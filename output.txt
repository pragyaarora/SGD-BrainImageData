lmda : 1  eta : 0.1
Logistic Loss
CROSS VALIDATION 0
Iteration :  0   Loss :  75856.3977165
Iteration :  1   Loss :  75852.7464519
Iteration :  2   Loss :  75852.7464252
[-0.07024048 -0.51245781  0.12728471 ...,  0.19160851 -0.19982005
 -0.10075717]
CROSS VALIDATION 1
Iteration :  0   Loss :  83901.6194552
Iteration :  1   Loss :  75852.7391614
Iteration :  2   Loss :  75852.7464252
Iteration :  3   Loss :  75852.7464252
[-0.07024048 -0.51245781  0.12728471 ...,  0.19160851 -0.19982005
 -0.10075717]
CROSS VALIDATION 2
Iteration :  0   Loss :  76302.6825956
Iteration :  1   Loss :  75852.7497075
Iteration :  2   Loss :  75852.7464252
Iteration :  3   Loss :  75852.7464252
[-0.07024048 -0.51245781  0.12728471 ...,  0.19160851 -0.19982005
 -0.10075717]
CROSS VALIDATION 3
Iteration :  0   Loss :  75860.6348988
Iteration :  1   Loss :  75852.746483
Iteration :  2   Loss :  75852.7464252
[-0.07024048 -0.51245781  0.12728471 ...,  0.19160851 -0.19982005
 -0.10075717]
CROSS VALIDATION 4
Iteration :  0   Loss :  75860.6348988
Iteration :  1   Loss :  75852.746483
Iteration :  2   Loss :  75852.7464252
[-0.07024048 -0.51245781  0.12728471 ...,  0.19160851 -0.19982005
 -0.10075717]
CROSS VALIDATION 5
Iteration :  0   Loss :  75860.6348988
Iteration :  1   Loss :  75852.746483
Iteration :  2   Loss :  75852.7464252
[-0.07024048 -0.51245781  0.12728471 ...,  0.19160851 -0.19982005
 -0.10075717]
CROSS VALIDATION 6
Iteration :  0   Loss :  72870.6344482
Iteration :  1   Loss :  83271.9809597
Iteration :  2   Loss :  83272.2182603
Iteration :  3   Loss :  83272.218262
[-0.21508333 -0.47275411 -0.10479216 ...,  0.16639644 -0.18896458
 -0.10756397]
CROSS VALIDATION 7
Iteration :  0   Loss :  75860.6348988
Iteration :  1   Loss :  75848.9794238
Iteration :  2   Loss :  75848.9793386
[-0.07025055 -0.51245995  0.12727476 ...,  0.19160269 -0.19982083
 -0.10075299]
CROSS VALIDATION 8
Iteration :  0   Loss :  75279.8576132
Iteration :  1   Loss :  74476.702503
Iteration :  2   Loss :  74476.9600962
Iteration :  3   Loss :  74476.9547719
Iteration :  4   Loss :  74476.9548789
Iteration :  5   Loss :  74476.9548767
[-0.06300272 -0.5156361   0.14916036 ...,  0.19294299 -0.19681788
 -0.09876445]
CROSS VALIDATION 9
Iteration :  0   Loss :  74659.6556522
Iteration :  1   Loss :  73958.2651242
Iteration :  2   Loss :  73958.6805808
Iteration :  3   Loss :  73958.6720796
Iteration :  4   Loss :  73958.6722504
Iteration :  5   Loss :  73958.672247
[-0.06300272 -0.5156361   0.14916036 ...,  0.19294299 -0.19681788
 -0.09876445]
CROSS VALIDATION 10
Iteration :  0   Loss :  73916.4646112
Iteration :  1   Loss :  73915.4579311
Iteration :  2   Loss :  73915.4579237
[-0.07021865 -0.51243399  0.12730971 ...,  0.19160667 -0.19983086
 -0.10075054]
CROSS VALIDATION 11
Iteration :  0   Loss :  75859.8899136
Iteration :  1   Loss :  75844.0646611
Iteration :  2   Loss :  75844.0645455
Iteration :  3   Loss :  75844.0645455
[-0.07024468 -0.5124418   0.12731737 ...,  0.19161237 -0.19981566
 -0.10075904]
CROSS VALIDATION 12
Iteration :  0   Loss :  72244.0044224
Iteration :  1   Loss :  72233.5760185
Iteration :  2   Loss :  72233.5759423
[-0.08801267 -0.51658407  0.11705607 ...,  0.20797454 -0.19339498
 -0.09960365]
CROSS VALIDATION 13
Iteration :  0   Loss :  75004.2702526
Iteration :  1   Loss :  83740.8468278
Iteration :  2   Loss :  83741.0689884
Iteration :  3   Loss :  83741.06899
[-0.19661941 -0.44548754 -0.10690041 ...,  0.18979865 -0.20011079
 -0.10064335]
CROSS VALIDATION 14
Iteration :  0   Loss :  83897.0235771
Iteration :  1   Loss :  79631.0660693
Iteration :  2   Loss :  79631.1041029
Iteration :  3   Loss :  79631.1041027
[-0.09408417 -0.53872172  0.11708975 ...,  0.18674852 -0.18378593
 -0.10624049]
CROSS VALIDATION 15
Iteration :  0   Loss :  83897.0235771
Iteration :  1   Loss :  79631.0660693
Iteration :  2   Loss :  79631.1041029
Iteration :  3   Loss :  79631.1041027
[-0.09408417 -0.53872172  0.11708975 ...,  0.18674852 -0.18378593
 -0.10624049]
CROSS VALIDATION 16
Iteration :  0   Loss :  80611.7426559
Iteration :  1   Loss :  77255.9533047
Iteration :  2   Loss :  77255.9973332
Iteration :  3   Loss :  77255.9973331
[-0.09408417 -0.53872172  0.11708975 ...,  0.18674852 -0.18378593
 -0.10624049]
CROSS VALIDATION 17
Iteration :  0   Loss :  79263.7503119
Iteration :  1   Loss :  73960.9924451
Iteration :  2   Loss :  73961.0229165
Iteration :  3   Loss :  73961.0229162
[-0.09408417 -0.53872172  0.11708975 ...,  0.18674852 -0.18378593
 -0.10624049]
CROSS VALIDATION 18
Iteration :  0   Loss :  72927.8753246
Iteration :  1   Loss :  74464.9796715
Iteration :  2   Loss :  74464.9914725
Iteration :  3   Loss :  74464.9914726
[-0.21504898 -0.4726451  -0.10488968 ...,  0.16627498 -0.1890239
 -0.10752102]
CROSS VALIDATION 19
Iteration :  0   Loss :  83897.0235771
Iteration :  1   Loss :  79610.8008456
Iteration :  2   Loss :  79610.8342382
Iteration :  3   Loss :  79610.8342384
[-0.09410774 -0.5387073   0.1171062  ...,  0.18672626 -0.18383132
 -0.10623035]
Accuracy (Logistic Loss):	0.6
Hinge Loss
CROSS VALIDATION 0
Iteration :  0   Loss :  75874.4553777
Iteration :  1   Loss :  75870.7465646
Iteration :  2   Loss :  75870.7465375
[-0.07024047 -0.5124578   0.12728471 ...,  0.19160851 -0.19982005
 -0.10075717]
CROSS VALIDATION 1
Iteration :  0   Loss :  75874.5736015
Iteration :  1   Loss :  75870.7465654
Iteration :  2   Loss :  75870.7465375
[-0.07024047 -0.5124578   0.12728471 ...,  0.19160851 -0.19982005
 -0.10075717]
CROSS VALIDATION 2
Iteration :  0   Loss :  75861.3678364
Iteration :  1   Loss :  75870.7464689
Iteration :  2   Loss :  75870.7465375
[-0.07024047 -0.5124578   0.12728471 ...,  0.19160851 -0.19982005
 -0.10075717]
CROSS VALIDATION 3
Iteration :  0   Loss :  75878.808491
Iteration :  1   Loss :  75870.7465964
Iteration :  2   Loss :  75870.7465375
[-0.07024047 -0.5124578   0.12728471 ...,  0.19160851 -0.19982005
 -0.10075717]
CROSS VALIDATION 4
Iteration :  0   Loss :  75878.808491
Iteration :  1   Loss :  75870.7465964
Iteration :  2   Loss :  75870.7465375
[-0.07024047 -0.5124578   0.12728471 ...,  0.19160851 -0.19982005
 -0.10075717]
CROSS VALIDATION 5
Iteration :  0   Loss :  75878.808491
Iteration :  1   Loss :  75870.7465964
Iteration :  2   Loss :  75870.7465375
[-0.07024047 -0.5124578   0.12728471 ...,  0.19160851 -0.19982005
 -0.10075717]
CROSS VALIDATION 6
Iteration :  0   Loss :  72887.8000246
Iteration :  1   Loss :  83286.9805593
Iteration :  2   Loss :  83287.2183465
Iteration :  3   Loss :  83287.2183482
[-0.21508333 -0.47275413 -0.10479216 ...,  0.16639643 -0.18896458
 -0.10756397]
CROSS VALIDATION 7
Iteration :  0   Loss :  75878.808491
Iteration :  1   Loss :  75866.9794251
Iteration :  2   Loss :  75866.9793386
[-0.07025055 -0.51245995  0.12727476 ...,  0.19160269 -0.19982083
 -0.10075299]
CROSS VALIDATION 8
Iteration :  0   Loss :  75296.9882754
Iteration :  1   Loss :  74493.5673559
Iteration :  2   Loss :  74493.5616516
Iteration :  3   Loss :  74493.5616516
[-0.06302316 -0.51565913  0.14915953 ...,  0.19294185 -0.19681464
 -0.0987696 ]
CROSS VALIDATION 9
Iteration :  0   Loss :  74676.8321639
Iteration :  1   Loss :  73975.0492366
Iteration :  2   Loss :  73975.044275
Iteration :  3   Loss :  73975.0442749
[-0.06302316 -0.51565913  0.14915953 ...,  0.19294185 -0.19681464
 -0.0987696 ]
CROSS VALIDATION 10
Iteration :  0   Loss :  73933.6461318
Iteration :  1   Loss :  73932.4579324
Iteration :  2   Loss :  73932.4579237
[-0.07021865 -0.51243399  0.12730971 ...,  0.19160667 -0.19983086
 -0.10075054]
CROSS VALIDATION 11
Iteration :  0   Loss :  75878.0635058
Iteration :  1   Loss :  75862.0646624
Iteration :  2   Loss :  75862.0645455
Iteration :  3   Loss :  75862.0645455
[-0.07024468 -0.5124418   0.12731737 ...,  0.19161237 -0.19981566
 -0.10075904]
CROSS VALIDATION 12
Iteration :  0   Loss :  72258.8242221
Iteration :  1   Loss :  72250.5760025
Iteration :  2   Loss :  72250.5759423
[-0.08801267 -0.51658407  0.11705607 ...,  0.20797454 -0.19339498
 -0.09960365]
CROSS VALIDATION 13
Iteration :  0   Loss :  75022.4439049
Iteration :  1   Loss :  83756.8406203
Iteration :  2   Loss :  83757.06273
Iteration :  3   Loss :  83757.0627316
[-0.19661932 -0.44548742 -0.10690042 ...,  0.18979875 -0.20011084
 -0.10064332]
CROSS VALIDATION 14
Iteration :  0   Loss :  83913.1779567
Iteration :  1   Loss :  79649.0660706
Iteration :  2   Loss :  79649.0996099
Iteration :  3   Loss :  79649.0996102
[-0.09408417 -0.53872171  0.11708973 ...,  0.1867485  -0.18378594
 -0.10624049]
CROSS VALIDATION 15
Iteration :  0   Loss :  83913.1779567
Iteration :  1   Loss :  79649.0660706
Iteration :  2   Loss :  79649.0996099
Iteration :  3   Loss :  79649.0996102
[-0.09408417 -0.53872171  0.11708973 ...,  0.1867485  -0.18378594
 -0.10624049]
CROSS VALIDATION 16
Iteration :  0   Loss :  80626.9167952
Iteration :  1   Loss :  77272.9533061
Iteration :  2   Loss :  77272.9934963
Iteration :  3   Loss :  77272.9934966
[-0.09408417 -0.53872171  0.11708973 ...,  0.1867485  -0.18378594
 -0.10624049]
CROSS VALIDATION 17
Iteration :  0   Loss :  79278.8838307
Iteration :  1   Loss :  73977.9924462
Iteration :  2   Loss :  73978.0184093
Iteration :  3   Loss :  73978.0184094
[-0.09408417 -0.53872171  0.11708973 ...,  0.1867485  -0.18378594
 -0.10624049]
CROSS VALIDATION 18
Iteration :  0   Loss :  72943.0248827
Iteration :  1   Loss :  74479.9785687
Iteration :  2   Loss :  74479.9903658
Iteration :  3   Loss :  74479.9903659
[-0.21504898 -0.4726451  -0.10488968 ...,  0.16627498 -0.1890239
 -0.10752102]
CROSS VALIDATION 19
Iteration :  0   Loss :  83913.1779567
Iteration :  1   Loss :  79628.8008469
Iteration :  2   Loss :  79628.8342382
Iteration :  3   Loss :  79628.8342384
[-0.09410774 -0.5387073   0.1171062  ...,  0.18672626 -0.18383132
 -0.10623035]
Accuracy (Hinge Loss):	0.6
lmda : 0.3  eta : 0.1
Logistic Loss
CROSS VALIDATION 0
Iteration :  0   Loss :  43056.7971149
Iteration :  1   Loss :  31344.9434271
Iteration :  2   Loss :  36930.2880628
Iteration :  3   Loss :  87217.9530487
Iteration :  4   Loss :  43040.9175845
Iteration :  5   Loss :  33949.3007208
Iteration :  6   Loss :  50950.871925
Iteration :  7   Loss :  17133.9968481
Iteration :  8   Loss :  48052.2124451
Iteration :  9   Loss :  54517.7242319
Iteration :  10   Loss :  43739.3675392
Iteration :  11   Loss :  26697.3300633
Iteration :  12   Loss :  62577.8772594
Iteration :  13   Loss :  32785.5685223
Iteration :  14   Loss :  46168.031546
Iteration :  15   Loss :  40976.0857717
Iteration :  16   Loss :  31981.2083087
Iteration :  17   Loss :  45279.3802869
Iteration :  18   Loss :  28090.4663413
Iteration :  19   Loss :  33298.0771694
Iteration :  20   Loss :  62891.848983
Iteration :  21   Loss :  30320.6958931
Iteration :  22   Loss :  58433.5717967
Iteration :  23   Loss :  43976.068659
Iteration :  24   Loss :  61386.6423798
Iteration :  25   Loss :  58271.5998171
Iteration :  26   Loss :  64875.7200716
Iteration :  27   Loss :  40463.0171058
Iteration :  28   Loss :  65239.3017806
Iteration :  29   Loss :  59753.1109175
Iteration :  30   Loss :  31852.2607608
Iteration :  31   Loss :  28812.0833585
Iteration :  32   Loss :  35656.6552388
Iteration :  33   Loss :  52223.7227711
Iteration :  34   Loss :  51310.7649085
Iteration :  35   Loss :  34203.2426227
Iteration :  36   Loss :  78456.3969905
Iteration :  37   Loss :  43654.5547849
Iteration :  38   Loss :  31105.6744292
Iteration :  39   Loss :  59809.7931494
Iteration :  40   Loss :  32931.5747978
Iteration :  41   Loss :  55186.999915
Iteration :  42   Loss :  31829.9551435
Iteration :  43   Loss :  40844.3500269
Iteration :  44   Loss :  22388.9384467
Iteration :  45   Loss :  71546.8812621
Iteration :  46   Loss :  49011.7886527
Iteration :  47   Loss :  40605.3240189
Iteration :  48   Loss :  72592.7094586
Iteration :  49   Loss :  47186.3888591
Iteration :  50   Loss :  29834.6803886
Iteration :  51   Loss :  42537.7413312
Iteration :  52   Loss :  50620.7712219
Iteration :  53   Loss :  23183.7695103
Iteration :  54   Loss :  54067.6368953
Iteration :  55   Loss :  23469.6811815
Iteration :  56   Loss :  46848.6392419
Iteration :  57   Loss :  28913.6458162
Iteration :  58   Loss :  37828.9589828
Iteration :  59   Loss :  64293.5616579
Iteration :  60   Loss :  33854.982659
Iteration :  61   Loss :  35338.0606157
Iteration :  62   Loss :  29117.9002473
Iteration :  63   Loss :  57341.8136517
Iteration :  64   Loss :  48824.3062977
Iteration :  65   Loss :  71650.5043123
Iteration :  66   Loss :  55029.0185939
Iteration :  67   Loss :  34405.7978173
Iteration :  68   Loss :  50031.0000559
Iteration :  69   Loss :  62773.4261725
Iteration :  70   Loss :  61473.5241144
Iteration :  71   Loss :  20528.434599
Iteration :  72   Loss :  46314.9136989
Iteration :  73   Loss :  53090.3322787
Iteration :  74   Loss :  35869.3329438
Iteration :  75   Loss :  54611.358977
Iteration :  76   Loss :  35446.5053534
Iteration :  77   Loss :  54613.9740969
Iteration :  78   Loss :  35445.9066969
Iteration :  79   Loss :  54613.9777958
Iteration :  80   Loss :  35445.9058484
Iteration :  81   Loss :  54613.977801
Iteration :  82   Loss :  35445.9058472
Iteration :  83   Loss :  54613.977801
Iteration :  84   Loss :  35445.9058472
Iteration :  85   Loss :  54613.977801
Iteration :  86   Loss :  35445.9058472
Iteration :  87   Loss :  54613.977801
Iteration :  88   Loss :  35445.9058472
Iteration :  89   Loss :  54613.977801
Iteration :  90   Loss :  35445.9058472
Iteration :  91   Loss :  54613.977801
Iteration :  92   Loss :  35445.9058472
Iteration :  93   Loss :  54613.977801
Iteration :  94   Loss :  35445.9058472
Iteration :  95   Loss :  54613.977801
Iteration :  96   Loss :  35445.9058472
Iteration :  97   Loss :  54613.977801
Iteration :  98   Loss :  35445.9058472
Iteration :  99   Loss :  54613.977801
[-0.30502843 -0.77501482 -0.25613441 ...,  0.43836455 -0.46571712
 -0.04239405]
CROSS VALIDATION 1
Iteration :  0   Loss :  36871.4767571
Iteration :  1   Loss :  88943.6171817
Iteration :  2   Loss :  38699.8862825
Iteration :  3   Loss :  86056.1584276
Iteration :  4   Loss :  38593.904048
Iteration :  5   Loss :  86057.7948592
Iteration :  6   Loss :  38593.7548132
Iteration :  7   Loss :  86057.7971804
Iteration :  8   Loss :  38593.7546017
Iteration :  9   Loss :  86057.7971837
Iteration :  10   Loss :  38593.7546014
Iteration :  11   Loss :  86057.7971837
Iteration :  12   Loss :  38593.7546014
Iteration :  13   Loss :  86057.7971837
Iteration :  14   Loss :  38593.7546014
Iteration :  15   Loss :  86057.7971837
Iteration :  16   Loss :  38593.7546014
Iteration :  17   Loss :  86057.7971837
Iteration :  18   Loss :  38593.7546014
Iteration :  19   Loss :  86057.7971837
Iteration :  20   Loss :  38593.7546014
Iteration :  21   Loss :  86057.7971837
Iteration :  22   Loss :  38593.7546014
Iteration :  23   Loss :  86057.7971837
Iteration :  24   Loss :  38593.7546014
Iteration :  25   Loss :  86057.7971837
Iteration :  26   Loss :  38593.7546014
Iteration :  27   Loss :  86057.7971837
Iteration :  28   Loss :  38593.7546014
Iteration :  29   Loss :  86057.7971837
Iteration :  30   Loss :  38593.7546014
Iteration :  31   Loss :  86057.7971837
Iteration :  32   Loss :  38593.7546014
Iteration :  33   Loss :  86057.7971837
Iteration :  34   Loss :  38593.7546014
Iteration :  35   Loss :  86057.7971837
Iteration :  36   Loss :  38593.7546014
Iteration :  37   Loss :  86057.7971837
Iteration :  38   Loss :  38593.7546014
Iteration :  39   Loss :  86057.7971837
Iteration :  40   Loss :  38593.7546014
Iteration :  41   Loss :  86057.7971837
Iteration :  42   Loss :  38593.7546014
Iteration :  43   Loss :  86057.7971837
Iteration :  44   Loss :  38593.7546014
Iteration :  45   Loss :  86057.7971837
Iteration :  46   Loss :  38593.7546014
Iteration :  47   Loss :  86057.7971837
Iteration :  48   Loss :  38593.7546014
Iteration :  49   Loss :  86057.7971837
Iteration :  50   Loss :  38593.7546014
Iteration :  51   Loss :  86057.7971837
Iteration :  52   Loss :  38593.7546014
Iteration :  53   Loss :  86057.7971837
Iteration :  54   Loss :  38593.7546014
Iteration :  55   Loss :  86057.7971837
Iteration :  56   Loss :  38593.7546014
Iteration :  57   Loss :  86057.7971837
Iteration :  58   Loss :  38593.7546014
Iteration :  59   Loss :  86057.7971837
Iteration :  60   Loss :  38593.7546014
Iteration :  61   Loss :  86057.7971837
Iteration :  62   Loss :  38593.7546014
Iteration :  63   Loss :  86057.7971837
Iteration :  64   Loss :  38593.7546014
Iteration :  65   Loss :  86057.7971837
Iteration :  66   Loss :  38593.7546014
Iteration :  67   Loss :  86057.7971837
Iteration :  68   Loss :  38593.7546014
Iteration :  69   Loss :  86057.7971837
Iteration :  70   Loss :  38593.7546014
Iteration :  71   Loss :  86057.7971837
Iteration :  72   Loss :  38593.7546014
Iteration :  73   Loss :  86057.7971837
Iteration :  74   Loss :  38593.7546014
Iteration :  75   Loss :  86057.7971837
Iteration :  76   Loss :  38593.7546014
Iteration :  77   Loss :  86057.7971837
Iteration :  78   Loss :  38593.7546014
Iteration :  79   Loss :  86057.7971837
Iteration :  80   Loss :  38593.7546014
Iteration :  81   Loss :  86057.7971837
Iteration :  82   Loss :  38593.7546014
Iteration :  83   Loss :  86057.7971837
Iteration :  84   Loss :  38593.7546014
Iteration :  85   Loss :  86057.7971837
Iteration :  86   Loss :  38593.7546014
Iteration :  87   Loss :  86057.7971837
Iteration :  88   Loss :  38593.7546014
Iteration :  89   Loss :  86057.7971837
Iteration :  90   Loss :  38593.7546014
Iteration :  91   Loss :  86057.7971837
Iteration :  92   Loss :  38593.7546014
Iteration :  93   Loss :  86057.7971837
Iteration :  94   Loss :  38593.7546014
Iteration :  95   Loss :  86057.7971837
Iteration :  96   Loss :  38593.7546014
Iteration :  97   Loss :  86057.7971837
Iteration :  98   Loss :  38593.7546014
Iteration :  99   Loss :  86057.7971837
[-0.52259015  0.03582248  0.21315283 ...,  0.34155618  0.49757702
  0.25795078]
CROSS VALIDATION 2
Iteration :  0   Loss :  33565.9102094
Iteration :  1   Loss :  36214.6297239
Iteration :  2   Loss :  37298.8178232
Iteration :  3   Loss :  29975.5949252
Iteration :  4   Loss :  44159.4517017
Iteration :  5   Loss :  26637.8591116
Iteration :  6   Loss :  35681.0926817
Iteration :  7   Loss :  31084.3881952
Iteration :  8   Loss :  27512.2991611
Iteration :  9   Loss :  44481.9464037
Iteration :  10   Loss :  56323.9222228
Iteration :  11   Loss :  52083.2622105
Iteration :  12   Loss :  38185.4597925
Iteration :  13   Loss :  59717.580221
Iteration :  14   Loss :  57541.4878531
Iteration :  15   Loss :  63051.6677947
Iteration :  16   Loss :  37486.774729
Iteration :  17   Loss :  34041.7001667
Iteration :  18   Loss :  56388.7030101
Iteration :  19   Loss :  9250.1427064
Iteration :  20   Loss :  51546.5777888
Iteration :  21   Loss :  20056.2009117
Iteration :  22   Loss :  41212.5287114
Iteration :  23   Loss :  31023.8154108
Iteration :  24   Loss :  27514.768545
Iteration :  25   Loss :  44482.0792912
Iteration :  26   Loss :  56323.9270672
Iteration :  27   Loss :  52083.262396
Iteration :  28   Loss :  38185.4597996
Iteration :  29   Loss :  59717.5802214
Iteration :  30   Loss :  57541.4878531
Iteration :  31   Loss :  63051.6677947
Iteration :  32   Loss :  37486.774729
Iteration :  33   Loss :  34041.7001667
Iteration :  34   Loss :  56388.7030101
Iteration :  35   Loss :  9250.1427064
Iteration :  36   Loss :  51546.5777888
Iteration :  37   Loss :  20056.2009117
Iteration :  38   Loss :  41212.5287114
Iteration :  39   Loss :  31023.8154108
Iteration :  40   Loss :  27514.768545
Iteration :  41   Loss :  44482.0792912
Iteration :  42   Loss :  56323.9270672
Iteration :  43   Loss :  52083.262396
Iteration :  44   Loss :  38185.4597996
Iteration :  45   Loss :  59717.5802214
Iteration :  46   Loss :  57541.4878531
Iteration :  47   Loss :  63051.6677947
Iteration :  48   Loss :  37486.774729
Iteration :  49   Loss :  34041.7001667
Iteration :  50   Loss :  56388.7030101
Iteration :  51   Loss :  9250.1427064
Iteration :  52   Loss :  51546.5777888
Iteration :  53   Loss :  20056.2009117
Iteration :  54   Loss :  41212.5287114
Iteration :  55   Loss :  31023.8154108
Iteration :  56   Loss :  27514.768545
Iteration :  57   Loss :  44482.0792912
Iteration :  58   Loss :  56323.9270672
Iteration :  59   Loss :  52083.262396
Iteration :  60   Loss :  38185.4597996
Iteration :  61   Loss :  59717.5802214
Iteration :  62   Loss :  57541.4878531
Iteration :  63   Loss :  63051.6677947
Iteration :  64   Loss :  37486.774729
Iteration :  65   Loss :  34041.7001667
Iteration :  66   Loss :  56388.7030101
Iteration :  67   Loss :  9250.1427064
Iteration :  68   Loss :  51546.5777888
Iteration :  69   Loss :  20056.2009117
Iteration :  70   Loss :  41212.5287114
Iteration :  71   Loss :  31023.8154108
Iteration :  72   Loss :  27514.768545
Iteration :  73   Loss :  44482.0792912
Iteration :  74   Loss :  56323.9270672
Iteration :  75   Loss :  52083.262396
Iteration :  76   Loss :  38185.4597996
Iteration :  77   Loss :  59717.5802214
Iteration :  78   Loss :  57541.4878531
Iteration :  79   Loss :  63051.6677947
Iteration :  80   Loss :  37486.774729
Iteration :  81   Loss :  34041.7001667
Iteration :  82   Loss :  56388.7030101
Iteration :  83   Loss :  9250.1427064
Iteration :  84   Loss :  51546.5777888
Iteration :  85   Loss :  20056.2009117
Iteration :  86   Loss :  41212.5287114
Iteration :  87   Loss :  31023.8154108
Iteration :  88   Loss :  27514.768545
Iteration :  89   Loss :  44482.0792912
Iteration :  90   Loss :  56323.9270672
Iteration :  91   Loss :  52083.262396
Iteration :  92   Loss :  38185.4597996
Iteration :  93   Loss :  59717.5802214
Iteration :  94   Loss :  57541.4878531
Iteration :  95   Loss :  63051.6677947
Iteration :  96   Loss :  37486.774729
Iteration :  97   Loss :  34041.7001667
Iteration :  98   Loss :  56388.7030101
Iteration :  99   Loss :  9250.1427064
[-0.29486499 -0.35537172  0.23698444 ..., -0.12692135  0.33044157
  0.01538898]
CROSS VALIDATION 3
Iteration :  0   Loss :  49515.7059459
Iteration :  1   Loss :  23114.3570246
Iteration :  2   Loss :  42401.8990203
Iteration :  3   Loss :  30584.0668556
Iteration :  4   Loss :  46961.3834086
Iteration :  5   Loss :  25865.903964
Iteration :  6   Loss :  35392.2048526
Iteration :  7   Loss :  36268.0250569
Iteration :  8   Loss :  23032.0248276
Iteration :  9   Loss :  39236.7755688
Iteration :  10   Loss :  40579.8785519
Iteration :  11   Loss :  33205.1495021
Iteration :  12   Loss :  30580.4630793
Iteration :  13   Loss :  44854.3274422
Iteration :  14   Loss :  41363.6710723
Iteration :  15   Loss :  59929.1519931
Iteration :  16   Loss :  32452.7684634
Iteration :  17   Loss :  49462.9903102
Iteration :  18   Loss :  43678.8684387
Iteration :  19   Loss :  38848.0881137
Iteration :  20   Loss :  32979.6154607
Iteration :  21   Loss :  36320.1731545
Iteration :  22   Loss :  31402.0095707
Iteration :  23   Loss :  47445.7996672
Iteration :  24   Loss :  35776.9546747
Iteration :  25   Loss :  13961.7303154
Iteration :  26   Loss :  62700.4865118
Iteration :  27   Loss :  66003.9536392
Iteration :  28   Loss :  42184.0408159
Iteration :  29   Loss :  23187.4135716
Iteration :  30   Loss :  62513.9889601
Iteration :  31   Loss :  44606.8707944
Iteration :  32   Loss :  50246.7552618
Iteration :  33   Loss :  36406.3297603
Iteration :  34   Loss :  28915.5888798
Iteration :  35   Loss :  48652.7195941
Iteration :  36   Loss :  45804.6998792
Iteration :  37   Loss :  29757.9853035
Iteration :  38   Loss :  62010.2905375
Iteration :  39   Loss :  11497.616557
Iteration :  40   Loss :  51926.3885439
Iteration :  41   Loss :  36900.7922408
Iteration :  42   Loss :  39551.6510339
Iteration :  43   Loss :  41471.6118184
Iteration :  44   Loss :  36704.1534858
Iteration :  45   Loss :  60511.3713228
Iteration :  46   Loss :  40824.4288152
Iteration :  47   Loss :  40464.7080185
Iteration :  48   Loss :  45097.099851
Iteration :  49   Loss :  35756.2008582
Iteration :  50   Loss :  54510.6643001
Iteration :  51   Loss :  35430.2921301
Iteration :  52   Loss :  54552.2373816
Iteration :  53   Loss :  35429.8320992
Iteration :  54   Loss :  54552.2962995
Iteration :  55   Loss :  35429.8314473
Iteration :  56   Loss :  54552.296383
Iteration :  57   Loss :  35429.8314464
Iteration :  58   Loss :  54552.2963831
Iteration :  59   Loss :  35429.8314464
Iteration :  60   Loss :  54552.2963831
Iteration :  61   Loss :  35429.8314464
Iteration :  62   Loss :  54552.2963831
Iteration :  63   Loss :  35429.8314464
Iteration :  64   Loss :  54552.2963831
Iteration :  65   Loss :  35429.8314464
Iteration :  66   Loss :  54552.2963831
Iteration :  67   Loss :  35429.8314464
Iteration :  68   Loss :  54552.2963831
Iteration :  69   Loss :  35429.8314464
Iteration :  70   Loss :  54552.2963831
Iteration :  71   Loss :  35429.8314464
Iteration :  72   Loss :  54552.2963831
Iteration :  73   Loss :  35429.8314464
Iteration :  74   Loss :  54552.2963831
Iteration :  75   Loss :  35429.8314464
Iteration :  76   Loss :  54552.2963831
Iteration :  77   Loss :  35429.8314464
Iteration :  78   Loss :  54552.2963831
Iteration :  79   Loss :  35429.8314464
Iteration :  80   Loss :  54552.2963831
Iteration :  81   Loss :  35429.8314464
Iteration :  82   Loss :  54552.2963831
Iteration :  83   Loss :  35429.8314464
Iteration :  84   Loss :  54552.2963831
Iteration :  85   Loss :  35429.8314464
Iteration :  86   Loss :  54552.2963831
Iteration :  87   Loss :  35429.8314464
Iteration :  88   Loss :  54552.2963831
Iteration :  89   Loss :  35429.8314464
Iteration :  90   Loss :  54552.2963831
Iteration :  91   Loss :  35429.8314464
Iteration :  92   Loss :  54552.2963831
Iteration :  93   Loss :  35429.8314464
Iteration :  94   Loss :  54552.2963831
Iteration :  95   Loss :  35429.8314464
Iteration :  96   Loss :  54552.2963831
Iteration :  97   Loss :  35429.8314464
Iteration :  98   Loss :  54552.2963831
Iteration :  99   Loss :  35429.8314464
[-0.51115432 -0.00128011 -0.21637761 ...,  0.29714714 -0.17351384
  0.05701844]
CROSS VALIDATION 4
Iteration :  0   Loss :  49515.7059459
Iteration :  1   Loss :  24544.7790345
Iteration :  2   Loss :  42401.8990203
Iteration :  3   Loss :  29124.005799
Iteration :  4   Loss :  46961.3834086
Iteration :  5   Loss :  25865.903964
Iteration :  6   Loss :  35392.2048526
Iteration :  7   Loss :  36268.0250569
Iteration :  8   Loss :  20119.0381945
Iteration :  9   Loss :  38125.9563723
Iteration :  10   Loss :  31076.9044595
Iteration :  11   Loss :  59827.6264212
Iteration :  12   Loss :  32957.1049442
Iteration :  13   Loss :  52317.9203888
Iteration :  14   Loss :  31840.7930963
Iteration :  15   Loss :  38348.9672406
Iteration :  16   Loss :  22475.8091408
Iteration :  17   Loss :  69276.5319482
Iteration :  18   Loss :  51947.7548438
Iteration :  19   Loss :  68104.2331214
Iteration :  20   Loss :  31954.9873653
Iteration :  21   Loss :  40761.3104277
Iteration :  22   Loss :  22032.4722018
Iteration :  23   Loss :  40813.2578782
Iteration :  24   Loss :  31021.7736106
Iteration :  25   Loss :  25947.4103525
Iteration :  26   Loss :  49858.0866827
Iteration :  27   Loss :  16193.2950776
Iteration :  28   Loss :  45132.5911875
Iteration :  29   Loss :  87693.3688339
Iteration :  30   Loss :  43291.3268912
Iteration :  31   Loss :  72643.1343397
Iteration :  32   Loss :  45991.250732
Iteration :  33   Loss :  41054.6994596
Iteration :  34   Loss :  22388.5174182
Iteration :  35   Loss :  69280.4369922
Iteration :  36   Loss :  51948.5477767
Iteration :  37   Loss :  68104.223347
Iteration :  38   Loss :  31954.9870343
Iteration :  39   Loss :  40761.3104316
Iteration :  40   Loss :  22032.4722014
Iteration :  41   Loss :  40813.2578782
Iteration :  42   Loss :  31021.7736106
Iteration :  43   Loss :  25947.4103525
Iteration :  44   Loss :  49858.0866827
Iteration :  45   Loss :  16193.2950776
Iteration :  46   Loss :  45132.5911875
Iteration :  47   Loss :  87693.3688339
Iteration :  48   Loss :  43291.3268912
Iteration :  49   Loss :  72643.1343397
Iteration :  50   Loss :  45991.250732
Iteration :  51   Loss :  41054.6994596
Iteration :  52   Loss :  22388.5174182
Iteration :  53   Loss :  69280.4369922
Iteration :  54   Loss :  51948.5477767
Iteration :  55   Loss :  68104.223347
Iteration :  56   Loss :  31954.9870343
Iteration :  57   Loss :  40761.3104316
Iteration :  58   Loss :  22032.4722014
Iteration :  59   Loss :  40813.2578782
Iteration :  60   Loss :  31021.7736106
Iteration :  61   Loss :  25947.4103525
Iteration :  62   Loss :  49858.0866827
Iteration :  63   Loss :  16193.2950776
Iteration :  64   Loss :  45132.5911875
Iteration :  65   Loss :  87693.3688339
Iteration :  66   Loss :  43291.3268912
Iteration :  67   Loss :  72643.1343397
Iteration :  68   Loss :  45991.250732
Iteration :  69   Loss :  41054.6994596
Iteration :  70   Loss :  22388.5174182
Iteration :  71   Loss :  69280.4369922
Iteration :  72   Loss :  51948.5477767
Iteration :  73   Loss :  68104.223347
Iteration :  74   Loss :  31954.9870343
Iteration :  75   Loss :  40761.3104316
Iteration :  76   Loss :  22032.4722014
Iteration :  77   Loss :  40813.2578782
Iteration :  78   Loss :  31021.7736106
Iteration :  79   Loss :  25947.4103525
Iteration :  80   Loss :  49858.0866827
Iteration :  81   Loss :  16193.2950776
Iteration :  82   Loss :  45132.5911875
Iteration :  83   Loss :  87693.3688339
Iteration :  84   Loss :  43291.3268912
Iteration :  85   Loss :  72643.1343397
Iteration :  86   Loss :  45991.250732
Iteration :  87   Loss :  41054.6994596
Iteration :  88   Loss :  22388.5174182
Iteration :  89   Loss :  69280.4369922
Iteration :  90   Loss :  51948.5477767
Iteration :  91   Loss :  68104.223347
Iteration :  92   Loss :  31954.9870343
Iteration :  93   Loss :  40761.3104316
Iteration :  94   Loss :  22032.4722014
Iteration :  95   Loss :  40813.2578782
Iteration :  96   Loss :  31021.7736106
Iteration :  97   Loss :  25947.4103525
Iteration :  98   Loss :  49858.0866827
Iteration :  99   Loss :  16193.2950776
[-0.32232799 -0.24189131  0.24676588 ...,  0.07259134  0.44700356
  0.04374171]
CROSS VALIDATION 5
Iteration :  0   Loss :  46848.6431967
Iteration :  1   Loss :  18454.6523766
Iteration :  2   Loss :  36614.3851344
Iteration :  3   Loss :  48408.5921087
Iteration :  4   Loss :  60489.2332567
Iteration :  5   Loss :  10102.5042268
Iteration :  6   Loss :  46248.7984893
Iteration :  7   Loss :  21000.1112231
Iteration :  8   Loss :  50992.4709495
Iteration :  9   Loss :  35030.8355939
Iteration :  10   Loss :  42497.8296179
Iteration :  11   Loss :  5848.44734299
Iteration :  12   Loss :  29819.9524365
Iteration :  13   Loss :  45749.858567
Iteration :  14   Loss :  44074.8604785
Iteration :  15   Loss :  39573.295533
Iteration :  16   Loss :  50465.9042463
Iteration :  17   Loss :  32421.2790634
Iteration :  18   Loss :  33387.5029958
Iteration :  19   Loss :  62720.7098078
Iteration :  20   Loss :  32459.3309711
Iteration :  21   Loss :  17499.7353003
Iteration :  22   Loss :  83068.4832465
Iteration :  23   Loss :  40506.8089725
Iteration :  24   Loss :  68458.3001231
Iteration :  25   Loss :  46027.6943192
Iteration :  26   Loss :  32392.1215781
Iteration :  27   Loss :  38321.3333672
Iteration :  28   Loss :  36275.4199657
Iteration :  29   Loss :  27538.5082925
Iteration :  30   Loss :  49726.2783587
Iteration :  31   Loss :  24730.791322
Iteration :  32   Loss :  47929.5317958
Iteration :  33   Loss :  25387.7652967
Iteration :  34   Loss :  61317.8945066
Iteration :  35   Loss :  31573.9680289
Iteration :  36   Loss :  41164.0305238
Iteration :  37   Loss :  28634.5252699
Iteration :  38   Loss :  45181.6688771
Iteration :  39   Loss :  110769.549392
Iteration :  40   Loss :  39596.9809074
Iteration :  41   Loss :  109865.396062
Iteration :  42   Loss :  33751.6709037
Iteration :  43   Loss :  47721.3525249
Iteration :  44   Loss :  60458.6425978
Iteration :  45   Loss :  10102.6561386
Iteration :  46   Loss :  46248.7749838
Iteration :  47   Loss :  21000.1110388
Iteration :  48   Loss :  50992.4709383
Iteration :  49   Loss :  35030.8355977
Iteration :  50   Loss :  42497.8296178
Iteration :  51   Loss :  5848.44734299
Iteration :  52   Loss :  29819.9524365
Iteration :  53   Loss :  45749.858567
Iteration :  54   Loss :  44074.8604785
Iteration :  55   Loss :  39573.295533
Iteration :  56   Loss :  50465.9042463
Iteration :  57   Loss :  32421.2790634
Iteration :  58   Loss :  33387.5029958
Iteration :  59   Loss :  62720.7098078
Iteration :  60   Loss :  32459.3309711
Iteration :  61   Loss :  17499.7353003
Iteration :  62   Loss :  83068.4832465
Iteration :  63   Loss :  40506.8089725
Iteration :  64   Loss :  68458.3001231
Iteration :  65   Loss :  46027.6943192
Iteration :  66   Loss :  32392.1215781
Iteration :  67   Loss :  38321.3333672
Iteration :  68   Loss :  36275.4199657
Iteration :  69   Loss :  27538.5082925
Iteration :  70   Loss :  49726.2783587
Iteration :  71   Loss :  24730.791322
Iteration :  72   Loss :  47929.5317958
Iteration :  73   Loss :  25387.7652967
Iteration :  74   Loss :  61317.8945066
Iteration :  75   Loss :  31573.9680289
Iteration :  76   Loss :  41164.0305238
Iteration :  77   Loss :  28634.5252699
Iteration :  78   Loss :  45181.6688771
Iteration :  79   Loss :  110769.549392
Iteration :  80   Loss :  39596.9809074
Iteration :  81   Loss :  109865.396062
Iteration :  82   Loss :  33751.6709037
Iteration :  83   Loss :  47721.3525249
Iteration :  84   Loss :  60458.6425978
Iteration :  85   Loss :  10102.6561386
Iteration :  86   Loss :  46248.7749838
Iteration :  87   Loss :  21000.1110388
Iteration :  88   Loss :  50992.4709383
Iteration :  89   Loss :  35030.8355977
Iteration :  90   Loss :  42497.8296178
Iteration :  91   Loss :  5848.44734299
Iteration :  92   Loss :  29819.9524365
Iteration :  93   Loss :  45749.858567
Iteration :  94   Loss :  44074.8604785
Iteration :  95   Loss :  39573.295533
Iteration :  96   Loss :  50465.9042463
Iteration :  97   Loss :  32421.2790634
Iteration :  98   Loss :  33387.5029958
Iteration :  99   Loss :  62720.7098078
[-0.20945908  0.03883868 -0.43900842 ...,  0.50638222  0.07601425
  0.07038481]
CROSS VALIDATION 6
Iteration :  0   Loss :  49515.7059459
Iteration :  1   Loss :  24544.7790345
Iteration :  2   Loss :  50923.1342347
Iteration :  3   Loss :  43272.7483656
Iteration :  4   Loss :  56268.7476671
Iteration :  5   Loss :  56534.7503272
Iteration :  6   Loss :  50812.3389459
Iteration :  7   Loss :  23368.2269966
Iteration :  8   Loss :  34509.7089633
Iteration :  9   Loss :  62745.2094121
Iteration :  10   Loss :  36521.307186
Iteration :  11   Loss :  43010.9351054
Iteration :  12   Loss :  16986.4146654
Iteration :  13   Loss :  47884.1486779
Iteration :  14   Loss :  16461.7132047
Iteration :  15   Loss :  47965.8740783
Iteration :  16   Loss :  16497.2415207
Iteration :  17   Loss :  47965.9901461
Iteration :  18   Loss :  16497.2918859
Iteration :  19   Loss :  47965.9903106
Iteration :  20   Loss :  16497.2919572
Iteration :  21   Loss :  47965.9903109
Iteration :  22   Loss :  16497.2919573
Iteration :  23   Loss :  47965.9903109
Iteration :  24   Loss :  16497.2919573
Iteration :  25   Loss :  47965.9903109
Iteration :  26   Loss :  16497.2919573
Iteration :  27   Loss :  47965.9903109
Iteration :  28   Loss :  16497.2919573
Iteration :  29   Loss :  47965.9903109
Iteration :  30   Loss :  16497.2919573
Iteration :  31   Loss :  47965.9903109
Iteration :  32   Loss :  16497.2919573
Iteration :  33   Loss :  47965.9903109
Iteration :  34   Loss :  16497.2919573
Iteration :  35   Loss :  47965.9903109
Iteration :  36   Loss :  16497.2919573
Iteration :  37   Loss :  47965.9903109
Iteration :  38   Loss :  16497.2919573
Iteration :  39   Loss :  47965.9903109
Iteration :  40   Loss :  16497.2919573
Iteration :  41   Loss :  47965.9903109
Iteration :  42   Loss :  16497.2919573
Iteration :  43   Loss :  47965.9903109
Iteration :  44   Loss :  16497.2919573
Iteration :  45   Loss :  47965.9903109
Iteration :  46   Loss :  16497.2919573
Iteration :  47   Loss :  47965.9903109
Iteration :  48   Loss :  16497.2919573
Iteration :  49   Loss :  47965.9903109
Iteration :  50   Loss :  16497.2919573
Iteration :  51   Loss :  47965.9903109
Iteration :  52   Loss :  16497.2919573
Iteration :  53   Loss :  47965.9903109
Iteration :  54   Loss :  16497.2919573
Iteration :  55   Loss :  47965.9903109
Iteration :  56   Loss :  16497.2919573
Iteration :  57   Loss :  47965.9903109
Iteration :  58   Loss :  16497.2919573
Iteration :  59   Loss :  47965.9903109
Iteration :  60   Loss :  16497.2919573
Iteration :  61   Loss :  47965.9903109
Iteration :  62   Loss :  16497.2919573
Iteration :  63   Loss :  47965.9903109
Iteration :  64   Loss :  16497.2919573
Iteration :  65   Loss :  47965.9903109
Iteration :  66   Loss :  16497.2919573
Iteration :  67   Loss :  47965.9903109
Iteration :  68   Loss :  16497.2919573
Iteration :  69   Loss :  47965.9903109
Iteration :  70   Loss :  16497.2919573
Iteration :  71   Loss :  47965.9903109
Iteration :  72   Loss :  16497.2919573
Iteration :  73   Loss :  47965.9903109
Iteration :  74   Loss :  16497.2919573
Iteration :  75   Loss :  47965.9903109
Iteration :  76   Loss :  16497.2919573
Iteration :  77   Loss :  47965.9903109
Iteration :  78   Loss :  16497.2919573
Iteration :  79   Loss :  47965.9903109
Iteration :  80   Loss :  16497.2919573
Iteration :  81   Loss :  47965.9903109
Iteration :  82   Loss :  16497.2919573
Iteration :  83   Loss :  47965.9903109
Iteration :  84   Loss :  16497.2919573
Iteration :  85   Loss :  47965.9903109
Iteration :  86   Loss :  16497.2919573
Iteration :  87   Loss :  47965.9903109
Iteration :  88   Loss :  16497.2919573
Iteration :  89   Loss :  47965.9903109
Iteration :  90   Loss :  16497.2919573
Iteration :  91   Loss :  47965.9903109
Iteration :  92   Loss :  16497.2919573
Iteration :  93   Loss :  47965.9903109
Iteration :  94   Loss :  16497.2919573
Iteration :  95   Loss :  47965.9903109
Iteration :  96   Loss :  16497.2919573
Iteration :  97   Loss :  47965.9903109
Iteration :  98   Loss :  16497.2919573
Iteration :  99   Loss :  47965.9903109
[-0.02368004 -0.85275892 -0.1262475  ...,  0.41272034 -0.46188979
 -0.07627624]
CROSS VALIDATION 7
Iteration :  0   Loss :  49515.7059459
Iteration :  1   Loss :  24544.7790345
Iteration :  2   Loss :  50923.1342347
Iteration :  3   Loss :  43272.7483656
Iteration :  4   Loss :  56268.7476671
Iteration :  5   Loss :  56534.7503272
Iteration :  6   Loss :  50812.3389459
Iteration :  7   Loss :  23368.2269966
Iteration :  8   Loss :  34509.7089633
Iteration :  9   Loss :  62745.2094121
Iteration :  10   Loss :  36369.6645517
Iteration :  11   Loss :  43010.9351054
Iteration :  12   Loss :  16986.4146654
Iteration :  13   Loss :  47884.1486779
Iteration :  14   Loss :  16461.7132047
Iteration :  15   Loss :  47965.8740783
Iteration :  16   Loss :  16497.2415207
Iteration :  17   Loss :  47965.9901461
Iteration :  18   Loss :  16497.2918859
Iteration :  19   Loss :  47965.9903106
Iteration :  20   Loss :  16497.2919572
Iteration :  21   Loss :  47965.9903109
Iteration :  22   Loss :  16497.2919573
Iteration :  23   Loss :  47965.9903109
Iteration :  24   Loss :  16497.2919573
Iteration :  25   Loss :  47965.9903109
Iteration :  26   Loss :  16497.2919573
Iteration :  27   Loss :  47965.9903109
Iteration :  28   Loss :  16497.2919573
Iteration :  29   Loss :  47965.9903109
Iteration :  30   Loss :  16497.2919573
Iteration :  31   Loss :  47965.9903109
Iteration :  32   Loss :  16497.2919573
Iteration :  33   Loss :  47965.9903109
Iteration :  34   Loss :  16497.2919573
Iteration :  35   Loss :  47965.9903109
Iteration :  36   Loss :  16497.2919573
Iteration :  37   Loss :  47965.9903109
Iteration :  38   Loss :  16497.2919573
Iteration :  39   Loss :  47965.9903109
Iteration :  40   Loss :  16497.2919573
Iteration :  41   Loss :  47965.9903109
Iteration :  42   Loss :  16497.2919573
Iteration :  43   Loss :  47965.9903109
Iteration :  44   Loss :  16497.2919573
Iteration :  45   Loss :  47965.9903109
Iteration :  46   Loss :  16497.2919573
Iteration :  47   Loss :  47965.9903109
Iteration :  48   Loss :  16497.2919573
Iteration :  49   Loss :  47965.9903109
Iteration :  50   Loss :  16497.2919573
Iteration :  51   Loss :  47965.9903109
Iteration :  52   Loss :  16497.2919573
Iteration :  53   Loss :  47965.9903109
Iteration :  54   Loss :  16497.2919573
Iteration :  55   Loss :  47965.9903109
Iteration :  56   Loss :  16497.2919573
Iteration :  57   Loss :  47965.9903109
Iteration :  58   Loss :  16497.2919573
Iteration :  59   Loss :  47965.9903109
Iteration :  60   Loss :  16497.2919573
Iteration :  61   Loss :  47965.9903109
Iteration :  62   Loss :  16497.2919573
Iteration :  63   Loss :  47965.9903109
Iteration :  64   Loss :  16497.2919573
Iteration :  65   Loss :  47965.9903109
Iteration :  66   Loss :  16497.2919573
Iteration :  67   Loss :  47965.9903109
Iteration :  68   Loss :  16497.2919573
Iteration :  69   Loss :  47965.9903109
Iteration :  70   Loss :  16497.2919573
Iteration :  71   Loss :  47965.9903109
Iteration :  72   Loss :  16497.2919573
Iteration :  73   Loss :  47965.9903109
Iteration :  74   Loss :  16497.2919573
Iteration :  75   Loss :  47965.9903109
Iteration :  76   Loss :  16497.2919573
Iteration :  77   Loss :  47965.9903109
Iteration :  78   Loss :  16497.2919573
Iteration :  79   Loss :  47965.9903109
Iteration :  80   Loss :  16497.2919573
Iteration :  81   Loss :  47965.9903109
Iteration :  82   Loss :  16497.2919573
Iteration :  83   Loss :  47965.9903109
Iteration :  84   Loss :  16497.2919573
Iteration :  85   Loss :  47965.9903109
Iteration :  86   Loss :  16497.2919573
Iteration :  87   Loss :  47965.9903109
Iteration :  88   Loss :  16497.2919573
Iteration :  89   Loss :  47965.9903109
Iteration :  90   Loss :  16497.2919573
Iteration :  91   Loss :  47965.9903109
Iteration :  92   Loss :  16497.2919573
Iteration :  93   Loss :  47965.9903109
Iteration :  94   Loss :  16497.2919573
Iteration :  95   Loss :  47965.9903109
Iteration :  96   Loss :  16497.2919573
Iteration :  97   Loss :  47965.9903109
Iteration :  98   Loss :  16497.2919573
Iteration :  99   Loss :  47965.9903109
[-0.02368004 -0.85275892 -0.1262475  ...,  0.41272034 -0.46188979
 -0.07627624]
CROSS VALIDATION 8
Iteration :  0   Loss :  48458.9781391
Iteration :  1   Loss :  31605.3641797
Iteration :  2   Loss :  38864.6350585
Iteration :  3   Loss :  36610.5448418
Iteration :  4   Loss :  35321.4960637
Iteration :  5   Loss :  26840.4268041
Iteration :  6   Loss :  38721.6705189
Iteration :  7   Loss :  56937.3546063
Iteration :  8   Loss :  48843.9192529
Iteration :  9   Loss :  32650.9860797
Iteration :  10   Loss :  28909.0269881
Iteration :  11   Loss :  48584.9947476
Iteration :  12   Loss :  32336.0521804
Iteration :  13   Loss :  47239.5384507
Iteration :  14   Loss :  63864.1139457
Iteration :  15   Loss :  36313.308585
Iteration :  16   Loss :  53351.6427518
Iteration :  17   Loss :  43849.3824531
Iteration :  18   Loss :  71888.6909747
Iteration :  19   Loss :  63682.4986352
Iteration :  20   Loss :  66149.4476568
Iteration :  21   Loss :  46681.7852819
Iteration :  22   Loss :  38062.0470349
Iteration :  23   Loss :  60497.6401482
Iteration :  24   Loss :  30088.9813449
Iteration :  25   Loss :  54984.4783238
Iteration :  26   Loss :  32573.5281084
Iteration :  27   Loss :  50636.2786581
Iteration :  28   Loss :  16592.4385281
Iteration :  29   Loss :  47956.3932148
Iteration :  30   Loss :  16497.3849189
Iteration :  31   Loss :  47965.976704
Iteration :  32   Loss :  16497.2920891
Iteration :  33   Loss :  47965.9902916
Iteration :  34   Loss :  16497.2919575
Iteration :  35   Loss :  47965.9903108
Iteration :  36   Loss :  16497.2919573
Iteration :  37   Loss :  47965.9903109
Iteration :  38   Loss :  16497.2919573
Iteration :  39   Loss :  47965.9903109
Iteration :  40   Loss :  16497.2919573
Iteration :  41   Loss :  47965.9903109
Iteration :  42   Loss :  16497.2919573
Iteration :  43   Loss :  47965.9903109
Iteration :  44   Loss :  16497.2919573
Iteration :  45   Loss :  47965.9903109
Iteration :  46   Loss :  16497.2919573
Iteration :  47   Loss :  47965.9903109
Iteration :  48   Loss :  16497.2919573
Iteration :  49   Loss :  47965.9903109
Iteration :  50   Loss :  16497.2919573
Iteration :  51   Loss :  47965.9903109
Iteration :  52   Loss :  16497.2919573
Iteration :  53   Loss :  47965.9903109
Iteration :  54   Loss :  16497.2919573
Iteration :  55   Loss :  47965.9903109
Iteration :  56   Loss :  16497.2919573
Iteration :  57   Loss :  47965.9903109
Iteration :  58   Loss :  16497.2919573
Iteration :  59   Loss :  47965.9903109
Iteration :  60   Loss :  16497.2919573
Iteration :  61   Loss :  47965.9903109
Iteration :  62   Loss :  16497.2919573
Iteration :  63   Loss :  47965.9903109
Iteration :  64   Loss :  16497.2919573
Iteration :  65   Loss :  47965.9903109
Iteration :  66   Loss :  16497.2919573
Iteration :  67   Loss :  47965.9903109
Iteration :  68   Loss :  16497.2919573
Iteration :  69   Loss :  47965.9903109
Iteration :  70   Loss :  16497.2919573
Iteration :  71   Loss :  47965.9903109
Iteration :  72   Loss :  16497.2919573
Iteration :  73   Loss :  47965.9903109
Iteration :  74   Loss :  16497.2919573
Iteration :  75   Loss :  47965.9903109
Iteration :  76   Loss :  16497.2919573
Iteration :  77   Loss :  47965.9903109
Iteration :  78   Loss :  16497.2919573
Iteration :  79   Loss :  47965.9903109
Iteration :  80   Loss :  16497.2919573
Iteration :  81   Loss :  47965.9903109
Iteration :  82   Loss :  16497.2919573
Iteration :  83   Loss :  47965.9903109
Iteration :  84   Loss :  16497.2919573
Iteration :  85   Loss :  47965.9903109
Iteration :  86   Loss :  16497.2919573
Iteration :  87   Loss :  47965.9903109
Iteration :  88   Loss :  16497.2919573
Iteration :  89   Loss :  47965.9903109
Iteration :  90   Loss :  16497.2919573
Iteration :  91   Loss :  47965.9903109
Iteration :  92   Loss :  16497.2919573
Iteration :  93   Loss :  47965.9903109
Iteration :  94   Loss :  16497.2919573
Iteration :  95   Loss :  47965.9903109
Iteration :  96   Loss :  16497.2919573
Iteration :  97   Loss :  47965.9903109
Iteration :  98   Loss :  16497.2919573
Iteration :  99   Loss :  47965.9903109
[-0.02368004 -0.85275892 -0.1262475  ...,  0.41272034 -0.46188979
 -0.07627624]
CROSS VALIDATION 9
Iteration :  0   Loss :  24419.8090578
Iteration :  1   Loss :  89727.636114
Iteration :  2   Loss :  36224.8905718
Iteration :  3   Loss :  86019.6009369
Iteration :  4   Loss :  36211.3965573
Iteration :  5   Loss :  86020.7016015
Iteration :  6   Loss :  36211.4503602
Iteration :  7   Loss :  86020.7031633
Iteration :  8   Loss :  36211.4504369
Iteration :  9   Loss :  86020.7031655
Iteration :  10   Loss :  36211.450437
Iteration :  11   Loss :  86020.7031655
Iteration :  12   Loss :  36211.450437
Iteration :  13   Loss :  86020.7031655
Iteration :  14   Loss :  36211.450437
Iteration :  15   Loss :  86020.7031655
Iteration :  16   Loss :  36211.450437
Iteration :  17   Loss :  86020.7031655
Iteration :  18   Loss :  36211.450437
Iteration :  19   Loss :  86020.7031655
Iteration :  20   Loss :  36211.450437
Iteration :  21   Loss :  86020.7031655
Iteration :  22   Loss :  36211.450437
Iteration :  23   Loss :  86020.7031655
Iteration :  24   Loss :  36211.450437
Iteration :  25   Loss :  86020.7031655
Iteration :  26   Loss :  36211.450437
Iteration :  27   Loss :  86020.7031655
Iteration :  28   Loss :  36211.450437
Iteration :  29   Loss :  86020.7031655
Iteration :  30   Loss :  36211.450437
Iteration :  31   Loss :  86020.7031655
Iteration :  32   Loss :  36211.450437
Iteration :  33   Loss :  86020.7031655
Iteration :  34   Loss :  36211.450437
Iteration :  35   Loss :  86020.7031655
Iteration :  36   Loss :  36211.450437
Iteration :  37   Loss :  86020.7031655
Iteration :  38   Loss :  36211.450437
Iteration :  39   Loss :  86020.7031655
Iteration :  40   Loss :  36211.450437
Iteration :  41   Loss :  86020.7031655
Iteration :  42   Loss :  36211.450437
Iteration :  43   Loss :  86020.7031655
Iteration :  44   Loss :  36211.450437
Iteration :  45   Loss :  86020.7031655
Iteration :  46   Loss :  36211.450437
Iteration :  47   Loss :  86020.7031655
Iteration :  48   Loss :  36211.450437
Iteration :  49   Loss :  86020.7031655
Iteration :  50   Loss :  36211.450437
Iteration :  51   Loss :  86020.7031655
Iteration :  52   Loss :  36211.450437
Iteration :  53   Loss :  86020.7031655
Iteration :  54   Loss :  36211.450437
Iteration :  55   Loss :  86020.7031655
Iteration :  56   Loss :  36211.450437
Iteration :  57   Loss :  86020.7031655
Iteration :  58   Loss :  36211.450437
Iteration :  59   Loss :  86020.7031655
Iteration :  60   Loss :  36211.450437
Iteration :  61   Loss :  86020.7031655
Iteration :  62   Loss :  36211.450437
Iteration :  63   Loss :  86020.7031655
Iteration :  64   Loss :  36211.450437
Iteration :  65   Loss :  86020.7031655
Iteration :  66   Loss :  36211.450437
Iteration :  67   Loss :  86020.7031655
Iteration :  68   Loss :  36211.450437
Iteration :  69   Loss :  86020.7031655
Iteration :  70   Loss :  36211.450437
Iteration :  71   Loss :  86020.7031655
Iteration :  72   Loss :  36211.450437
Iteration :  73   Loss :  86020.7031655
Iteration :  74   Loss :  36211.450437
Iteration :  75   Loss :  86020.7031655
Iteration :  76   Loss :  36211.450437
Iteration :  77   Loss :  86020.7031655
Iteration :  78   Loss :  36211.450437
Iteration :  79   Loss :  86020.7031655
Iteration :  80   Loss :  36211.450437
Iteration :  81   Loss :  86020.7031655
Iteration :  82   Loss :  36211.450437
Iteration :  83   Loss :  86020.7031655
Iteration :  84   Loss :  36211.450437
Iteration :  85   Loss :  86020.7031655
Iteration :  86   Loss :  36211.450437
Iteration :  87   Loss :  86020.7031655
Iteration :  88   Loss :  36211.450437
Iteration :  89   Loss :  86020.7031655
Iteration :  90   Loss :  36211.450437
Iteration :  91   Loss :  86020.7031655
Iteration :  92   Loss :  36211.450437
Iteration :  93   Loss :  86020.7031655
Iteration :  94   Loss :  36211.450437
Iteration :  95   Loss :  86020.7031655
Iteration :  96   Loss :  36211.450437
Iteration :  97   Loss :  86020.7031655
Iteration :  98   Loss :  36211.450437
Iteration :  99   Loss :  86020.7031655
[-0.52404041  0.03577654  0.21303792 ...,  0.34292698  0.49928093
  0.25761125]
CROSS VALIDATION 10
Iteration :  0   Loss :  49241.0731926
Iteration :  1   Loss :  26820.4183456
Iteration :  2   Loss :  50611.7720566
Iteration :  3   Loss :  17966.5712623
Iteration :  4   Loss :  47292.1306516
Iteration :  5   Loss :  30277.5531919
Iteration :  6   Loss :  49429.8427723
Iteration :  7   Loss :  30210.3425854
Iteration :  8   Loss :  43767.7775133
Iteration :  9   Loss :  39905.0863424
Iteration :  10   Loss :  51163.0235848
Iteration :  11   Loss :  43520.0367832
Iteration :  12   Loss :  53714.1376356
Iteration :  13   Loss :  49503.9494327
Iteration :  14   Loss :  29199.0780536
Iteration :  15   Loss :  64212.3116335
Iteration :  16   Loss :  45312.8023362
Iteration :  17   Loss :  85917.9393104
Iteration :  18   Loss :  29810.5219772
Iteration :  19   Loss :  24422.2892796
Iteration :  20   Loss :  50271.9070052
Iteration :  21   Loss :  20288.8373465
Iteration :  22   Loss :  49668.3768474
Iteration :  23   Loss :  78125.230084
Iteration :  24   Loss :  36017.8806841
Iteration :  25   Loss :  32243.7285667
Iteration :  26   Loss :  36210.0969447
Iteration :  27   Loss :  45095.0973666
Iteration :  28   Loss :  24365.7253344
Iteration :  29   Loss :  16558.9560783
Iteration :  30   Loss :  70013.168403
Iteration :  31   Loss :  26884.1808495
Iteration :  32   Loss :  59322.2073286
Iteration :  33   Loss :  20497.161531
Iteration :  34   Loss :  58761.0548454
Iteration :  35   Loss :  57282.4307968
Iteration :  36   Loss :  59339.0372848
Iteration :  37   Loss :  41728.1829876
Iteration :  38   Loss :  63863.9163897
Iteration :  39   Loss :  29142.0915831
Iteration :  40   Loss :  37680.8540084
Iteration :  41   Loss :  48697.8532117
Iteration :  42   Loss :  46005.9551028
Iteration :  43   Loss :  74686.1897013
Iteration :  44   Loss :  45170.6582102
Iteration :  45   Loss :  28278.869892
Iteration :  46   Loss :  50677.1994176
Iteration :  47   Loss :  17969.9021849
Iteration :  48   Loss :  47292.1029654
Iteration :  49   Loss :  30277.5538567
Iteration :  50   Loss :  49429.8427203
Iteration :  51   Loss :  30210.3425873
Iteration :  52   Loss :  43767.7775132
Iteration :  53   Loss :  39905.0863424
Iteration :  54   Loss :  51163.0235848
Iteration :  55   Loss :  43520.0367832
Iteration :  56   Loss :  53714.1376356
Iteration :  57   Loss :  49503.9494327
Iteration :  58   Loss :  29199.0780536
Iteration :  59   Loss :  64212.3116335
Iteration :  60   Loss :  45312.8023362
Iteration :  61   Loss :  85917.9393104
Iteration :  62   Loss :  29810.5219772
Iteration :  63   Loss :  24422.2892796
Iteration :  64   Loss :  50271.9070052
Iteration :  65   Loss :  20288.8373465
Iteration :  66   Loss :  49668.3768474
Iteration :  67   Loss :  78125.230084
Iteration :  68   Loss :  36017.8806841
Iteration :  69   Loss :  32243.7285667
Iteration :  70   Loss :  36210.0969447
Iteration :  71   Loss :  45095.0973666
Iteration :  72   Loss :  24365.7253344
Iteration :  73   Loss :  16558.9560783
Iteration :  74   Loss :  70013.168403
Iteration :  75   Loss :  26884.1808495
Iteration :  76   Loss :  59322.2073286
Iteration :  77   Loss :  20497.161531
Iteration :  78   Loss :  58761.0548454
Iteration :  79   Loss :  57282.4307968
Iteration :  80   Loss :  59339.0372848
Iteration :  81   Loss :  41728.1829876
Iteration :  82   Loss :  63863.9163897
Iteration :  83   Loss :  29142.0915831
Iteration :  84   Loss :  37680.8540084
Iteration :  85   Loss :  48697.8532117
Iteration :  86   Loss :  46005.9551028
Iteration :  87   Loss :  74686.1897013
Iteration :  88   Loss :  45170.6582102
Iteration :  89   Loss :  28278.869892
Iteration :  90   Loss :  50677.1994176
Iteration :  91   Loss :  17969.9021849
Iteration :  92   Loss :  47292.1029654
Iteration :  93   Loss :  30277.5538567
Iteration :  94   Loss :  49429.8427203
Iteration :  95   Loss :  30210.3425873
Iteration :  96   Loss :  43767.7775132
Iteration :  97   Loss :  39905.0863424
Iteration :  98   Loss :  51163.0235848
Iteration :  99   Loss :  43520.0367832
[-0.69262007  0.01062566 -0.57190567 ...,  0.42879119 -0.19944575
  0.1389136 ]
CROSS VALIDATION 11
Iteration :  0   Loss :  49241.0731926
Iteration :  1   Loss :  27087.577247
Iteration :  2   Loss :  50611.1638338
Iteration :  3   Loss :  35772.2439641
Iteration :  4   Loss :  27234.7665607
Iteration :  5   Loss :  45400.6297058
Iteration :  6   Loss :  38608.7003173
Iteration :  7   Loss :  51494.0686337
Iteration :  8   Loss :  34165.4818774
Iteration :  9   Loss :  74061.3062081
Iteration :  10   Loss :  65821.3578937
Iteration :  11   Loss :  31611.0899
Iteration :  12   Loss :  6938.72066595
Iteration :  13   Loss :  32079.7888139
Iteration :  14   Loss :  27514.9700814
Iteration :  15   Loss :  59038.8439019
Iteration :  16   Loss :  26710.5423821
Iteration :  17   Loss :  41094.3347817
Iteration :  18   Loss :  65371.5976741
Iteration :  19   Loss :  50512.0482105
Iteration :  20   Loss :  16723.5389916
Iteration :  21   Loss :  48474.6237342
Iteration :  22   Loss :  13752.3390127
Iteration :  23   Loss :  47969.2609573
Iteration :  24   Loss :  13783.7287414
Iteration :  25   Loss :  47968.54531
Iteration :  26   Loss :  13783.7732404
Iteration :  27   Loss :  47968.5442955
Iteration :  28   Loss :  13783.7733035
Iteration :  29   Loss :  47968.5442941
Iteration :  30   Loss :  13783.7733036
Iteration :  31   Loss :  47968.5442941
Iteration :  32   Loss :  13783.7733036
Iteration :  33   Loss :  47968.5442941
Iteration :  34   Loss :  13783.7733036
Iteration :  35   Loss :  47968.5442941
Iteration :  36   Loss :  13783.7733036
Iteration :  37   Loss :  47968.5442941
Iteration :  38   Loss :  13783.7733036
Iteration :  39   Loss :  47968.5442941
Iteration :  40   Loss :  13783.7733036
Iteration :  41   Loss :  47968.5442941
Iteration :  42   Loss :  13783.7733036
Iteration :  43   Loss :  47968.5442941
Iteration :  44   Loss :  13783.7733036
Iteration :  45   Loss :  47968.5442941
Iteration :  46   Loss :  13783.7733036
Iteration :  47   Loss :  47968.5442941
Iteration :  48   Loss :  13783.7733036
Iteration :  49   Loss :  47968.5442941
Iteration :  50   Loss :  13783.7733036
Iteration :  51   Loss :  47968.5442941
Iteration :  52   Loss :  13783.7733036
Iteration :  53   Loss :  47968.5442941
Iteration :  54   Loss :  13783.7733036
Iteration :  55   Loss :  47968.5442941
Iteration :  56   Loss :  13783.7733036
Iteration :  57   Loss :  47968.5442941
Iteration :  58   Loss :  13783.7733036
Iteration :  59   Loss :  47968.5442941
Iteration :  60   Loss :  13783.7733036
Iteration :  61   Loss :  47968.5442941
Iteration :  62   Loss :  13783.7733036
Iteration :  63   Loss :  47968.5442941
Iteration :  64   Loss :  13783.7733036
Iteration :  65   Loss :  47968.5442941
Iteration :  66   Loss :  13783.7733036
Iteration :  67   Loss :  47968.5442941
Iteration :  68   Loss :  13783.7733036
Iteration :  69   Loss :  47968.5442941
Iteration :  70   Loss :  13783.7733036
Iteration :  71   Loss :  47968.5442941
Iteration :  72   Loss :  13783.7733036
Iteration :  73   Loss :  47968.5442941
Iteration :  74   Loss :  13783.7733036
Iteration :  75   Loss :  47968.5442941
Iteration :  76   Loss :  13783.7733036
Iteration :  77   Loss :  47968.5442941
Iteration :  78   Loss :  13783.7733036
Iteration :  79   Loss :  47968.5442941
Iteration :  80   Loss :  13783.7733036
Iteration :  81   Loss :  47968.5442941
Iteration :  82   Loss :  13783.7733036
Iteration :  83   Loss :  47968.5442941
Iteration :  84   Loss :  13783.7733036
Iteration :  85   Loss :  47968.5442941
Iteration :  86   Loss :  13783.7733036
Iteration :  87   Loss :  47968.5442941
Iteration :  88   Loss :  13783.7733036
Iteration :  89   Loss :  47968.5442941
Iteration :  90   Loss :  13783.7733036
Iteration :  91   Loss :  47968.5442941
Iteration :  92   Loss :  13783.7733036
Iteration :  93   Loss :  47968.5442941
Iteration :  94   Loss :  13783.7733036
Iteration :  95   Loss :  47968.5442941
Iteration :  96   Loss :  13783.7733036
Iteration :  97   Loss :  47968.5442941
Iteration :  98   Loss :  13783.7733036
Iteration :  99   Loss :  47968.5442941
[-0.02373151 -0.85277435 -0.12623236 ...,  0.41273162 -0.46185973
 -0.07629306]
CROSS VALIDATION 12
Iteration :  0   Loss :  28154.7165108
Iteration :  1   Loss :  37662.6293155
Iteration :  2   Loss :  36133.3984724
Iteration :  3   Loss :  56713.367681
Iteration :  4   Loss :  50747.8551499
Iteration :  5   Loss :  34639.888767
Iteration :  6   Loss :  66006.2237454
Iteration :  7   Loss :  37018.6278368
Iteration :  8   Loss :  41235.867821
Iteration :  9   Loss :  35190.5637761
Iteration :  10   Loss :  56877.3887725
Iteration :  11   Loss :  62228.7448165
Iteration :  12   Loss :  52339.7509395
Iteration :  13   Loss :  39282.1652597
Iteration :  14   Loss :  39240.9113465
Iteration :  15   Loss :  43836.2786767
Iteration :  16   Loss :  41898.5893589
Iteration :  17   Loss :  33281.5613089
Iteration :  18   Loss :  20685.1181561
Iteration :  19   Loss :  52580.0399299
Iteration :  20   Loss :  32229.80308
Iteration :  21   Loss :  49283.7827205
Iteration :  22   Loss :  19922.3714463
Iteration :  23   Loss :  56070.0928509
Iteration :  24   Loss :  20422.8946023
Iteration :  25   Loss :  33429.6837726
Iteration :  26   Loss :  54944.8526523
Iteration :  27   Loss :  30852.3929009
Iteration :  28   Loss :  47779.9421702
Iteration :  29   Loss :  48355.1965771
Iteration :  30   Loss :  65149.2748333
Iteration :  31   Loss :  39350.3417443
Iteration :  32   Loss :  30887.0046295
Iteration :  33   Loss :  72011.0377482
Iteration :  34   Loss :  56665.9230886
Iteration :  35   Loss :  51889.3000415
Iteration :  36   Loss :  18910.4638489
Iteration :  37   Loss :  43189.2753803
Iteration :  38   Loss :  34482.2828136
Iteration :  39   Loss :  42283.1287403
Iteration :  40   Loss :  34620.4958978
Iteration :  41   Loss :  25098.9721739
Iteration :  42   Loss :  33304.2664402
Iteration :  43   Loss :  24112.7744737
Iteration :  44   Loss :  39852.026632
Iteration :  45   Loss :  27745.8289132
Iteration :  46   Loss :  61180.2329772
Iteration :  47   Loss :  33668.5966596
Iteration :  48   Loss :  43941.3086492
Iteration :  49   Loss :  42027.0017384
Iteration :  50   Loss :  46457.3055077
Iteration :  51   Loss :  65415.8875427
Iteration :  52   Loss :  39326.9469745
Iteration :  53   Loss :  30886.2443089
Iteration :  54   Loss :  72011.0221526
Iteration :  55   Loss :  56665.9231128
Iteration :  56   Loss :  51889.3000359
Iteration :  57   Loss :  18910.4638476
Iteration :  58   Loss :  43189.2753803
Iteration :  59   Loss :  34482.2828136
Iteration :  60   Loss :  42283.1287403
Iteration :  61   Loss :  34620.4958978
Iteration :  62   Loss :  25098.9721739
Iteration :  63   Loss :  33304.2664402
Iteration :  64   Loss :  24112.7744737
Iteration :  65   Loss :  39852.026632
Iteration :  66   Loss :  27745.8289132
Iteration :  67   Loss :  61180.2329772
Iteration :  68   Loss :  33668.5966596
Iteration :  69   Loss :  43941.3086492
Iteration :  70   Loss :  42027.0017384
Iteration :  71   Loss :  46457.3055077
Iteration :  72   Loss :  65415.8875427
Iteration :  73   Loss :  39326.9469745
Iteration :  74   Loss :  30886.2443089
Iteration :  75   Loss :  72011.0221526
Iteration :  76   Loss :  56665.9231128
Iteration :  77   Loss :  51889.3000359
Iteration :  78   Loss :  18910.4638476
Iteration :  79   Loss :  43189.2753803
Iteration :  80   Loss :  34482.2828136
Iteration :  81   Loss :  42283.1287403
Iteration :  82   Loss :  34620.4958978
Iteration :  83   Loss :  25098.9721739
Iteration :  84   Loss :  33304.2664402
Iteration :  85   Loss :  24112.7744737
Iteration :  86   Loss :  39852.026632
Iteration :  87   Loss :  27745.8289132
Iteration :  88   Loss :  61180.2329772
Iteration :  89   Loss :  33668.5966596
Iteration :  90   Loss :  43941.3086492
Iteration :  91   Loss :  42027.0017384
Iteration :  92   Loss :  46457.3055077
Iteration :  93   Loss :  65415.8875427
Iteration :  94   Loss :  39326.9469745
Iteration :  95   Loss :  30886.2443089
Iteration :  96   Loss :  72011.0221526
Iteration :  97   Loss :  56665.9231128
Iteration :  98   Loss :  51889.3000359
Iteration :  99   Loss :  18910.4638476
[-0.10330891 -0.69613066  0.13384142 ...,  0.11260752 -0.24922931
 -0.0208611 ]
CROSS VALIDATION 13
Iteration :  0   Loss :  36768.580702
Iteration :  1   Loss :  26988.7430383
Iteration :  2   Loss :  49004.5336262
Iteration :  3   Loss :  22356.8136937
Iteration :  4   Loss :  50581.6818371
Iteration :  5   Loss :  32135.1319344
Iteration :  6   Loss :  49514.2833384
Iteration :  7   Loss :  30422.5443378
Iteration :  8   Loss :  52081.9267702
Iteration :  9   Loss :  31035.802826
Iteration :  10   Loss :  49563.9862527
Iteration :  11   Loss :  34903.6228816
Iteration :  12   Loss :  9225.88766095
Iteration :  13   Loss :  41763.7503644
Iteration :  14   Loss :  30416.9131271
Iteration :  15   Loss :  50795.5541596
Iteration :  16   Loss :  32103.1304553
Iteration :  17   Loss :  49512.9414161
Iteration :  18   Loss :  30422.5728412
Iteration :  19   Loss :  52081.9248399
Iteration :  20   Loss :  31035.8027476
Iteration :  21   Loss :  49563.9862517
Iteration :  22   Loss :  34903.6228817
Iteration :  23   Loss :  9225.88766096
Iteration :  24   Loss :  41763.7503644
Iteration :  25   Loss :  30416.9131271
Iteration :  26   Loss :  50795.5541596
Iteration :  27   Loss :  32103.1304553
Iteration :  28   Loss :  49512.9414161
Iteration :  29   Loss :  30422.5728412
Iteration :  30   Loss :  52081.9248399
Iteration :  31   Loss :  31035.8027476
Iteration :  32   Loss :  49563.9862517
Iteration :  33   Loss :  34903.6228817
Iteration :  34   Loss :  9225.88766096
Iteration :  35   Loss :  41763.7503644
Iteration :  36   Loss :  30416.9131271
Iteration :  37   Loss :  50795.5541596
Iteration :  38   Loss :  32103.1304553
Iteration :  39   Loss :  49512.9414161
Iteration :  40   Loss :  30422.5728412
Iteration :  41   Loss :  52081.9248399
Iteration :  42   Loss :  31035.8027476
Iteration :  43   Loss :  49563.9862517
Iteration :  44   Loss :  34903.6228817
Iteration :  45   Loss :  9225.88766096
Iteration :  46   Loss :  41763.7503644
Iteration :  47   Loss :  30416.9131271
Iteration :  48   Loss :  50795.5541596
Iteration :  49   Loss :  32103.1304553
Iteration :  50   Loss :  49512.9414161
Iteration :  51   Loss :  30422.5728412
Iteration :  52   Loss :  52081.9248399
Iteration :  53   Loss :  31035.8027476
Iteration :  54   Loss :  49563.9862517
Iteration :  55   Loss :  34903.6228817
Iteration :  56   Loss :  9225.88766096
Iteration :  57   Loss :  41763.7503644
Iteration :  58   Loss :  30416.9131271
Iteration :  59   Loss :  50795.5541596
Iteration :  60   Loss :  32103.1304553
Iteration :  61   Loss :  49512.9414161
Iteration :  62   Loss :  30422.5728412
Iteration :  63   Loss :  52081.9248399
Iteration :  64   Loss :  31035.8027476
Iteration :  65   Loss :  49563.9862517
Iteration :  66   Loss :  34903.6228817
Iteration :  67   Loss :  9225.88766096
Iteration :  68   Loss :  41763.7503644
Iteration :  69   Loss :  30416.9131271
Iteration :  70   Loss :  50795.5541596
Iteration :  71   Loss :  32103.1304553
Iteration :  72   Loss :  49512.9414161
Iteration :  73   Loss :  30422.5728412
Iteration :  74   Loss :  52081.9248399
Iteration :  75   Loss :  31035.8027476
Iteration :  76   Loss :  49563.9862517
Iteration :  77   Loss :  34903.6228817
Iteration :  78   Loss :  9225.88766096
Iteration :  79   Loss :  41763.7503644
Iteration :  80   Loss :  30416.9131271
Iteration :  81   Loss :  50795.5541596
Iteration :  82   Loss :  32103.1304553
Iteration :  83   Loss :  49512.9414161
Iteration :  84   Loss :  30422.5728412
Iteration :  85   Loss :  52081.9248399
Iteration :  86   Loss :  31035.8027476
Iteration :  87   Loss :  49563.9862517
Iteration :  88   Loss :  34903.6228817
Iteration :  89   Loss :  9225.88766096
Iteration :  90   Loss :  41763.7503644
Iteration :  91   Loss :  30416.9131271
Iteration :  92   Loss :  50795.5541596
Iteration :  93   Loss :  32103.1304553
Iteration :  94   Loss :  49512.9414161
Iteration :  95   Loss :  30422.5728412
Iteration :  96   Loss :  52081.9248399
Iteration :  97   Loss :  31035.8027476
Iteration :  98   Loss :  49563.9862517
Iteration :  99   Loss :  34903.6228817
[ 0.14411458 -0.58966082  0.19966678 ...,  0.150839   -0.30827135
 -0.00861527]
CROSS VALIDATION 14
Iteration :  0   Loss :  46942.2349021
Iteration :  1   Loss :  23869.8126976
Iteration :  2   Loss :  41407.5191433
Iteration :  3   Loss :  29731.8831393
Iteration :  4   Loss :  46900.1812501
Iteration :  5   Loss :  38711.4083675
Iteration :  6   Loss :  26013.1278509
Iteration :  7   Loss :  24097.1049702
Iteration :  8   Loss :  30963.7323274
Iteration :  9   Loss :  43054.4086096
Iteration :  10   Loss :  42772.7947671
Iteration :  11   Loss :  62723.3742226
Iteration :  12   Loss :  33644.8285117
Iteration :  13   Loss :  38357.8643121
Iteration :  14   Loss :  33195.4716851
Iteration :  15   Loss :  31590.1637622
Iteration :  16   Loss :  40874.652365
Iteration :  17   Loss :  46289.677268
Iteration :  18   Loss :  31987.7792023
Iteration :  19   Loss :  35461.6927002
Iteration :  20   Loss :  40624.7323857
Iteration :  21   Loss :  24535.0408938
Iteration :  22   Loss :  43300.6239686
Iteration :  23   Loss :  52104.5599919
Iteration :  24   Loss :  46207.7779496
Iteration :  25   Loss :  43945.1036887
Iteration :  26   Loss :  26349.0484734
Iteration :  27   Loss :  41600.7044986
Iteration :  28   Loss :  40217.3649646
Iteration :  29   Loss :  46899.1440136
Iteration :  30   Loss :  11129.4107968
Iteration :  31   Loss :  55455.2914094
Iteration :  32   Loss :  42475.801391
Iteration :  33   Loss :  62119.4091277
Iteration :  34   Loss :  27532.4511477
Iteration :  35   Loss :  40489.6368645
Iteration :  36   Loss :  62451.0290279
Iteration :  37   Loss :  40491.5861421
Iteration :  38   Loss :  45192.0707322
Iteration :  39   Loss :  43941.0051591
Iteration :  40   Loss :  26347.5650742
Iteration :  41   Loss :  41600.6601613
Iteration :  42   Loss :  40217.36441
Iteration :  43   Loss :  46899.1439508
Iteration :  44   Loss :  11129.410796
Iteration :  45   Loss :  55455.2914093
Iteration :  46   Loss :  42475.801391
Iteration :  47   Loss :  62119.4091277
Iteration :  48   Loss :  27532.4511477
Iteration :  49   Loss :  40489.6368645
Iteration :  50   Loss :  62451.0290279
Iteration :  51   Loss :  40491.5861421
Iteration :  52   Loss :  45192.0707322
Iteration :  53   Loss :  43941.0051591
Iteration :  54   Loss :  26347.5650742
Iteration :  55   Loss :  41600.6601613
Iteration :  56   Loss :  40217.36441
Iteration :  57   Loss :  46899.1439508
Iteration :  58   Loss :  11129.410796
Iteration :  59   Loss :  55455.2914093
Iteration :  60   Loss :  42475.801391
Iteration :  61   Loss :  62119.4091277
Iteration :  62   Loss :  27532.4511477
Iteration :  63   Loss :  40489.6368645
Iteration :  64   Loss :  62451.0290279
Iteration :  65   Loss :  40491.5861421
Iteration :  66   Loss :  45192.0707322
Iteration :  67   Loss :  43941.0051591
Iteration :  68   Loss :  26347.5650742
Iteration :  69   Loss :  41600.6601613
Iteration :  70   Loss :  40217.36441
Iteration :  71   Loss :  46899.1439508
Iteration :  72   Loss :  11129.410796
Iteration :  73   Loss :  55455.2914093
Iteration :  74   Loss :  42475.801391
Iteration :  75   Loss :  62119.4091277
Iteration :  76   Loss :  27532.4511477
Iteration :  77   Loss :  40489.6368645
Iteration :  78   Loss :  62451.0290279
Iteration :  79   Loss :  40491.5861421
Iteration :  80   Loss :  45192.0707322
Iteration :  81   Loss :  43941.0051591
Iteration :  82   Loss :  26347.5650742
Iteration :  83   Loss :  41600.6601613
Iteration :  84   Loss :  40217.36441
Iteration :  85   Loss :  46899.1439508
Iteration :  86   Loss :  11129.410796
Iteration :  87   Loss :  55455.2914093
Iteration :  88   Loss :  42475.801391
Iteration :  89   Loss :  62119.4091277
Iteration :  90   Loss :  27532.4511477
Iteration :  91   Loss :  40489.6368645
Iteration :  92   Loss :  62451.0290279
Iteration :  93   Loss :  40491.5861421
Iteration :  94   Loss :  45192.0707322
Iteration :  95   Loss :  43941.0051591
Iteration :  96   Loss :  26347.5650742
Iteration :  97   Loss :  41600.6601613
Iteration :  98   Loss :  40217.36441
Iteration :  99   Loss :  46899.1439508
[-0.27279756 -0.56284912 -0.07647129 ...,  0.43101121 -0.34147964
 -0.01963616]
CROSS VALIDATION 15
Iteration :  0   Loss :  49238.8725453
Iteration :  1   Loss :  23613.8805803
Iteration :  2   Loss :  42277.1973738
Iteration :  3   Loss :  30392.7875196
Iteration :  4   Loss :  46900.1812501
Iteration :  5   Loss :  25753.9204606
Iteration :  6   Loss :  34219.3471714
Iteration :  7   Loss :  36398.4137967
Iteration :  8   Loss :  22161.7701807
Iteration :  9   Loss :  38078.250573
Iteration :  10   Loss :  29148.1890675
Iteration :  11   Loss :  59551.1913833
Iteration :  12   Loss :  43760.8690195
Iteration :  13   Loss :  53017.032194
Iteration :  14   Loss :  59692.2047331
Iteration :  15   Loss :  22543.9324706
Iteration :  16   Loss :  38906.3612253
Iteration :  17   Loss :  64296.9549339
Iteration :  18   Loss :  33478.0815597
Iteration :  19   Loss :  35316.8781443
Iteration :  20   Loss :  28917.160084
Iteration :  21   Loss :  57085.4802608
Iteration :  22   Loss :  48649.5436068
Iteration :  23   Loss :  65339.0311601
Iteration :  24   Loss :  21039.1661755
Iteration :  25   Loss :  41563.1817864
Iteration :  26   Loss :  31421.6702067
Iteration :  27   Loss :  52114.3411216
Iteration :  28   Loss :  15517.018158
Iteration :  29   Loss :  53340.0753854
Iteration :  30   Loss :  50761.1204448
Iteration :  31   Loss :  44396.1705083
Iteration :  32   Loss :  40822.3387649
Iteration :  33   Loss :  58220.0249292
Iteration :  34   Loss :  58121.8918172
Iteration :  35   Loss :  41010.4174402
Iteration :  36   Loss :  29326.5088279
Iteration :  37   Loss :  59581.0340681
Iteration :  38   Loss :  29131.0612311
Iteration :  39   Loss :  38713.8660068
Iteration :  40   Loss :  51170.5320208
Iteration :  41   Loss :  7427.33433201
Iteration :  42   Loss :  34103.2204045
Iteration :  43   Loss :  22419.5697346
Iteration :  44   Loss :  66170.5045636
Iteration :  45   Loss :  48522.9291064
Iteration :  46   Loss :  32610.3702516
Iteration :  47   Loss :  43744.6553346
Iteration :  48   Loss :  61516.7387114
Iteration :  49   Loss :  42149.8292621
Iteration :  50   Loss :  51604.9397354
Iteration :  51   Loss :  48788.8033139
Iteration :  52   Loss :  56711.7050134
Iteration :  53   Loss :  21407.3729131
Iteration :  54   Loss :  34878.3219426
Iteration :  55   Loss :  46847.2001598
Iteration :  56   Loss :  49425.4795589
Iteration :  57   Loss :  46115.8667933
Iteration :  58   Loss :  29665.2819716
Iteration :  59   Loss :  61287.3323737
Iteration :  60   Loss :  11414.8778981
Iteration :  61   Loss :  72470.7993706
Iteration :  62   Loss :  9781.41246552
Iteration :  63   Loss :  55787.4897139
Iteration :  64   Loss :  29835.0171696
Iteration :  65   Loss :  38839.4612073
Iteration :  66   Loss :  43720.5779101
Iteration :  67   Loss :  43885.4302685
Iteration :  68   Loss :  48937.5726267
Iteration :  69   Loss :  72032.4591959
Iteration :  70   Loss :  31350.9925369
Iteration :  71   Loss :  70752.5504304
Iteration :  72   Loss :  32074.6860669
Iteration :  73   Loss :  76687.5427285
Iteration :  74   Loss :  24422.3154826
Iteration :  75   Loss :  103763.655156
Iteration :  76   Loss :  40999.4565838
Iteration :  77   Loss :  38777.5818448
Iteration :  78   Loss :  46882.6833553
Iteration :  79   Loss :  25391.5879824
Iteration :  80   Loss :  72704.622964
Iteration :  81   Loss :  52895.2136076
Iteration :  82   Loss :  49295.7537736
Iteration :  83   Loss :  42390.8829985
Iteration :  84   Loss :  67053.2478179
Iteration :  85   Loss :  49937.2736397
Iteration :  86   Loss :  49061.5623421
Iteration :  87   Loss :  29140.3135666
Iteration :  88   Loss :  54306.3591372
Iteration :  89   Loss :  60207.0145
Iteration :  90   Loss :  47483.6622666
Iteration :  91   Loss :  57271.450387
Iteration :  92   Loss :  42351.723082
Iteration :  93   Loss :  54983.6078367
Iteration :  94   Loss :  40382.7540952
Iteration :  95   Loss :  49463.9506817
Iteration :  96   Loss :  36119.3322853
Iteration :  97   Loss :  52389.3973697
Iteration :  98   Loss :  30750.4595313
Iteration :  99   Loss :  27993.9844858
[-0.11823132 -0.09941029 -0.25592094 ...,  0.56202942 -0.01889262
  0.08557381]
CROSS VALIDATION 16
Iteration :  0   Loss :  49238.8725453
Iteration :  1   Loss :  24429.9156516
Iteration :  2   Loss :  26015.1254039
Iteration :  3   Loss :  41910.6748916
Iteration :  4   Loss :  14841.8867619
Iteration :  5   Loss :  51194.9878481
Iteration :  6   Loss :  25704.5785416
Iteration :  7   Loss :  79783.3983579
Iteration :  8   Loss :  33880.0972983
Iteration :  9   Loss :  34883.7323436
Iteration :  10   Loss :  48397.6446721
Iteration :  11   Loss :  40489.4215852
Iteration :  12   Loss :  67706.239053
Iteration :  13   Loss :  36554.4831374
Iteration :  14   Loss :  52323.5551624
Iteration :  15   Loss :  38003.7166975
Iteration :  16   Loss :  68173.6652542
Iteration :  17   Loss :  48212.5914194
Iteration :  18   Loss :  40407.5969537
Iteration :  19   Loss :  67937.7938132
Iteration :  20   Loss :  36611.3358793
Iteration :  21   Loss :  52393.9136272
Iteration :  22   Loss :  38005.3792541
Iteration :  23   Loss :  68172.8514165
Iteration :  24   Loss :  48212.592961
Iteration :  25   Loss :  40407.5969709
Iteration :  26   Loss :  67937.7938137
Iteration :  27   Loss :  36611.3350477
Iteration :  28   Loss :  52393.9133561
Iteration :  29   Loss :  38005.3792476
Iteration :  30   Loss :  68172.8514197
Iteration :  31   Loss :  48212.592961
Iteration :  32   Loss :  40407.5969709
Iteration :  33   Loss :  67937.7938137
Iteration :  34   Loss :  36611.3350477
Iteration :  35   Loss :  52393.9133561
Iteration :  36   Loss :  38005.3792476
Iteration :  37   Loss :  68172.8514197
Iteration :  38   Loss :  48212.592961
Iteration :  39   Loss :  40407.5969709
Iteration :  40   Loss :  67937.7938137
Iteration :  41   Loss :  36611.3350477
Iteration :  42   Loss :  52393.9133561
Iteration :  43   Loss :  38005.3792476
Iteration :  44   Loss :  68172.8514197
Iteration :  45   Loss :  48212.592961
Iteration :  46   Loss :  40407.5969709
Iteration :  47   Loss :  67937.7938137
Iteration :  48   Loss :  36611.3350477
Iteration :  49   Loss :  52393.9133561
Iteration :  50   Loss :  38005.3792476
Iteration :  51   Loss :  68172.8514197
Iteration :  52   Loss :  48212.592961
Iteration :  53   Loss :  40407.5969709
Iteration :  54   Loss :  67937.7938137
Iteration :  55   Loss :  36611.3350477
Iteration :  56   Loss :  52393.9133561
Iteration :  57   Loss :  38005.3792476
Iteration :  58   Loss :  68172.8514197
Iteration :  59   Loss :  48212.592961
Iteration :  60   Loss :  40407.5969709
Iteration :  61   Loss :  67937.7938137
Iteration :  62   Loss :  36611.3350477
Iteration :  63   Loss :  52393.9133561
Iteration :  64   Loss :  38005.3792476
Iteration :  65   Loss :  68172.8514197
Iteration :  66   Loss :  48212.592961
Iteration :  67   Loss :  40407.5969709
Iteration :  68   Loss :  67937.7938137
Iteration :  69   Loss :  36611.3350477
Iteration :  70   Loss :  52393.9133561
Iteration :  71   Loss :  38005.3792476
Iteration :  72   Loss :  68172.8514197
Iteration :  73   Loss :  48212.592961
Iteration :  74   Loss :  40407.5969709
Iteration :  75   Loss :  67937.7938137
Iteration :  76   Loss :  36611.3350477
Iteration :  77   Loss :  52393.9133561
Iteration :  78   Loss :  38005.3792476
Iteration :  79   Loss :  68172.8514197
Iteration :  80   Loss :  48212.592961
Iteration :  81   Loss :  40407.5969709
Iteration :  82   Loss :  67937.7938137
Iteration :  83   Loss :  36611.3350477
Iteration :  84   Loss :  52393.9133561
Iteration :  85   Loss :  38005.3792476
Iteration :  86   Loss :  68172.8514197
Iteration :  87   Loss :  48212.592961
Iteration :  88   Loss :  40407.5969709
Iteration :  89   Loss :  67937.7938137
Iteration :  90   Loss :  36611.3350477
Iteration :  91   Loss :  52393.9133561
Iteration :  92   Loss :  38005.3792476
Iteration :  93   Loss :  68172.8514197
Iteration :  94   Loss :  48212.592961
Iteration :  95   Loss :  40407.5969709
Iteration :  96   Loss :  67937.7938137
Iteration :  97   Loss :  36611.3350477
Iteration :  98   Loss :  52393.9133561
Iteration :  99   Loss :  38005.3792476
[-0.17495669 -0.81845981 -0.2226258  ...,  0.31371283 -0.38833625
 -0.0542279 ]
CROSS VALIDATION 17
Iteration :  0   Loss :  48282.5884314
Iteration :  1   Loss :  29903.0645901
Iteration :  2   Loss :  40456.684567
Iteration :  3   Loss :  43755.4802685
Iteration :  4   Loss :  43885.2035636
Iteration :  5   Loss :  48937.6458358
Iteration :  6   Loss :  72032.4642315
Iteration :  7   Loss :  48285.300915
Iteration :  8   Loss :  42884.1789417
Iteration :  9   Loss :  52071.1643507
Iteration :  10   Loss :  2802.23606073
Iteration :  11   Loss :  44907.1566909
Iteration :  12   Loss :  28444.9029259
Iteration :  13   Loss :  39832.5612769
Iteration :  14   Loss :  69157.0286682
Iteration :  15   Loss :  25279.7594092
Iteration :  16   Loss :  57715.4810296
Iteration :  17   Loss :  46202.2484559
Iteration :  18   Loss :  49551.2302455
Iteration :  19   Loss :  43385.7377135
Iteration :  20   Loss :  62232.3436697
Iteration :  21   Loss :  41021.4447883
Iteration :  22   Loss :  62403.6234311
Iteration :  23   Loss :  41019.9869621
Iteration :  24   Loss :  62403.866965
Iteration :  25   Loss :  41019.9848965
Iteration :  26   Loss :  62403.8673102
Iteration :  27   Loss :  41019.9848936
Iteration :  28   Loss :  62403.8673107
Iteration :  29   Loss :  41019.9848936
Iteration :  30   Loss :  62403.8673107
Iteration :  31   Loss :  41019.9848936
Iteration :  32   Loss :  62403.8673107
Iteration :  33   Loss :  41019.9848936
Iteration :  34   Loss :  62403.8673107
Iteration :  35   Loss :  41019.9848936
Iteration :  36   Loss :  62403.8673107
Iteration :  37   Loss :  41019.9848936
Iteration :  38   Loss :  62403.8673107
Iteration :  39   Loss :  41019.9848936
Iteration :  40   Loss :  62403.8673107
Iteration :  41   Loss :  41019.9848936
Iteration :  42   Loss :  62403.8673107
Iteration :  43   Loss :  41019.9848936
Iteration :  44   Loss :  62403.8673107
Iteration :  45   Loss :  41019.9848936
Iteration :  46   Loss :  62403.8673107
Iteration :  47   Loss :  41019.9848936
Iteration :  48   Loss :  62403.8673107
Iteration :  49   Loss :  41019.9848936
Iteration :  50   Loss :  62403.8673107
Iteration :  51   Loss :  41019.9848936
Iteration :  52   Loss :  62403.8673107
Iteration :  53   Loss :  41019.9848936
Iteration :  54   Loss :  62403.8673107
Iteration :  55   Loss :  41019.9848936
Iteration :  56   Loss :  62403.8673107
Iteration :  57   Loss :  41019.9848936
Iteration :  58   Loss :  62403.8673107
Iteration :  59   Loss :  41019.9848936
Iteration :  60   Loss :  62403.8673107
Iteration :  61   Loss :  41019.9848936
Iteration :  62   Loss :  62403.8673107
Iteration :  63   Loss :  41019.9848936
Iteration :  64   Loss :  62403.8673107
Iteration :  65   Loss :  41019.9848936
Iteration :  66   Loss :  62403.8673107
Iteration :  67   Loss :  41019.9848936
Iteration :  68   Loss :  62403.8673107
Iteration :  69   Loss :  41019.9848936
Iteration :  70   Loss :  62403.8673107
Iteration :  71   Loss :  41019.9848936
Iteration :  72   Loss :  62403.8673107
Iteration :  73   Loss :  41019.9848936
Iteration :  74   Loss :  62403.8673107
Iteration :  75   Loss :  41019.9848936
Iteration :  76   Loss :  62403.8673107
Iteration :  77   Loss :  41019.9848936
Iteration :  78   Loss :  62403.8673107
Iteration :  79   Loss :  41019.9848936
Iteration :  80   Loss :  62403.8673107
Iteration :  81   Loss :  41019.9848936
Iteration :  82   Loss :  62403.8673107
Iteration :  83   Loss :  41019.9848936
Iteration :  84   Loss :  62403.8673107
Iteration :  85   Loss :  41019.9848936
Iteration :  86   Loss :  62403.8673107
Iteration :  87   Loss :  41019.9848936
Iteration :  88   Loss :  62403.8673107
Iteration :  89   Loss :  41019.9848936
Iteration :  90   Loss :  62403.8673107
Iteration :  91   Loss :  41019.9848936
Iteration :  92   Loss :  62403.8673107
Iteration :  93   Loss :  41019.9848936
Iteration :  94   Loss :  62403.8673107
Iteration :  95   Loss :  41019.9848936
Iteration :  96   Loss :  62403.8673107
Iteration :  97   Loss :  41019.9848936
Iteration :  98   Loss :  62403.8673107
Iteration :  99   Loss :  41019.9848936
[-0.17813607 -0.8293133  -0.23425012 ...,  0.37142883 -0.50630114
 -0.02782964]
CROSS VALIDATION 18
Iteration :  0   Loss :  48637.5683463
Iteration :  1   Loss :  26982.2857046
Iteration :  2   Loss :  41538.7943667
Iteration :  3   Loss :  42882.5393068
Iteration :  4   Loss :  39197.7638089
Iteration :  5   Loss :  57945.1600727
Iteration :  6   Loss :  41906.0519166
Iteration :  7   Loss :  29246.205773
Iteration :  8   Loss :  42027.2861177
Iteration :  9   Loss :  26288.7560361
Iteration :  10   Loss :  41796.9306068
Iteration :  11   Loss :  26969.3121463
Iteration :  12   Loss :  43937.0033494
Iteration :  13   Loss :  44260.6637461
Iteration :  14   Loss :  23699.4665114
Iteration :  15   Loss :  33529.8411611
Iteration :  16   Loss :  73692.079121
Iteration :  17   Loss :  36797.7265979
Iteration :  18   Loss :  49598.0202199
Iteration :  19   Loss :  63364.2447413
Iteration :  20   Loss :  46855.3416739
Iteration :  21   Loss :  42988.4953702
Iteration :  22   Loss :  23719.381872
Iteration :  23   Loss :  33528.2696128
Iteration :  24   Loss :  73692.0728488
Iteration :  25   Loss :  36797.7261373
Iteration :  26   Loss :  49598.0202436
Iteration :  27   Loss :  63364.2447393
Iteration :  28   Loss :  46855.341674
Iteration :  29   Loss :  42988.4953702
Iteration :  30   Loss :  23719.381872
Iteration :  31   Loss :  33528.2696128
Iteration :  32   Loss :  73692.0728488
Iteration :  33   Loss :  36797.7261373
Iteration :  34   Loss :  49598.0202436
Iteration :  35   Loss :  63364.2447393
Iteration :  36   Loss :  46855.341674
Iteration :  37   Loss :  42988.4953702
Iteration :  38   Loss :  23719.381872
Iteration :  39   Loss :  33528.2696128
Iteration :  40   Loss :  73692.0728488
Iteration :  41   Loss :  36797.7261373
Iteration :  42   Loss :  49598.0202436
Iteration :  43   Loss :  63364.2447393
Iteration :  44   Loss :  46855.341674
Iteration :  45   Loss :  42988.4953702
Iteration :  46   Loss :  23719.381872
Iteration :  47   Loss :  33528.2696128
Iteration :  48   Loss :  73692.0728488
Iteration :  49   Loss :  36797.7261373
Iteration :  50   Loss :  49598.0202436
Iteration :  51   Loss :  63364.2447393
Iteration :  52   Loss :  46855.341674
Iteration :  53   Loss :  42988.4953702
Iteration :  54   Loss :  23719.381872
Iteration :  55   Loss :  33528.2696128
Iteration :  56   Loss :  73692.0728488
Iteration :  57   Loss :  36797.7261373
Iteration :  58   Loss :  49598.0202436
Iteration :  59   Loss :  63364.2447393
Iteration :  60   Loss :  46855.341674
Iteration :  61   Loss :  42988.4953702
Iteration :  62   Loss :  23719.381872
Iteration :  63   Loss :  33528.2696128
Iteration :  64   Loss :  73692.0728488
Iteration :  65   Loss :  36797.7261373
Iteration :  66   Loss :  49598.0202436
Iteration :  67   Loss :  63364.2447393
Iteration :  68   Loss :  46855.341674
Iteration :  69   Loss :  42988.4953702
Iteration :  70   Loss :  23719.381872
Iteration :  71   Loss :  33528.2696128
Iteration :  72   Loss :  73692.0728488
Iteration :  73   Loss :  36797.7261373
Iteration :  74   Loss :  49598.0202436
Iteration :  75   Loss :  63364.2447393
Iteration :  76   Loss :  46855.341674
Iteration :  77   Loss :  42988.4953702
Iteration :  78   Loss :  23719.381872
Iteration :  79   Loss :  33528.2696128
Iteration :  80   Loss :  73692.0728488
Iteration :  81   Loss :  36797.7261373
Iteration :  82   Loss :  49598.0202436
Iteration :  83   Loss :  63364.2447393
Iteration :  84   Loss :  46855.341674
Iteration :  85   Loss :  42988.4953702
Iteration :  86   Loss :  23719.381872
Iteration :  87   Loss :  33528.2696128
Iteration :  88   Loss :  73692.0728488
Iteration :  89   Loss :  36797.7261373
Iteration :  90   Loss :  49598.0202436
Iteration :  91   Loss :  63364.2447393
Iteration :  92   Loss :  46855.341674
Iteration :  93   Loss :  42988.4953702
Iteration :  94   Loss :  23719.381872
Iteration :  95   Loss :  33528.2696128
Iteration :  96   Loss :  73692.0728488
Iteration :  97   Loss :  36797.7261373
Iteration :  98   Loss :  49598.0202436
Iteration :  99   Loss :  63364.2447393
[-0.34512707 -0.47977547  0.0970114  ...,  0.48787968  0.14780533
  0.00176231]
CROSS VALIDATION 19
Iteration :  0   Loss :  49116.003729
Iteration :  1   Loss :  26984.3445668
Iteration :  2   Loss :  51409.8331876
Iteration :  3   Loss :  14624.3249467
Iteration :  4   Loss :  29729.1351339
Iteration :  5   Loss :  43977.2256082
Iteration :  6   Loss :  43113.0458797
Iteration :  7   Loss :  28539.5562756
Iteration :  8   Loss :  46386.9303341
Iteration :  9   Loss :  18582.5714765
Iteration :  10   Loss :  56907.3792316
Iteration :  11   Loss :  42525.3851886
Iteration :  12   Loss :  60434.6814874
Iteration :  13   Loss :  17176.5729297
Iteration :  14   Loss :  47426.4242414
Iteration :  15   Loss :  36261.4351598
Iteration :  16   Loss :  55360.6844619
Iteration :  17   Loss :  15055.5185427
Iteration :  18   Loss :  29729.7403506
Iteration :  19   Loss :  43978.1200594
Iteration :  20   Loss :  43113.0572929
Iteration :  21   Loss :  28539.5565076
Iteration :  22   Loss :  46386.9303524
Iteration :  23   Loss :  18582.5714781
Iteration :  24   Loss :  56907.3792316
Iteration :  25   Loss :  42525.3851886
Iteration :  26   Loss :  60434.6814874
Iteration :  27   Loss :  17176.5729297
Iteration :  28   Loss :  47426.4242414
Iteration :  29   Loss :  36261.4351598
Iteration :  30   Loss :  55360.6844619
Iteration :  31   Loss :  15055.5185427
Iteration :  32   Loss :  29729.7403506
Iteration :  33   Loss :  43978.1200594
Iteration :  34   Loss :  43113.0572929
Iteration :  35   Loss :  28539.5565076
Iteration :  36   Loss :  46386.9303524
Iteration :  37   Loss :  18582.5714781
Iteration :  38   Loss :  56907.3792316
Iteration :  39   Loss :  42525.3851886
Iteration :  40   Loss :  60434.6814874
Iteration :  41   Loss :  17176.5729297
Iteration :  42   Loss :  47426.4242414
Iteration :  43   Loss :  36261.4351598
Iteration :  44   Loss :  55360.6844619
Iteration :  45   Loss :  15055.5185427
Iteration :  46   Loss :  29729.7403506
Iteration :  47   Loss :  43978.1200594
Iteration :  48   Loss :  43113.0572929
Iteration :  49   Loss :  28539.5565076
Iteration :  50   Loss :  46386.9303524
Iteration :  51   Loss :  18582.5714781
Iteration :  52   Loss :  56907.3792316
Iteration :  53   Loss :  42525.3851886
Iteration :  54   Loss :  60434.6814874
Iteration :  55   Loss :  17176.5729297
Iteration :  56   Loss :  47426.4242414
Iteration :  57   Loss :  36261.4351598
Iteration :  58   Loss :  55360.6844619
Iteration :  59   Loss :  15055.5185427
Iteration :  60   Loss :  29729.7403506
Iteration :  61   Loss :  43978.1200594
Iteration :  62   Loss :  43113.0572929
Iteration :  63   Loss :  28539.5565076
Iteration :  64   Loss :  46386.9303524
Iteration :  65   Loss :  18582.5714781
Iteration :  66   Loss :  56907.3792316
Iteration :  67   Loss :  42525.3851886
Iteration :  68   Loss :  60434.6814874
Iteration :  69   Loss :  17176.5729297
Iteration :  70   Loss :  47426.4242414
Iteration :  71   Loss :  36261.4351598
Iteration :  72   Loss :  55360.6844619
Iteration :  73   Loss :  15055.5185427
Iteration :  74   Loss :  29729.7403506
Iteration :  75   Loss :  43978.1200594
Iteration :  76   Loss :  43113.0572929
Iteration :  77   Loss :  28539.5565076
Iteration :  78   Loss :  46386.9303524
Iteration :  79   Loss :  18582.5714781
Iteration :  80   Loss :  56907.3792316
Iteration :  81   Loss :  42525.3851886
Iteration :  82   Loss :  60434.6814874
Iteration :  83   Loss :  17176.5729297
Iteration :  84   Loss :  47426.4242414
Iteration :  85   Loss :  36261.4351598
Iteration :  86   Loss :  55360.6844619
Iteration :  87   Loss :  15055.5185427
Iteration :  88   Loss :  29729.7403506
Iteration :  89   Loss :  43978.1200594
Iteration :  90   Loss :  43113.0572929
Iteration :  91   Loss :  28539.5565076
Iteration :  92   Loss :  46386.9303524
Iteration :  93   Loss :  18582.5714781
Iteration :  94   Loss :  56907.3792316
Iteration :  95   Loss :  42525.3851886
Iteration :  96   Loss :  60434.6814874
Iteration :  97   Loss :  17176.5729297
Iteration :  98   Loss :  47426.4242414
Iteration :  99   Loss :  36261.4351598
[-0.2613111   0.12088805  0.22271977 ...,  0.17099992  0.23666002
  0.09819163]
Accuracy (Logistic Loss):	0.75
Hinge Loss
CROSS VALIDATION 0
Iteration :  0   Loss :  49484.7747371
Iteration :  1   Loss :  46103.2258926
Iteration :  2   Loss :  29740.2074812
Iteration :  3   Loss :  62022.561269
Iteration :  4   Loss :  11504.3537463
Iteration :  5   Loss :  72352.8041938
Iteration :  6   Loss :  9759.91119328
Iteration :  7   Loss :  51643.5772768
Iteration :  8   Loss :  20064.0142007
Iteration :  9   Loss :  41224.3407854
Iteration :  10   Loss :  30947.1839865
Iteration :  11   Loss :  27671.3962509
Iteration :  12   Loss :  49832.0267285
Iteration :  13   Loss :  16191.8779966
Iteration :  14   Loss :  45142.6613009
Iteration :  15   Loss :  92454.4327421
Iteration :  16   Loss :  43058.9226869
Iteration :  17   Loss :  33939.9911996
Iteration :  18   Loss :  50962.8671038
Iteration :  19   Loss :  17140.9495381
Iteration :  20   Loss :  48063.2125139
Iteration :  21   Loss :  54524.7241627
Iteration :  22   Loss :  43748.3675393
Iteration :  23   Loss :  26708.3300632
Iteration :  24   Loss :  62584.8772594
Iteration :  25   Loss :  32794.5685223
Iteration :  26   Loss :  46176.031546
Iteration :  27   Loss :  40983.0857717
Iteration :  28   Loss :  31990.2076895
Iteration :  29   Loss :  45291.380225
Iteration :  30   Loss :  28101.9503786
Iteration :  31   Loss :  33304.2370902
Iteration :  32   Loss :  62901.224282
Iteration :  33   Loss :  30327.6980561
Iteration :  34   Loss :  58449.5722058
Iteration :  35   Loss :  43984.068679
Iteration :  36   Loss :  61399.6423797
Iteration :  37   Loss :  58289.5316757
Iteration :  38   Loss :  64860.6380993
Iteration :  39   Loss :  40470.1903462
Iteration :  40   Loss :  65253.3528045
Iteration :  41   Loss :  59765.1118597
Iteration :  42   Loss :  31860.2607733
Iteration :  43   Loss :  28821.0833595
Iteration :  44   Loss :  35662.6552388
Iteration :  45   Loss :  52236.7227711
Iteration :  46   Loss :  51318.7649085
Iteration :  47   Loss :  34209.2426227
Iteration :  48   Loss :  78475.3986064
Iteration :  49   Loss :  43663.5875846
Iteration :  50   Loss :  31113.6728188
Iteration :  51   Loss :  59819.7918713
Iteration :  52   Loss :  32938.5748002
Iteration :  53   Loss :  55196.0354276
Iteration :  54   Loss :  31838.9555437
Iteration :  55   Loss :  40854.3500639
Iteration :  56   Loss :  22393.9384465
Iteration :  57   Loss :  71557.8812636
Iteration :  58   Loss :  49020.6842531
Iteration :  59   Loss :  40617.3261966
Iteration :  60   Loss :  72602.1379689
Iteration :  61   Loss :  47198.4470353
Iteration :  62   Loss :  29841.6755203
Iteration :  63   Loss :  42547.7413617
Iteration :  64   Loss :  50630.7712231
Iteration :  65   Loss :  23188.76951
Iteration :  66   Loss :  54078.6368953
Iteration :  67   Loss :  23475.6811815
Iteration :  68   Loss :  46856.6392419
Iteration :  69   Loss :  28920.6458162
Iteration :  70   Loss :  37840.9589828
Iteration :  71   Loss :  64305.5616579
Iteration :  72   Loss :  33866.982659
Iteration :  73   Loss :  35347.0606157
Iteration :  74   Loss :  29126.9002473
Iteration :  75   Loss :  57351.8136517
Iteration :  76   Loss :  48834.3062977
Iteration :  77   Loss :  71666.5043123
Iteration :  78   Loss :  55042.0185939
Iteration :  79   Loss :  34412.7978411
Iteration :  80   Loss :  50044.0001088
Iteration :  81   Loss :  62786.4261737
Iteration :  82   Loss :  61487.5241142
Iteration :  83   Loss :  20535.434599
Iteration :  84   Loss :  46324.9136989
Iteration :  85   Loss :  53101.3322735
Iteration :  86   Loss :  35877.3329438
Iteration :  87   Loss :  54622.358977
Iteration :  88   Loss :  35454.5031744
Iteration :  89   Loss :  54624.9743175
Iteration :  90   Loss :  35453.9044235
Iteration :  91   Loss :  54624.978026
Iteration :  92   Loss :  35453.9035747
Iteration :  93   Loss :  54624.9780312
Iteration :  94   Loss :  35453.9035735
Iteration :  95   Loss :  54624.9780312
Iteration :  96   Loss :  35453.9035735
Iteration :  97   Loss :  54624.9780312
Iteration :  98   Loss :  35453.9035735
Iteration :  99   Loss :  54624.9780312
[-0.30502843 -0.77501482 -0.25613441 ...,  0.43836455 -0.46571712
 -0.04239405]
CROSS VALIDATION 1
Iteration :  0   Loss :  42318.6062483
Iteration :  1   Loss :  115711.393778
Iteration :  2   Loss :  33334.729436
Iteration :  3   Loss :  49766.0424591
Iteration :  4   Loss :  39666.218146
Iteration :  5   Loss :  38747.2957502
Iteration :  6   Loss :  73418.0417982
Iteration :  7   Loss :  30683.9308459
Iteration :  8   Loss :  73969.655153
Iteration :  9   Loss :  27010.8068388
Iteration :  10   Loss :  55023.9583421
Iteration :  11   Loss :  54498.1519206
Iteration :  12   Loss :  45473.7170999
Iteration :  13   Loss :  15808.1377803
Iteration :  14   Loss :  28354.3940595
Iteration :  15   Loss :  41778.2647595
Iteration :  16   Loss :  59000.7639088
Iteration :  17   Loss :  36817.4420376
Iteration :  18   Loss :  55476.4892493
Iteration :  19   Loss :  53314.2497915
Iteration :  20   Loss :  29320.5511288
Iteration :  21   Loss :  53766.2949089
Iteration :  22   Loss :  43868.8373397
Iteration :  23   Loss :  46428.9042558
Iteration :  24   Loss :  35281.2013087
Iteration :  25   Loss :  23702.2666331
Iteration :  26   Loss :  55784.5405979
Iteration :  27   Loss :  35157.4946557
Iteration :  28   Loss :  51601.2932707
Iteration :  29   Loss :  30963.5175072
Iteration :  30   Loss :  43140.6497074
Iteration :  31   Loss :  52290.2644419
Iteration :  32   Loss :  72731.9367288
Iteration :  33   Loss :  46935.9894943
Iteration :  34   Loss :  31011.2988941
Iteration :  35   Loss :  49572.449249
Iteration :  36   Loss :  88609.9306777
Iteration :  37   Loss :  43689.2578207
Iteration :  38   Loss :  40268.1836648
Iteration :  39   Loss :  54149.8191609
Iteration :  40   Loss :  37702.4271066
Iteration :  41   Loss :  73482.1666268
Iteration :  42   Loss :  30684.8300308
Iteration :  43   Loss :  73969.7446605
Iteration :  44   Loss :  27010.807058
Iteration :  45   Loss :  55023.9584484
Iteration :  46   Loss :  54498.1519225
Iteration :  47   Loss :  45473.7171001
Iteration :  48   Loss :  15808.1377803
Iteration :  49   Loss :  28354.3940595
Iteration :  50   Loss :  41778.2647595
Iteration :  51   Loss :  59000.7639088
Iteration :  52   Loss :  36817.4420376
Iteration :  53   Loss :  55476.4892493
Iteration :  54   Loss :  53314.2497915
Iteration :  55   Loss :  29320.5511288
Iteration :  56   Loss :  53766.2949089
Iteration :  57   Loss :  43868.8373397
Iteration :  58   Loss :  46428.9042558
Iteration :  59   Loss :  35281.2013087
Iteration :  60   Loss :  23702.2666331
Iteration :  61   Loss :  55784.5405979
Iteration :  62   Loss :  35157.4946557
Iteration :  63   Loss :  51601.2932707
Iteration :  64   Loss :  30963.5175072
Iteration :  65   Loss :  43140.6497074
Iteration :  66   Loss :  52290.2644419
Iteration :  67   Loss :  72731.9367288
Iteration :  68   Loss :  46935.9894943
Iteration :  69   Loss :  31011.2988941
Iteration :  70   Loss :  49572.449249
Iteration :  71   Loss :  88609.9306777
Iteration :  72   Loss :  43689.2578207
Iteration :  73   Loss :  40268.1836648
Iteration :  74   Loss :  54149.8191609
Iteration :  75   Loss :  37702.4271066
Iteration :  76   Loss :  73482.1666268
Iteration :  77   Loss :  30684.8300308
Iteration :  78   Loss :  73969.7446605
Iteration :  79   Loss :  27010.807058
Iteration :  80   Loss :  55023.9584484
Iteration :  81   Loss :  54498.1519225
Iteration :  82   Loss :  45473.7171001
Iteration :  83   Loss :  15808.1377803
Iteration :  84   Loss :  28354.3940595
Iteration :  85   Loss :  41778.2647595
Iteration :  86   Loss :  59000.7639088
Iteration :  87   Loss :  36817.4420376
Iteration :  88   Loss :  55476.4892493
Iteration :  89   Loss :  53314.2497915
Iteration :  90   Loss :  29320.5511288
Iteration :  91   Loss :  53766.2949089
Iteration :  92   Loss :  43868.8373397
Iteration :  93   Loss :  46428.9042558
Iteration :  94   Loss :  35281.2013087
Iteration :  95   Loss :  23702.2666331
Iteration :  96   Loss :  55784.5405979
Iteration :  97   Loss :  35157.4946557
Iteration :  98   Loss :  51601.2932707
Iteration :  99   Loss :  30963.5175072
[-0.20951972 -0.60391497  0.3193692  ...,  0.3276184   0.10176325
  0.15952736]
CROSS VALIDATION 2
Iteration :  0   Loss :  27086.0077165
Iteration :  1   Loss :  44980.8471165
Iteration :  2   Loss :  36809.0113846
Iteration :  3   Loss :  38425.3787344
Iteration :  4   Loss :  18018.0718091
Iteration :  5   Loss :  48336.9572575
Iteration :  6   Loss :  53436.9231895
Iteration :  7   Loss :  42634.3105653
Iteration :  8   Loss :  41992.3017822
Iteration :  9   Loss :  65925.0357432
Iteration :  10   Loss :  33331.2848053
Iteration :  11   Loss :  58756.317958
Iteration :  12   Loss :  42658.784122
Iteration :  13   Loss :  43176.3034252
Iteration :  14   Loss :  44035.8778859
Iteration :  15   Loss :  25653.7304434
Iteration :  16   Loss :  37675.4800247
Iteration :  17   Loss :  44250.4722803
Iteration :  18   Loss :  38719.084652
Iteration :  19   Loss :  8547.67288057
Iteration :  20   Loss :  66326.5633415
Iteration :  21   Loss :  28079.6796042
Iteration :  22   Loss :  24773.6469852
Iteration :  23   Loss :  50437.0324995
Iteration :  24   Loss :  48744.1962845
Iteration :  25   Loss :  45610.6606773
Iteration :  26   Loss :  76559.6427479
Iteration :  27   Loss :  46499.1900039
Iteration :  28   Loss :  69979.1559486
Iteration :  29   Loss :  28313.2069881
Iteration :  30   Loss :  66930.4315378
Iteration :  31   Loss :  51849.705445
Iteration :  32   Loss :  68115.6412142
Iteration :  33   Loss :  31960.0349261
Iteration :  34   Loss :  40768.3007116
Iteration :  35   Loss :  22038.4721169
Iteration :  36   Loss :  41593.1509152
Iteration :  37   Loss :  31026.7708862
Iteration :  38   Loss :  27538.5733414
Iteration :  39   Loss :  44493.2648256
Iteration :  40   Loss :  56332.9279736
Iteration :  41   Loss :  52092.2619825
Iteration :  42   Loss :  38195.4598062
Iteration :  43   Loss :  59729.580221
Iteration :  44   Loss :  57553.4878531
Iteration :  45   Loss :  63066.6677947
Iteration :  46   Loss :  37495.774729
Iteration :  47   Loss :  34050.7001667
Iteration :  48   Loss :  56401.7030101
Iteration :  49   Loss :  9254.1427064
Iteration :  50   Loss :  51556.5777888
Iteration :  51   Loss :  20062.2009117
Iteration :  52   Loss :  41223.5287114
Iteration :  53   Loss :  31028.8149451
Iteration :  54   Loss :  27538.3108837
Iteration :  55   Loss :  44493.247563
Iteration :  56   Loss :  56332.9280593
Iteration :  57   Loss :  52092.2619554
Iteration :  58   Loss :  38195.4598068
Iteration :  59   Loss :  59729.5802209
Iteration :  60   Loss :  57553.4878531
Iteration :  61   Loss :  63066.6677947
Iteration :  62   Loss :  37495.774729
Iteration :  63   Loss :  34050.7001667
Iteration :  64   Loss :  56401.7030101
Iteration :  65   Loss :  9254.1427064
Iteration :  66   Loss :  51556.5777888
Iteration :  67   Loss :  20062.2009117
Iteration :  68   Loss :  41223.5287114
Iteration :  69   Loss :  31028.8149451
Iteration :  70   Loss :  27538.3108837
Iteration :  71   Loss :  44493.247563
Iteration :  72   Loss :  56332.9280593
Iteration :  73   Loss :  52092.2619554
Iteration :  74   Loss :  38195.4598068
Iteration :  75   Loss :  59729.5802209
Iteration :  76   Loss :  57553.4878531
Iteration :  77   Loss :  63066.6677947
Iteration :  78   Loss :  37495.774729
Iteration :  79   Loss :  34050.7001667
Iteration :  80   Loss :  56401.7030101
Iteration :  81   Loss :  9254.1427064
Iteration :  82   Loss :  51556.5777888
Iteration :  83   Loss :  20062.2009117
Iteration :  84   Loss :  41223.5287114
Iteration :  85   Loss :  31028.8149451
Iteration :  86   Loss :  27538.3108837
Iteration :  87   Loss :  44493.247563
Iteration :  88   Loss :  56332.9280593
Iteration :  89   Loss :  52092.2619554
Iteration :  90   Loss :  38195.4598068
Iteration :  91   Loss :  59729.5802209
Iteration :  92   Loss :  57553.4878531
Iteration :  93   Loss :  63066.6677947
Iteration :  94   Loss :  37495.774729
Iteration :  95   Loss :  34050.7001667
Iteration :  96   Loss :  56401.7030101
Iteration :  97   Loss :  9254.1427064
Iteration :  98   Loss :  51556.5777888
Iteration :  99   Loss :  20062.2009117
[ 0.04211974 -0.57946292  0.17553664 ...,  0.2533735  -0.31148394
  0.00386846]
CROSS VALIDATION 3
Iteration :  0   Loss :  42258.8719488
Iteration :  1   Loss :  109326.748741
Iteration :  2   Loss :  33334.8541896
Iteration :  3   Loss :  49766.0449749
Iteration :  4   Loss :  39666.218289
Iteration :  5   Loss :  44109.9547876
Iteration :  6   Loss :  60491.3367199
Iteration :  7   Loss :  22401.2723629
Iteration :  8   Loss :  46594.5256239
Iteration :  9   Loss :  45174.1590661
Iteration :  10   Loss :  47676.3603267
Iteration :  11   Loss :  62488.3439072
Iteration :  12   Loss :  36848.0060236
Iteration :  13   Loss :  38222.8665949
Iteration :  14   Loss :  34636.3370929
Iteration :  15   Loss :  53559.2321142
Iteration :  16   Loss :  50821.3856269
Iteration :  17   Loss :  36373.2594023
Iteration :  18   Loss :  28925.9042415
Iteration :  19   Loss :  48660.7133566
Iteration :  20   Loss :  45813.7002116
Iteration :  21   Loss :  29765.9852734
Iteration :  22   Loss :  62021.2905401
Iteration :  23   Loss :  11503.6165571
Iteration :  24   Loss :  51938.3885439
Iteration :  25   Loss :  36906.7922408
Iteration :  26   Loss :  39563.6510339
Iteration :  27   Loss :  41482.6118184
Iteration :  28   Loss :  36714.1534858
Iteration :  29   Loss :  60520.3713228
Iteration :  30   Loss :  40833.4288152
Iteration :  31   Loss :  40474.7080185
Iteration :  32   Loss :  45106.099851
Iteration :  33   Loss :  35764.2008582
Iteration :  34   Loss :  54521.6643001
Iteration :  35   Loss :  35438.2907986
Iteration :  36   Loss :  54563.2375164
Iteration :  37   Loss :  35437.830583
Iteration :  38   Loss :  54563.296453
Iteration :  39   Loss :  35437.8299306
Iteration :  40   Loss :  54563.2965365
Iteration :  41   Loss :  35437.8299297
Iteration :  42   Loss :  54563.2965367
Iteration :  43   Loss :  35437.8299297
Iteration :  44   Loss :  54563.2965367
Iteration :  45   Loss :  35437.8299297
Iteration :  46   Loss :  54563.2965367
Iteration :  47   Loss :  35437.8299297
Iteration :  48   Loss :  54563.2965367
Iteration :  49   Loss :  35437.8299297
Iteration :  50   Loss :  54563.2965367
Iteration :  51   Loss :  35437.8299297
Iteration :  52   Loss :  54563.2965367
Iteration :  53   Loss :  35437.8299297
Iteration :  54   Loss :  54563.2965367
Iteration :  55   Loss :  35437.8299297
Iteration :  56   Loss :  54563.2965367
Iteration :  57   Loss :  35437.8299297
Iteration :  58   Loss :  54563.2965367
Iteration :  59   Loss :  35437.8299297
Iteration :  60   Loss :  54563.2965367
Iteration :  61   Loss :  35437.8299297
Iteration :  62   Loss :  54563.2965367
Iteration :  63   Loss :  35437.8299297
Iteration :  64   Loss :  54563.2965367
Iteration :  65   Loss :  35437.8299297
Iteration :  66   Loss :  54563.2965367
Iteration :  67   Loss :  35437.8299297
Iteration :  68   Loss :  54563.2965367
Iteration :  69   Loss :  35437.8299297
Iteration :  70   Loss :  54563.2965367
Iteration :  71   Loss :  35437.8299297
Iteration :  72   Loss :  54563.2965367
Iteration :  73   Loss :  35437.8299297
Iteration :  74   Loss :  54563.2965367
Iteration :  75   Loss :  35437.8299297
Iteration :  76   Loss :  54563.2965367
Iteration :  77   Loss :  35437.8299297
Iteration :  78   Loss :  54563.2965367
Iteration :  79   Loss :  35437.8299297
Iteration :  80   Loss :  54563.2965367
Iteration :  81   Loss :  35437.8299297
Iteration :  82   Loss :  54563.2965367
Iteration :  83   Loss :  35437.8299297
Iteration :  84   Loss :  54563.2965367
Iteration :  85   Loss :  35437.8299297
Iteration :  86   Loss :  54563.2965367
Iteration :  87   Loss :  35437.8299297
Iteration :  88   Loss :  54563.2965367
Iteration :  89   Loss :  35437.8299297
Iteration :  90   Loss :  54563.2965367
Iteration :  91   Loss :  35437.8299297
Iteration :  92   Loss :  54563.2965367
Iteration :  93   Loss :  35437.8299297
Iteration :  94   Loss :  54563.2965367
Iteration :  95   Loss :  35437.8299297
Iteration :  96   Loss :  54563.2965367
Iteration :  97   Loss :  35437.8299297
Iteration :  98   Loss :  54563.2965367
Iteration :  99   Loss :  35437.8299297
[-0.51115432 -0.00128012 -0.2163776  ...,  0.29714714 -0.17351389
  0.05701843]
CROSS VALIDATION 4
Iteration :  0   Loss :  42258.8719488
Iteration :  1   Loss :  109612.552695
Iteration :  2   Loss :  51994.1723061
Iteration :  3   Loss :  24017.9106654
Iteration :  4   Loss :  38363.4135519
Iteration :  5   Loss :  59305.8057558
Iteration :  6   Loss :  68786.5417508
Iteration :  7   Loss :  24914.7090333
Iteration :  8   Loss :  51921.5642344
Iteration :  9   Loss :  37558.0360581
Iteration :  10   Loss :  38436.8914874
Iteration :  11   Loss :  18325.2635451
Iteration :  12   Loss :  48689.8423787
Iteration :  13   Loss :  39484.5080431
Iteration :  14   Loss :  72019.651735
Iteration :  15   Loss :  31510.1033624
Iteration :  16   Loss :  70871.1884795
Iteration :  17   Loss :  32096.477352
Iteration :  18   Loss :  73713.3644161
Iteration :  19   Loss :  35975.211745
Iteration :  20   Loss :  49912.1183563
Iteration :  21   Loss :  56394.8933069
Iteration :  22   Loss :  51269.1153158
Iteration :  23   Loss :  44141.9851571
Iteration :  24   Loss :  43724.3373248
Iteration :  25   Loss :  43802.6595996
Iteration :  26   Loss :  78595.8533562
Iteration :  27   Loss :  61955.9569194
Iteration :  28   Loss :  40225.8834229
Iteration :  29   Loss :  43743.418808
Iteration :  30   Loss :  34847.2903384
Iteration :  31   Loss :  56191.9871406
Iteration :  32   Loss :  35119.0852663
Iteration :  33   Loss :  60446.6205261
Iteration :  34   Loss :  32967.1401373
Iteration :  35   Loss :  52326.7630352
Iteration :  36   Loss :  31849.8243036
Iteration :  37   Loss :  38357.9673473
Iteration :  38   Loss :  22480.8091852
Iteration :  39   Loss :  69286.5319489
Iteration :  40   Loss :  51957.7564275
Iteration :  41   Loss :  68115.233053
Iteration :  42   Loss :  31959.9873663
Iteration :  43   Loss :  40768.3104276
Iteration :  44   Loss :  22038.4722018
Iteration :  45   Loss :  40823.2578782
Iteration :  46   Loss :  31026.7708863
Iteration :  47   Loss :  25970.631519
Iteration :  48   Loss :  49866.1193783
Iteration :  49   Loss :  16197.2939603
Iteration :  50   Loss :  45142.5913357
Iteration :  51   Loss :  87709.3688206
Iteration :  52   Loss :  43302.3268915
Iteration :  53   Loss :  72659.1343397
Iteration :  54   Loss :  49928.8627275
Iteration :  55   Loss :  27321.2392757
Iteration :  56   Loss :  27106.8821492
Iteration :  57   Loss :  24884.8717583
Iteration :  58   Loss :  50452.8538122
Iteration :  59   Loss :  48743.5924088
Iteration :  60   Loss :  45592.5421522
Iteration :  61   Loss :  76559.639429
Iteration :  62   Loss :  46499.1901061
Iteration :  63   Loss :  64960.7229639
Iteration :  64   Loss :  45496.9714237
Iteration :  65   Loss :  31983.7138607
Iteration :  66   Loss :  51094.3127147
Iteration :  67   Loss :  54450.233366
Iteration :  68   Loss :  35452.9773731
Iteration :  69   Loss :  31763.751141
Iteration :  70   Loss :  29430.4497641
Iteration :  71   Loss :  56522.1458142
Iteration :  72   Loss :  47980.880004
Iteration :  73   Loss :  41680.7563943
Iteration :  74   Loss :  42794.9941747
Iteration :  75   Loss :  20404.23401
Iteration :  76   Loss :  38326.11289
Iteration :  77   Loss :  71121.5406194
Iteration :  78   Loss :  53974.8916101
Iteration :  79   Loss :  30414.2527492
Iteration :  80   Loss :  16522.0633618
Iteration :  81   Loss :  26755.7776424
Iteration :  82   Loss :  44990.0542141
Iteration :  83   Loss :  44189.3451746
Iteration :  84   Loss :  38513.6239687
Iteration :  85   Loss :  70694.082818
Iteration :  86   Loss :  38937.3118639
Iteration :  87   Loss :  28812.6760247
Iteration :  88   Loss :  24757.9266881
Iteration :  89   Loss :  80308.8603697
Iteration :  90   Loss :  36289.1287093
Iteration :  91   Loss :  37077.488642
Iteration :  92   Loss :  77274.4765452
Iteration :  93   Loss :  42015.5083296
Iteration :  94   Loss :  64000.6394059
Iteration :  95   Loss :  40544.9989087
Iteration :  96   Loss :  65889.724661
Iteration :  97   Loss :  33700.8325135
Iteration :  98   Loss :  37410.5405147
Iteration :  99   Loss :  30283.8405849
[-0.20533113 -0.80435657  0.12011856 ...,  0.00801223  0.03499325
  0.05721466]
CROSS VALIDATION 5
Iteration :  0   Loss :  34312.6767049
Iteration :  1   Loss :  27952.1353823
Iteration :  2   Loss :  35937.9331167
Iteration :  3   Loss :  44369.6432938
Iteration :  4   Loss :  49768.5469535
Iteration :  5   Loss :  13632.5289416
Iteration :  6   Loss :  51270.2223189
Iteration :  7   Loss :  31596.4716133
Iteration :  8   Loss :  61545.5206785
Iteration :  9   Loss :  42284.5021304
Iteration :  10   Loss :  65982.0861453
Iteration :  11   Loss :  35849.2659835
Iteration :  12   Loss :  35011.5103639
Iteration :  13   Loss :  27556.3913029
Iteration :  14   Loss :  26063.5420642
Iteration :  15   Loss :  24656.5334243
Iteration :  16   Loss :  17325.2598522
Iteration :  17   Loss :  48762.1419689
Iteration :  18   Loss :  21715.6701828
Iteration :  19   Loss :  75076.5732819
Iteration :  20   Loss :  22131.9525822
Iteration :  21   Loss :  29303.1319616
Iteration :  22   Loss :  49905.1546779
Iteration :  23   Loss :  33209.8344575
Iteration :  24   Loss :  38552.7279476
Iteration :  25   Loss :  31522.1213235
Iteration :  26   Loss :  37163.6088476
Iteration :  27   Loss :  51963.2963727
Iteration :  28   Loss :  32129.8480947
Iteration :  29   Loss :  54324.0435161
Iteration :  30   Loss :  9370.94751605
Iteration :  31   Loss :  46235.0910959
Iteration :  32   Loss :  21006.4336468
Iteration :  33   Loss :  50999.4034139
Iteration :  34   Loss :  35036.835989
Iteration :  35   Loss :  42508.8295984
Iteration :  36   Loss :  5853.44733456
Iteration :  37   Loss :  29823.9524364
Iteration :  38   Loss :  45757.858567
Iteration :  39   Loss :  44082.8604785
Iteration :  40   Loss :  39582.295533
Iteration :  41   Loss :  50473.9042463
Iteration :  42   Loss :  32428.6601109
Iteration :  43   Loss :  33411.5396457
Iteration :  44   Loss :  62729.1775173
Iteration :  45   Loss :  32466.3396984
Iteration :  46   Loss :  17503.7353222
Iteration :  47   Loss :  83078.4831156
Iteration :  48   Loss :  40500.8715662
Iteration :  49   Loss :  68476.3674548
Iteration :  50   Loss :  46032.702637
Iteration :  51   Loss :  32403.1211171
Iteration :  52   Loss :  38330.3333393
Iteration :  53   Loss :  36285.4199653
Iteration :  54   Loss :  27546.5082924
Iteration :  55   Loss :  49735.2783587
Iteration :  56   Loss :  24737.791322
Iteration :  57   Loss :  47935.5317958
Iteration :  58   Loss :  25395.7652967
Iteration :  59   Loss :  61328.8945066
Iteration :  60   Loss :  31584.9680289
Iteration :  61   Loss :  41172.0305238
Iteration :  62   Loss :  28642.5252699
Iteration :  63   Loss :  45192.6688513
Iteration :  64   Loss :  110791.549392
Iteration :  65   Loss :  39606.9809074
Iteration :  66   Loss :  109887.396062
Iteration :  67   Loss :  33758.6709037
Iteration :  68   Loss :  47732.3525249
Iteration :  69   Loss :  60472.6425978
Iteration :  70   Loss :  10106.6561386
Iteration :  71   Loss :  46257.7749838
Iteration :  72   Loss :  21007.1110388
Iteration :  73   Loss :  50999.4709383
Iteration :  74   Loss :  35036.8355977
Iteration :  75   Loss :  42508.8296178
Iteration :  76   Loss :  5853.44733735
Iteration :  77   Loss :  29823.9524364
Iteration :  78   Loss :  45757.858567
Iteration :  79   Loss :  44082.8604785
Iteration :  80   Loss :  39582.295533
Iteration :  81   Loss :  50473.9042463
Iteration :  82   Loss :  32428.6601109
Iteration :  83   Loss :  33411.5396457
Iteration :  84   Loss :  62729.1775173
Iteration :  85   Loss :  32466.3396984
Iteration :  86   Loss :  17503.7353222
Iteration :  87   Loss :  83078.4831156
Iteration :  88   Loss :  40500.8715662
Iteration :  89   Loss :  68476.3674548
Iteration :  90   Loss :  46032.702637
Iteration :  91   Loss :  32403.1211171
Iteration :  92   Loss :  38330.3333393
Iteration :  93   Loss :  36285.4199653
Iteration :  94   Loss :  27546.5082924
Iteration :  95   Loss :  49735.2783587
Iteration :  96   Loss :  24737.791322
Iteration :  97   Loss :  47935.5317958
Iteration :  98   Loss :  25395.7652967
Iteration :  99   Loss :  61328.8945066
[-0.41662867 -0.21876042  0.08659924 ...,  0.35026712  0.31002682
  0.14914884]
CROSS VALIDATION 6
Iteration :  0   Loss :  42212.6171604
Iteration :  1   Loss :  116185.162914
Iteration :  2   Loss :  34897.9784536
Iteration :  3   Loss :  97526.4441813
Iteration :  4   Loss :  27001.3515284
Iteration :  5   Loss :  50193.7028862
Iteration :  6   Loss :  75898.3248173
Iteration :  7   Loss :  57517.9906152
Iteration :  8   Loss :  59058.7781603
Iteration :  9   Loss :  42555.8496133
Iteration :  10   Loss :  55108.143688
Iteration :  11   Loss :  40310.8511886
Iteration :  12   Loss :  52174.1253679
Iteration :  13   Loss :  31497.641897
Iteration :  14   Loss :  14860.6433471
Iteration :  15   Loss :  44291.3714159
Iteration :  16   Loss :  42258.6388201
Iteration :  17   Loss :  88191.3714383
Iteration :  18   Loss :  37982.0555955
Iteration :  19   Loss :  27595.3729299
Iteration :  20   Loss :  49069.0130106
Iteration :  21   Loss :  26895.406091
Iteration :  22   Loss :  48092.875438
Iteration :  23   Loss :  26906.2476277
Iteration :  24   Loss :  48091.4950326
Iteration :  25   Loss :  26906.2630014
Iteration :  26   Loss :  48091.4930758
Iteration :  27   Loss :  26906.2630231
Iteration :  28   Loss :  48091.493073
Iteration :  29   Loss :  26906.2630232
Iteration :  30   Loss :  48091.493073
Iteration :  31   Loss :  26906.2630232
Iteration :  32   Loss :  48091.493073
Iteration :  33   Loss :  26906.2630232
Iteration :  34   Loss :  48091.493073
Iteration :  35   Loss :  26906.2630232
Iteration :  36   Loss :  48091.493073
Iteration :  37   Loss :  26906.2630232
Iteration :  38   Loss :  48091.493073
Iteration :  39   Loss :  26906.2630232
Iteration :  40   Loss :  48091.493073
Iteration :  41   Loss :  26906.2630232
Iteration :  42   Loss :  48091.493073
Iteration :  43   Loss :  26906.2630232
Iteration :  44   Loss :  48091.493073
Iteration :  45   Loss :  26906.2630232
Iteration :  46   Loss :  48091.493073
Iteration :  47   Loss :  26906.2630232
Iteration :  48   Loss :  48091.493073
Iteration :  49   Loss :  26906.2630232
Iteration :  50   Loss :  48091.493073
Iteration :  51   Loss :  26906.2630232
Iteration :  52   Loss :  48091.493073
Iteration :  53   Loss :  26906.2630232
Iteration :  54   Loss :  48091.493073
Iteration :  55   Loss :  26906.2630232
Iteration :  56   Loss :  48091.493073
Iteration :  57   Loss :  26906.2630232
Iteration :  58   Loss :  48091.493073
Iteration :  59   Loss :  26906.2630232
Iteration :  60   Loss :  48091.493073
Iteration :  61   Loss :  26906.2630232
Iteration :  62   Loss :  48091.493073
Iteration :  63   Loss :  26906.2630232
Iteration :  64   Loss :  48091.493073
Iteration :  65   Loss :  26906.2630232
Iteration :  66   Loss :  48091.493073
Iteration :  67   Loss :  26906.2630232
Iteration :  68   Loss :  48091.493073
Iteration :  69   Loss :  26906.2630232
Iteration :  70   Loss :  48091.493073
Iteration :  71   Loss :  26906.2630232
Iteration :  72   Loss :  48091.493073
Iteration :  73   Loss :  26906.2630232
Iteration :  74   Loss :  48091.493073
Iteration :  75   Loss :  26906.2630232
Iteration :  76   Loss :  48091.493073
Iteration :  77   Loss :  26906.2630232
Iteration :  78   Loss :  48091.493073
Iteration :  79   Loss :  26906.2630232
Iteration :  80   Loss :  48091.493073
Iteration :  81   Loss :  26906.2630232
Iteration :  82   Loss :  48091.493073
Iteration :  83   Loss :  26906.2630232
Iteration :  84   Loss :  48091.493073
Iteration :  85   Loss :  26906.2630232
Iteration :  86   Loss :  48091.493073
Iteration :  87   Loss :  26906.2630232
Iteration :  88   Loss :  48091.493073
Iteration :  89   Loss :  26906.2630232
Iteration :  90   Loss :  48091.493073
Iteration :  91   Loss :  26906.2630232
Iteration :  92   Loss :  48091.493073
Iteration :  93   Loss :  26906.2630232
Iteration :  94   Loss :  48091.493073
Iteration :  95   Loss :  26906.2630232
Iteration :  96   Loss :  48091.493073
Iteration :  97   Loss :  26906.2630232
Iteration :  98   Loss :  48091.493073
Iteration :  99   Loss :  26906.2630232
[-0.54842322  0.07276916 -0.35087363 ...,  0.23764935 -0.09045332
  0.07036534]
CROSS VALIDATION 7
Iteration :  0   Loss :  42212.6171604
Iteration :  1   Loss :  112601.840923
Iteration :  2   Loss :  34897.9784536
Iteration :  3   Loss :  93109.9385568
Iteration :  4   Loss :  27001.3515284
Iteration :  5   Loss :  50193.7028862
Iteration :  6   Loss :  75898.3248173
Iteration :  7   Loss :  57517.9906152
Iteration :  8   Loss :  59058.7781603
Iteration :  9   Loss :  42555.8496133
Iteration :  10   Loss :  55108.143688
Iteration :  11   Loss :  40310.8511886
Iteration :  12   Loss :  55689.2690354
Iteration :  13   Loss :  31497.641897
Iteration :  14   Loss :  14860.6433471
Iteration :  15   Loss :  44291.3714159
Iteration :  16   Loss :  42258.6388201
Iteration :  17   Loss :  85316.5201552
Iteration :  18   Loss :  37982.0555955
Iteration :  19   Loss :  27595.3729299
Iteration :  20   Loss :  49069.0130106
Iteration :  21   Loss :  26895.406091
Iteration :  22   Loss :  48092.875438
Iteration :  23   Loss :  26906.2476277
Iteration :  24   Loss :  48091.4950326
Iteration :  25   Loss :  26906.2630014
Iteration :  26   Loss :  48091.4930758
Iteration :  27   Loss :  26906.2630231
Iteration :  28   Loss :  48091.493073
Iteration :  29   Loss :  26906.2630232
Iteration :  30   Loss :  48091.493073
Iteration :  31   Loss :  26906.2630232
Iteration :  32   Loss :  48091.493073
Iteration :  33   Loss :  26906.2630232
Iteration :  34   Loss :  48091.493073
Iteration :  35   Loss :  26906.2630232
Iteration :  36   Loss :  48091.493073
Iteration :  37   Loss :  26906.2630232
Iteration :  38   Loss :  48091.493073
Iteration :  39   Loss :  26906.2630232
Iteration :  40   Loss :  48091.493073
Iteration :  41   Loss :  26906.2630232
Iteration :  42   Loss :  48091.493073
Iteration :  43   Loss :  26906.2630232
Iteration :  44   Loss :  48091.493073
Iteration :  45   Loss :  26906.2630232
Iteration :  46   Loss :  48091.493073
Iteration :  47   Loss :  26906.2630232
Iteration :  48   Loss :  48091.493073
Iteration :  49   Loss :  26906.2630232
Iteration :  50   Loss :  48091.493073
Iteration :  51   Loss :  26906.2630232
Iteration :  52   Loss :  48091.493073
Iteration :  53   Loss :  26906.2630232
Iteration :  54   Loss :  48091.493073
Iteration :  55   Loss :  26906.2630232
Iteration :  56   Loss :  48091.493073
Iteration :  57   Loss :  26906.2630232
Iteration :  58   Loss :  48091.493073
Iteration :  59   Loss :  26906.2630232
Iteration :  60   Loss :  48091.493073
Iteration :  61   Loss :  26906.2630232
Iteration :  62   Loss :  48091.493073
Iteration :  63   Loss :  26906.2630232
Iteration :  64   Loss :  48091.493073
Iteration :  65   Loss :  26906.2630232
Iteration :  66   Loss :  48091.493073
Iteration :  67   Loss :  26906.2630232
Iteration :  68   Loss :  48091.493073
Iteration :  69   Loss :  26906.2630232
Iteration :  70   Loss :  48091.493073
Iteration :  71   Loss :  26906.2630232
Iteration :  72   Loss :  48091.493073
Iteration :  73   Loss :  26906.2630232
Iteration :  74   Loss :  48091.493073
Iteration :  75   Loss :  26906.2630232
Iteration :  76   Loss :  48091.493073
Iteration :  77   Loss :  26906.2630232
Iteration :  78   Loss :  48091.493073
Iteration :  79   Loss :  26906.2630232
Iteration :  80   Loss :  48091.493073
Iteration :  81   Loss :  26906.2630232
Iteration :  82   Loss :  48091.493073
Iteration :  83   Loss :  26906.2630232
Iteration :  84   Loss :  48091.493073
Iteration :  85   Loss :  26906.2630232
Iteration :  86   Loss :  48091.493073
Iteration :  87   Loss :  26906.2630232
Iteration :  88   Loss :  48091.493073
Iteration :  89   Loss :  26906.2630232
Iteration :  90   Loss :  48091.493073
Iteration :  91   Loss :  26906.2630232
Iteration :  92   Loss :  48091.493073
Iteration :  93   Loss :  26906.2630232
Iteration :  94   Loss :  48091.493073
Iteration :  95   Loss :  26906.2630232
Iteration :  96   Loss :  48091.493073
Iteration :  97   Loss :  26906.2630232
Iteration :  98   Loss :  48091.493073
Iteration :  99   Loss :  26906.2630232
[-0.54842322  0.07276916 -0.35087363 ...,  0.23764935 -0.09045332
  0.07036534]
CROSS VALIDATION 8
Iteration :  0   Loss :  41017.7606078
Iteration :  1   Loss :  33506.4889317
Iteration :  2   Loss :  43541.7853712
Iteration :  3   Loss :  35787.465684
Iteration :  4   Loss :  45207.4686
Iteration :  5   Loss :  31255.3512765
Iteration :  6   Loss :  91469.8516419
Iteration :  7   Loss :  38424.2322172
Iteration :  8   Loss :  86040.4418057
Iteration :  9   Loss :  38412.0016787
Iteration :  10   Loss :  86040.2884792
Iteration :  11   Loss :  38412.0604637
Iteration :  12   Loss :  86040.2882633
Iteration :  13   Loss :  38412.060547
Iteration :  14   Loss :  86040.288263
Iteration :  15   Loss :  38412.0605471
Iteration :  16   Loss :  86040.288263
Iteration :  17   Loss :  38412.0605471
Iteration :  18   Loss :  86040.288263
Iteration :  19   Loss :  38412.0605471
Iteration :  20   Loss :  86040.288263
Iteration :  21   Loss :  38412.0605471
Iteration :  22   Loss :  86040.288263
Iteration :  23   Loss :  38412.0605471
Iteration :  24   Loss :  86040.288263
Iteration :  25   Loss :  38412.0605471
Iteration :  26   Loss :  86040.288263
Iteration :  27   Loss :  38412.0605471
Iteration :  28   Loss :  86040.288263
Iteration :  29   Loss :  38412.0605471
Iteration :  30   Loss :  86040.288263
Iteration :  31   Loss :  38412.0605471
Iteration :  32   Loss :  86040.288263
Iteration :  33   Loss :  38412.0605471
Iteration :  34   Loss :  86040.288263
Iteration :  35   Loss :  38412.0605471
Iteration :  36   Loss :  86040.288263
Iteration :  37   Loss :  38412.0605471
Iteration :  38   Loss :  86040.288263
Iteration :  39   Loss :  38412.0605471
Iteration :  40   Loss :  86040.288263
Iteration :  41   Loss :  38412.0605471
Iteration :  42   Loss :  86040.288263
Iteration :  43   Loss :  38412.0605471
Iteration :  44   Loss :  86040.288263
Iteration :  45   Loss :  38412.0605471
Iteration :  46   Loss :  86040.288263
Iteration :  47   Loss :  38412.0605471
Iteration :  48   Loss :  86040.288263
Iteration :  49   Loss :  38412.0605471
Iteration :  50   Loss :  86040.288263
Iteration :  51   Loss :  38412.0605471
Iteration :  52   Loss :  86040.288263
Iteration :  53   Loss :  38412.0605471
Iteration :  54   Loss :  86040.288263
Iteration :  55   Loss :  38412.0605471
Iteration :  56   Loss :  86040.288263
Iteration :  57   Loss :  38412.0605471
Iteration :  58   Loss :  86040.288263
Iteration :  59   Loss :  38412.0605471
Iteration :  60   Loss :  86040.288263
Iteration :  61   Loss :  38412.0605471
Iteration :  62   Loss :  86040.288263
Iteration :  63   Loss :  38412.0605471
Iteration :  64   Loss :  86040.288263
Iteration :  65   Loss :  38412.0605471
Iteration :  66   Loss :  86040.288263
Iteration :  67   Loss :  38412.0605471
Iteration :  68   Loss :  86040.288263
Iteration :  69   Loss :  38412.0605471
Iteration :  70   Loss :  86040.288263
Iteration :  71   Loss :  38412.0605471
Iteration :  72   Loss :  86040.288263
Iteration :  73   Loss :  38412.0605471
Iteration :  74   Loss :  86040.288263
Iteration :  75   Loss :  38412.0605471
Iteration :  76   Loss :  86040.288263
Iteration :  77   Loss :  38412.0605471
Iteration :  78   Loss :  86040.288263
Iteration :  79   Loss :  38412.0605471
Iteration :  80   Loss :  86040.288263
Iteration :  81   Loss :  38412.0605471
Iteration :  82   Loss :  86040.288263
Iteration :  83   Loss :  38412.0605471
Iteration :  84   Loss :  86040.288263
Iteration :  85   Loss :  38412.0605471
Iteration :  86   Loss :  86040.288263
Iteration :  87   Loss :  38412.0605471
Iteration :  88   Loss :  86040.288263
Iteration :  89   Loss :  38412.0605471
Iteration :  90   Loss :  86040.288263
Iteration :  91   Loss :  38412.0605471
Iteration :  92   Loss :  86040.288263
Iteration :  93   Loss :  38412.0605471
Iteration :  94   Loss :  86040.288263
Iteration :  95   Loss :  38412.0605471
Iteration :  96   Loss :  86040.288263
Iteration :  97   Loss :  38412.0605471
Iteration :  98   Loss :  86040.288263
Iteration :  99   Loss :  38412.0605471
[-0.07982941 -0.98562775 -0.00272251 ...,  0.19843088 -0.45742601
 -0.08099403]
CROSS VALIDATION 9
Iteration :  0   Loss :  39264.5663429
Iteration :  1   Loss :  37207.4783282
Iteration :  2   Loss :  33495.5118252
Iteration :  3   Loss :  55696.8307844
Iteration :  4   Loss :  164.264417527
Iteration :  5   Loss :  59382.6198981
Iteration :  6   Loss :  11741.7097421
Iteration :  7   Loss :  53661.0761473
Iteration :  8   Loss :  42474.3087993
Iteration :  9   Loss :  63602.7967864
Iteration :  10   Loss :  33754.5052134
Iteration :  11   Loss :  44696.2579012
Iteration :  12   Loss :  43693.639723
Iteration :  13   Loss :  33341.0638885
Iteration :  14   Loss :  35165.2853368
Iteration :  15   Loss :  33605.3648799
Iteration :  16   Loss :  56701.0253292
Iteration :  17   Loss :  26524.0198704
Iteration :  18   Loss :  31546.3065175
Iteration :  19   Loss :  29948.1308206
Iteration :  20   Loss :  26013.5652417
Iteration :  21   Loss :  42142.9939918
Iteration :  22   Loss :  54244.9707838
Iteration :  23   Loss :  31220.0968291
Iteration :  24   Loss :  30855.1708154
Iteration :  25   Loss :  27561.2947292
Iteration :  26   Loss :  58654.6697242
Iteration :  27   Loss :  45957.8027079
Iteration :  28   Loss :  21672.6198616
Iteration :  29   Loss :  40278.540255
Iteration :  30   Loss :  17730.1959705
Iteration :  31   Loss :  35044.8778014
Iteration :  32   Loss :  25140.4016164
Iteration :  33   Loss :  28959.3195981
Iteration :  34   Loss :  63375.2788806
Iteration :  35   Loss :  38127.7030147
Iteration :  36   Loss :  10051.7419865
Iteration :  37   Loss :  33567.9641383
Iteration :  38   Loss :  19592.1176226
Iteration :  39   Loss :  40276.2944987
Iteration :  40   Loss :  25875.5595692
Iteration :  41   Loss :  26824.9558903
Iteration :  42   Loss :  46544.1053514
Iteration :  43   Loss :  87402.6866372
Iteration :  44   Loss :  32981.4056577
Iteration :  45   Loss :  29602.5029476
Iteration :  46   Loss :  37885.8417522
Iteration :  47   Loss :  42631.4643765
Iteration :  48   Loss :  34770.4546253
Iteration :  49   Loss :  15385.8610408
Iteration :  50   Loss :  45045.521735
Iteration :  51   Loss :  49546.3250871
Iteration :  52   Loss :  66115.2076595
Iteration :  53   Loss :  60262.1079855
Iteration :  54   Loss :  29236.6699922
Iteration :  55   Loss :  35901.2584234
Iteration :  56   Loss :  17955.8128287
Iteration :  57   Loss :  45421.9507314
Iteration :  58   Loss :  16388.0471225
Iteration :  59   Loss :  45033.5615227
Iteration :  60   Loss :  16422.3182868
Iteration :  61   Loss :  45033.0117317
Iteration :  62   Loss :  16422.3668706
Iteration :  63   Loss :  45033.0109523
Iteration :  64   Loss :  16422.3669395
Iteration :  65   Loss :  45033.0109512
Iteration :  66   Loss :  16422.3669396
Iteration :  67   Loss :  45033.0109512
Iteration :  68   Loss :  16422.3669396
Iteration :  69   Loss :  45033.0109512
Iteration :  70   Loss :  16422.3669396
Iteration :  71   Loss :  45033.0109512
Iteration :  72   Loss :  16422.3669396
Iteration :  73   Loss :  45033.0109512
Iteration :  74   Loss :  16422.3669396
Iteration :  75   Loss :  45033.0109512
Iteration :  76   Loss :  16422.3669396
Iteration :  77   Loss :  45033.0109512
Iteration :  78   Loss :  16422.3669396
Iteration :  79   Loss :  45033.0109512
Iteration :  80   Loss :  16422.3669396
Iteration :  81   Loss :  45033.0109512
Iteration :  82   Loss :  16422.3669396
Iteration :  83   Loss :  45033.0109512
Iteration :  84   Loss :  16422.3669396
Iteration :  85   Loss :  45033.0109512
Iteration :  86   Loss :  16422.3669396
Iteration :  87   Loss :  45033.0109512
Iteration :  88   Loss :  16422.3669396
Iteration :  89   Loss :  45033.0109512
Iteration :  90   Loss :  16422.3669396
Iteration :  91   Loss :  45033.0109512
Iteration :  92   Loss :  16422.3669396
Iteration :  93   Loss :  45033.0109512
Iteration :  94   Loss :  16422.3669396
Iteration :  95   Loss :  45033.0109512
Iteration :  96   Loss :  16422.3669396
Iteration :  97   Loss :  45033.0109512
Iteration :  98   Loss :  16422.3669396
Iteration :  99   Loss :  45033.0109512
[-0.02368004 -0.85275892 -0.1262475  ...,  0.41272034 -0.46188979
 -0.07627624]
CROSS VALIDATION 10
Iteration :  0   Loss :  47137.3292689
Iteration :  1   Loss :  41072.2395415
Iteration :  2   Loss :  33548.1943994
Iteration :  3   Loss :  89010.8651295
Iteration :  4   Loss :  42195.6350445
Iteration :  5   Loss :  44331.3996665
Iteration :  6   Loss :  49506.7788834
Iteration :  7   Loss :  87624.4598759
Iteration :  8   Loss :  50181.3074939
Iteration :  9   Loss :  23448.2957373
Iteration :  10   Loss :  48608.3374243
Iteration :  11   Loss :  33810.8501613
Iteration :  12   Loss :  70782.6546069
Iteration :  13   Loss :  36681.5859164
Iteration :  14   Loss :  55598.1686754
Iteration :  15   Loss :  22569.8310742
Iteration :  16   Loss :  67592.5467588
Iteration :  17   Loss :  47290.9167792
Iteration :  18   Loss :  65530.9419197
Iteration :  19   Loss :  46973.9209315
Iteration :  20   Loss :  29096.6651151
Iteration :  21   Loss :  62465.1201424
Iteration :  22   Loss :  40477.6431189
Iteration :  23   Loss :  49849.7873642
Iteration :  24   Loss :  21838.416213
Iteration :  25   Loss :  41620.1069877
Iteration :  26   Loss :  31015.3741076
Iteration :  27   Loss :  52037.5086299
Iteration :  28   Loss :  39583.567441
Iteration :  29   Loss :  68996.3944907
Iteration :  30   Loss :  28449.4009532
Iteration :  31   Loss :  65524.2004535
Iteration :  32   Loss :  31796.842169
Iteration :  33   Loss :  39653.1193768
Iteration :  34   Loss :  39107.0590804
Iteration :  35   Loss :  34495.2123166
Iteration :  36   Loss :  38190.9317332
Iteration :  37   Loss :  45240.7579568
Iteration :  38   Loss :  33498.1143643
Iteration :  39   Loss :  45382.4187292
Iteration :  40   Loss :  26558.5180631
Iteration :  41   Loss :  64562.8195792
Iteration :  42   Loss :  34056.1398521
Iteration :  43   Loss :  67370.7125846
Iteration :  44   Loss :  29857.4705539
Iteration :  45   Loss :  36201.9869595
Iteration :  46   Loss :  53481.0887369
Iteration :  47   Loss :  33587.162707
Iteration :  48   Loss :  55251.8268691
Iteration :  49   Loss :  27837.6534759
Iteration :  50   Loss :  34578.0980556
Iteration :  51   Loss :  30098.4522174
Iteration :  52   Loss :  49038.6452169
Iteration :  53   Loss :  44329.4283124
Iteration :  54   Loss :  42120.9597238
Iteration :  55   Loss :  64915.648642
Iteration :  56   Loss :  47676.9633167
Iteration :  57   Loss :  40938.2521613
Iteration :  58   Loss :  30695.5029042
Iteration :  59   Loss :  36811.6768197
Iteration :  60   Loss :  96346.2537941
Iteration :  61   Loss :  27299.1759628
Iteration :  62   Loss :  41568.421812
Iteration :  63   Loss :  59112.6980068
Iteration :  64   Loss :  44718.3260455
Iteration :  65   Loss :  41159.6610224
Iteration :  66   Loss :  58114.7882236
Iteration :  67   Loss :  43115.3794389
Iteration :  68   Loss :  34390.6128138
Iteration :  69   Loss :  41027.6129103
Iteration :  70   Loss :  23052.9137841
Iteration :  71   Loss :  71466.1553355
Iteration :  72   Loss :  51954.9269136
Iteration :  73   Loss :  68111.3278434
Iteration :  74   Loss :  31849.530723
Iteration :  75   Loss :  29456.6375642
Iteration :  76   Loss :  29845.4368691
Iteration :  77   Loss :  42518.786846
Iteration :  78   Loss :  43627.6377869
Iteration :  79   Loss :  49529.1974771
Iteration :  80   Loss :  87624.1838107
Iteration :  81   Loss :  50181.3118447
Iteration :  82   Loss :  23448.2949002
Iteration :  83   Loss :  48608.3375484
Iteration :  84   Loss :  33810.8501621
Iteration :  85   Loss :  70782.6546072
Iteration :  86   Loss :  36681.5859164
Iteration :  87   Loss :  55598.1686754
Iteration :  88   Loss :  22569.8310742
Iteration :  89   Loss :  67592.5467588
Iteration :  90   Loss :  47290.9167792
Iteration :  91   Loss :  65530.9419197
Iteration :  92   Loss :  46973.9209315
Iteration :  93   Loss :  29096.6651151
Iteration :  94   Loss :  62465.1201424
Iteration :  95   Loss :  40477.6431189
Iteration :  96   Loss :  49849.7873642
Iteration :  97   Loss :  21838.416213
Iteration :  98   Loss :  41620.1069877
Iteration :  99   Loss :  31015.3741076
[-0.16600938 -0.37518052 -0.42859796 ...,  0.31479204 -0.34917462
  0.03648492]
CROSS VALIDATION 11
Iteration :  0   Loss :  42255.7269951
Iteration :  1   Loss :  108583.737695
Iteration :  2   Loss :  34927.4126669
Iteration :  3   Loss :  89769.6286569
Iteration :  4   Loss :  47418.815541
Iteration :  5   Loss :  110828.575208
Iteration :  6   Loss :  36432.8538266
Iteration :  7   Loss :  35708.7835238
Iteration :  8   Loss :  46512.4726971
Iteration :  9   Loss :  40988.0992269
Iteration :  10   Loss :  28816.6307843
Iteration :  11   Loss :  41380.4963839
Iteration :  12   Loss :  27662.6885137
Iteration :  13   Loss :  49813.4879961
Iteration :  14   Loss :  24637.7120227
Iteration :  15   Loss :  24414.1480878
Iteration :  16   Loss :  28307.3350348
Iteration :  17   Loss :  28483.7856258
Iteration :  18   Loss :  26796.9396742
Iteration :  19   Loss :  76518.777231
Iteration :  20   Loss :  45538.9786778
Iteration :  21   Loss :  46381.6516283
Iteration :  22   Loss :  48791.3111933
Iteration :  23   Loss :  42760.5719862
Iteration :  24   Loss :  45599.2175577
Iteration :  25   Loss :  34505.6130547
Iteration :  26   Loss :  23084.337958
Iteration :  27   Loss :  57194.3504191
Iteration :  28   Loss :  44617.9816696
Iteration :  29   Loss :  62120.6230965
Iteration :  30   Loss :  30004.2457486
Iteration :  31   Loss :  40346.1874798
Iteration :  32   Loss :  51305.1118781
Iteration :  33   Loss :  38610.1314189
Iteration :  34   Loss :  71982.498666
Iteration :  35   Loss :  31441.9813306
Iteration :  36   Loss :  57887.9938813
Iteration :  37   Loss :  44980.6545238
Iteration :  38   Loss :  40463.0677208
Iteration :  39   Loss :  28806.7382015
Iteration :  40   Loss :  41379.9621889
Iteration :  41   Loss :  27662.7623015
Iteration :  42   Loss :  49813.4866734
Iteration :  43   Loss :  24637.7119905
Iteration :  44   Loss :  24414.1480874
Iteration :  45   Loss :  28307.3350348
Iteration :  46   Loss :  28483.7856258
Iteration :  47   Loss :  26796.9396742
Iteration :  48   Loss :  76518.777231
Iteration :  49   Loss :  45538.9786778
Iteration :  50   Loss :  46381.6516283
Iteration :  51   Loss :  48791.3111933
Iteration :  52   Loss :  42760.5719862
Iteration :  53   Loss :  45599.2175577
Iteration :  54   Loss :  34505.6130547
Iteration :  55   Loss :  23084.337958
Iteration :  56   Loss :  57194.3504191
Iteration :  57   Loss :  44617.9816696
Iteration :  58   Loss :  62120.6230965
Iteration :  59   Loss :  30004.2457486
Iteration :  60   Loss :  40346.1874798
Iteration :  61   Loss :  51305.1118781
Iteration :  62   Loss :  38610.1314189
Iteration :  63   Loss :  71982.498666
Iteration :  64   Loss :  31441.9813306
Iteration :  65   Loss :  57887.9938813
Iteration :  66   Loss :  44980.6545238
Iteration :  67   Loss :  40463.0677208
Iteration :  68   Loss :  28806.7382015
Iteration :  69   Loss :  41379.9621889
Iteration :  70   Loss :  27662.7623015
Iteration :  71   Loss :  49813.4866734
Iteration :  72   Loss :  24637.7119905
Iteration :  73   Loss :  24414.1480874
Iteration :  74   Loss :  28307.3350348
Iteration :  75   Loss :  28483.7856258
Iteration :  76   Loss :  26796.9396742
Iteration :  77   Loss :  76518.777231
Iteration :  78   Loss :  45538.9786778
Iteration :  79   Loss :  46381.6516283
Iteration :  80   Loss :  48791.3111933
Iteration :  81   Loss :  42760.5719862
Iteration :  82   Loss :  45599.2175577
Iteration :  83   Loss :  34505.6130547
Iteration :  84   Loss :  23084.337958
Iteration :  85   Loss :  57194.3504191
Iteration :  86   Loss :  44617.9816696
Iteration :  87   Loss :  62120.6230965
Iteration :  88   Loss :  30004.2457486
Iteration :  89   Loss :  40346.1874798
Iteration :  90   Loss :  51305.1118781
Iteration :  91   Loss :  38610.1314189
Iteration :  92   Loss :  71982.498666
Iteration :  93   Loss :  31441.9813306
Iteration :  94   Loss :  57887.9938813
Iteration :  95   Loss :  44980.6545238
Iteration :  96   Loss :  40463.0677208
Iteration :  97   Loss :  28806.7382015
Iteration :  98   Loss :  41379.9621889
Iteration :  99   Loss :  27662.7623015
[-0.03696105 -0.77142167  0.26438046 ...,  0.05478568  0.00117589
  0.02713711]
CROSS VALIDATION 12
Iteration :  0   Loss :  34105.9941481
Iteration :  1   Loss :  11250.1511875
Iteration :  2   Loss :  53248.8473577
Iteration :  3   Loss :  24799.5517174
Iteration :  4   Loss :  43608.1468623
Iteration :  5   Loss :  37582.7418477
Iteration :  6   Loss :  30771.4702295
Iteration :  7   Loss :  72000.6474448
Iteration :  8   Loss :  56676.565689
Iteration :  9   Loss :  51897.298545
Iteration :  10   Loss :  18914.4637574
Iteration :  11   Loss :  43199.2753571
Iteration :  12   Loss :  34488.2828115
Iteration :  13   Loss :  42295.1287386
Iteration :  14   Loss :  34630.4958154
Iteration :  15   Loss :  25103.9717859
Iteration :  16   Loss :  33312.2664165
Iteration :  17   Loss :  24120.7744757
Iteration :  18   Loss :  40198.2697446
Iteration :  19   Loss :  27745.0984278
Iteration :  20   Loss :  61196.9521026
Iteration :  21   Loss :  33679.561018
Iteration :  22   Loss :  43949.3079027
Iteration :  23   Loss :  42033.0017273
Iteration :  24   Loss :  46469.3055079
Iteration :  25   Loss :  65425.8875427
Iteration :  26   Loss :  39369.1738486
Iteration :  27   Loss :  30892.3311568
Iteration :  28   Loss :  72020.9391283
Iteration :  29   Loss :  56676.9247473
Iteration :  30   Loss :  51897.2999036
Iteration :  31   Loss :  18914.4638489
Iteration :  32   Loss :  43199.2753804
Iteration :  33   Loss :  34488.2828136
Iteration :  34   Loss :  42295.1287386
Iteration :  35   Loss :  34630.4958154
Iteration :  36   Loss :  25103.9717859
Iteration :  37   Loss :  33312.2664165
Iteration :  38   Loss :  24120.7744757
Iteration :  39   Loss :  40198.2697446
Iteration :  40   Loss :  27745.0984278
Iteration :  41   Loss :  61196.9521026
Iteration :  42   Loss :  33679.561018
Iteration :  43   Loss :  43949.3079027
Iteration :  44   Loss :  42033.0017273
Iteration :  45   Loss :  46469.3055079
Iteration :  46   Loss :  65425.8875427
Iteration :  47   Loss :  39369.1738486
Iteration :  48   Loss :  30892.3311568
Iteration :  49   Loss :  72020.9391283
Iteration :  50   Loss :  56676.9247473
Iteration :  51   Loss :  51897.2999036
Iteration :  52   Loss :  18914.4638489
Iteration :  53   Loss :  43199.2753804
Iteration :  54   Loss :  34488.2828136
Iteration :  55   Loss :  42295.1287386
Iteration :  56   Loss :  34630.4958154
Iteration :  57   Loss :  25103.9717859
Iteration :  58   Loss :  33312.2664165
Iteration :  59   Loss :  24120.7744757
Iteration :  60   Loss :  40198.2697446
Iteration :  61   Loss :  27745.0984278
Iteration :  62   Loss :  61196.9521026
Iteration :  63   Loss :  33679.561018
Iteration :  64   Loss :  43949.3079027
Iteration :  65   Loss :  42033.0017273
Iteration :  66   Loss :  46469.3055079
Iteration :  67   Loss :  65425.8875427
Iteration :  68   Loss :  39369.1738486
Iteration :  69   Loss :  30892.3311568
Iteration :  70   Loss :  72020.9391283
Iteration :  71   Loss :  56676.9247473
Iteration :  72   Loss :  51897.2999036
Iteration :  73   Loss :  18914.4638489
Iteration :  74   Loss :  43199.2753804
Iteration :  75   Loss :  34488.2828136
Iteration :  76   Loss :  42295.1287386
Iteration :  77   Loss :  34630.4958154
Iteration :  78   Loss :  25103.9717859
Iteration :  79   Loss :  33312.2664165
Iteration :  80   Loss :  24120.7744757
Iteration :  81   Loss :  40198.2697446
Iteration :  82   Loss :  27745.0984278
Iteration :  83   Loss :  61196.9521026
Iteration :  84   Loss :  33679.561018
Iteration :  85   Loss :  43949.3079027
Iteration :  86   Loss :  42033.0017273
Iteration :  87   Loss :  46469.3055079
Iteration :  88   Loss :  65425.8875427
Iteration :  89   Loss :  39369.1738486
Iteration :  90   Loss :  30892.3311568
Iteration :  91   Loss :  72020.9391283
Iteration :  92   Loss :  56676.9247473
Iteration :  93   Loss :  51897.2999036
Iteration :  94   Loss :  18914.4638489
Iteration :  95   Loss :  43199.2753804
Iteration :  96   Loss :  34488.2828136
Iteration :  97   Loss :  42295.1287386
Iteration :  98   Loss :  34630.4958154
Iteration :  99   Loss :  25103.9717859
[ 0.01813501 -0.27503637  0.3439869  ..., -0.21577949  0.24774911
  0.0611819 ]
CROSS VALIDATION 13
Iteration :  0   Loss :  39786.2933788
Iteration :  1   Loss :  116397.702971
Iteration :  2   Loss :  36503.1919017
Iteration :  3   Loss :  40221.3191877
Iteration :  4   Loss :  26948.3941488
Iteration :  5   Loss :  51237.6021863
Iteration :  6   Loss :  51173.0422421
Iteration :  7   Loss :  27911.9784549
Iteration :  8   Loss :  48183.8773886
Iteration :  9   Loss :  54655.2835705
Iteration :  10   Loss :  50860.1472493
Iteration :  11   Loss :  31257.0146994
Iteration :  12   Loss :  47339.0022099
Iteration :  13   Loss :  40497.5321465
Iteration :  14   Loss :  41594.3516959
Iteration :  15   Loss :  51153.896485
Iteration :  16   Loss :  47894.2701182
Iteration :  17   Loss :  28728.2323463
Iteration :  18   Loss :  49876.7641748
Iteration :  19   Loss :  38434.5100893
Iteration :  20   Loss :  30171.1561357
Iteration :  21   Loss :  48628.8896484
Iteration :  22   Loss :  41261.6749436
Iteration :  23   Loss :  31189.7554049
Iteration :  24   Loss :  63091.5093137
Iteration :  25   Loss :  51979.7059269
Iteration :  26   Loss :  80004.3177086
Iteration :  27   Loss :  42237.2198577
Iteration :  28   Loss :  50410.0042331
Iteration :  29   Loss :  28256.1603668
Iteration :  30   Loss :  33374.0014875
Iteration :  31   Loss :  51028.5550103
Iteration :  32   Loss :  45129.9149737
Iteration :  33   Loss :  51013.6299822
Iteration :  34   Loss :  67912.4927484
Iteration :  35   Loss :  36639.4548951
Iteration :  36   Loss :  55418.6476082
Iteration :  37   Loss :  29901.1647018
Iteration :  38   Loss :  38278.6472433
Iteration :  39   Loss :  49386.7620788
Iteration :  40   Loss :  39896.5362583
Iteration :  41   Loss :  44237.7339081
Iteration :  42   Loss :  37689.3633383
Iteration :  43   Loss :  6856.40066431
Iteration :  44   Loss :  59241.3016421
Iteration :  45   Loss :  42393.5288192
Iteration :  46   Loss :  33576.4404259
Iteration :  47   Loss :  31346.2661819
Iteration :  48   Loss :  87360.7530167
Iteration :  49   Loss :  59362.9882284
Iteration :  50   Loss :  29849.5909061
Iteration :  51   Loss :  43985.2731652
Iteration :  52   Loss :  65740.6585453
Iteration :  53   Loss :  26836.401771
Iteration :  54   Loss :  52040.981521
Iteration :  55   Loss :  45362.4032316
Iteration :  56   Loss :  32216.2262684
Iteration :  57   Loss :  60725.8119996
Iteration :  58   Loss :  25803.8349503
Iteration :  59   Loss :  51901.2154002
Iteration :  60   Loss :  45197.5700528
Iteration :  61   Loss :  28973.6722635
Iteration :  62   Loss :  26570.2991583
Iteration :  63   Loss :  42131.8100722
Iteration :  64   Loss :  30528.384017
Iteration :  65   Loss :  41682.7948058
Iteration :  66   Loss :  41563.8890724
Iteration :  67   Loss :  49022.3673868
Iteration :  68   Loss :  25733.3931484
Iteration :  69   Loss :  27879.58945
Iteration :  70   Loss :  50846.1012977
Iteration :  71   Loss :  34938.9347853
Iteration :  72   Loss :  9234.82002749
Iteration :  73   Loss :  41776.7965177
Iteration :  74   Loss :  30425.9127243
Iteration :  75   Loss :  50804.5542345
Iteration :  76   Loss :  32112.1304543
Iteration :  77   Loss :  49527.1825151
Iteration :  78   Loss :  30436.5801722
Iteration :  79   Loss :  52095.9251917
Iteration :  80   Loss :  31042.6266832
Iteration :  81   Loss :  49569.9862517
Iteration :  82   Loss :  34911.6228817
Iteration :  83   Loss :  9234.88766096
Iteration :  84   Loss :  41776.7503644
Iteration :  85   Loss :  30425.9131271
Iteration :  86   Loss :  50804.5541596
Iteration :  87   Loss :  32112.1304553
Iteration :  88   Loss :  49527.1825153
Iteration :  89   Loss :  30436.5801722
Iteration :  90   Loss :  52095.9251917
Iteration :  91   Loss :  31042.6266832
Iteration :  92   Loss :  49569.9862517
Iteration :  93   Loss :  34911.6228817
Iteration :  94   Loss :  9234.88766096
Iteration :  95   Loss :  41776.7503644
Iteration :  96   Loss :  30425.9131271
Iteration :  97   Loss :  50804.5541596
Iteration :  98   Loss :  32112.1304553
Iteration :  99   Loss :  49527.1825153
[-0.0654931  -0.75266922 -0.08588768 ...,  0.43904273 -0.48510409
 -0.06830764]
CROSS VALIDATION 14
Iteration :  0   Loss :  39463.8752291
Iteration :  1   Loss :  114446.347619
Iteration :  2   Loss :  33182.7463521
Iteration :  3   Loss :  21027.8845313
Iteration :  4   Loss :  35491.5253886
Iteration :  5   Loss :  31809.692261
Iteration :  6   Loss :  40911.9987385
Iteration :  7   Loss :  46297.1282529
Iteration :  8   Loss :  31994.8260686
Iteration :  9   Loss :  35470.6924149
Iteration :  10   Loss :  40630.7324075
Iteration :  11   Loss :  24543.0408939
Iteration :  12   Loss :  43310.6239687
Iteration :  13   Loss :  52114.5599919
Iteration :  14   Loss :  46218.7779496
Iteration :  15   Loss :  43953.1036887
Iteration :  16   Loss :  26358.0484734
Iteration :  17   Loss :  41608.7044986
Iteration :  18   Loss :  40226.3649646
Iteration :  19   Loss :  46908.1440136
Iteration :  20   Loss :  11135.4107968
Iteration :  21   Loss :  55466.2914094
Iteration :  22   Loss :  42480.801391
Iteration :  23   Loss :  62135.4091277
Iteration :  24   Loss :  27538.4511477
Iteration :  25   Loss :  40500.6368645
Iteration :  26   Loss :  62467.0290279
Iteration :  27   Loss :  40499.5861421
Iteration :  28   Loss :  45203.0707322
Iteration :  29   Loss :  43949.0051591
Iteration :  30   Loss :  26356.5650742
Iteration :  31   Loss :  41608.6601613
Iteration :  32   Loss :  40226.36441
Iteration :  33   Loss :  46908.1439508
Iteration :  34   Loss :  11135.410796
Iteration :  35   Loss :  55466.2914093
Iteration :  36   Loss :  42480.801391
Iteration :  37   Loss :  62135.4091277
Iteration :  38   Loss :  27538.4511477
Iteration :  39   Loss :  40500.6368645
Iteration :  40   Loss :  62467.0290279
Iteration :  41   Loss :  40499.5861421
Iteration :  42   Loss :  45203.0707322
Iteration :  43   Loss :  43949.0051591
Iteration :  44   Loss :  26356.5650742
Iteration :  45   Loss :  41608.6601613
Iteration :  46   Loss :  40226.36441
Iteration :  47   Loss :  46908.1439508
Iteration :  48   Loss :  11135.410796
Iteration :  49   Loss :  55466.2914093
Iteration :  50   Loss :  42480.801391
Iteration :  51   Loss :  62135.4091277
Iteration :  52   Loss :  27538.4511477
Iteration :  53   Loss :  40500.6368645
Iteration :  54   Loss :  62467.0290279
Iteration :  55   Loss :  40499.5861421
Iteration :  56   Loss :  45203.0707322
Iteration :  57   Loss :  43949.0051591
Iteration :  58   Loss :  26356.5650742
Iteration :  59   Loss :  41608.6601613
Iteration :  60   Loss :  40226.36441
Iteration :  61   Loss :  46908.1439508
Iteration :  62   Loss :  11135.410796
Iteration :  63   Loss :  55466.2914093
Iteration :  64   Loss :  42480.801391
Iteration :  65   Loss :  62135.4091277
Iteration :  66   Loss :  27538.4511477
Iteration :  67   Loss :  40500.6368645
Iteration :  68   Loss :  62467.0290279
Iteration :  69   Loss :  40499.5861421
Iteration :  70   Loss :  45203.0707322
Iteration :  71   Loss :  43949.0051591
Iteration :  72   Loss :  26356.5650742
Iteration :  73   Loss :  41608.6601613
Iteration :  74   Loss :  40226.36441
Iteration :  75   Loss :  46908.1439508
Iteration :  76   Loss :  11135.410796
Iteration :  77   Loss :  55466.2914093
Iteration :  78   Loss :  42480.801391
Iteration :  79   Loss :  62135.4091277
Iteration :  80   Loss :  27538.4511477
Iteration :  81   Loss :  40500.6368645
Iteration :  82   Loss :  62467.0290279
Iteration :  83   Loss :  40499.5861421
Iteration :  84   Loss :  45203.0707322
Iteration :  85   Loss :  43949.0051591
Iteration :  86   Loss :  26356.5650742
Iteration :  87   Loss :  41608.6601613
Iteration :  88   Loss :  40226.36441
Iteration :  89   Loss :  46908.1439508
Iteration :  90   Loss :  11135.410796
Iteration :  91   Loss :  55466.2914093
Iteration :  92   Loss :  42480.801391
Iteration :  93   Loss :  62135.4091277
Iteration :  94   Loss :  27538.4511477
Iteration :  95   Loss :  40500.6368645
Iteration :  96   Loss :  62467.0290279
Iteration :  97   Loss :  40499.5861421
Iteration :  98   Loss :  45203.0707322
Iteration :  99   Loss :  43949.0051591
[-0.40134458 -0.67643458 -0.6379449  ...,  0.44848712 -0.58418899
 -0.03073039]
CROSS VALIDATION 15
Iteration :  0   Loss :  42243.0288314
Iteration :  1   Loss :  112858.165824
Iteration :  2   Loss :  33286.0795392
Iteration :  3   Loss :  47544.1274016
Iteration :  4   Loss :  62051.7618859
Iteration :  5   Loss :  37036.9835599
Iteration :  6   Loss :  33900.0859955
Iteration :  7   Loss :  56446.1525565
Iteration :  8   Loss :  9272.40254756
Iteration :  9   Loss :  51500.5110348
Iteration :  10   Loss :  19898.3320372
Iteration :  11   Loss :  41293.6023747
Iteration :  12   Loss :  30873.8849751
Iteration :  13   Loss :  51165.5594367
Iteration :  14   Loss :  32141.871369
Iteration :  15   Loss :  38386.9058671
Iteration :  16   Loss :  49366.3923042
Iteration :  17   Loss :  30014.0725125
Iteration :  18   Loss :  41571.6696129
Iteration :  19   Loss :  55306.5535332
Iteration :  20   Loss :  43658.0948025
Iteration :  21   Loss :  21100.9379544
Iteration :  22   Loss :  58426.6740923
Iteration :  23   Loss :  20655.2988694
Iteration :  24   Loss :  38763.745056
Iteration :  25   Loss :  69773.0370617
Iteration :  26   Loss :  45860.1238889
Iteration :  27   Loss :  24943.5978309
Iteration :  28   Loss :  47990.4737126
Iteration :  29   Loss :  30039.4121415
Iteration :  30   Loss :  41573.7432822
Iteration :  31   Loss :  55306.2796362
Iteration :  32   Loss :  43658.0959252
Iteration :  33   Loss :  21100.9381008
Iteration :  34   Loss :  58426.6740862
Iteration :  35   Loss :  20655.2988691
Iteration :  36   Loss :  38763.745056
Iteration :  37   Loss :  69773.0370617
Iteration :  38   Loss :  45860.1238889
Iteration :  39   Loss :  24943.5978309
Iteration :  40   Loss :  47990.4737126
Iteration :  41   Loss :  30039.4121415
Iteration :  42   Loss :  41573.7432822
Iteration :  43   Loss :  55306.2796362
Iteration :  44   Loss :  43658.0959252
Iteration :  45   Loss :  21100.9381008
Iteration :  46   Loss :  58426.6740862
Iteration :  47   Loss :  20655.2988691
Iteration :  48   Loss :  38763.745056
Iteration :  49   Loss :  69773.0370617
Iteration :  50   Loss :  45860.1238889
Iteration :  51   Loss :  24943.5978309
Iteration :  52   Loss :  47990.4737126
Iteration :  53   Loss :  30039.4121415
Iteration :  54   Loss :  41573.7432822
Iteration :  55   Loss :  55306.2796362
Iteration :  56   Loss :  43658.0959252
Iteration :  57   Loss :  21100.9381008
Iteration :  58   Loss :  58426.6740862
Iteration :  59   Loss :  20655.2988691
Iteration :  60   Loss :  38763.745056
Iteration :  61   Loss :  69773.0370617
Iteration :  62   Loss :  45860.1238889
Iteration :  63   Loss :  24943.5978309
Iteration :  64   Loss :  47990.4737126
Iteration :  65   Loss :  30039.4121415
Iteration :  66   Loss :  41573.7432822
Iteration :  67   Loss :  55306.2796362
Iteration :  68   Loss :  43658.0959252
Iteration :  69   Loss :  21100.9381008
Iteration :  70   Loss :  58426.6740862
Iteration :  71   Loss :  20655.2988691
Iteration :  72   Loss :  38763.745056
Iteration :  73   Loss :  69773.0370617
Iteration :  74   Loss :  45860.1238889
Iteration :  75   Loss :  24943.5978309
Iteration :  76   Loss :  47990.4737126
Iteration :  77   Loss :  30039.4121415
Iteration :  78   Loss :  41573.7432822
Iteration :  79   Loss :  55306.2796362
Iteration :  80   Loss :  43658.0959252
Iteration :  81   Loss :  21100.9381008
Iteration :  82   Loss :  58426.6740862
Iteration :  83   Loss :  20655.2988691
Iteration :  84   Loss :  38763.745056
Iteration :  85   Loss :  69773.0370617
Iteration :  86   Loss :  45860.1238889
Iteration :  87   Loss :  24943.5978309
Iteration :  88   Loss :  47990.4737126
Iteration :  89   Loss :  30039.4121415
Iteration :  90   Loss :  41573.7432822
Iteration :  91   Loss :  55306.2796362
Iteration :  92   Loss :  43658.0959252
Iteration :  93   Loss :  21100.9381008
Iteration :  94   Loss :  58426.6740862
Iteration :  95   Loss :  20655.2988691
Iteration :  96   Loss :  38763.745056
Iteration :  97   Loss :  69773.0370617
Iteration :  98   Loss :  45860.1238889
Iteration :  99   Loss :  24943.5978309
[-0.48654485  0.00470411 -0.13572836 ...,  0.30626717 -0.1883139
  0.05919059]
CROSS VALIDATION 16
Iteration :  0   Loss :  42243.0288314
Iteration :  1   Loss :  55854.551964
Iteration :  2   Loss :  41822.2406229
Iteration :  3   Loss :  38276.8303246
Iteration :  4   Loss :  26058.9506308
Iteration :  5   Loss :  100419.231235
Iteration :  6   Loss :  37397.9374466
Iteration :  7   Loss :  103207.306101
Iteration :  8   Loss :  35206.3031415
Iteration :  9   Loss :  25892.8771548
Iteration :  10   Loss :  59568.2036298
Iteration :  11   Loss :  36780.1174731
Iteration :  12   Loss :  31752.0288913
Iteration :  13   Loss :  56446.1246967
Iteration :  14   Loss :  31204.8186235
Iteration :  15   Loss :  47917.1030469
Iteration :  16   Loss :  36083.0146971
Iteration :  17   Loss :  31723.5891429
Iteration :  18   Loss :  56445.9499342
Iteration :  19   Loss :  31204.8188745
Iteration :  20   Loss :  47917.1027599
Iteration :  21   Loss :  36083.0146444
Iteration :  22   Loss :  31723.5891407
Iteration :  23   Loss :  56445.9499342
Iteration :  24   Loss :  31204.8188745
Iteration :  25   Loss :  47917.1027599
Iteration :  26   Loss :  36083.0146444
Iteration :  27   Loss :  31723.5891407
Iteration :  28   Loss :  56445.9499342
Iteration :  29   Loss :  31204.8188745
Iteration :  30   Loss :  47917.1027599
Iteration :  31   Loss :  36083.0146444
Iteration :  32   Loss :  31723.5891407
Iteration :  33   Loss :  56445.9499342
Iteration :  34   Loss :  31204.8188745
Iteration :  35   Loss :  47917.1027599
Iteration :  36   Loss :  36083.0146444
Iteration :  37   Loss :  31723.5891407
Iteration :  38   Loss :  56445.9499342
Iteration :  39   Loss :  31204.8188745
Iteration :  40   Loss :  47917.1027599
Iteration :  41   Loss :  36083.0146444
Iteration :  42   Loss :  31723.5891407
Iteration :  43   Loss :  56445.9499342
Iteration :  44   Loss :  31204.8188745
Iteration :  45   Loss :  47917.1027599
Iteration :  46   Loss :  36083.0146444
Iteration :  47   Loss :  31723.5891407
Iteration :  48   Loss :  56445.9499342
Iteration :  49   Loss :  31204.8188745
Iteration :  50   Loss :  47917.1027599
Iteration :  51   Loss :  36083.0146444
Iteration :  52   Loss :  31723.5891407
Iteration :  53   Loss :  56445.9499342
Iteration :  54   Loss :  31204.8188745
Iteration :  55   Loss :  47917.1027599
Iteration :  56   Loss :  36083.0146444
Iteration :  57   Loss :  31723.5891407
Iteration :  58   Loss :  56445.9499342
Iteration :  59   Loss :  31204.8188745
Iteration :  60   Loss :  47917.1027599
Iteration :  61   Loss :  36083.0146444
Iteration :  62   Loss :  31723.5891407
Iteration :  63   Loss :  56445.9499342
Iteration :  64   Loss :  31204.8188745
Iteration :  65   Loss :  47917.1027599
Iteration :  66   Loss :  36083.0146444
Iteration :  67   Loss :  31723.5891407
Iteration :  68   Loss :  56445.9499342
Iteration :  69   Loss :  31204.8188745
Iteration :  70   Loss :  47917.1027599
Iteration :  71   Loss :  36083.0146444
Iteration :  72   Loss :  31723.5891407
Iteration :  73   Loss :  56445.9499342
Iteration :  74   Loss :  31204.8188745
Iteration :  75   Loss :  47917.1027599
Iteration :  76   Loss :  36083.0146444
Iteration :  77   Loss :  31723.5891407
Iteration :  78   Loss :  56445.9499342
Iteration :  79   Loss :  31204.8188745
Iteration :  80   Loss :  47917.1027599
Iteration :  81   Loss :  36083.0146444
Iteration :  82   Loss :  31723.5891407
Iteration :  83   Loss :  56445.9499342
Iteration :  84   Loss :  31204.8188745
Iteration :  85   Loss :  47917.1027599
Iteration :  86   Loss :  36083.0146444
Iteration :  87   Loss :  31723.5891407
Iteration :  88   Loss :  56445.9499342
Iteration :  89   Loss :  31204.8188745
Iteration :  90   Loss :  47917.1027599
Iteration :  91   Loss :  36083.0146444
Iteration :  92   Loss :  31723.5891407
Iteration :  93   Loss :  56445.9499342
Iteration :  94   Loss :  31204.8188745
Iteration :  95   Loss :  47917.1027599
Iteration :  96   Loss :  36083.0146444
Iteration :  97   Loss :  31723.5891407
Iteration :  98   Loss :  56445.9499342
Iteration :  99   Loss :  31204.8188745
[-0.64996287  0.09980621 -0.15564735 ...,  0.26878902 -0.2037186   0.0802795 ]
CROSS VALIDATION 17
Iteration :  0   Loss :  40983.8509212
Iteration :  1   Loss :  116351.51372
Iteration :  2   Loss :  33288.6595125
Iteration :  3   Loss :  99516.7719727
Iteration :  4   Loss :  34795.7396815
Iteration :  5   Loss :  16723.2442325
Iteration :  6   Loss :  59804.8389776
Iteration :  7   Loss :  82230.7231677
Iteration :  8   Loss :  29745.3468191
Iteration :  9   Loss :  33899.5459496
Iteration :  10   Loss :  41957.0496194
Iteration :  11   Loss :  43910.4157388
Iteration :  12   Loss :  40798.9284577
Iteration :  13   Loss :  20030.192324
Iteration :  14   Loss :  52011.8205957
Iteration :  15   Loss :  29895.3421367
Iteration :  16   Loss :  46915.3719018
Iteration :  17   Loss :  49743.1575496
Iteration :  18   Loss :  39366.1155355
Iteration :  19   Loss :  48727.4908347
Iteration :  20   Loss :  16861.6841901
Iteration :  21   Loss :  59789.5900326
Iteration :  22   Loss :  82229.0777215
Iteration :  23   Loss :  29745.1693784
Iteration :  24   Loss :  33899.5541236
Iteration :  25   Loss :  41957.0494197
Iteration :  26   Loss :  43910.4157279
Iteration :  27   Loss :  40798.9284575
Iteration :  28   Loss :  20030.192324
Iteration :  29   Loss :  52011.8205957
Iteration :  30   Loss :  29895.3421367
Iteration :  31   Loss :  46915.3719018
Iteration :  32   Loss :  49743.1575496
Iteration :  33   Loss :  39366.1155355
Iteration :  34   Loss :  48727.4908347
Iteration :  35   Loss :  16861.6841901
Iteration :  36   Loss :  59789.5900326
Iteration :  37   Loss :  82229.0777215
Iteration :  38   Loss :  29745.1693784
Iteration :  39   Loss :  33899.5541236
Iteration :  40   Loss :  41957.0494197
Iteration :  41   Loss :  43910.4157279
Iteration :  42   Loss :  40798.9284575
Iteration :  43   Loss :  20030.192324
Iteration :  44   Loss :  52011.8205957
Iteration :  45   Loss :  29895.3421367
Iteration :  46   Loss :  46915.3719018
Iteration :  47   Loss :  49743.1575496
Iteration :  48   Loss :  39366.1155355
Iteration :  49   Loss :  48727.4908347
Iteration :  50   Loss :  16861.6841901
Iteration :  51   Loss :  59789.5900326
Iteration :  52   Loss :  82229.0777215
Iteration :  53   Loss :  29745.1693784
Iteration :  54   Loss :  33899.5541236
Iteration :  55   Loss :  41957.0494197
Iteration :  56   Loss :  43910.4157279
Iteration :  57   Loss :  40798.9284575
Iteration :  58   Loss :  20030.192324
Iteration :  59   Loss :  52011.8205957
Iteration :  60   Loss :  29895.3421367
Iteration :  61   Loss :  46915.3719018
Iteration :  62   Loss :  49743.1575496
Iteration :  63   Loss :  39366.1155355
Iteration :  64   Loss :  48727.4908347
Iteration :  65   Loss :  16861.6841901
Iteration :  66   Loss :  59789.5900326
Iteration :  67   Loss :  82229.0777215
Iteration :  68   Loss :  29745.1693784
Iteration :  69   Loss :  33899.5541236
Iteration :  70   Loss :  41957.0494197
Iteration :  71   Loss :  43910.4157279
Iteration :  72   Loss :  40798.9284575
Iteration :  73   Loss :  20030.192324
Iteration :  74   Loss :  52011.8205957
Iteration :  75   Loss :  29895.3421367
Iteration :  76   Loss :  46915.3719018
Iteration :  77   Loss :  49743.1575496
Iteration :  78   Loss :  39366.1155355
Iteration :  79   Loss :  48727.4908347
Iteration :  80   Loss :  16861.6841901
Iteration :  81   Loss :  59789.5900326
Iteration :  82   Loss :  82229.0777215
Iteration :  83   Loss :  29745.1693784
Iteration :  84   Loss :  33899.5541236
Iteration :  85   Loss :  41957.0494197
Iteration :  86   Loss :  43910.4157279
Iteration :  87   Loss :  40798.9284575
Iteration :  88   Loss :  20030.192324
Iteration :  89   Loss :  52011.8205957
Iteration :  90   Loss :  29895.3421367
Iteration :  91   Loss :  46915.3719018
Iteration :  92   Loss :  49743.1575496
Iteration :  93   Loss :  39366.1155355
Iteration :  94   Loss :  48727.4908347
Iteration :  95   Loss :  16861.6841901
Iteration :  96   Loss :  59789.5900326
Iteration :  97   Loss :  82229.0777215
Iteration :  98   Loss :  29745.1693784
Iteration :  99   Loss :  33899.5541236
[-0.60591775  0.00518472 -0.18616185 ...,  0.29666683 -0.20935551
  0.07963135]
CROSS VALIDATION 18
Iteration :  0   Loss :  30660.2952585
Iteration :  1   Loss :  23444.7312639
Iteration :  2   Loss :  50385.4774258
Iteration :  3   Loss :  26472.8626919
Iteration :  4   Loss :  7608.56172554
Iteration :  5   Loss :  43302.7478267
Iteration :  6   Loss :  27700.685814
Iteration :  7   Loss :  41502.2256072
Iteration :  8   Loss :  37134.1421921
Iteration :  9   Loss :  30039.2945222
Iteration :  10   Loss :  34445.422885
Iteration :  11   Loss :  16811.3166179
Iteration :  12   Loss :  46122.8799997
Iteration :  13   Loss :  35592.9508425
Iteration :  14   Loss :  45980.7392581
Iteration :  15   Loss :  38659.4502118
Iteration :  16   Loss :  23442.6797988
Iteration :  17   Loss :  45682.2392558
Iteration :  18   Loss :  26769.8939579
Iteration :  19   Loss :  42532.583419
Iteration :  20   Loss :  44392.489208
Iteration :  21   Loss :  52728.5076321
Iteration :  22   Loss :  192.681604306
Iteration :  23   Loss :  52373.2475208
Iteration :  24   Loss :  19958.9552907
Iteration :  25   Loss :  41191.648273
Iteration :  26   Loss :  22154.3735107
Iteration :  27   Loss :  40638.5218136
Iteration :  28   Loss :  22228.0272528
Iteration :  29   Loss :  40637.7425646
Iteration :  30   Loss :  22228.1316709
Iteration :  31   Loss :  40637.7414599
Iteration :  32   Loss :  22228.131819
Iteration :  33   Loss :  40637.7414583
Iteration :  34   Loss :  22228.1318192
Iteration :  35   Loss :  40637.7414583
Iteration :  36   Loss :  22228.1318192
Iteration :  37   Loss :  40637.7414583
Iteration :  38   Loss :  22228.1318192
Iteration :  39   Loss :  40637.7414583
Iteration :  40   Loss :  22228.1318192
Iteration :  41   Loss :  40637.7414583
Iteration :  42   Loss :  22228.1318192
Iteration :  43   Loss :  40637.7414583
Iteration :  44   Loss :  22228.1318192
Iteration :  45   Loss :  40637.7414583
Iteration :  46   Loss :  22228.1318192
Iteration :  47   Loss :  40637.7414583
Iteration :  48   Loss :  22228.1318192
Iteration :  49   Loss :  40637.7414583
Iteration :  50   Loss :  22228.1318192
Iteration :  51   Loss :  40637.7414583
Iteration :  52   Loss :  22228.1318192
Iteration :  53   Loss :  40637.7414583
Iteration :  54   Loss :  22228.1318192
Iteration :  55   Loss :  40637.7414583
Iteration :  56   Loss :  22228.1318192
Iteration :  57   Loss :  40637.7414583
Iteration :  58   Loss :  22228.1318192
Iteration :  59   Loss :  40637.7414583
Iteration :  60   Loss :  22228.1318192
Iteration :  61   Loss :  40637.7414583
Iteration :  62   Loss :  22228.1318192
Iteration :  63   Loss :  40637.7414583
Iteration :  64   Loss :  22228.1318192
Iteration :  65   Loss :  40637.7414583
Iteration :  66   Loss :  22228.1318192
Iteration :  67   Loss :  40637.7414583
Iteration :  68   Loss :  22228.1318192
Iteration :  69   Loss :  40637.7414583
Iteration :  70   Loss :  22228.1318192
Iteration :  71   Loss :  40637.7414583
Iteration :  72   Loss :  22228.1318192
Iteration :  73   Loss :  40637.7414583
Iteration :  74   Loss :  22228.1318192
Iteration :  75   Loss :  40637.7414583
Iteration :  76   Loss :  22228.1318192
Iteration :  77   Loss :  40637.7414583
Iteration :  78   Loss :  22228.1318192
Iteration :  79   Loss :  40637.7414583
Iteration :  80   Loss :  22228.1318192
Iteration :  81   Loss :  40637.7414583
Iteration :  82   Loss :  22228.1318192
Iteration :  83   Loss :  40637.7414583
Iteration :  84   Loss :  22228.1318192
Iteration :  85   Loss :  40637.7414583
Iteration :  86   Loss :  22228.1318192
Iteration :  87   Loss :  40637.7414583
Iteration :  88   Loss :  22228.1318192
Iteration :  89   Loss :  40637.7414583
Iteration :  90   Loss :  22228.1318192
Iteration :  91   Loss :  40637.7414583
Iteration :  92   Loss :  22228.1318192
Iteration :  93   Loss :  40637.7414583
Iteration :  94   Loss :  22228.1318192
Iteration :  95   Loss :  40637.7414583
Iteration :  96   Loss :  22228.1318192
Iteration :  97   Loss :  40637.7414583
Iteration :  98   Loss :  22228.1318192
Iteration :  99   Loss :  40637.7414583
[-0.18579588 -0.85613967 -0.26746084 ...,  0.39247772 -0.46197958
 -0.03351256]
CROSS VALIDATION 19
Iteration :  0   Loss :  43255.9779801
Iteration :  1   Loss :  31029.3781826
Iteration :  2   Loss :  37224.3178458
Iteration :  3   Loss :  37781.934998
Iteration :  4   Loss :  30230.9449032
Iteration :  5   Loss :  43260.8990467
Iteration :  6   Loss :  44804.9564085
Iteration :  7   Loss :  94683.0053874
Iteration :  8   Loss :  28188.8849121
Iteration :  9   Loss :  32186.5500532
Iteration :  10   Loss :  47105.2490172
Iteration :  11   Loss :  33931.7002456
Iteration :  12   Loss :  52948.281924
Iteration :  13   Loss :  38461.2627448
Iteration :  14   Loss :  36806.3411293
Iteration :  15   Loss :  30216.2328673
Iteration :  16   Loss :  43259.069359
Iteration :  17   Loss :  44805.052526
Iteration :  18   Loss :  94683.0024048
Iteration :  19   Loss :  28188.8847636
Iteration :  20   Loss :  32186.5500533
Iteration :  21   Loss :  47105.2490172
Iteration :  22   Loss :  33931.7002456
Iteration :  23   Loss :  52948.281924
Iteration :  24   Loss :  38461.2627448
Iteration :  25   Loss :  36806.3411293
Iteration :  26   Loss :  30216.2328673
Iteration :  27   Loss :  43259.069359
Iteration :  28   Loss :  44805.052526
Iteration :  29   Loss :  94683.0024048
Iteration :  30   Loss :  28188.8847636
Iteration :  31   Loss :  32186.5500533
Iteration :  32   Loss :  47105.2490172
Iteration :  33   Loss :  33931.7002456
Iteration :  34   Loss :  52948.281924
Iteration :  35   Loss :  38461.2627448
Iteration :  36   Loss :  36806.3411293
Iteration :  37   Loss :  30216.2328673
Iteration :  38   Loss :  43259.069359
Iteration :  39   Loss :  44805.052526
Iteration :  40   Loss :  94683.0024048
Iteration :  41   Loss :  28188.8847636
Iteration :  42   Loss :  32186.5500533
Iteration :  43   Loss :  47105.2490172
Iteration :  44   Loss :  33931.7002456
Iteration :  45   Loss :  52948.281924
Iteration :  46   Loss :  38461.2627448
Iteration :  47   Loss :  36806.3411293
Iteration :  48   Loss :  30216.2328673
Iteration :  49   Loss :  43259.069359
Iteration :  50   Loss :  44805.052526
Iteration :  51   Loss :  94683.0024048
Iteration :  52   Loss :  28188.8847636
Iteration :  53   Loss :  32186.5500533
Iteration :  54   Loss :  47105.2490172
Iteration :  55   Loss :  33931.7002456
Iteration :  56   Loss :  52948.281924
Iteration :  57   Loss :  38461.2627448
Iteration :  58   Loss :  36806.3411293
Iteration :  59   Loss :  30216.2328673
Iteration :  60   Loss :  43259.069359
Iteration :  61   Loss :  44805.052526
Iteration :  62   Loss :  94683.0024048
Iteration :  63   Loss :  28188.8847636
Iteration :  64   Loss :  32186.5500533
Iteration :  65   Loss :  47105.2490172
Iteration :  66   Loss :  33931.7002456
Iteration :  67   Loss :  52948.281924
Iteration :  68   Loss :  38461.2627448
Iteration :  69   Loss :  36806.3411293
Iteration :  70   Loss :  30216.2328673
Iteration :  71   Loss :  43259.069359
Iteration :  72   Loss :  44805.052526
Iteration :  73   Loss :  94683.0024048
Iteration :  74   Loss :  28188.8847636
Iteration :  75   Loss :  32186.5500533
Iteration :  76   Loss :  47105.2490172
Iteration :  77   Loss :  33931.7002456
Iteration :  78   Loss :  52948.281924
Iteration :  79   Loss :  38461.2627448
Iteration :  80   Loss :  36806.3411293
Iteration :  81   Loss :  30216.2328673
Iteration :  82   Loss :  43259.069359
Iteration :  83   Loss :  44805.052526
Iteration :  84   Loss :  94683.0024048
Iteration :  85   Loss :  28188.8847636
Iteration :  86   Loss :  32186.5500533
Iteration :  87   Loss :  47105.2490172
Iteration :  88   Loss :  33931.7002456
Iteration :  89   Loss :  52948.281924
Iteration :  90   Loss :  38461.2627448
Iteration :  91   Loss :  36806.3411293
Iteration :  92   Loss :  30216.2328673
Iteration :  93   Loss :  43259.069359
Iteration :  94   Loss :  44805.052526
Iteration :  95   Loss :  94683.0024048
Iteration :  96   Loss :  28188.8847636
Iteration :  97   Loss :  32186.5500533
Iteration :  98   Loss :  47105.2490172
Iteration :  99   Loss :  33931.7002456
[-0.08747401 -0.52040011  0.00248436 ...,  0.269012   -0.26662311
  0.01340332]
Accuracy (Hinge Loss):	0.75
lmda : 0.2  eta : 0.1
Logistic Loss
CROSS VALIDATION 0
Iteration :  0   Loss :  37194.4276639
Iteration :  1   Loss :  14414.9484308
Iteration :  2   Loss :  42353.9346303
Iteration :  3   Loss :  53355.1675925
Iteration :  4   Loss :  18246.5222818
Iteration :  5   Loss :  18447.6263491
Iteration :  6   Loss :  34006.1617014
Iteration :  7   Loss :  59534.8067229
Iteration :  8   Loss :  9167.87618789
Iteration :  9   Loss :  25537.57953
Iteration :  10   Loss :  27672.5026212
Iteration :  11   Loss :  47767.3221327
Iteration :  12   Loss :  20675.6992957
Iteration :  13   Loss :  35276.9678812
Iteration :  14   Loss :  15967.2059701
Iteration :  15   Loss :  25939.2382016
Iteration :  16   Loss :  35756.7318742
Iteration :  17   Loss :  45847.7835419
Iteration :  18   Loss :  56542.3396835
Iteration :  19   Loss :  34505.9073872
Iteration :  20   Loss :  33542.2652247
Iteration :  21   Loss :  16281.3657584
Iteration :  22   Loss :  39611.995661
Iteration :  23   Loss :  12712.8468186
Iteration :  24   Loss :  33639.794692
Iteration :  25   Loss :  20025.9065153
Iteration :  26   Loss :  22831.4760181
Iteration :  27   Loss :  21371.8157396
Iteration :  28   Loss :  23088.6537442
Iteration :  29   Loss :  61445.6467778
Iteration :  30   Loss :  38689.9442062
Iteration :  31   Loss :  33677.5620526
Iteration :  32   Loss :  52932.5056213
Iteration :  33   Loss :  33009.9611498
Iteration :  34   Loss :  12726.1670979
Iteration :  35   Loss :  40701.1177185
Iteration :  36   Loss :  23522.1357256
Iteration :  37   Loss :  75739.5820798
Iteration :  38   Loss :  16554.4129107
Iteration :  39   Loss :  21600.9371905
Iteration :  40   Loss :  12291.1491398
Iteration :  41   Loss :  48916.8123728
Iteration :  42   Loss :  37303.2689987
Iteration :  43   Loss :  45094.4506108
Iteration :  44   Loss :  33151.4023263
Iteration :  45   Loss :  27876.2645855
Iteration :  46   Loss :  57670.0404208
Iteration :  47   Loss :  14725.8436686
Iteration :  48   Loss :  26375.1391145
Iteration :  49   Loss :  37796.8953977
Iteration :  50   Loss :  23416.4799131
Iteration :  51   Loss :  39967.1080549
Iteration :  52   Loss :  30587.2175258
Iteration :  53   Loss :  2072.33678226
Iteration :  54   Loss :  27.365982125
Iteration :  55   Loss :  0.454975626721
Iteration :  56   Loss :  23730.7067609
Iteration :  57   Loss :  5688.44779135
Iteration :  58   Loss :  39416.8475905
Iteration :  59   Loss :  39779.863295
Iteration :  60   Loss :  42662.9380405
Iteration :  61   Loss :  24074.8283988
Iteration :  62   Loss :  40693.4780593
Iteration :  63   Loss :  38714.1526683
Iteration :  64   Loss :  54489.3065448
Iteration :  65   Loss :  18263.0761762
Iteration :  66   Loss :  43119.9658873
Iteration :  67   Loss :  38312.4813307
Iteration :  68   Loss :  15928.9626518
Iteration :  69   Loss :  28994.7556578
Iteration :  70   Loss :  19888.3384295
Iteration :  71   Loss :  21159.5838038
Iteration :  72   Loss :  27760.6491195
Iteration :  73   Loss :  29910.0881987
Iteration :  74   Loss :  24766.2945488
Iteration :  75   Loss :  31961.260627
Iteration :  76   Loss :  26189.31576
Iteration :  77   Loss :  23841.4122377
Iteration :  78   Loss :  25274.6477878
Iteration :  79   Loss :  47534.8472764
Iteration :  80   Loss :  11062.4205117
Iteration :  81   Loss :  41403.6214859
Iteration :  82   Loss :  23262.6783372
Iteration :  83   Loss :  32153.7829045
Iteration :  84   Loss :  949.19850519
Iteration :  85   Loss :  12.5375017971
Iteration :  86   Loss :  45435.4262747
Iteration :  87   Loss :  35225.3122034
Iteration :  88   Loss :  30960.1340116
Iteration :  89   Loss :  57082.8020826
Iteration :  90   Loss :  24258.8074503
Iteration :  91   Loss :  44110.0923632
Iteration :  92   Loss :  15442.8640738
Iteration :  93   Loss :  26433.6521216
Iteration :  94   Loss :  13785.0671085
Iteration :  95   Loss :  42330.1497757
Iteration :  96   Loss :  61050.0496754
Iteration :  97   Loss :  35397.0259158
Iteration :  98   Loss :  27213.8692787
Iteration :  99   Loss :  22543.2443725
[-0.74676082 -0.77408928 -0.32917234 ...,  0.16648387 -0.09669144
 -0.06296081]
CROSS VALIDATION 1
Iteration :  0   Loss :  30914.8521853
Iteration :  1   Loss :  6412.25711348
Iteration :  2   Loss :  31505.9766312
Iteration :  3   Loss :  27892.5198186
Iteration :  4   Loss :  44248.9015333
Iteration :  5   Loss :  21628.1473276
Iteration :  6   Loss :  16672.0025637
Iteration :  7   Loss :  55608.0899704
Iteration :  8   Loss :  24495.0120331
Iteration :  9   Loss :  4859.07744837
Iteration :  10   Loss :  67808.7067672
Iteration :  11   Loss :  2353.18938447
Iteration :  12   Loss :  23625.3964806
Iteration :  13   Loss :  46877.1486063
Iteration :  14   Loss :  19954.9256832
Iteration :  15   Loss :  5748.41265894
Iteration :  16   Loss :  46493.5343345
Iteration :  17   Loss :  37148.216593
Iteration :  18   Loss :  45789.7515582
Iteration :  19   Loss :  25602.8158781
Iteration :  20   Loss :  15006.934076
Iteration :  21   Loss :  39293.5534985
Iteration :  22   Loss :  81375.3633174
Iteration :  23   Loss :  31668.5575948
Iteration :  24   Loss :  8281.1952437
Iteration :  25   Loss :  31913.411524
Iteration :  26   Loss :  53137.328988
Iteration :  27   Loss :  37557.6026668
Iteration :  28   Loss :  54339.1554603
Iteration :  29   Loss :  17716.6534248
Iteration :  30   Loss :  24579.4151087
Iteration :  31   Loss :  24456.182935
Iteration :  32   Loss :  48289.056603
Iteration :  33   Loss :  51488.6705802
Iteration :  34   Loss :  34604.9962322
Iteration :  35   Loss :  43056.5094048
Iteration :  36   Loss :  70090.2443638
Iteration :  37   Loss :  12978.1442152
Iteration :  38   Loss :  22361.9307538
Iteration :  39   Loss :  25577.6518963
Iteration :  40   Loss :  23554.1457015
Iteration :  41   Loss :  23215.5455703
Iteration :  42   Loss :  43429.8069102
Iteration :  43   Loss :  20017.220102
Iteration :  44   Loss :  27369.7820584
Iteration :  45   Loss :  20672.3053424
Iteration :  46   Loss :  42094.1692504
Iteration :  47   Loss :  37413.3348874
Iteration :  48   Loss :  21453.0638807
Iteration :  49   Loss :  18079.9765628
Iteration :  50   Loss :  27922.6523426
Iteration :  51   Loss :  16408.4811896
Iteration :  52   Loss :  35648.0648649
Iteration :  53   Loss :  28601.8683306
Iteration :  54   Loss :  36332.0903968
Iteration :  55   Loss :  24473.6543822
Iteration :  56   Loss :  45438.8796745
Iteration :  57   Loss :  58737.4167139
Iteration :  58   Loss :  15718.3953739
Iteration :  59   Loss :  34296.3466316
Iteration :  60   Loss :  18711.874537
Iteration :  61   Loss :  37455.1249114
Iteration :  62   Loss :  46054.4629724
Iteration :  63   Loss :  22378.8437388
Iteration :  64   Loss :  39512.4078146
Iteration :  65   Loss :  20581.2386193
Iteration :  66   Loss :  32257.1806963
Iteration :  67   Loss :  19344.9545196
Iteration :  68   Loss :  34617.4713221
Iteration :  69   Loss :  30426.2741237
Iteration :  70   Loss :  38832.0055383
Iteration :  71   Loss :  9674.35621518
Iteration :  72   Loss :  31508.6125055
Iteration :  73   Loss :  22119.6845446
Iteration :  74   Loss :  22398.3638148
Iteration :  75   Loss :  107636.693425
Iteration :  76   Loss :  32373.8492605
Iteration :  77   Loss :  34360.3183847
Iteration :  78   Loss :  26201.730167
Iteration :  79   Loss :  23390.9391784
Iteration :  80   Loss :  48501.0926452
Iteration :  81   Loss :  23925.2002451
Iteration :  82   Loss :  33199.2501437
Iteration :  83   Loss :  14580.6115895
Iteration :  84   Loss :  33675.6635724
Iteration :  85   Loss :  13032.4655685
Iteration :  86   Loss :  27359.4291362
Iteration :  87   Loss :  48683.5593966
Iteration :  88   Loss :  28588.4579454
Iteration :  89   Loss :  26366.089443
Iteration :  90   Loss :  38725.2708607
Iteration :  91   Loss :  23760.2531643
Iteration :  92   Loss :  18836.4833011
Iteration :  93   Loss :  93198.2690955
Iteration :  94   Loss :  77467.746896
Iteration :  95   Loss :  21208.4910512
Iteration :  96   Loss :  39800.6859895
Iteration :  97   Loss :  31896.481297
Iteration :  98   Loss :  23005.358052
Iteration :  99   Loss :  33877.4461144
[-0.71058378 -0.38055455  0.02383854 ...,  0.17132918 -0.02798744
 -0.04965779]
CROSS VALIDATION 2
Iteration :  0   Loss :  30079.6635242
Iteration :  1   Loss :  28340.9849882
Iteration :  2   Loss :  39709.6690113
Iteration :  3   Loss :  35094.6491779
Iteration :  4   Loss :  48096.7522922
Iteration :  5   Loss :  65913.1025874
Iteration :  6   Loss :  40318.1176165
Iteration :  7   Loss :  29245.1562437
Iteration :  8   Loss :  63032.7704949
Iteration :  9   Loss :  23681.1527032
Iteration :  10   Loss :  31657.5538407
Iteration :  11   Loss :  13065.3345336
Iteration :  12   Loss :  35182.0378339
Iteration :  13   Loss :  41605.6654646
Iteration :  14   Loss :  47075.2580338
Iteration :  15   Loss :  68270.7077482
Iteration :  16   Loss :  24947.8901995
Iteration :  17   Loss :  44967.7233868
Iteration :  18   Loss :  27527.2943657
Iteration :  19   Loss :  34813.2948576
Iteration :  20   Loss :  12962.2235176
Iteration :  21   Loss :  42330.6282645
Iteration :  22   Loss :  17862.03167
Iteration :  23   Loss :  30663.3409581
Iteration :  24   Loss :  21046.6122551
Iteration :  25   Loss :  34773.588142
Iteration :  26   Loss :  37076.7343993
Iteration :  27   Loss :  18600.1762873
Iteration :  28   Loss :  18177.310883
Iteration :  29   Loss :  39170.7936932
Iteration :  30   Loss :  28696.1633886
Iteration :  31   Loss :  44175.9497014
Iteration :  32   Loss :  16123.3435838
Iteration :  33   Loss :  43068.7438511
Iteration :  34   Loss :  33223.4502477
Iteration :  35   Loss :  50053.4799437
Iteration :  36   Loss :  3834.3572993
Iteration :  37   Loss :  34438.8482358
Iteration :  38   Loss :  27478.8154141
Iteration :  39   Loss :  32647.1622064
Iteration :  40   Loss :  8562.66918767
Iteration :  41   Loss :  34149.8583382
Iteration :  42   Loss :  49478.8406219
Iteration :  43   Loss :  21438.8879284
Iteration :  44   Loss :  39239.2093964
Iteration :  45   Loss :  50781.966048
Iteration :  46   Loss :  15269.1118944
Iteration :  47   Loss :  38729.2085473
Iteration :  48   Loss :  54004.8190393
Iteration :  49   Loss :  47595.962553
Iteration :  50   Loss :  19962.509651
Iteration :  51   Loss :  30673.582957
Iteration :  52   Loss :  23282.4449416
Iteration :  53   Loss :  51037.1144539
Iteration :  54   Loss :  9066.92133351
Iteration :  55   Loss :  31672.9977086
Iteration :  56   Loss :  26477.8818903
Iteration :  57   Loss :  34489.2797463
Iteration :  58   Loss :  14649.0829149
Iteration :  59   Loss :  28886.3547965
Iteration :  60   Loss :  27423.6505808
Iteration :  61   Loss :  33913.2382628
Iteration :  62   Loss :  34510.1428117
Iteration :  63   Loss :  42298.0896639
Iteration :  64   Loss :  28249.1597711
Iteration :  65   Loss :  38131.0014494
Iteration :  66   Loss :  9893.81633158
Iteration :  67   Loss :  23153.5744201
Iteration :  68   Loss :  20720.4704861
Iteration :  69   Loss :  38070.1965525
Iteration :  70   Loss :  22297.3475148
Iteration :  71   Loss :  38821.7227013
Iteration :  72   Loss :  34879.3562712
Iteration :  73   Loss :  14340.5387043
Iteration :  74   Loss :  22110.0726542
Iteration :  75   Loss :  55977.3347665
Iteration :  76   Loss :  21845.1103696
Iteration :  77   Loss :  36962.8060708
Iteration :  78   Loss :  46316.9303933
Iteration :  79   Loss :  5654.02610946
Iteration :  80   Loss :  21990.8411656
Iteration :  81   Loss :  23042.0315691
Iteration :  82   Loss :  35043.6390184
Iteration :  83   Loss :  8652.69343383
Iteration :  84   Loss :  44750.6813363
Iteration :  85   Loss :  27485.1596967
Iteration :  86   Loss :  50085.0513064
Iteration :  87   Loss :  64300.3034079
Iteration :  88   Loss :  29980.4837502
Iteration :  89   Loss :  21894.8215684
Iteration :  90   Loss :  39762.9962063
Iteration :  91   Loss :  36467.3947171
Iteration :  92   Loss :  30421.874231
Iteration :  93   Loss :  11178.8051514
Iteration :  94   Loss :  14363.0085354
Iteration :  95   Loss :  31887.3381611
Iteration :  96   Loss :  32934.4299559
Iteration :  97   Loss :  32521.0389561
Iteration :  98   Loss :  25227.312962
Iteration :  99   Loss :  67554.9928878
[ 0.15225431 -0.56032573 -0.19028107 ...,  0.38911047 -0.06244642
  0.20469352]
CROSS VALIDATION 3
Iteration :  0   Loss :  34942.6711106
Iteration :  1   Loss :  31812.4595082
Iteration :  2   Loss :  48456.7566506
Iteration :  3   Loss :  30033.5213432
Iteration :  4   Loss :  37317.4013262
Iteration :  5   Loss :  78342.0672071
Iteration :  6   Loss :  32148.8300288
Iteration :  7   Loss :  17444.0918274
Iteration :  8   Loss :  47472.911686
Iteration :  9   Loss :  70825.7725246
Iteration :  10   Loss :  33204.3870981
Iteration :  11   Loss :  26477.9527813
Iteration :  12   Loss :  71245.6627556
Iteration :  13   Loss :  67564.0029296
Iteration :  14   Loss :  30266.5941768
Iteration :  15   Loss :  13024.1273268
Iteration :  16   Loss :  21987.6695577
Iteration :  17   Loss :  33003.0847851
Iteration :  18   Loss :  55250.5002255
Iteration :  19   Loss :  81772.7484516
Iteration :  20   Loss :  20496.0163326
Iteration :  21   Loss :  34265.3663875
Iteration :  22   Loss :  49414.8133035
Iteration :  23   Loss :  7174.57211191
Iteration :  24   Loss :  34716.856599
Iteration :  25   Loss :  28495.7147734
Iteration :  26   Loss :  36212.9978029
Iteration :  27   Loss :  21823.9275423
Iteration :  28   Loss :  35690.6771483
Iteration :  29   Loss :  21184.8302253
Iteration :  30   Loss :  78660.2812357
Iteration :  31   Loss :  29217.7097269
Iteration :  32   Loss :  27133.1140338
Iteration :  33   Loss :  17992.6998586
Iteration :  34   Loss :  31497.0641684
Iteration :  35   Loss :  28260.7410785
Iteration :  36   Loss :  36231.9796353
Iteration :  37   Loss :  21826.6722232
Iteration :  38   Loss :  35690.6651891
Iteration :  39   Loss :  21184.7599919
Iteration :  40   Loss :  78660.2844129
Iteration :  41   Loss :  29217.7099146
Iteration :  42   Loss :  27133.11404
Iteration :  43   Loss :  17992.6998743
Iteration :  44   Loss :  31497.0641681
Iteration :  45   Loss :  28260.7410784
Iteration :  46   Loss :  36231.9796353
Iteration :  47   Loss :  21826.6722232
Iteration :  48   Loss :  35690.6651891
Iteration :  49   Loss :  21184.7599919
Iteration :  50   Loss :  78660.2844129
Iteration :  51   Loss :  29217.7099146
Iteration :  52   Loss :  27133.11404
Iteration :  53   Loss :  17992.6998743
Iteration :  54   Loss :  31497.0641681
Iteration :  55   Loss :  28260.7410784
Iteration :  56   Loss :  36231.9796353
Iteration :  57   Loss :  21826.6722232
Iteration :  58   Loss :  35690.6651891
Iteration :  59   Loss :  21184.7599919
Iteration :  60   Loss :  78660.2844129
Iteration :  61   Loss :  29217.7099146
Iteration :  62   Loss :  27133.11404
Iteration :  63   Loss :  17992.6998743
Iteration :  64   Loss :  31497.0641681
Iteration :  65   Loss :  28260.7410784
Iteration :  66   Loss :  36231.9796353
Iteration :  67   Loss :  21826.6722232
Iteration :  68   Loss :  35690.6651891
Iteration :  69   Loss :  21184.7599919
Iteration :  70   Loss :  78660.2844129
Iteration :  71   Loss :  29217.7099146
Iteration :  72   Loss :  27133.11404
Iteration :  73   Loss :  17992.6998743
Iteration :  74   Loss :  31497.0641681
Iteration :  75   Loss :  28260.7410784
Iteration :  76   Loss :  36231.9796353
Iteration :  77   Loss :  21826.6722232
Iteration :  78   Loss :  35690.6651891
Iteration :  79   Loss :  21184.7599919
Iteration :  80   Loss :  78660.2844129
Iteration :  81   Loss :  29217.7099146
Iteration :  82   Loss :  27133.11404
Iteration :  83   Loss :  17992.6998743
Iteration :  84   Loss :  31497.0641681
Iteration :  85   Loss :  28260.7410784
Iteration :  86   Loss :  36231.9796353
Iteration :  87   Loss :  21826.6722232
Iteration :  88   Loss :  35690.6651891
Iteration :  89   Loss :  21184.7599919
Iteration :  90   Loss :  78660.2844129
Iteration :  91   Loss :  29217.7099146
Iteration :  92   Loss :  27133.11404
Iteration :  93   Loss :  17992.6998743
Iteration :  94   Loss :  31497.0641681
Iteration :  95   Loss :  28260.7410784
Iteration :  96   Loss :  36231.9796353
Iteration :  97   Loss :  21826.6722232
Iteration :  98   Loss :  35690.6651891
Iteration :  99   Loss :  21184.7599919
[-0.60075119 -0.66483733  0.57865742 ...,  0.21071701  0.05149476
 -0.12705079]
CROSS VALIDATION 4
Iteration :  0   Loss :  34942.6711106
Iteration :  1   Loss :  31812.4595082
Iteration :  2   Loss :  48456.7566506
Iteration :  3   Loss :  30033.5213432
Iteration :  4   Loss :  37317.4013262
Iteration :  5   Loss :  71624.9003522
Iteration :  6   Loss :  34832.0892577
Iteration :  7   Loss :  4079.42549215
Iteration :  8   Loss :  17603.8836573
Iteration :  9   Loss :  17743.5960811
Iteration :  10   Loss :  28177.8974468
Iteration :  11   Loss :  9491.36271391
Iteration :  12   Loss :  24018.0234
Iteration :  13   Loss :  30563.4301186
Iteration :  14   Loss :  33806.7083471
Iteration :  15   Loss :  25573.0310029
Iteration :  16   Loss :  36727.2078837
Iteration :  17   Loss :  25876.2668672
Iteration :  18   Loss :  40064.160502
Iteration :  19   Loss :  19787.4490744
Iteration :  20   Loss :  47100.3695169
Iteration :  21   Loss :  12592.3104449
Iteration :  22   Loss :  24760.6848561
Iteration :  23   Loss :  25206.6041149
Iteration :  24   Loss :  43065.8686459
Iteration :  25   Loss :  3111.67492512
Iteration :  26   Loss :  50554.9065892
Iteration :  27   Loss :  20240.9622759
Iteration :  28   Loss :  39394.9138327
Iteration :  29   Loss :  43207.8347312
Iteration :  30   Loss :  24817.5297963
Iteration :  31   Loss :  13336.6091401
Iteration :  32   Loss :  49132.5286797
Iteration :  33   Loss :  34723.2642484
Iteration :  34   Loss :  30947.7401485
Iteration :  35   Loss :  7411.56323374
Iteration :  36   Loss :  55066.0740504
Iteration :  37   Loss :  7326.56902744
Iteration :  38   Loss :  22346.9329653
Iteration :  39   Loss :  28852.53136
Iteration :  40   Loss :  43633.4973285
Iteration :  41   Loss :  39980.8909709
Iteration :  42   Loss :  96700.10984
Iteration :  43   Loss :  49756.317803
Iteration :  44   Loss :  15547.2673575
Iteration :  45   Loss :  31126.6028032
Iteration :  46   Loss :  53101.545259
Iteration :  47   Loss :  28877.3328616
Iteration :  48   Loss :  17525.6237545
Iteration :  49   Loss :  22607.022178
Iteration :  50   Loss :  31460.2833778
Iteration :  51   Loss :  55221.1600016
Iteration :  52   Loss :  80652.6553458
Iteration :  53   Loss :  18713.0428245
Iteration :  54   Loss :  63145.4674219
Iteration :  55   Loss :  61122.3148956
Iteration :  56   Loss :  47810.0952771
Iteration :  57   Loss :  22122.0807257
Iteration :  58   Loss :  36070.3148699
Iteration :  59   Loss :  33032.096989
Iteration :  60   Loss :  53404.7847742
Iteration :  61   Loss :  29412.0379012
Iteration :  62   Loss :  60059.2224815
Iteration :  63   Loss :  44400.5721401
Iteration :  64   Loss :  44940.1923416
Iteration :  65   Loss :  27101.3758016
Iteration :  66   Loss :  29236.0513651
Iteration :  67   Loss :  9023.41412195
Iteration :  68   Loss :  68430.0064673
Iteration :  69   Loss :  38276.7232437
Iteration :  70   Loss :  17289.8438627
Iteration :  71   Loss :  45717.82981
Iteration :  72   Loss :  63018.9973921
Iteration :  73   Loss :  37518.5158259
Iteration :  74   Loss :  17542.6005696
Iteration :  75   Loss :  36857.3418665
Iteration :  76   Loss :  5419.49776996
Iteration :  77   Loss :  27879.7395013
Iteration :  78   Loss :  22991.3663612
Iteration :  79   Loss :  30453.5304926
Iteration :  80   Loss :  13305.4346421
Iteration :  81   Loss :  60923.8773276
Iteration :  82   Loss :  48549.5160838
Iteration :  83   Loss :  59926.8267089
Iteration :  84   Loss :  34100.5635545
Iteration :  85   Loss :  68640.0710821
Iteration :  86   Loss :  53081.8585579
Iteration :  87   Loss :  13748.695623
Iteration :  88   Loss :  90996.4246076
Iteration :  89   Loss :  54543.2240673
Iteration :  90   Loss :  13968.506488
Iteration :  91   Loss :  27583.9028283
Iteration :  92   Loss :  27582.162814
Iteration :  93   Loss :  47062.4779774
Iteration :  94   Loss :  19431.5487607
Iteration :  95   Loss :  2282.31855216
Iteration :  96   Loss :  55618.6273073
Iteration :  97   Loss :  21817.8352535
Iteration :  98   Loss :  42175.5997313
Iteration :  99   Loss :  12139.8862281
[-0.19119074 -0.0088389   0.57936797 ...,  0.31648725  0.03451915
  0.06878086]
CROSS VALIDATION 5
Iteration :  0   Loss :  31217.7145693
Iteration :  1   Loss :  27649.8328682
Iteration :  2   Loss :  48580.6527508
Iteration :  3   Loss :  10922.6500564
Iteration :  4   Loss :  30319.469922
Iteration :  5   Loss :  42533.5138727
Iteration :  6   Loss :  26913.7384258
Iteration :  7   Loss :  13813.4400386
Iteration :  8   Loss :  40079.7220259
Iteration :  9   Loss :  9278.37342655
Iteration :  10   Loss :  29555.4244708
Iteration :  11   Loss :  13790.9487621
Iteration :  12   Loss :  27428.9199242
Iteration :  13   Loss :  37160.2718767
Iteration :  14   Loss :  48849.3134733
Iteration :  15   Loss :  25439.656523
Iteration :  16   Loss :  74015.0318733
Iteration :  17   Loss :  29703.5196254
Iteration :  18   Loss :  15049.5374702
Iteration :  19   Loss :  29743.9223283
Iteration :  20   Loss :  31380.8005015
Iteration :  21   Loss :  18867.0955663
Iteration :  22   Loss :  29481.7252956
Iteration :  23   Loss :  4850.70552652
Iteration :  24   Loss :  24205.8959115
Iteration :  25   Loss :  52332.4092411
Iteration :  26   Loss :  29684.4310797
Iteration :  27   Loss :  40334.1932868
Iteration :  28   Loss :  4597.98327698
Iteration :  29   Loss :  33778.2523701
Iteration :  30   Loss :  62913.4676496
Iteration :  31   Loss :  19554.6195582
Iteration :  32   Loss :  58163.9190258
Iteration :  33   Loss :  13273.4493856
Iteration :  34   Loss :  32364.6737976
Iteration :  35   Loss :  25327.7400204
Iteration :  36   Loss :  34337.5742559
Iteration :  37   Loss :  18775.2041171
Iteration :  38   Loss :  21057.4838835
Iteration :  39   Loss :  63958.0753272
Iteration :  40   Loss :  27305.41043
Iteration :  41   Loss :  22680.4147177
Iteration :  42   Loss :  36088.7359225
Iteration :  43   Loss :  71205.8495695
Iteration :  44   Loss :  27569.0342204
Iteration :  45   Loss :  26893.0365925
Iteration :  46   Loss :  33404.7977316
Iteration :  47   Loss :  1436.40905028
Iteration :  48   Loss :  18.9683185723
Iteration :  49   Loss :  98197.6805216
Iteration :  50   Loss :  78639.9456576
Iteration :  51   Loss :  7745.04027017
Iteration :  52   Loss :  32340.7575542
Iteration :  53   Loss :  15121.1196575
Iteration :  54   Loss :  59048.9184593
Iteration :  55   Loss :  8012.15279003
Iteration :  56   Loss :  25293.5237355
Iteration :  57   Loss :  12965.9966421
Iteration :  58   Loss :  26191.2492909
Iteration :  59   Loss :  49860.6875102
Iteration :  60   Loss :  49329.1378651
Iteration :  61   Loss :  70668.4885627
Iteration :  62   Loss :  32829.1717765
Iteration :  63   Loss :  9852.25448188
Iteration :  64   Loss :  27616.9377131
Iteration :  65   Loss :  13318.2499281
Iteration :  66   Loss :  46619.3224915
Iteration :  67   Loss :  11059.7075109
Iteration :  68   Loss :  33216.6844484
Iteration :  69   Loss :  8249.88998479
Iteration :  70   Loss :  38550.0624699
Iteration :  71   Loss :  27461.0462606
Iteration :  72   Loss :  26552.5223192
Iteration :  73   Loss :  63224.7414635
Iteration :  74   Loss :  37259.2470941
Iteration :  75   Loss :  27634.2530784
Iteration :  76   Loss :  28861.3814983
Iteration :  77   Loss :  46385.3665006
Iteration :  78   Loss :  1896.67357592
Iteration :  79   Loss :  41078.2782608
Iteration :  80   Loss :  10344.4679658
Iteration :  81   Loss :  27818.4470254
Iteration :  82   Loss :  75696.2159389
Iteration :  83   Loss :  3597.10229042
Iteration :  84   Loss :  30312.3774303
Iteration :  85   Loss :  19398.9155247
Iteration :  86   Loss :  29397.9025346
Iteration :  87   Loss :  1411.68096246
Iteration :  88   Loss :  38405.4519639
Iteration :  89   Loss :  6452.69801811
Iteration :  90   Loss :  19288.2185308
Iteration :  91   Loss :  40270.8334189
Iteration :  92   Loss :  31631.299081
Iteration :  93   Loss :  53002.8227467
Iteration :  94   Loss :  23472.455441
Iteration :  95   Loss :  23489.7015884
Iteration :  96   Loss :  51483.2852586
Iteration :  97   Loss :  3924.96823553
Iteration :  98   Loss :  51740.444855
Iteration :  99   Loss :  21211.3432855
[-0.34426539 -0.77800445  0.17581606 ...,  0.32572853 -0.03504983
  0.06225433]
CROSS VALIDATION 6
Iteration :  0   Loss :  34942.6711106
Iteration :  1   Loss :  31812.4595082
Iteration :  2   Loss :  48363.5212941
Iteration :  3   Loss :  30043.443717
Iteration :  4   Loss :  37304.8354938
Iteration :  5   Loss :  78352.63002
Iteration :  6   Loss :  31924.393706
Iteration :  7   Loss :  20640.2921199
Iteration :  8   Loss :  44523.9266053
Iteration :  9   Loss :  20221.3224422
Iteration :  10   Loss :  39453.80804
Iteration :  11   Loss :  10332.4363434
Iteration :  12   Loss :  75153.8123972
Iteration :  13   Loss :  51333.9285412
Iteration :  14   Loss :  25228.4032705
Iteration :  15   Loss :  47316.6554703
Iteration :  16   Loss :  25921.7576942
Iteration :  17   Loss :  43916.6083922
Iteration :  18   Loss :  15564.1446747
Iteration :  19   Loss :  33525.1163053
Iteration :  20   Loss :  22244.0272737
Iteration :  21   Loss :  33971.5995205
Iteration :  22   Loss :  18223.1942427
Iteration :  23   Loss :  18830.168304
Iteration :  24   Loss :  25236.0114522
Iteration :  25   Loss :  34064.2147253
Iteration :  26   Loss :  38309.9311836
Iteration :  27   Loss :  20099.5797397
Iteration :  28   Loss :  29412.0936345
Iteration :  29   Loss :  21754.1066521
Iteration :  30   Loss :  11848.702694
Iteration :  31   Loss :  34617.9525338
Iteration :  32   Loss :  41933.4938966
Iteration :  33   Loss :  57956.3893518
Iteration :  34   Loss :  6992.3558982
Iteration :  35   Loss :  42760.5773577
Iteration :  36   Loss :  33557.8566213
Iteration :  37   Loss :  32974.6527111
Iteration :  38   Loss :  14895.6308786
Iteration :  39   Loss :  40332.0739381
Iteration :  40   Loss :  27560.2992174
Iteration :  41   Loss :  31873.7928882
Iteration :  42   Loss :  24527.6868454
Iteration :  43   Loss :  25420.7726614
Iteration :  44   Loss :  47545.4145129
Iteration :  45   Loss :  31079.4870184
Iteration :  46   Loss :  36849.7168731
Iteration :  47   Loss :  49212.2218956
Iteration :  48   Loss :  15937.9251701
Iteration :  49   Loss :  40683.1797638
Iteration :  50   Loss :  19196.8729841
Iteration :  51   Loss :  36576.8997677
Iteration :  52   Loss :  20070.5648998
Iteration :  53   Loss :  20995.827022
Iteration :  54   Loss :  38676.318183
Iteration :  55   Loss :  17505.5364684
Iteration :  56   Loss :  40259.9611664
Iteration :  57   Loss :  35661.4731536
Iteration :  58   Loss :  42975.1420396
Iteration :  59   Loss :  39549.5279692
Iteration :  60   Loss :  23383.6556461
Iteration :  61   Loss :  19633.6189216
Iteration :  62   Loss :  26384.2583401
Iteration :  63   Loss :  31896.9886071
Iteration :  64   Loss :  34204.9990149
Iteration :  65   Loss :  27980.0198817
Iteration :  66   Loss :  41048.7317364
Iteration :  67   Loss :  55720.092247
Iteration :  68   Loss :  48509.9346606
Iteration :  69   Loss :  36108.2715614
Iteration :  70   Loss :  38782.9206483
Iteration :  71   Loss :  38389.7156189
Iteration :  72   Loss :  43073.3258473
Iteration :  73   Loss :  36882.8870189
Iteration :  74   Loss :  19124.7101274
Iteration :  75   Loss :  6956.16034681
Iteration :  76   Loss :  35442.3792174
Iteration :  77   Loss :  35393.4052711
Iteration :  78   Loss :  38832.9345828
Iteration :  79   Loss :  16078.6699583
Iteration :  80   Loss :  48071.9988201
Iteration :  81   Loss :  8343.98232407
Iteration :  82   Loss :  24601.6680436
Iteration :  83   Loss :  38002.7615447
Iteration :  84   Loss :  36526.1727392
Iteration :  85   Loss :  26452.981008
Iteration :  86   Loss :  7514.13791607
Iteration :  87   Loss :  36366.0442853
Iteration :  88   Loss :  8941.86766356
Iteration :  89   Loss :  31351.2011373
Iteration :  90   Loss :  30751.7052288
Iteration :  91   Loss :  63398.903839
Iteration :  92   Loss :  8932.19776376
Iteration :  93   Loss :  36164.3718233
Iteration :  94   Loss :  25012.4936147
Iteration :  95   Loss :  25401.3218381
Iteration :  96   Loss :  34905.2162407
Iteration :  97   Loss :  4745.09834495
Iteration :  98   Loss :  46315.4060072
Iteration :  99   Loss :  24989.0080838
[-0.60714909 -0.67261065  0.33889067 ...,  0.23028311  0.28218901
  0.0045198 ]
CROSS VALIDATION 7
Iteration :  0   Loss :  34942.6711106
Iteration :  1   Loss :  31812.4595082
Iteration :  2   Loss :  48363.5212941
Iteration :  3   Loss :  28071.2033032
Iteration :  4   Loss :  36704.9344669
Iteration :  5   Loss :  8437.14157177
Iteration :  6   Loss :  42142.4774045
Iteration :  7   Loss :  19884.5200518
Iteration :  8   Loss :  39578.8594583
Iteration :  9   Loss :  40841.8968096
Iteration :  10   Loss :  39839.7235782
Iteration :  11   Loss :  18985.247664
Iteration :  12   Loss :  22586.2581128
Iteration :  13   Loss :  35695.5665001
Iteration :  14   Loss :  27344.8965615
Iteration :  15   Loss :  39345.1865983
Iteration :  16   Loss :  43976.5202778
Iteration :  17   Loss :  38594.7336617
Iteration :  18   Loss :  23372.3219128
Iteration :  19   Loss :  19633.7573251
Iteration :  20   Loss :  26384.1022497
Iteration :  21   Loss :  31897.0050491
Iteration :  22   Loss :  34204.9975043
Iteration :  23   Loss :  27980.0199846
Iteration :  24   Loss :  41048.7317694
Iteration :  25   Loss :  55720.0922475
Iteration :  26   Loss :  48509.9346608
Iteration :  27   Loss :  34769.3554999
Iteration :  28   Loss :  43855.2936513
Iteration :  29   Loss :  34457.4146651
Iteration :  30   Loss :  43073.3258473
Iteration :  31   Loss :  36882.8870189
Iteration :  32   Loss :  19124.7101274
Iteration :  33   Loss :  6956.16034681
Iteration :  34   Loss :  35442.3792174
Iteration :  35   Loss :  35393.4052711
Iteration :  36   Loss :  38832.9345828
Iteration :  37   Loss :  16078.6699583
Iteration :  38   Loss :  48071.9988201
Iteration :  39   Loss :  8254.23332053
Iteration :  40   Loss :  46770.4126041
Iteration :  41   Loss :  12493.238169
Iteration :  42   Loss :  44636.9579316
Iteration :  43   Loss :  26889.2170213
Iteration :  44   Loss :  17275.0128579
Iteration :  45   Loss :  27768.6883771
Iteration :  46   Loss :  39288.9348798
Iteration :  47   Loss :  53829.9913913
Iteration :  48   Loss :  25357.0362334
Iteration :  49   Loss :  9327.71923999
Iteration :  50   Loss :  57927.9052068
Iteration :  51   Loss :  29459.2613462
Iteration :  52   Loss :  25799.5225773
Iteration :  53   Loss :  33222.0968541
Iteration :  54   Loss :  44064.9815348
Iteration :  55   Loss :  51687.1274009
Iteration :  56   Loss :  13934.4422565
Iteration :  57   Loss :  24152.3331122
Iteration :  58   Loss :  20880.1291735
Iteration :  59   Loss :  34046.0220416
Iteration :  60   Loss :  23876.3310258
Iteration :  61   Loss :  29914.5507032
Iteration :  62   Loss :  36802.9992038
Iteration :  63   Loss :  38725.3363659
Iteration :  64   Loss :  31822.00941
Iteration :  65   Loss :  18412.1029382
Iteration :  66   Loss :  40986.4415449
Iteration :  67   Loss :  41554.2183311
Iteration :  68   Loss :  21378.1236189
Iteration :  69   Loss :  14916.0155365
Iteration :  70   Loss :  17769.6318433
Iteration :  71   Loss :  36021.6049373
Iteration :  72   Loss :  26202.6471669
Iteration :  73   Loss :  38933.4740078
Iteration :  74   Loss :  55285.6755679
Iteration :  75   Loss :  30229.1180307
Iteration :  76   Loss :  14213.4131014
Iteration :  77   Loss :  31388.4238839
Iteration :  78   Loss :  40736.2340035
Iteration :  79   Loss :  13951.1861885
Iteration :  80   Loss :  5278.78949998
Iteration :  81   Loss :  28853.2002755
Iteration :  82   Loss :  47308.7350488
Iteration :  83   Loss :  39722.4470728
Iteration :  84   Loss :  41645.9866087
Iteration :  85   Loss :  34033.4144294
Iteration :  86   Loss :  20404.9938606
Iteration :  87   Loss :  23333.0343228
Iteration :  88   Loss :  22069.6454146
Iteration :  89   Loss :  21722.8093509
Iteration :  90   Loss :  26032.5229821
Iteration :  91   Loss :  18487.3369945
Iteration :  92   Loss :  86096.7733497
Iteration :  93   Loss :  32854.2370791
Iteration :  94   Loss :  11452.2903477
Iteration :  95   Loss :  37205.8935393
Iteration :  96   Loss :  32522.7901127
Iteration :  97   Loss :  21656.6357095
Iteration :  98   Loss :  46362.1478328
Iteration :  99   Loss :  44254.0223824
[-1.35630149  0.20377444 -0.37894531 ...,  0.64459051 -0.14134173
  0.11423242]
CROSS VALIDATION 8
Iteration :  0   Loss :  34942.6711106
Iteration :  1   Loss :  3523.13776186
Iteration :  2   Loss :  32372.840459
Iteration :  3   Loss :  2876.64948345
Iteration :  4   Loss :  32670.8903094
Iteration :  5   Loss :  10203.0715229
Iteration :  6   Loss :  42647.6952274
Iteration :  7   Loss :  14995.6426429
Iteration :  8   Loss :  30639.5706412
Iteration :  9   Loss :  5469.36721898
Iteration :  10   Loss :  50986.5131832
Iteration :  11   Loss :  52752.7218861
Iteration :  12   Loss :  27626.8768421
Iteration :  13   Loss :  40217.9895463
Iteration :  14   Loss :  28048.6403278
Iteration :  15   Loss :  27085.4806137
Iteration :  16   Loss :  43078.6680399
Iteration :  17   Loss :  21707.3010723
Iteration :  18   Loss :  22432.7319963
Iteration :  19   Loss :  18751.322957
Iteration :  20   Loss :  46677.5978195
Iteration :  21   Loss :  9505.43254374
Iteration :  22   Loss :  65473.0917259
Iteration :  23   Loss :  29220.8373365
Iteration :  24   Loss :  43380.5137347
Iteration :  25   Loss :  20177.2806825
Iteration :  26   Loss :  27600.1304765
Iteration :  27   Loss :  12024.9803198
Iteration :  28   Loss :  39667.5856468
Iteration :  29   Loss :  45637.4726212
Iteration :  30   Loss :  34332.0258821
Iteration :  31   Loss :  6769.80355045
Iteration :  32   Loss :  27479.9320385
Iteration :  33   Loss :  16894.3504525
Iteration :  34   Loss :  24663.6759033
Iteration :  35   Loss :  63658.1502987
Iteration :  36   Loss :  16949.8871332
Iteration :  37   Loss :  24094.2103355
Iteration :  38   Loss :  33348.2633063
Iteration :  39   Loss :  15569.9233451
Iteration :  40   Loss :  46242.4978545
Iteration :  41   Loss :  13401.8194193
Iteration :  42   Loss :  39814.9535109
Iteration :  43   Loss :  8758.12259153
Iteration :  44   Loss :  52008.1118098
Iteration :  45   Loss :  26439.7152079
Iteration :  46   Loss :  41601.5262827
Iteration :  47   Loss :  8737.74095484
Iteration :  48   Loss :  34138.706938
Iteration :  49   Loss :  14700.6592516
Iteration :  50   Loss :  29538.9279044
Iteration :  51   Loss :  22874.5301605
Iteration :  52   Loss :  41632.5344797
Iteration :  53   Loss :  20409.7706342
Iteration :  54   Loss :  20980.1837783
Iteration :  55   Loss :  54217.5331606
Iteration :  56   Loss :  28482.2386288
Iteration :  57   Loss :  22184.431019
Iteration :  58   Loss :  46919.8235489
Iteration :  59   Loss :  9164.91458087
Iteration :  60   Loss :  44309.2333623
Iteration :  61   Loss :  26209.1413528
Iteration :  62   Loss :  43877.8635156
Iteration :  63   Loss :  21692.2016194
Iteration :  64   Loss :  22444.7749547
Iteration :  65   Loss :  18750.8073896
Iteration :  66   Loss :  46677.9669923
Iteration :  67   Loss :  9505.42951774
Iteration :  68   Loss :  65473.0908581
Iteration :  69   Loss :  29220.8375311
Iteration :  70   Loss :  43380.5137243
Iteration :  71   Loss :  20177.2806877
Iteration :  72   Loss :  27600.1304765
Iteration :  73   Loss :  12024.9803198
Iteration :  74   Loss :  39667.5856468
Iteration :  75   Loss :  45637.4726212
Iteration :  76   Loss :  34332.0258821
Iteration :  77   Loss :  6769.80355045
Iteration :  78   Loss :  27479.9320385
Iteration :  79   Loss :  16894.3504525
Iteration :  80   Loss :  24663.6759033
Iteration :  81   Loss :  63658.1502987
Iteration :  82   Loss :  16949.8871332
Iteration :  83   Loss :  24094.2103355
Iteration :  84   Loss :  33348.2633063
Iteration :  85   Loss :  15569.9233451
Iteration :  86   Loss :  46242.4978545
Iteration :  87   Loss :  13401.8194193
Iteration :  88   Loss :  39814.9535109
Iteration :  89   Loss :  8758.12259153
Iteration :  90   Loss :  52008.1118098
Iteration :  91   Loss :  26439.7152079
Iteration :  92   Loss :  41601.5262827
Iteration :  93   Loss :  8737.74095484
Iteration :  94   Loss :  34138.706938
Iteration :  95   Loss :  14700.6592516
Iteration :  96   Loss :  29538.9279044
Iteration :  97   Loss :  22874.5301605
Iteration :  98   Loss :  41632.5344797
Iteration :  99   Loss :  20409.7706342
[-0.61301015 -0.22198945  0.53595776 ...,  0.23399013  0.32447475
  0.02787064]
CROSS VALIDATION 9
Iteration :  0   Loss :  34629.1406608
Iteration :  1   Loss :  19232.5038607
Iteration :  2   Loss :  20249.1103976
Iteration :  3   Loss :  23927.6371596
Iteration :  4   Loss :  32910.4118134
Iteration :  5   Loss :  30697.6176316
Iteration :  6   Loss :  12420.8952689
Iteration :  7   Loss :  39688.1714696
Iteration :  8   Loss :  14627.1400426
Iteration :  9   Loss :  40988.8131448
Iteration :  10   Loss :  13284.344575
Iteration :  11   Loss :  37352.8926375
Iteration :  12   Loss :  24625.1961792
Iteration :  13   Loss :  24534.3463082
Iteration :  14   Loss :  32232.7973419
Iteration :  15   Loss :  27924.1882032
Iteration :  16   Loss :  10057.7868367
Iteration :  17   Loss :  67367.6620818
Iteration :  18   Loss :  17717.2905078
Iteration :  19   Loss :  37818.4816314
Iteration :  20   Loss :  10237.9419547
Iteration :  21   Loss :  33068.5738193
Iteration :  22   Loss :  6596.41276683
Iteration :  23   Loss :  50663.2733808
Iteration :  24   Loss :  14240.1245784
Iteration :  25   Loss :  37377.6408899
Iteration :  26   Loss :  47679.3878134
Iteration :  27   Loss :  14853.4595516
Iteration :  28   Loss :  21741.6950863
Iteration :  29   Loss :  21979.781463
Iteration :  30   Loss :  53260.1148986
Iteration :  31   Loss :  29857.3839708
Iteration :  32   Loss :  8927.81781914
Iteration :  33   Loss :  34364.9856407
Iteration :  34   Loss :  10233.0321564
Iteration :  35   Loss :  53009.1002527
Iteration :  36   Loss :  31978.151207
Iteration :  37   Loss :  35407.1277445
Iteration :  38   Loss :  48769.1986904
Iteration :  39   Loss :  18882.6775431
Iteration :  40   Loss :  34195.3962651
Iteration :  41   Loss :  44987.3858162
Iteration :  42   Loss :  18648.5890552
Iteration :  43   Loss :  24564.0402436
Iteration :  44   Loss :  21618.5859368
Iteration :  45   Loss :  36699.0220122
Iteration :  46   Loss :  1010.98517537
Iteration :  47   Loss :  52874.9252065
Iteration :  48   Loss :  31114.7060313
Iteration :  49   Loss :  27820.8304069
Iteration :  50   Loss :  68786.9673422
Iteration :  51   Loss :  45633.297735
Iteration :  52   Loss :  24056.2866663
Iteration :  53   Loss :  18447.2797949
Iteration :  54   Loss :  16352.0947703
Iteration :  55   Loss :  30123.8241312
Iteration :  56   Loss :  57992.0665533
Iteration :  57   Loss :  20391.6340565
Iteration :  58   Loss :  29477.6377511
Iteration :  59   Loss :  73156.446884
Iteration :  60   Loss :  8404.8012917
Iteration :  61   Loss :  31357.9782538
Iteration :  62   Loss :  45691.1746307
Iteration :  63   Loss :  1914.33827997
Iteration :  64   Loss :  50300.652415
Iteration :  65   Loss :  12739.7887909
Iteration :  66   Loss :  25160.0472815
Iteration :  67   Loss :  24124.1380993
Iteration :  68   Loss :  43304.3598388
Iteration :  69   Loss :  36157.7515935
Iteration :  70   Loss :  45413.2598213
Iteration :  71   Loss :  37169.2113442
Iteration :  72   Loss :  15962.7426391
Iteration :  73   Loss :  30967.3173105
Iteration :  74   Loss :  20298.0613077
Iteration :  75   Loss :  27183.8251708
Iteration :  76   Loss :  37062.8472181
Iteration :  77   Loss :  71333.0232189
Iteration :  78   Loss :  51564.8218552
Iteration :  79   Loss :  29313.5459019
Iteration :  80   Loss :  51361.9802758
Iteration :  81   Loss :  52229.633894
Iteration :  82   Loss :  41925.51516
Iteration :  83   Loss :  19261.8117524
Iteration :  84   Loss :  12374.4325246
Iteration :  85   Loss :  51386.9353297
Iteration :  86   Loss :  28015.7962103
Iteration :  87   Loss :  23595.4658935
Iteration :  88   Loss :  56092.0975263
Iteration :  89   Loss :  33204.8767344
Iteration :  90   Loss :  2667.5689418
Iteration :  91   Loss :  29665.591179
Iteration :  92   Loss :  15609.9059381
Iteration :  93   Loss :  20331.1752796
Iteration :  94   Loss :  25038.535992
Iteration :  95   Loss :  22579.402461
Iteration :  96   Loss :  21776.6889635
Iteration :  97   Loss :  44336.4554761
Iteration :  98   Loss :  38329.5577714
Iteration :  99   Loss :  18001.7050583
[-0.97185669 -1.18343941 -0.47432466 ...,  0.60775706 -0.38731515
  0.02842211]
CROSS VALIDATION 10
Iteration :  0   Loss :  38107.4920461
Iteration :  1   Loss :  4354.71060736
Iteration :  2   Loss :  37035.2707417
Iteration :  3   Loss :  15533.6907377
Iteration :  4   Loss :  29996.8393155
Iteration :  5   Loss :  4349.77813363
Iteration :  6   Loss :  23056.2232792
Iteration :  7   Loss :  19922.6012816
Iteration :  8   Loss :  30786.657525
Iteration :  9   Loss :  9795.57250006
Iteration :  10   Loss :  24030.9885118
Iteration :  11   Loss :  26702.6890088
Iteration :  12   Loss :  25606.6551339
Iteration :  13   Loss :  28306.3818878
Iteration :  14   Loss :  45119.5430628
Iteration :  15   Loss :  23274.9419207
Iteration :  16   Loss :  20724.1138012
Iteration :  17   Loss :  20390.2526876
Iteration :  18   Loss :  41934.6227091
Iteration :  19   Loss :  22387.3137474
Iteration :  20   Loss :  65420.8344643
Iteration :  21   Loss :  16082.9371124
Iteration :  22   Loss :  56017.4695886
Iteration :  23   Loss :  23732.0383917
Iteration :  24   Loss :  36034.9292019
Iteration :  25   Loss :  14668.1493854
Iteration :  26   Loss :  24321.3051456
Iteration :  27   Loss :  71679.6071662
Iteration :  28   Loss :  49402.6348952
Iteration :  29   Loss :  25050.3112639
Iteration :  30   Loss :  19030.9506846
Iteration :  31   Loss :  30215.6025956
Iteration :  32   Loss :  32558.7072083
Iteration :  33   Loss :  8986.80912873
Iteration :  34   Loss :  43997.0383545
Iteration :  35   Loss :  25738.7191945
Iteration :  36   Loss :  21714.1553936
Iteration :  37   Loss :  15769.2053867
Iteration :  38   Loss :  45674.3341505
Iteration :  39   Loss :  13968.7007708
Iteration :  40   Loss :  23540.4283532
Iteration :  41   Loss :  41726.0875552
Iteration :  42   Loss :  24752.2557236
Iteration :  43   Loss :  24914.725877
Iteration :  44   Loss :  40479.2352268
Iteration :  45   Loss :  20863.7175638
Iteration :  46   Loss :  36906.8443225
Iteration :  47   Loss :  36929.4589334
Iteration :  48   Loss :  33110.1779528
Iteration :  49   Loss :  24452.8294718
Iteration :  50   Loss :  37301.2006043
Iteration :  51   Loss :  14434.2998278
Iteration :  52   Loss :  37304.2356568
Iteration :  53   Loss :  60152.2033317
Iteration :  54   Loss :  20629.9879918
Iteration :  55   Loss :  2454.4444644
Iteration :  56   Loss :  17794.5224277
Iteration :  57   Loss :  37350.9480011
Iteration :  58   Loss :  10444.9147292
Iteration :  59   Loss :  45843.7015668
Iteration :  60   Loss :  40114.5644423
Iteration :  61   Loss :  28074.3691662
Iteration :  62   Loss :  26101.1449427
Iteration :  63   Loss :  42946.9758665
Iteration :  64   Loss :  18249.9629317
Iteration :  65   Loss :  38121.8994872
Iteration :  66   Loss :  24533.0002566
Iteration :  67   Loss :  45684.7032348
Iteration :  68   Loss :  8074.77331004
Iteration :  69   Loss :  43543.1115173
Iteration :  70   Loss :  19283.1951316
Iteration :  71   Loss :  48604.519569
Iteration :  72   Loss :  9437.69305824
Iteration :  73   Loss :  27861.3687423
Iteration :  74   Loss :  36319.0208389
Iteration :  75   Loss :  64185.531445
Iteration :  76   Loss :  43981.4604452
Iteration :  77   Loss :  39500.7586096
Iteration :  78   Loss :  25035.0098164
Iteration :  79   Loss :  23686.224513
Iteration :  80   Loss :  27234.6977213
Iteration :  81   Loss :  28842.7623987
Iteration :  82   Loss :  46773.8470241
Iteration :  83   Loss :  14842.3938305
Iteration :  84   Loss :  26909.6188727
Iteration :  85   Loss :  71154.5209213
Iteration :  86   Loss :  40707.0885187
Iteration :  87   Loss :  13174.9968123
Iteration :  88   Loss :  29611.3662236
Iteration :  89   Loss :  56501.8022179
Iteration :  90   Loss :  28509.7987134
Iteration :  91   Loss :  34721.5603854
Iteration :  92   Loss :  43592.7301085
Iteration :  93   Loss :  62326.920713
Iteration :  94   Loss :  16840.7349437
Iteration :  95   Loss :  15035.0883389
Iteration :  96   Loss :  43213.53071
Iteration :  97   Loss :  37090.3979417
Iteration :  98   Loss :  22125.5737023
Iteration :  99   Loss :  41025.8017775
[-0.64908157 -0.84234514  0.00196675 ...,  0.45876295 -0.31873743
  0.10222764]
CROSS VALIDATION 11
Iteration :  0   Loss :  38107.4920461
Iteration :  1   Loss :  52501.827714
Iteration :  2   Loss :  40882.0978936
Iteration :  3   Loss :  22921.360248
Iteration :  4   Loss :  35120.5982713
Iteration :  5   Loss :  23972.7167145
Iteration :  6   Loss :  20309.3524607
Iteration :  7   Loss :  55437.1000656
Iteration :  8   Loss :  26464.3150149
Iteration :  9   Loss :  14375.1846986
Iteration :  10   Loss :  31147.7912802
Iteration :  11   Loss :  19753.4837461
Iteration :  12   Loss :  30280.022127
Iteration :  13   Loss :  19140.0167002
Iteration :  14   Loss :  32537.1778794
Iteration :  15   Loss :  23328.7640366
Iteration :  16   Loss :  25611.3589279
Iteration :  17   Loss :  19422.7377051
Iteration :  18   Loss :  45907.8814021
Iteration :  19   Loss :  41135.5499571
Iteration :  20   Loss :  13139.9255959
Iteration :  21   Loss :  51060.0451888
Iteration :  22   Loss :  25843.2062093
Iteration :  23   Loss :  43991.388169
Iteration :  24   Loss :  35554.2349428
Iteration :  25   Loss :  16257.7961497
Iteration :  26   Loss :  43696.2843914
Iteration :  27   Loss :  37607.752856
Iteration :  28   Loss :  33307.1866044
Iteration :  29   Loss :  54363.7423249
Iteration :  30   Loss :  22402.2524684
Iteration :  31   Loss :  26513.4893115
Iteration :  32   Loss :  19010.9891421
Iteration :  33   Loss :  48504.3006742
Iteration :  34   Loss :  27214.5662499
Iteration :  35   Loss :  31753.9234599
Iteration :  36   Loss :  26632.7888709
Iteration :  37   Loss :  35630.9465964
Iteration :  38   Loss :  15204.3965693
Iteration :  39   Loss :  51455.5823023
Iteration :  40   Loss :  47283.6464465
Iteration :  41   Loss :  5401.82223445
Iteration :  42   Loss :  32403.4533687
Iteration :  43   Loss :  37905.5990031
Iteration :  44   Loss :  38513.6484743
Iteration :  45   Loss :  42606.6312399
Iteration :  46   Loss :  36879.6009461
Iteration :  47   Loss :  49226.5817443
Iteration :  48   Loss :  10730.1364265
Iteration :  49   Loss :  12718.40714
Iteration :  50   Loss :  22038.8827955
Iteration :  51   Loss :  45934.1582508
Iteration :  52   Loss :  19690.1350737
Iteration :  53   Loss :  18055.106504
Iteration :  54   Loss :  51595.6425824
Iteration :  55   Loss :  25506.793918
Iteration :  56   Loss :  26302.915165
Iteration :  57   Loss :  34103.9222879
Iteration :  58   Loss :  19176.5916395
Iteration :  59   Loss :  14276.0081454
Iteration :  60   Loss :  90524.7044551
Iteration :  61   Loss :  31625.9798194
Iteration :  62   Loss :  50886.8654134
Iteration :  63   Loss :  4001.45445785
Iteration :  64   Loss :  53861.5019623
Iteration :  65   Loss :  15913.2012683
Iteration :  66   Loss :  41798.6728625
Iteration :  67   Loss :  19816.9493278
Iteration :  68   Loss :  53385.5877623
Iteration :  69   Loss :  20141.9651475
Iteration :  70   Loss :  44978.4353647
Iteration :  71   Loss :  20131.6654231
Iteration :  72   Loss :  6063.24206643
Iteration :  73   Loss :  29895.7477141
Iteration :  74   Loss :  8769.38376453
Iteration :  75   Loss :  39554.4852005
Iteration :  76   Loss :  27795.9762123
Iteration :  77   Loss :  38787.7103656
Iteration :  78   Loss :  40640.9511582
Iteration :  79   Loss :  42248.3147638
Iteration :  80   Loss :  34386.6316005
Iteration :  81   Loss :  30245.7726228
Iteration :  82   Loss :  29494.2506953
Iteration :  83   Loss :  36203.2605372
Iteration :  84   Loss :  8826.13875656
Iteration :  85   Loss :  39969.6261727
Iteration :  86   Loss :  46513.6324143
Iteration :  87   Loss :  47463.7144039
Iteration :  88   Loss :  42067.5565595
Iteration :  89   Loss :  21656.3114273
Iteration :  90   Loss :  42385.5017503
Iteration :  91   Loss :  34680.8572083
Iteration :  92   Loss :  72730.6228459
Iteration :  93   Loss :  66298.1345168
Iteration :  94   Loss :  32279.2259536
Iteration :  95   Loss :  23292.191954
Iteration :  96   Loss :  8964.04912389
Iteration :  97   Loss :  27113.4663724
Iteration :  98   Loss :  20789.8608771
Iteration :  99   Loss :  24708.4464459
[-0.82200735 -0.2259169  -0.49609319 ...,  0.17209367 -0.59935138
 -0.01030424]
CROSS VALIDATION 12
Iteration :  0   Loss :  30167.9960994
Iteration :  1   Loss :  37770.0547424
Iteration :  2   Loss :  16120.3030274
Iteration :  3   Loss :  27179.6657027
Iteration :  4   Loss :  40997.1874483
Iteration :  5   Loss :  26766.4728088
Iteration :  6   Loss :  30372.3120523
Iteration :  7   Loss :  54888.3509184
Iteration :  8   Loss :  38191.5106663
Iteration :  9   Loss :  12304.0643261
Iteration :  10   Loss :  28533.4500829
Iteration :  11   Loss :  37635.5212514
Iteration :  12   Loss :  22278.2002796
Iteration :  13   Loss :  13605.0910915
Iteration :  14   Loss :  17177.0228315
Iteration :  15   Loss :  62952.0795311
Iteration :  16   Loss :  28587.750494
Iteration :  17   Loss :  21008.4104918
Iteration :  18   Loss :  31947.9184004
Iteration :  19   Loss :  27498.6970191
Iteration :  20   Loss :  60355.8144404
Iteration :  21   Loss :  26507.7794491
Iteration :  22   Loss :  37354.5527021
Iteration :  23   Loss :  44652.1258464
Iteration :  24   Loss :  18742.8388255
Iteration :  25   Loss :  44978.2850465
Iteration :  26   Loss :  30078.1808946
Iteration :  27   Loss :  26085.2651165
Iteration :  28   Loss :  17366.8540736
Iteration :  29   Loss :  31350.0766156
Iteration :  30   Loss :  6071.21621433
Iteration :  31   Loss :  23814.1747965
Iteration :  32   Loss :  13221.099644
Iteration :  33   Loss :  33767.6092692
Iteration :  34   Loss :  67618.7799839
Iteration :  35   Loss :  24053.6275434
Iteration :  36   Loss :  40674.4654975
Iteration :  37   Loss :  14201.1628474
Iteration :  38   Loss :  36240.0932229
Iteration :  39   Loss :  27658.5831468
Iteration :  40   Loss :  74521.903636
Iteration :  41   Loss :  12069.8172835
Iteration :  42   Loss :  34319.2324954
Iteration :  43   Loss :  21414.2609109
Iteration :  44   Loss :  44712.3761794
Iteration :  45   Loss :  26468.3829561
Iteration :  46   Loss :  25122.3083332
Iteration :  47   Loss :  40645.6783584
Iteration :  48   Loss :  50369.7341267
Iteration :  49   Loss :  19397.8645823
Iteration :  50   Loss :  18134.9789433
Iteration :  51   Loss :  44334.8828303
Iteration :  52   Loss :  23668.5427386
Iteration :  53   Loss :  32250.3727503
Iteration :  54   Loss :  16050.9750245
Iteration :  55   Loss :  36734.2348992
Iteration :  56   Loss :  18804.3012611
Iteration :  57   Loss :  77115.5600372
Iteration :  58   Loss :  18397.1092993
Iteration :  59   Loss :  46622.7602586
Iteration :  60   Loss :  16921.1201237
Iteration :  61   Loss :  2423.10120851
Iteration :  62   Loss :  66396.4020785
Iteration :  63   Loss :  5510.0895028
Iteration :  64   Loss :  31040.5552044
Iteration :  65   Loss :  23423.7122724
Iteration :  66   Loss :  19455.8712512
Iteration :  67   Loss :  22206.1053489
Iteration :  68   Loss :  19383.6882996
Iteration :  69   Loss :  26891.1610834
Iteration :  70   Loss :  42968.0075417
Iteration :  71   Loss :  18550.5957306
Iteration :  72   Loss :  35096.5002128
Iteration :  73   Loss :  25919.7791582
Iteration :  74   Loss :  37292.7497172
Iteration :  75   Loss :  7509.77912923
Iteration :  76   Loss :  30933.0764469
Iteration :  77   Loss :  14061.0435844
Iteration :  78   Loss :  24650.4595119
Iteration :  79   Loss :  42951.6493899
Iteration :  80   Loss :  25212.2838044
Iteration :  81   Loss :  42259.4246034
Iteration :  82   Loss :  16954.653021
Iteration :  83   Loss :  37711.9170637
Iteration :  84   Loss :  22077.4116023
Iteration :  85   Loss :  37932.5255741
Iteration :  86   Loss :  5080.13102735
Iteration :  87   Loss :  17087.0546492
Iteration :  88   Loss :  36898.7363693
Iteration :  89   Loss :  29296.3833211
Iteration :  90   Loss :  18920.4405771
Iteration :  91   Loss :  48741.409646
Iteration :  92   Loss :  2042.05518964
Iteration :  93   Loss :  45079.7890995
Iteration :  94   Loss :  22748.578515
Iteration :  95   Loss :  41889.4239065
Iteration :  96   Loss :  14957.7164568
Iteration :  97   Loss :  22208.3876911
Iteration :  98   Loss :  38502.6564506
Iteration :  99   Loss :  49653.9721813
[-0.20000858 -0.07709474  0.50416758 ...,  0.01278851 -0.0246335
  0.04625637]
CROSS VALIDATION 13
Iteration :  0   Loss :  40020.6887253
Iteration :  1   Loss :  14313.9998455
Iteration :  2   Loss :  32804.92307
Iteration :  3   Loss :  35125.1300502
Iteration :  4   Loss :  38101.4419934
Iteration :  5   Loss :  31657.8840605
Iteration :  6   Loss :  18320.3251993
Iteration :  7   Loss :  40780.5361249
Iteration :  8   Loss :  20112.2975807
Iteration :  9   Loss :  13423.8479775
Iteration :  10   Loss :  23785.3044542
Iteration :  11   Loss :  3921.53749836
Iteration :  12   Loss :  32356.4045653
Iteration :  13   Loss :  59815.0204463
Iteration :  14   Loss :  750.879159138
Iteration :  15   Loss :  9.91564004477
Iteration :  16   Loss :  93424.6542912
Iteration :  17   Loss :  30697.623852
Iteration :  18   Loss :  38765.515504
Iteration :  19   Loss :  88818.9827586
Iteration :  20   Loss :  48496.5579081
Iteration :  21   Loss :  15568.0749941
Iteration :  22   Loss :  38105.8192165
Iteration :  23   Loss :  58762.0044863
Iteration :  24   Loss :  16910.2898152
Iteration :  25   Loss :  23059.045222
Iteration :  26   Loss :  1919.12336065
Iteration :  27   Loss :  25.3465603641
Iteration :  28   Loss :  1460.77689481
Iteration :  29   Loss :  38244.0674697
Iteration :  30   Loss :  28158.8543725
Iteration :  31   Loss :  35262.6334201
Iteration :  32   Loss :  56227.8560662
Iteration :  33   Loss :  23869.735206
Iteration :  34   Loss :  41065.8835039
Iteration :  35   Loss :  15698.8423645
Iteration :  36   Loss :  25688.1292599
Iteration :  37   Loss :  24724.382383
Iteration :  38   Loss :  45841.6414318
Iteration :  39   Loss :  83962.3084718
Iteration :  40   Loss :  53420.9360917
Iteration :  41   Loss :  10368.9389397
Iteration :  42   Loss :  17718.8697808
Iteration :  43   Loss :  29491.6481014
Iteration :  44   Loss :  28546.865384
Iteration :  45   Loss :  33049.5752479
Iteration :  46   Loss :  25456.661208
Iteration :  47   Loss :  32922.3843959
Iteration :  48   Loss :  37390.9059104
Iteration :  49   Loss :  14006.151577
Iteration :  50   Loss :  40188.6935978
Iteration :  51   Loss :  17529.2903285
Iteration :  52   Loss :  38055.6770749
Iteration :  53   Loss :  22291.4574803
Iteration :  54   Loss :  53595.1857954
Iteration :  55   Loss :  56432.5225772
Iteration :  56   Loss :  16155.1615186
Iteration :  57   Loss :  78276.7206982
Iteration :  58   Loss :  49302.5433728
Iteration :  59   Loss :  22715.1162145
Iteration :  60   Loss :  39472.0716065
Iteration :  61   Loss :  17761.9018155
Iteration :  62   Loss :  19969.0406644
Iteration :  63   Loss :  37120.5452615
Iteration :  64   Loss :  50805.5944977
Iteration :  65   Loss :  6824.6923048
Iteration :  66   Loss :  37462.0530695
Iteration :  67   Loss :  32127.678161
Iteration :  68   Loss :  21994.4696034
Iteration :  69   Loss :  29600.5952799
Iteration :  70   Loss :  30527.8659295
Iteration :  71   Loss :  24744.1549219
Iteration :  72   Loss :  61760.6115237
Iteration :  73   Loss :  40782.1685632
Iteration :  74   Loss :  7372.69421881
Iteration :  75   Loss :  32044.826212
Iteration :  76   Loss :  20875.0874401
Iteration :  77   Loss :  15600.0117836
Iteration :  78   Loss :  26293.7535972
Iteration :  79   Loss :  36552.5450748
Iteration :  80   Loss :  41612.7656684
Iteration :  81   Loss :  7490.11430972
Iteration :  82   Loss :  77944.3257662
Iteration :  83   Loss :  25135.9268706
Iteration :  84   Loss :  25903.6885815
Iteration :  85   Loss :  27536.5298824
Iteration :  86   Loss :  39041.0113744
Iteration :  87   Loss :  22386.6327779
Iteration :  88   Loss :  59438.4715262
Iteration :  89   Loss :  23080.1054757
Iteration :  90   Loss :  31874.6344818
Iteration :  91   Loss :  13149.4666672
Iteration :  92   Loss :  59634.4248874
Iteration :  93   Loss :  29445.3302895
Iteration :  94   Loss :  26022.2667481
Iteration :  95   Loss :  12144.2247728
Iteration :  96   Loss :  77081.0784345
Iteration :  97   Loss :  30361.6848186
Iteration :  98   Loss :  27092.6088815
Iteration :  99   Loss :  48498.8756404
[ 0.13025059 -0.67225221 -0.12658078 ...,  0.4055779  -0.40539365
  0.02103715]
CROSS VALIDATION 14
Iteration :  0   Loss :  32352.0238956
Iteration :  1   Loss :  28181.6223631
Iteration :  2   Loss :  48648.9579949
Iteration :  3   Loss :  17561.8019689
Iteration :  4   Loss :  27344.9851784
Iteration :  5   Loss :  33539.8380617
Iteration :  6   Loss :  33840.766467
Iteration :  7   Loss :  19526.9204652
Iteration :  8   Loss :  32826.6188642
Iteration :  9   Loss :  54364.2413608
Iteration :  10   Loss :  29008.9299066
Iteration :  11   Loss :  17469.1005426
Iteration :  12   Loss :  22411.9757793
Iteration :  13   Loss :  8004.90432354
Iteration :  14   Loss :  24655.698606
Iteration :  15   Loss :  63854.6806417
Iteration :  16   Loss :  33146.0773736
Iteration :  17   Loss :  46215.5860314
Iteration :  18   Loss :  20730.6171877
Iteration :  19   Loss :  26955.1264359
Iteration :  20   Loss :  33563.5709788
Iteration :  21   Loss :  34153.8819466
Iteration :  22   Loss :  16751.5545785
Iteration :  23   Loss :  54561.8014157
Iteration :  24   Loss :  14490.4850917
Iteration :  25   Loss :  37113.1902071
Iteration :  26   Loss :  23243.9742044
Iteration :  27   Loss :  21246.0937064
Iteration :  28   Loss :  23809.7786772
Iteration :  29   Loss :  25414.0504513
Iteration :  30   Loss :  28045.181046
Iteration :  31   Loss :  38295.3950803
Iteration :  32   Loss :  15732.0738452
Iteration :  33   Loss :  46634.6863782
Iteration :  34   Loss :  7257.00477214
Iteration :  35   Loss :  48711.9951095
Iteration :  36   Loss :  46992.8990728
Iteration :  37   Loss :  40979.5373049
Iteration :  38   Loss :  42201.2654327
Iteration :  39   Loss :  21841.9502747
Iteration :  40   Loss :  35222.1339785
Iteration :  41   Loss :  22024.5679734
Iteration :  42   Loss :  23243.0081075
Iteration :  43   Loss :  26305.0228725
Iteration :  44   Loss :  53735.142739
Iteration :  45   Loss :  41777.7978161
Iteration :  46   Loss :  51783.8434434
Iteration :  47   Loss :  14513.8211044
Iteration :  48   Loss :  32618.0977434
Iteration :  49   Loss :  26595.9093569
Iteration :  50   Loss :  52182.3046193
Iteration :  51   Loss :  25461.3865945
Iteration :  52   Loss :  30041.56165
Iteration :  53   Loss :  27288.0383633
Iteration :  54   Loss :  54343.3843343
Iteration :  55   Loss :  30678.7812805
Iteration :  56   Loss :  7971.14573524
Iteration :  57   Loss :  37911.7217702
Iteration :  58   Loss :  16273.32735
Iteration :  59   Loss :  42825.212282
Iteration :  60   Loss :  28847.2780995
Iteration :  61   Loss :  35122.992304
Iteration :  62   Loss :  30696.1985207
Iteration :  63   Loss :  32890.1263069
Iteration :  64   Loss :  27998.7684365
Iteration :  65   Loss :  28640.1422745
Iteration :  66   Loss :  49759.8913358
Iteration :  67   Loss :  9311.19529687
Iteration :  68   Loss :  22715.7753786
Iteration :  69   Loss :  67717.5561536
Iteration :  70   Loss :  33557.8243229
Iteration :  71   Loss :  16083.8155349
Iteration :  72   Loss :  41152.1508356
Iteration :  73   Loss :  14486.8409102
Iteration :  74   Loss :  31964.3894377
Iteration :  75   Loss :  50307.0934836
Iteration :  76   Loss :  2018.76741694
Iteration :  77   Loss :  26.6585785433
Iteration :  78   Loss :  0.369545080557
Iteration :  79   Loss :  32762.1524473
Iteration :  80   Loss :  34404.6271037
Iteration :  81   Loss :  55285.0764883
Iteration :  82   Loss :  12889.2312002
Iteration :  83   Loss :  12708.2879214
Iteration :  84   Loss :  24634.9289047
Iteration :  85   Loss :  37008.293005
Iteration :  86   Loss :  9020.0497282
Iteration :  87   Loss :  28995.4973352
Iteration :  88   Loss :  23576.002794
Iteration :  89   Loss :  16766.1215008
Iteration :  90   Loss :  6492.76701504
Iteration :  91   Loss :  31120.9683612
Iteration :  92   Loss :  23877.8474792
Iteration :  93   Loss :  18629.4490738
Iteration :  94   Loss :  38050.0889129
Iteration :  95   Loss :  37356.5610881
Iteration :  96   Loss :  25437.0899204
Iteration :  97   Loss :  24076.4276415
Iteration :  98   Loss :  22779.909396
Iteration :  99   Loss :  28911.4908204
[-0.3544921  -0.29823496 -0.74149748 ...,  0.38346193 -0.54100397
  0.06125713]
CROSS VALIDATION 15
Iteration :  0   Loss :  34830.7288523
Iteration :  1   Loss :  31519.2886556
Iteration :  2   Loss :  48224.9304015
Iteration :  3   Loss :  29998.9843387
Iteration :  4   Loss :  37216.2697605
Iteration :  5   Loss :  75828.1721226
Iteration :  6   Loss :  32205.685681
Iteration :  7   Loss :  17435.2457963
Iteration :  8   Loss :  47274.7190303
Iteration :  9   Loss :  66783.7174491
Iteration :  10   Loss :  32716.7659298
Iteration :  11   Loss :  26290.9090869
Iteration :  12   Loss :  71153.5880185
Iteration :  13   Loss :  68484.508206
Iteration :  14   Loss :  30200.8617618
Iteration :  15   Loss :  10587.8278138
Iteration :  16   Loss :  27166.7873571
Iteration :  17   Loss :  23934.0080144
Iteration :  18   Loss :  20384.0867246
Iteration :  19   Loss :  24235.8912868
Iteration :  20   Loss :  30072.2252597
Iteration :  21   Loss :  45542.9893072
Iteration :  22   Loss :  30622.0650022
Iteration :  23   Loss :  7108.02652862
Iteration :  24   Loss :  28596.7316297
Iteration :  25   Loss :  66675.5159257
Iteration :  26   Loss :  82285.142202
Iteration :  27   Loss :  24280.5600134
Iteration :  28   Loss :  53115.0399041
Iteration :  29   Loss :  62131.0367971
Iteration :  30   Loss :  30568.9777762
Iteration :  31   Loss :  30213.5297449
Iteration :  32   Loss :  19625.2970771
Iteration :  33   Loss :  22687.6102043
Iteration :  34   Loss :  51931.4422953
Iteration :  35   Loss :  28426.3202112
Iteration :  36   Loss :  14149.055593
Iteration :  37   Loss :  20296.3257255
Iteration :  38   Loss :  56469.8624647
Iteration :  39   Loss :  17126.0041267
Iteration :  40   Loss :  13118.6625693
Iteration :  41   Loss :  48837.9335806
Iteration :  42   Loss :  21580.9031543
Iteration :  43   Loss :  32388.7929852
Iteration :  44   Loss :  14502.8638374
Iteration :  45   Loss :  37295.7494183
Iteration :  46   Loss :  24177.9176067
Iteration :  47   Loss :  45336.0785048
Iteration :  48   Loss :  14159.4164472
Iteration :  49   Loss :  37569.357825
Iteration :  50   Loss :  18344.853032
Iteration :  51   Loss :  38386.2531937
Iteration :  52   Loss :  38895.5966569
Iteration :  53   Loss :  73024.663219
Iteration :  54   Loss :  31529.9007639
Iteration :  55   Loss :  50901.4691671
Iteration :  56   Loss :  12103.2019791
Iteration :  57   Loss :  54238.9355469
Iteration :  58   Loss :  34077.8920291
Iteration :  59   Loss :  20943.938759
Iteration :  60   Loss :  27888.0347287
Iteration :  61   Loss :  46319.3596735
Iteration :  62   Loss :  25042.9795094
Iteration :  63   Loss :  27012.9775775
Iteration :  64   Loss :  55224.1019686
Iteration :  65   Loss :  14928.7819584
Iteration :  66   Loss :  15316.1951168
Iteration :  67   Loss :  52960.0451452
Iteration :  68   Loss :  66773.4242904
Iteration :  69   Loss :  32724.270306
Iteration :  70   Loss :  26291.7565541
Iteration :  71   Loss :  71153.8116148
Iteration :  72   Loss :  68484.5952574
Iteration :  73   Loss :  30200.8636225
Iteration :  74   Loss :  10587.8280493
Iteration :  75   Loss :  27166.7874799
Iteration :  76   Loss :  23934.0080138
Iteration :  77   Loss :  20384.0867265
Iteration :  78   Loss :  24235.8912869
Iteration :  79   Loss :  30072.2252597
Iteration :  80   Loss :  45542.9893072
Iteration :  81   Loss :  30622.0650022
Iteration :  82   Loss :  7108.02652862
Iteration :  83   Loss :  28596.7316297
Iteration :  84   Loss :  66675.5159257
Iteration :  85   Loss :  82285.142202
Iteration :  86   Loss :  24280.5600134
Iteration :  87   Loss :  53115.0399041
Iteration :  88   Loss :  62131.0367971
Iteration :  89   Loss :  30568.9777762
Iteration :  90   Loss :  30213.5297449
Iteration :  91   Loss :  19625.2970771
Iteration :  92   Loss :  22687.6102043
Iteration :  93   Loss :  51931.4422953
Iteration :  94   Loss :  28426.3202112
Iteration :  95   Loss :  14149.055593
Iteration :  96   Loss :  20296.3257255
Iteration :  97   Loss :  56469.8624647
Iteration :  98   Loss :  17126.0041267
Iteration :  99   Loss :  13118.6625693
[-0.14378914 -0.42680067  0.1872749  ...,  0.18629075  0.16108081
 -0.03498851]
CROSS VALIDATION 16
Iteration :  0   Loss :  34830.7288523
Iteration :  1   Loss :  18756.6203979
Iteration :  2   Loss :  35972.5014137
Iteration :  3   Loss :  17135.4884549
Iteration :  4   Loss :  29486.1059644
Iteration :  5   Loss :  31319.2463302
Iteration :  6   Loss :  53104.8243717
Iteration :  7   Loss :  19367.1866223
Iteration :  8   Loss :  36414.310406
Iteration :  9   Loss :  9899.60432509
Iteration :  10   Loss :  21684.9959243
Iteration :  11   Loss :  15199.9607903
Iteration :  12   Loss :  47295.5498196
Iteration :  13   Loss :  27806.3806965
Iteration :  14   Loss :  33821.7390299
Iteration :  15   Loss :  30732.6800398
Iteration :  16   Loss :  20087.8668766
Iteration :  17   Loss :  29923.4729587
Iteration :  18   Loss :  50748.2466307
Iteration :  19   Loss :  23491.0918919
Iteration :  20   Loss :  34807.6633476
Iteration :  21   Loss :  19655.2575348
Iteration :  22   Loss :  21039.9073621
Iteration :  23   Loss :  21489.1347018
Iteration :  24   Loss :  49283.1584436
Iteration :  25   Loss :  3077.96333714
Iteration :  26   Loss :  22315.0638716
Iteration :  27   Loss :  13901.4014785
Iteration :  28   Loss :  43356.5015515
Iteration :  29   Loss :  12616.542907
Iteration :  30   Loss :  53941.9925926
Iteration :  31   Loss :  27093.5675999
Iteration :  32   Loss :  40483.5823539
Iteration :  33   Loss :  23975.041756
Iteration :  34   Loss :  30771.1788119
Iteration :  35   Loss :  13521.7718138
Iteration :  36   Loss :  30945.1826658
Iteration :  37   Loss :  53944.1683564
Iteration :  38   Loss :  49132.931369
Iteration :  39   Loss :  12550.4089872
Iteration :  40   Loss :  54530.0895344
Iteration :  41   Loss :  20992.4903758
Iteration :  42   Loss :  23359.8572783
Iteration :  43   Loss :  19269.5604376
Iteration :  44   Loss :  27028.5105544
Iteration :  45   Loss :  35880.8095373
Iteration :  46   Loss :  26135.2225435
Iteration :  47   Loss :  47909.3103031
Iteration :  48   Loss :  13910.1007491
Iteration :  49   Loss :  65286.5658155
Iteration :  50   Loss :  15890.3323796
Iteration :  51   Loss :  21539.9045227
Iteration :  52   Loss :  17343.1017666
Iteration :  53   Loss :  78368.1548285
Iteration :  54   Loss :  42313.213257
Iteration :  55   Loss :  44445.4868368
Iteration :  56   Loss :  44121.1032964
Iteration :  57   Loss :  25754.0184118
Iteration :  58   Loss :  19148.1910389
Iteration :  59   Loss :  60974.3988486
Iteration :  60   Loss :  10002.8573626
Iteration :  61   Loss :  38091.1996518
Iteration :  62   Loss :  28930.1756218
Iteration :  63   Loss :  32810.8851734
Iteration :  64   Loss :  6572.32161246
Iteration :  65   Loss :  35580.2422379
Iteration :  66   Loss :  22365.604545
Iteration :  67   Loss :  12376.8782886
Iteration :  68   Loss :  25552.9426632
Iteration :  69   Loss :  39070.9826541
Iteration :  70   Loss :  15363.5729384
Iteration :  71   Loss :  28188.2113501
Iteration :  72   Loss :  10300.7380304
Iteration :  73   Loss :  43722.1999413
Iteration :  74   Loss :  36695.8587457
Iteration :  75   Loss :  68990.3655824
Iteration :  76   Loss :  33290.4052251
Iteration :  77   Loss :  8632.45871643
Iteration :  78   Loss :  77891.7687332
Iteration :  79   Loss :  20446.595785
Iteration :  80   Loss :  48361.7472497
Iteration :  81   Loss :  17812.2295011
Iteration :  82   Loss :  9547.66929119
Iteration :  83   Loss :  48394.6221514
Iteration :  84   Loss :  65572.8575282
Iteration :  85   Loss :  18861.8802579
Iteration :  86   Loss :  21096.5887064
Iteration :  87   Loss :  27153.3710099
Iteration :  88   Loss :  34506.8023154
Iteration :  89   Loss :  24413.3093127
Iteration :  90   Loss :  55237.064202
Iteration :  91   Loss :  28416.0921546
Iteration :  92   Loss :  46782.2343862
Iteration :  93   Loss :  44598.4319265
Iteration :  94   Loss :  3800.27220461
Iteration :  95   Loss :  38650.4290454
Iteration :  96   Loss :  13589.0010197
Iteration :  97   Loss :  37901.9419008
Iteration :  98   Loss :  47016.2995795
Iteration :  99   Loss :  9204.00825183
[-0.71330068  0.00961511  0.27151753 ...,  0.50859575  0.05870925
  0.10199487]
CROSS VALIDATION 17
Iteration :  0   Loss :  34830.7288523
Iteration :  1   Loss :  31803.2259801
Iteration :  2   Loss :  48237.8888957
Iteration :  3   Loss :  12126.8137288
Iteration :  4   Loss :  21789.0102481
Iteration :  5   Loss :  16635.5582236
Iteration :  6   Loss :  34448.8395843
Iteration :  7   Loss :  22832.740468
Iteration :  8   Loss :  36696.318643
Iteration :  9   Loss :  35755.8262643
Iteration :  10   Loss :  30592.5719943
Iteration :  11   Loss :  43233.3883018
Iteration :  12   Loss :  21035.8931033
Iteration :  13   Loss :  18923.8408919
Iteration :  14   Loss :  23275.6988558
Iteration :  15   Loss :  55251.9348492
Iteration :  16   Loss :  29413.3959335
Iteration :  17   Loss :  36055.0759297
Iteration :  18   Loss :  31300.6011589
Iteration :  19   Loss :  78391.5000138
Iteration :  20   Loss :  3771.05011295
Iteration :  21   Loss :  49.7981382043
Iteration :  22   Loss :  67671.5705581
Iteration :  23   Loss :  20827.9594449
Iteration :  24   Loss :  41903.2702374
Iteration :  25   Loss :  28376.2812613
Iteration :  26   Loss :  82962.306571
Iteration :  27   Loss :  51720.0228893
Iteration :  28   Loss :  34147.3199073
Iteration :  29   Loss :  31323.1117213
Iteration :  30   Loss :  21474.7082075
Iteration :  31   Loss :  25691.1091472
Iteration :  32   Loss :  23249.2603657
Iteration :  33   Loss :  40843.0925819
Iteration :  34   Loss :  63865.4619107
Iteration :  35   Loss :  15007.6618416
Iteration :  36   Loss :  50678.7628725
Iteration :  37   Loss :  17031.6815527
Iteration :  38   Loss :  31513.2649902
Iteration :  39   Loss :  24873.0066567
Iteration :  40   Loss :  23390.1061182
Iteration :  41   Loss :  8994.47689719
Iteration :  42   Loss :  37125.3967473
Iteration :  43   Loss :  50095.8137324
Iteration :  44   Loss :  69176.2835349
Iteration :  45   Loss :  9851.24918536
Iteration :  46   Loss :  28609.4262109
Iteration :  47   Loss :  46715.0118823
Iteration :  48   Loss :  32385.6815444
Iteration :  49   Loss :  11275.7343347
Iteration :  50   Loss :  35105.8313654
Iteration :  51   Loss :  25252.504006
Iteration :  52   Loss :  30357.8060707
Iteration :  53   Loss :  29310.3189661
Iteration :  54   Loss :  39522.3245319
Iteration :  55   Loss :  6256.10708531
Iteration :  56   Loss :  903.618769792
Iteration :  57   Loss :  11.9326237117
Iteration :  58   Loss :  72154.9937784
Iteration :  59   Loss :  22284.5382785
Iteration :  60   Loss :  28548.6382269
Iteration :  61   Loss :  2804.01937308
Iteration :  62   Loss :  30536.6089865
Iteration :  63   Loss :  11618.1359362
Iteration :  64   Loss :  19346.5603282
Iteration :  65   Loss :  52483.4453649
Iteration :  66   Loss :  7347.52708005
Iteration :  67   Loss :  31736.7433553
Iteration :  68   Loss :  24592.5849649
Iteration :  69   Loss :  49525.6775425
Iteration :  70   Loss :  3858.6377683
Iteration :  71   Loss :  22930.6869628
Iteration :  72   Loss :  19776.981544
Iteration :  73   Loss :  58258.6719
Iteration :  74   Loss :  37020.9066902
Iteration :  75   Loss :  79586.4244763
Iteration :  76   Loss :  13920.7253802
Iteration :  77   Loss :  33036.0614238
Iteration :  78   Loss :  20335.5037118
Iteration :  79   Loss :  39982.463434
Iteration :  80   Loss :  14704.4370791
Iteration :  81   Loss :  39554.5862538
Iteration :  82   Loss :  13040.0269955
Iteration :  83   Loss :  36163.9570053
Iteration :  84   Loss :  15577.4953453
Iteration :  85   Loss :  17873.6297763
Iteration :  86   Loss :  51872.2290074
Iteration :  87   Loss :  28663.9094794
Iteration :  88   Loss :  17118.4787263
Iteration :  89   Loss :  16300.8327047
Iteration :  90   Loss :  9927.20277235
Iteration :  91   Loss :  52459.3930087
Iteration :  92   Loss :  7711.32855253
Iteration :  93   Loss :  19868.4405373
Iteration :  94   Loss :  29626.2101332
Iteration :  95   Loss :  48975.5975257
Iteration :  96   Loss :  27014.7108281
Iteration :  97   Loss :  52171.639584
Iteration :  98   Loss :  14711.124246
Iteration :  99   Loss :  31599.4167442
[-0.50126767 -0.95430146  0.07762897 ..., -0.08955511 -0.0149117
  0.08404811]
CROSS VALIDATION 18
Iteration :  0   Loss :  39053.1157572
Iteration :  1   Loss :  44254.3458671
Iteration :  2   Loss :  42216.4802241
Iteration :  3   Loss :  25952.6776743
Iteration :  4   Loss :  65213.8433431
Iteration :  5   Loss :  80776.2435328
Iteration :  6   Loss :  6342.84529888
Iteration :  7   Loss :  44648.5482534
Iteration :  8   Loss :  2029.42062437
Iteration :  9   Loss :  38779.3582861
Iteration :  10   Loss :  20651.9329089
Iteration :  11   Loss :  16338.1627617
Iteration :  12   Loss :  41373.9670732
Iteration :  13   Loss :  23364.2518513
Iteration :  14   Loss :  27353.0453847
Iteration :  15   Loss :  38909.2157061
Iteration :  16   Loss :  16462.8287632
Iteration :  17   Loss :  24759.3464561
Iteration :  18   Loss :  18356.4935195
Iteration :  19   Loss :  33760.6073273
Iteration :  20   Loss :  21825.2290306
Iteration :  21   Loss :  31141.4360441
Iteration :  22   Loss :  20534.793864
Iteration :  23   Loss :  47113.9649803
Iteration :  24   Loss :  5386.82825378
Iteration :  25   Loss :  48502.1060485
Iteration :  26   Loss :  19908.447635
Iteration :  27   Loss :  32075.2894536
Iteration :  28   Loss :  19105.9435605
Iteration :  29   Loss :  28116.5109594
Iteration :  30   Loss :  19507.0381642
Iteration :  31   Loss :  37320.603886
Iteration :  32   Loss :  9025.76636033
Iteration :  33   Loss :  34864.50859
Iteration :  34   Loss :  22229.0401788
Iteration :  35   Loss :  3550.56603386
Iteration :  36   Loss :  46.8865520081
Iteration :  37   Loss :  0.650356743077
Iteration :  38   Loss :  43094.9751046
Iteration :  39   Loss :  18228.0235388
Iteration :  40   Loss :  31434.8555221
Iteration :  41   Loss :  28154.7511945
Iteration :  42   Loss :  23603.3427851
Iteration :  43   Loss :  22957.2208127
Iteration :  44   Loss :  26442.118046
Iteration :  45   Loss :  13728.1181883
Iteration :  46   Loss :  59496.7474286
Iteration :  47   Loss :  20564.001098
Iteration :  48   Loss :  17302.9098415
Iteration :  49   Loss :  31332.8708801
Iteration :  50   Loss :  21565.095137
Iteration :  51   Loss :  18738.8897504
Iteration :  52   Loss :  27523.8003513
Iteration :  53   Loss :  24987.2356251
Iteration :  54   Loss :  44443.3083656
Iteration :  55   Loss :  26861.8415324
Iteration :  56   Loss :  52801.677476
Iteration :  57   Loss :  24979.3721686
Iteration :  58   Loss :  42067.7483259
Iteration :  59   Loss :  27160.7077426
Iteration :  60   Loss :  25392.6849349
Iteration :  61   Loss :  66781.1652466
Iteration :  62   Loss :  12374.9811145
Iteration :  63   Loss :  47138.4319587
Iteration :  64   Loss :  40430.9100321
Iteration :  65   Loss :  5846.44402908
Iteration :  66   Loss :  36581.9062464
Iteration :  67   Loss :  12535.9853551
Iteration :  68   Loss :  36798.1126428
Iteration :  69   Loss :  25055.2002679
Iteration :  70   Loss :  51258.7774657
Iteration :  71   Loss :  35871.9452873
Iteration :  72   Loss :  21511.2530056
Iteration :  73   Loss :  30955.306704
Iteration :  74   Loss :  51698.1001519
Iteration :  75   Loss :  27532.4436096
Iteration :  76   Loss :  40112.1295204
Iteration :  77   Loss :  80346.8063066
Iteration :  78   Loss :  8366.41333029
Iteration :  79   Loss :  61674.4412224
Iteration :  80   Loss :  20084.7240168
Iteration :  81   Loss :  30823.6965452
Iteration :  82   Loss :  6517.38993715
Iteration :  83   Loss :  31323.9656691
Iteration :  84   Loss :  17190.7445941
Iteration :  85   Loss :  24668.8581755
Iteration :  86   Loss :  11856.2470737
Iteration :  87   Loss :  54654.9031117
Iteration :  88   Loss :  47551.0469632
Iteration :  89   Loss :  47727.6452315
Iteration :  90   Loss :  32920.427619
Iteration :  91   Loss :  17273.9285282
Iteration :  92   Loss :  73032.0200743
Iteration :  93   Loss :  6169.07211063
Iteration :  94   Loss :  30533.1958656
Iteration :  95   Loss :  22522.1729205
Iteration :  96   Loss :  31880.0260141
Iteration :  97   Loss :  15196.2682614
Iteration :  98   Loss :  44444.3887843
Iteration :  99   Loss :  28911.0774223
[-0.15163249 -0.03045312 -0.65556026 ...,  0.53855899 -0.04799629
  0.12466639]
CROSS VALIDATION 19
Iteration :  0   Loss :  34600.7781147
Iteration :  1   Loss :  29651.1161504
Iteration :  2   Loss :  48210.3802058
Iteration :  3   Loss :  12291.4864905
Iteration :  4   Loss :  42619.2651071
Iteration :  5   Loss :  7051.33709987
Iteration :  6   Loss :  48804.7190451
Iteration :  7   Loss :  32473.0189534
Iteration :  8   Loss :  29412.2672381
Iteration :  9   Loss :  10666.2616343
Iteration :  10   Loss :  28103.3226329
Iteration :  11   Loss :  48041.6024927
Iteration :  12   Loss :  34647.2504182
Iteration :  13   Loss :  24156.4590579
Iteration :  14   Loss :  40018.7223423
Iteration :  15   Loss :  21124.7737184
Iteration :  16   Loss :  43435.095249
Iteration :  17   Loss :  10135.2086773
Iteration :  18   Loss :  38062.7101489
Iteration :  19   Loss :  30627.2171557
Iteration :  20   Loss :  41315.4313504
Iteration :  21   Loss :  13597.1036406
Iteration :  22   Loss :  46122.0045194
Iteration :  23   Loss :  34230.1630987
Iteration :  24   Loss :  35058.7843301
Iteration :  25   Loss :  21129.0508498
Iteration :  26   Loss :  54929.559748
Iteration :  27   Loss :  22689.8565625
Iteration :  28   Loss :  48841.679371
Iteration :  29   Loss :  3920.53783858
Iteration :  30   Loss :  36086.2353368
Iteration :  31   Loss :  17017.2740249
Iteration :  32   Loss :  22321.6143613
Iteration :  33   Loss :  50008.9550776
Iteration :  34   Loss :  34786.0946792
Iteration :  35   Loss :  8177.10055559
Iteration :  36   Loss :  21932.2394733
Iteration :  37   Loss :  24597.9267663
Iteration :  38   Loss :  37177.1860941
Iteration :  39   Loss :  66548.5164922
Iteration :  40   Loss :  2595.0890634
Iteration :  41   Loss :  34832.6563447
Iteration :  42   Loss :  22040.9908078
Iteration :  43   Loss :  35807.716515
Iteration :  44   Loss :  16816.6487965
Iteration :  45   Loss :  40673.167272
Iteration :  46   Loss :  41071.0933659
Iteration :  47   Loss :  2263.85705791
Iteration :  48   Loss :  42668.2787115
Iteration :  49   Loss :  35407.9388241
Iteration :  50   Loss :  28583.7560235
Iteration :  51   Loss :  57492.4009251
Iteration :  52   Loss :  12697.6498942
Iteration :  53   Loss :  22182.1811342
Iteration :  54   Loss :  19667.0397402
Iteration :  55   Loss :  49320.7337597
Iteration :  56   Loss :  31099.3220173
Iteration :  57   Loss :  24048.5778535
Iteration :  58   Loss :  24762.8334023
Iteration :  59   Loss :  49090.2354973
Iteration :  60   Loss :  26731.8990301
Iteration :  61   Loss :  33044.7504529
Iteration :  62   Loss :  14270.8126776
Iteration :  63   Loss :  41164.1665765
Iteration :  64   Loss :  21941.2669025
Iteration :  65   Loss :  38680.2563672
Iteration :  66   Loss :  7172.69171225
Iteration :  67   Loss :  31719.7916519
Iteration :  68   Loss :  30872.4330295
Iteration :  69   Loss :  28564.1838302
Iteration :  70   Loss :  25409.6351885
Iteration :  71   Loss :  52171.6840697
Iteration :  72   Loss :  42166.2330486
Iteration :  73   Loss :  21644.2452754
Iteration :  74   Loss :  38332.6103286
Iteration :  75   Loss :  28666.4012819
Iteration :  76   Loss :  41188.8014331
Iteration :  77   Loss :  13906.603442
Iteration :  78   Loss :  33832.4564915
Iteration :  79   Loss :  17948.596494
Iteration :  80   Loss :  46287.5812837
Iteration :  81   Loss :  49804.9540544
Iteration :  82   Loss :  37445.8868988
Iteration :  83   Loss :  22725.5945553
Iteration :  84   Loss :  4408.48820621
Iteration :  85   Loss :  29768.8742325
Iteration :  86   Loss :  17904.4472915
Iteration :  87   Loss :  40924.9715495
Iteration :  88   Loss :  28524.8501318
Iteration :  89   Loss :  58127.109554
Iteration :  90   Loss :  16834.7239943
Iteration :  91   Loss :  29944.638383
Iteration :  92   Loss :  39148.2035928
Iteration :  93   Loss :  22084.007708
Iteration :  94   Loss :  25185.425989
Iteration :  95   Loss :  28130.5399669
Iteration :  96   Loss :  63008.2536946
Iteration :  97   Loss :  17811.1865278
Iteration :  98   Loss :  39398.5870866
Iteration :  99   Loss :  40571.0779818
[-0.72492271 -0.04058654  0.31316238 ...,  0.01415238  0.08212874
  0.09718128]
Accuracy (Logistic Loss):	0.7
Hinge Loss
CROSS VALIDATION 0
Iteration :  0   Loss :  34195.6757926
Iteration :  1   Loss :  19357.9317306
Iteration :  2   Loss :  44936.8683876
Iteration :  3   Loss :  54801.8884421
Iteration :  4   Loss :  37881.8117776
Iteration :  5   Loss :  51002.5885378
Iteration :  6   Loss :  17432.2730627
Iteration :  7   Loss :  16629.1779737
Iteration :  8   Loss :  32546.6048942
Iteration :  9   Loss :  39535.6558861
Iteration :  10   Loss :  13240.7061602
Iteration :  11   Loss :  67002.2078073
Iteration :  12   Loss :  11151.6180505
Iteration :  13   Loss :  40583.9666835
Iteration :  14   Loss :  25766.8394785
Iteration :  15   Loss :  27345.3457709
Iteration :  16   Loss :  14578.5237055
Iteration :  17   Loss :  17143.086986
Iteration :  18   Loss :  19814.1194416
Iteration :  19   Loss :  42191.9925917
Iteration :  20   Loss :  12604.2162273
Iteration :  21   Loss :  56428.6527429
Iteration :  22   Loss :  32275.8220644
Iteration :  23   Loss :  46815.3837827
Iteration :  24   Loss :  45174.5213445
Iteration :  25   Loss :  61505.4699962
Iteration :  26   Loss :  46220.6661125
Iteration :  27   Loss :  50968.552786
Iteration :  28   Loss :  4053.75506233
Iteration :  29   Loss :  53.5313512678
Iteration :  30   Loss :  0.706901508478
Iteration :  31   Loss :  1.21569176844
Iteration :  32   Loss :  43897.5069679
Iteration :  33   Loss :  36397.7179431
Iteration :  34   Loss :  28989.8063028
Iteration :  35   Loss :  38605.8420206
Iteration :  36   Loss :  16895.7295153
Iteration :  37   Loss :  21485.3352384
Iteration :  38   Loss :  23077.2971838
Iteration :  39   Loss :  29760.1213869
Iteration :  40   Loss :  55092.53102
Iteration :  41   Loss :  25976.7618053
Iteration :  42   Loss :  38213.9725915
Iteration :  43   Loss :  82615.0147694
Iteration :  44   Loss :  26962.1076826
Iteration :  45   Loss :  34754.9502497
Iteration :  46   Loss :  58295.08974
Iteration :  47   Loss :  69019.7911362
Iteration :  48   Loss :  30513.5752873
Iteration :  49   Loss :  40518.2522604
Iteration :  50   Loss :  15113.9445816
Iteration :  51   Loss :  47853.4109002
Iteration :  52   Loss :  56030.7167816
Iteration :  53   Loss :  35571.2293738
Iteration :  54   Loss :  8356.30257674
Iteration :  55   Loss :  32331.6770299
Iteration :  56   Loss :  57679.2051584
Iteration :  57   Loss :  35185.2924867
Iteration :  58   Loss :  40078.2421458
Iteration :  59   Loss :  50461.2662881
Iteration :  60   Loss :  35732.6484698
Iteration :  61   Loss :  57941.0798533
Iteration :  62   Loss :  24618.1315012
Iteration :  63   Loss :  32675.0019723
Iteration :  64   Loss :  27992.3663354
Iteration :  65   Loss :  30861.0134974
Iteration :  66   Loss :  45494.9470003
Iteration :  67   Loss :  43565.7672317
Iteration :  68   Loss :  13238.998125
Iteration :  69   Loss :  47476.8589857
Iteration :  70   Loss :  16852.4297306
Iteration :  71   Loss :  38218.3361251
Iteration :  72   Loss :  31555.2725685
Iteration :  73   Loss :  39045.0261305
Iteration :  74   Loss :  9074.66406172
Iteration :  75   Loss :  18415.0518597
Iteration :  76   Loss :  44734.7816486
Iteration :  77   Loss :  37703.4537581
Iteration :  78   Loss :  18190.6440333
Iteration :  79   Loss :  45563.8163056
Iteration :  80   Loss :  19477.8605028
Iteration :  81   Loss :  39365.7659506
Iteration :  82   Loss :  10750.7790556
Iteration :  83   Loss :  31134.0918544
Iteration :  84   Loss :  16209.2795676
Iteration :  85   Loss :  16853.0205936
Iteration :  86   Loss :  20877.8380064
Iteration :  87   Loss :  20790.2923101
Iteration :  88   Loss :  15148.6972703
Iteration :  89   Loss :  41163.7828927
Iteration :  90   Loss :  49790.6575535
Iteration :  91   Loss :  9377.25273007
Iteration :  92   Loss :  45069.0680893
Iteration :  93   Loss :  29165.5211404
Iteration :  94   Loss :  32365.3899987
Iteration :  95   Loss :  94649.2934626
Iteration :  96   Loss :  17496.5281842
Iteration :  97   Loss :  49159.2328084
Iteration :  98   Loss :  47204.3797754
Iteration :  99   Loss :  62256.13586
[-0.59174165 -0.53277738  0.04018996 ...,  0.7963583  -0.23901792
 -0.00720634]
CROSS VALIDATION 1
Iteration :  0   Loss :  37767.9153958
Iteration :  1   Loss :  16780.6079867
Iteration :  2   Loss :  21282.7916211
Iteration :  3   Loss :  46825.5763016
Iteration :  4   Loss :  29686.0748517
Iteration :  5   Loss :  17127.6953777
Iteration :  6   Loss :  32022.12699
Iteration :  7   Loss :  9671.28881572
Iteration :  8   Loss :  45030.0798139
Iteration :  9   Loss :  24283.8788587
Iteration :  10   Loss :  64227.4954062
Iteration :  11   Loss :  30242.2901392
Iteration :  12   Loss :  17057.2708076
Iteration :  13   Loss :  40344.7673266
Iteration :  14   Loss :  16040.7585615
Iteration :  15   Loss :  29265.2501472
Iteration :  16   Loss :  3995.41918952
Iteration :  17   Loss :  67192.2632342
Iteration :  18   Loss :  42342.19989
Iteration :  19   Loss :  6103.85258095
Iteration :  20   Loss :  25683.4567207
Iteration :  21   Loss :  17735.4043052
Iteration :  22   Loss :  34662.7093112
Iteration :  23   Loss :  22412.5985473
Iteration :  24   Loss :  40920.4672359
Iteration :  25   Loss :  47149.6834793
Iteration :  26   Loss :  42946.5931863
Iteration :  27   Loss :  29603.9694575
Iteration :  28   Loss :  20804.5735688
Iteration :  29   Loss :  25840.6976723
Iteration :  30   Loss :  19219.9346072
Iteration :  31   Loss :  34687.0603301
Iteration :  32   Loss :  19453.4325966
Iteration :  33   Loss :  40217.9050874
Iteration :  34   Loss :  10548.1889279
Iteration :  35   Loss :  31832.6082925
Iteration :  36   Loss :  36618.7182462
Iteration :  37   Loss :  45680.8815874
Iteration :  38   Loss :  17975.0729307
Iteration :  39   Loss :  34316.1965179
Iteration :  40   Loss :  50196.400346
Iteration :  41   Loss :  43681.6901286
Iteration :  42   Loss :  30332.5522926
Iteration :  43   Loss :  43090.2621772
Iteration :  44   Loss :  34224.3590126
Iteration :  45   Loss :  4334.21446651
Iteration :  46   Loss :  25444.4167061
Iteration :  47   Loss :  12039.7159757
Iteration :  48   Loss :  28741.2707
Iteration :  49   Loss :  36894.8727432
Iteration :  50   Loss :  20634.8130306
Iteration :  51   Loss :  26174.1884878
Iteration :  52   Loss :  26909.4076327
Iteration :  53   Loss :  74501.1805102
Iteration :  54   Loss :  38909.6420101
Iteration :  55   Loss :  28039.5927831
Iteration :  56   Loss :  38472.7760615
Iteration :  57   Loss :  24122.2873552
Iteration :  58   Loss :  11088.032232
Iteration :  59   Loss :  23533.6913621
Iteration :  60   Loss :  38679.46181
Iteration :  61   Loss :  30017.0873562
Iteration :  62   Loss :  41379.6145927
Iteration :  63   Loss :  73190.97329
Iteration :  64   Loss :  39250.8306247
Iteration :  65   Loss :  29473.0333184
Iteration :  66   Loss :  37792.588298
Iteration :  67   Loss :  51848.2185169
Iteration :  68   Loss :  34576.4461859
Iteration :  69   Loss :  29091.8233356
Iteration :  70   Loss :  31184.6563305
Iteration :  71   Loss :  46919.7856838
Iteration :  72   Loss :  18998.2970376
Iteration :  73   Loss :  17609.9884146
Iteration :  74   Loss :  29765.7960368
Iteration :  75   Loss :  17033.6150526
Iteration :  76   Loss :  22207.1749244
Iteration :  77   Loss :  28924.8353151
Iteration :  78   Loss :  25299.1238299
Iteration :  79   Loss :  16309.7066856
Iteration :  80   Loss :  24309.0099159
Iteration :  81   Loss :  10057.5051488
Iteration :  82   Loss :  57105.2173557
Iteration :  83   Loss :  18232.7160379
Iteration :  84   Loss :  15219.8359798
Iteration :  85   Loss :  42183.8468014
Iteration :  86   Loss :  34905.1545516
Iteration :  87   Loss :  29928.6707548
Iteration :  88   Loss :  29470.2420235
Iteration :  89   Loss :  26050.8189872
Iteration :  90   Loss :  5218.79242321
Iteration :  91   Loss :  68.9161052173
Iteration :  92   Loss :  0.91006293663
Iteration :  93   Loss :  0.038208068416
Iteration :  94   Loss :  42959.3744429
Iteration :  95   Loss :  42358.5451726
Iteration :  96   Loss :  56266.2641531
Iteration :  97   Loss :  38129.1060597
Iteration :  98   Loss :  13922.9466345
Iteration :  99   Loss :  47917.6974882
[-0.27028773 -0.81127377 -0.29747228 ...,  0.47413792 -0.43761095
 -0.05250103]
CROSS VALIDATION 2
Iteration :  0   Loss :  19301.6380581
Iteration :  1   Loss :  35883.6281251
Iteration :  2   Loss :  38022.7515718
Iteration :  3   Loss :  61009.5900301
Iteration :  4   Loss :  18627.7444574
Iteration :  5   Loss :  3576.008092
Iteration :  6   Loss :  38685.0448564
Iteration :  7   Loss :  18534.7433484
Iteration :  8   Loss :  38692.6506467
Iteration :  9   Loss :  22969.8488816
Iteration :  10   Loss :  48671.3089068
Iteration :  11   Loss :  4286.2765246
Iteration :  12   Loss :  56.6018841152
Iteration :  13   Loss :  0.747449042778
Iteration :  14   Loss :  1.44247419519
Iteration :  15   Loss :  25371.5999916
Iteration :  16   Loss :  34100.1994015
Iteration :  17   Loss :  48647.4881987
Iteration :  18   Loss :  21025.1475778
Iteration :  19   Loss :  21207.5482782
Iteration :  20   Loss :  21892.0168989
Iteration :  21   Loss :  14327.8689721
Iteration :  22   Loss :  41697.0546805
Iteration :  23   Loss :  56285.5732638
Iteration :  24   Loss :  11130.134688
Iteration :  25   Loss :  36299.9191011
Iteration :  26   Loss :  24280.4951426
Iteration :  27   Loss :  37683.3291117
Iteration :  28   Loss :  16441.5623965
Iteration :  29   Loss :  28643.1110874
Iteration :  30   Loss :  26994.3048099
Iteration :  31   Loss :  37570.4261204
Iteration :  32   Loss :  50353.2731902
Iteration :  33   Loss :  24271.6005057
Iteration :  34   Loss :  32590.5936096
Iteration :  35   Loss :  60689.3623474
Iteration :  36   Loss :  23317.3855076
Iteration :  37   Loss :  21125.5879882
Iteration :  38   Loss :  48772.2170746
Iteration :  39   Loss :  24023.0795017
Iteration :  40   Loss :  36075.1860681
Iteration :  41   Loss :  22832.109725
Iteration :  42   Loss :  25227.5434831
Iteration :  43   Loss :  23848.8925554
Iteration :  44   Loss :  8194.91781348
Iteration :  45   Loss :  46135.0447045
Iteration :  46   Loss :  35442.7585202
Iteration :  47   Loss :  73433.3275293
Iteration :  48   Loss :  19300.7254801
Iteration :  49   Loss :  37826.2462617
Iteration :  50   Loss :  42959.844302
Iteration :  51   Loss :  38163.1325284
Iteration :  52   Loss :  13064.6485646
Iteration :  53   Loss :  29635.5185966
Iteration :  54   Loss :  16075.4812253
Iteration :  55   Loss :  39789.1646733
Iteration :  56   Loss :  21989.0308964
Iteration :  57   Loss :  18409.5160098
Iteration :  58   Loss :  32840.9999793
Iteration :  59   Loss :  26453.882938
Iteration :  60   Loss :  9004.64933295
Iteration :  61   Loss :  38652.4264042
Iteration :  62   Loss :  37082.0805701
Iteration :  63   Loss :  59459.4784365
Iteration :  64   Loss :  23359.2886292
Iteration :  65   Loss :  52471.5756978
Iteration :  66   Loss :  21050.9745637
Iteration :  67   Loss :  50463.4649432
Iteration :  68   Loss :  35501.5377925
Iteration :  69   Loss :  13874.9794102
Iteration :  70   Loss :  15332.2146763
Iteration :  71   Loss :  26606.1391964
Iteration :  72   Loss :  23948.3499115
Iteration :  73   Loss :  25305.6451575
Iteration :  74   Loss :  10126.1579463
Iteration :  75   Loss :  37530.6296317
Iteration :  76   Loss :  43052.9417441
Iteration :  77   Loss :  30278.3039562
Iteration :  78   Loss :  27313.6020491
Iteration :  79   Loss :  16618.9494286
Iteration :  80   Loss :  41058.6648077
Iteration :  81   Loss :  23487.6524126
Iteration :  82   Loss :  20701.1845111
Iteration :  83   Loss :  38504.108023
Iteration :  84   Loss :  12246.9633154
Iteration :  85   Loss :  36196.1358071
Iteration :  86   Loss :  13392.4800484
Iteration :  87   Loss :  37914.3010556
Iteration :  88   Loss :  20623.0409893
Iteration :  89   Loss :  36394.5397966
Iteration :  90   Loss :  18845.7304383
Iteration :  91   Loss :  17097.5475362
Iteration :  92   Loss :  32268.6903565
Iteration :  93   Loss :  25073.4620257
Iteration :  94   Loss :  14086.1866598
Iteration :  95   Loss :  53367.9842681
Iteration :  96   Loss :  24210.6058922
Iteration :  97   Loss :  21868.7890341
Iteration :  98   Loss :  91692.4352499
Iteration :  99   Loss :  5899.56533933
[-0.80413195 -0.47162955  0.14342688 ...,  0.28712149  0.02200752
  0.07779889]
CROSS VALIDATION 3
Iteration :  0   Loss :  37802.1730106
Iteration :  1   Loss :  19020.9953668
Iteration :  2   Loss :  51917.8468174
Iteration :  3   Loss :  3750.28709184
Iteration :  4   Loss :  38161.5197715
Iteration :  5   Loss :  36427.68909
Iteration :  6   Loss :  15723.1386462
Iteration :  7   Loss :  42490.7885213
Iteration :  8   Loss :  29542.376635
Iteration :  9   Loss :  20595.8072468
Iteration :  10   Loss :  35055.6275938
Iteration :  11   Loss :  51035.158137
Iteration :  12   Loss :  24347.9242323
Iteration :  13   Loss :  32588.6327547
Iteration :  14   Loss :  20605.4424414
Iteration :  15   Loss :  22728.0758785
Iteration :  16   Loss :  51995.3300763
Iteration :  17   Loss :  30074.6486813
Iteration :  18   Loss :  5434.21914898
Iteration :  19   Loss :  29028.3354722
Iteration :  20   Loss :  31881.8394934
Iteration :  21   Loss :  26444.105741
Iteration :  22   Loss :  21509.063224
Iteration :  23   Loss :  16876.211192
Iteration :  24   Loss :  35098.9695962
Iteration :  25   Loss :  40138.7008167
Iteration :  26   Loss :  39132.8938697
Iteration :  27   Loss :  11294.6079319
Iteration :  28   Loss :  31768.7002686
Iteration :  29   Loss :  26415.9163146
Iteration :  30   Loss :  36301.220857
Iteration :  31   Loss :  27275.8845474
Iteration :  32   Loss :  21960.7112409
Iteration :  33   Loss :  30959.6779865
Iteration :  34   Loss :  31146.887334
Iteration :  35   Loss :  85125.9894694
Iteration :  36   Loss :  44099.3915055
Iteration :  37   Loss :  46188.6204655
Iteration :  38   Loss :  47386.3224407
Iteration :  39   Loss :  5517.75375143
Iteration :  40   Loss :  31303.5539526
Iteration :  41   Loss :  38076.5838153
Iteration :  42   Loss :  18692.1196777
Iteration :  43   Loss :  30874.3172407
Iteration :  44   Loss :  23110.4308556
Iteration :  45   Loss :  33233.4677844
Iteration :  46   Loss :  24764.8239333
Iteration :  47   Loss :  33421.5504541
Iteration :  48   Loss :  28225.1407491
Iteration :  49   Loss :  45103.0936123
Iteration :  50   Loss :  42917.8346564
Iteration :  51   Loss :  47842.2762619
Iteration :  52   Loss :  49078.6192853
Iteration :  53   Loss :  24148.0390774
Iteration :  54   Loss :  23713.6959592
Iteration :  55   Loss :  27831.9857047
Iteration :  56   Loss :  17430.5840649
Iteration :  57   Loss :  34103.1252411
Iteration :  58   Loss :  25118.6489725
Iteration :  59   Loss :  42851.7178603
Iteration :  60   Loss :  7311.75491783
Iteration :  61   Loss :  52032.9579413
Iteration :  62   Loss :  60391.9519176
Iteration :  63   Loss :  24413.1619158
Iteration :  64   Loss :  38772.2226001
Iteration :  65   Loss :  16569.6772825
Iteration :  66   Loss :  4710.65441688
Iteration :  67   Loss :  49819.0464943
Iteration :  68   Loss :  10161.6150605
Iteration :  69   Loss :  51918.7239961
Iteration :  70   Loss :  29160.2819239
Iteration :  71   Loss :  35586.2071423
Iteration :  72   Loss :  35624.740095
Iteration :  73   Loss :  5829.49842879
Iteration :  74   Loss :  54174.8383382
Iteration :  75   Loss :  31679.9654457
Iteration :  76   Loss :  26968.1660552
Iteration :  77   Loss :  25468.9463435
Iteration :  78   Loss :  31492.5933675
Iteration :  79   Loss :  21350.637511
Iteration :  80   Loss :  43791.6493619
Iteration :  81   Loss :  17830.4168881
Iteration :  82   Loss :  29335.3532606
Iteration :  83   Loss :  21116.3473958
Iteration :  84   Loss :  29792.9499824
Iteration :  85   Loss :  1099.18096048
Iteration :  86   Loss :  14.515095559
Iteration :  87   Loss :  100885.928579
Iteration :  88   Loss :  29350.5657727
Iteration :  89   Loss :  41312.1521105
Iteration :  90   Loss :  21954.0756904
Iteration :  91   Loss :  31033.3698175
Iteration :  92   Loss :  24289.4969089
Iteration :  93   Loss :  40552.9848526
Iteration :  94   Loss :  40237.6191322
Iteration :  95   Loss :  43779.0828864
Iteration :  96   Loss :  19737.403342
Iteration :  97   Loss :  18429.5494197
Iteration :  98   Loss :  40591.5171443
Iteration :  99   Loss :  9639.32537819
[-0.19204016 -0.35509905  0.32832879 ...,  0.14123182  0.12945064
  0.02843509]
CROSS VALIDATION 4
Iteration :  0   Loss :  37802.1730106
Iteration :  1   Loss :  19020.9953668
Iteration :  2   Loss :  51917.8468174
Iteration :  3   Loss :  3750.28709184
Iteration :  4   Loss :  38161.5197715
Iteration :  5   Loss :  36427.68909
Iteration :  6   Loss :  15723.1386462
Iteration :  7   Loss :  42490.7885213
Iteration :  8   Loss :  29542.376635
Iteration :  9   Loss :  20595.8072468
Iteration :  10   Loss :  35055.6275938
Iteration :  11   Loss :  51035.158137
Iteration :  12   Loss :  24347.9242323
Iteration :  13   Loss :  29447.6380649
Iteration :  14   Loss :  20605.4424414
Iteration :  15   Loss :  20070.9945037
Iteration :  16   Loss :  45606.1022664
Iteration :  17   Loss :  18044.0474535
Iteration :  18   Loss :  48282.4509372
Iteration :  19   Loss :  26201.5626369
Iteration :  20   Loss :  44424.2850273
Iteration :  21   Loss :  17007.2443495
Iteration :  22   Loss :  37228.4676878
Iteration :  23   Loss :  5144.71171497
Iteration :  24   Loss :  29443.9961296
Iteration :  25   Loss :  15350.1593479
Iteration :  26   Loss :  27785.9535869
Iteration :  27   Loss :  2845.47430474
Iteration :  28   Loss :  26557.6840656
Iteration :  29   Loss :  48347.9093433
Iteration :  30   Loss :  17925.301524
Iteration :  31   Loss :  26632.3185094
Iteration :  32   Loss :  22045.9835086
Iteration :  33   Loss :  25728.3875016
Iteration :  34   Loss :  17387.1412554
Iteration :  35   Loss :  10522.0192417
Iteration :  36   Loss :  34830.7975773
Iteration :  37   Loss :  17553.6501941
Iteration :  38   Loss :  30764.8564987
Iteration :  39   Loss :  22723.6069733
Iteration :  40   Loss :  19155.4800018
Iteration :  41   Loss :  25753.8857128
Iteration :  42   Loss :  33958.0401089
Iteration :  43   Loss :  25915.1661089
Iteration :  44   Loss :  25298.1863068
Iteration :  45   Loss :  46709.5194557
Iteration :  46   Loss :  7841.50534942
Iteration :  47   Loss :  30523.5727384
Iteration :  48   Loss :  31825.8024343
Iteration :  49   Loss :  27031.9181542
Iteration :  50   Loss :  16439.0135038
Iteration :  51   Loss :  72652.9509438
Iteration :  52   Loss :  27720.2690563
Iteration :  53   Loss :  26592.1088759
Iteration :  54   Loss :  25566.8401588
Iteration :  55   Loss :  31441.438206
Iteration :  56   Loss :  21349.8658041
Iteration :  57   Loss :  41672.1574359
Iteration :  58   Loss :  17830.4373006
Iteration :  59   Loss :  29335.3535985
Iteration :  60   Loss :  19985.9713072
Iteration :  61   Loss :  19662.637798
Iteration :  62   Loss :  34022.6594873
Iteration :  63   Loss :  53425.2000739
Iteration :  64   Loss :  18172.5834118
Iteration :  65   Loss :  19006.1318605
Iteration :  66   Loss :  12300.2238473
Iteration :  67   Loss :  55759.5217851
Iteration :  68   Loss :  17180.8198542
Iteration :  69   Loss :  30393.1664521
Iteration :  70   Loss :  39734.5696492
Iteration :  71   Loss :  57304.1826593
Iteration :  72   Loss :  17097.9339798
Iteration :  73   Loss :  50165.5875873
Iteration :  74   Loss :  16684.5792624
Iteration :  75   Loss :  31150.6377194
Iteration :  76   Loss :  76386.2921325
Iteration :  77   Loss :  46829.1859049
Iteration :  78   Loss :  24460.1579762
Iteration :  79   Loss :  26154.6747356
Iteration :  80   Loss :  50998.5612647
Iteration :  81   Loss :  11470.2159645
Iteration :  82   Loss :  26272.0195023
Iteration :  83   Loss :  19874.8001503
Iteration :  84   Loss :  26787.4946507
Iteration :  85   Loss :  40771.9368087
Iteration :  86   Loss :  40293.7228719
Iteration :  87   Loss :  19030.1477609
Iteration :  88   Loss :  34237.2957664
Iteration :  89   Loss :  19520.4430339
Iteration :  90   Loss :  16774.9479254
Iteration :  91   Loss :  20099.6866629
Iteration :  92   Loss :  37353.8464047
Iteration :  93   Loss :  43076.850717
Iteration :  94   Loss :  45469.9389757
Iteration :  95   Loss :  40299.4747162
Iteration :  96   Loss :  16291.1793586
Iteration :  97   Loss :  25103.509439
Iteration :  98   Loss :  33493.9815646
Iteration :  99   Loss :  38714.9782918
[-0.95629141 -0.18570602  0.22933143 ...,  0.55733124  0.37590765
  0.21969087]
CROSS VALIDATION 5
Iteration :  0   Loss :  27908.5691294
Iteration :  1   Loss :  19319.2517061
Iteration :  2   Loss :  21117.2814515
Iteration :  3   Loss :  34491.4272582
Iteration :  4   Loss :  6259.57303789
Iteration :  5   Loss :  25067.7954709
Iteration :  6   Loss :  25099.6022951
Iteration :  7   Loss :  30203.3075723
Iteration :  8   Loss :  46435.9823204
Iteration :  9   Loss :  21248.7738
Iteration :  10   Loss :  39890.9025004
Iteration :  11   Loss :  49525.4863689
Iteration :  12   Loss :  25936.5546294
Iteration :  13   Loss :  74008.1677839
Iteration :  14   Loss :  29708.21672
Iteration :  15   Loss :  15056.6701902
Iteration :  16   Loss :  29749.9431515
Iteration :  17   Loss :  31388.798385
Iteration :  18   Loss :  18879.9460015
Iteration :  19   Loss :  29492.7252175
Iteration :  20   Loss :  4852.70552673
Iteration :  21   Loss :  24212.8959127
Iteration :  22   Loss :  52340.4092413
Iteration :  23   Loss :  29691.4310797
Iteration :  24   Loss :  40341.1932868
Iteration :  25   Loss :  4598.98327697
Iteration :  26   Loss :  33787.2523695
Iteration :  27   Loss :  62931.4676496
Iteration :  28   Loss :  19557.6195582
Iteration :  29   Loss :  58178.9190258
Iteration :  30   Loss :  13278.4493856
Iteration :  31   Loss :  32372.6737976
Iteration :  32   Loss :  25335.7400204
Iteration :  33   Loss :  34344.5742559
Iteration :  34   Loss :  18782.2041171
Iteration :  35   Loss :  21062.4838835
Iteration :  36   Loss :  63970.0753272
Iteration :  37   Loss :  27310.41043
Iteration :  38   Loss :  22686.4147177
Iteration :  39   Loss :  36096.7359223
Iteration :  40   Loss :  71219.8491328
Iteration :  41   Loss :  27577.0342047
Iteration :  42   Loss :  26899.0365925
Iteration :  43   Loss :  33412.7977316
Iteration :  44   Loss :  1436.40905028
Iteration :  45   Loss :  18.968318572
Iteration :  46   Loss :  0.250483738861
Iteration :  47   Loss :  57870.8623667
Iteration :  48   Loss :  31031.8887737
Iteration :  49   Loss :  18162.5837889
Iteration :  50   Loss :  14950.7822688
Iteration :  51   Loss :  63379.4725593
Iteration :  52   Loss :  46509.4050766
Iteration :  53   Loss :  66711.4823469
Iteration :  54   Loss :  37814.4165929
Iteration :  55   Loss :  22168.3302981
Iteration :  56   Loss :  54500.5693379
Iteration :  57   Loss :  5638.94816458
Iteration :  58   Loss :  30463.3559768
Iteration :  59   Loss :  42203.8557969
Iteration :  60   Loss :  26955.6912082
Iteration :  61   Loss :  13815.0252412
Iteration :  62   Loss :  40083.7187352
Iteration :  63   Loss :  9282.37055897
Iteration :  64   Loss :  29566.4246909
Iteration :  65   Loss :  13793.9487542
Iteration :  66   Loss :  25836.891001
Iteration :  67   Loss :  18802.17174
Iteration :  68   Loss :  29831.5757505
Iteration :  69   Loss :  38793.1914122
Iteration :  70   Loss :  42810.9480538
Iteration :  71   Loss :  33751.0203846
Iteration :  72   Loss :  30444.7655352
Iteration :  73   Loss :  28387.1566782
Iteration :  74   Loss :  46047.615447
Iteration :  75   Loss :  20887.3652487
Iteration :  76   Loss :  21807.0677045
Iteration :  77   Loss :  25795.403768
Iteration :  78   Loss :  55228.4319721
Iteration :  79   Loss :  24673.1118238
Iteration :  80   Loss :  44084.5315088
Iteration :  81   Loss :  21685.3094311
Iteration :  82   Loss :  31145.9326383
Iteration :  83   Loss :  14993.1897759
Iteration :  84   Loss :  12676.2773496
Iteration :  85   Loss :  22195.7572464
Iteration :  86   Loss :  40876.8661945
Iteration :  87   Loss :  58310.1177314
Iteration :  88   Loss :  35683.0567963
Iteration :  89   Loss :  37049.6942232
Iteration :  90   Loss :  26183.1899255
Iteration :  91   Loss :  79504.9000273
Iteration :  92   Loss :  34093.6599866
Iteration :  93   Loss :  49403.7910626
Iteration :  94   Loss :  12103.0097197
Iteration :  95   Loss :  48329.4329579
Iteration :  96   Loss :  12860.7516855
Iteration :  97   Loss :  17658.7663727
Iteration :  98   Loss :  5464.24365192
Iteration :  99   Loss :  29729.4292267
[-0.95671728 -1.23967462 -0.15287001 ...,  0.26999776 -0.10480701
 -0.00823661]
CROSS VALIDATION 6
Iteration :  0   Loss :  37705.0765172
Iteration :  1   Loss :  18904.6425732
Iteration :  2   Loss :  51905.6189942
Iteration :  3   Loss :  3728.54326016
Iteration :  4   Loss :  38158.6043586
Iteration :  5   Loss :  36413.3994715
Iteration :  6   Loss :  15764.7507295
Iteration :  7   Loss :  42373.0055122
Iteration :  8   Loss :  29542.9337144
Iteration :  9   Loss :  20594.350588
Iteration :  10   Loss :  34981.6496534
Iteration :  11   Loss :  51048.1799799
Iteration :  12   Loss :  24335.3013287
Iteration :  13   Loss :  32435.7396064
Iteration :  14   Loss :  20486.6053629
Iteration :  15   Loss :  22712.6817322
Iteration :  16   Loss :  29074.6817907
Iteration :  17   Loss :  1024.95822764
Iteration :  18   Loss :  45696.2056034
Iteration :  19   Loss :  46505.1151854
Iteration :  20   Loss :  13981.4622979
Iteration :  21   Loss :  5291.61739204
Iteration :  22   Loss :  28860.9289207
Iteration :  23   Loss :  47306.9461338
Iteration :  24   Loss :  41584.0791053
Iteration :  25   Loss :  41655.0619457
Iteration :  26   Loss :  39290.409843
Iteration :  27   Loss :  20409.9949767
Iteration :  28   Loss :  23342.0343314
Iteration :  29   Loss :  22075.6454132
Iteration :  30   Loss :  21732.8092305
Iteration :  31   Loss :  26037.6589413
Iteration :  32   Loss :  18493.2690391
Iteration :  33   Loss :  86108.7711638
Iteration :  34   Loss :  32858.2370405
Iteration :  35   Loss :  11454.2903622
Iteration :  36   Loss :  37211.893541
Iteration :  37   Loss :  37154.3441754
Iteration :  38   Loss :  21662.284856
Iteration :  39   Loss :  46369.194764
Iteration :  40   Loss :  44259.0293157
Iteration :  41   Loss :  40900.8531192
Iteration :  42   Loss :  31507.1223223
Iteration :  43   Loss :  24754.6317309
Iteration :  44   Loss :  33814.9991619
Iteration :  45   Loss :  25910.5223779
Iteration :  46   Loss :  12274.3647947
Iteration :  47   Loss :  58757.994622
Iteration :  48   Loss :  64575.1759932
Iteration :  49   Loss :  24987.7991833
Iteration :  50   Loss :  27493.003798
Iteration :  51   Loss :  35388.9992724
Iteration :  52   Loss :  39096.8444469
Iteration :  53   Loss :  14783.8470029
Iteration :  54   Loss :  52183.6879987
Iteration :  55   Loss :  24887.5709094
Iteration :  56   Loss :  48178.5848842
Iteration :  57   Loss :  18484.7303842
Iteration :  58   Loss :  47944.0065791
Iteration :  59   Loss :  62490.4567723
Iteration :  60   Loss :  17393.4019934
Iteration :  61   Loss :  8358.81426895
Iteration :  62   Loss :  26121.3789827
Iteration :  63   Loss :  59832.6277103
Iteration :  64   Loss :  29298.8799959
Iteration :  65   Loss :  14040.9609722
Iteration :  66   Loss :  32547.385845
Iteration :  67   Loss :  20614.213835
Iteration :  68   Loss :  40247.7099768
Iteration :  69   Loss :  1109.35815365
Iteration :  70   Loss :  22355.5133249
Iteration :  71   Loss :  2019.47945008
Iteration :  72   Loss :  56652.037809
Iteration :  73   Loss :  28188.9134735
Iteration :  74   Loss :  43520.7756062
Iteration :  75   Loss :  21648.2951451
Iteration :  76   Loss :  22451.0897376
Iteration :  77   Loss :  18757.8009485
Iteration :  78   Loss :  42872.6310844
Iteration :  79   Loss :  6855.46325485
Iteration :  80   Loss :  58165.3168159
Iteration :  81   Loss :  38879.6471993
Iteration :  82   Loss :  27942.7266988
Iteration :  83   Loss :  46726.1277708
Iteration :  84   Loss :  18967.2426367
Iteration :  85   Loss :  27930.9568162
Iteration :  86   Loss :  5132.5198546
Iteration :  87   Loss :  67.7768436923
Iteration :  88   Loss :  0.89501856223
Iteration :  89   Loss :  0.561738759966
Iteration :  90   Loss :  43884.8982616
Iteration :  91   Loss :  29029.8679161
Iteration :  92   Loss :  14710.184239
Iteration :  93   Loss :  31501.0151744
Iteration :  94   Loss :  28095.6890218
Iteration :  95   Loss :  22593.163115
Iteration :  96   Loss :  16050.0894513
Iteration :  97   Loss :  24111.4281912
Iteration :  98   Loss :  10813.6989864
Iteration :  99   Loss :  44801.6183807
[-0.5341777  -0.76935474 -0.42471893 ...,  0.52427489 -0.61587257
 -0.06133166]
CROSS VALIDATION 7
Iteration :  0   Loss :  37705.0765172
Iteration :  1   Loss :  18904.6425732
Iteration :  2   Loss :  51905.6189942
Iteration :  3   Loss :  3728.54326016
Iteration :  4   Loss :  38158.6043586
Iteration :  5   Loss :  36413.3994715
Iteration :  6   Loss :  15764.7507295
Iteration :  7   Loss :  42373.0055122
Iteration :  8   Loss :  29542.9337144
Iteration :  9   Loss :  20594.350588
Iteration :  10   Loss :  34981.6496534
Iteration :  11   Loss :  50307.3503914
Iteration :  12   Loss :  24335.3013287
Iteration :  13   Loss :  32435.7396064
Iteration :  14   Loss :  20486.6053629
Iteration :  15   Loss :  22712.6817322
Iteration :  16   Loss :  51932.5587724
Iteration :  17   Loss :  29958.5622241
Iteration :  18   Loss :  5438.32048816
Iteration :  19   Loss :  29014.6084892
Iteration :  20   Loss :  31880.4018328
Iteration :  21   Loss :  26347.6154373
Iteration :  22   Loss :  21505.9641191
Iteration :  23   Loss :  16874.5727466
Iteration :  24   Loss :  35097.4631562
Iteration :  25   Loss :  40155.3769797
Iteration :  26   Loss :  39015.1199947
Iteration :  27   Loss :  11002.3369488
Iteration :  28   Loss :  46523.7768242
Iteration :  29   Loss :  16450.9066311
Iteration :  30   Loss :  39675.9629368
Iteration :  31   Loss :  34935.8377973
Iteration :  32   Loss :  47489.0498913
Iteration :  33   Loss :  50321.5626226
Iteration :  34   Loss :  17669.5597122
Iteration :  35   Loss :  28339.3401681
Iteration :  36   Loss :  7950.42438069
Iteration :  37   Loss :  47739.4317191
Iteration :  38   Loss :  17676.7048156
Iteration :  39   Loss :  25546.7158534
Iteration :  40   Loss :  31281.6583108
Iteration :  41   Loss :  69025.8683865
Iteration :  42   Loss :  2496.11743525
Iteration :  43   Loss :  30525.3482404
Iteration :  44   Loss :  23514.6458198
Iteration :  45   Loss :  15163.5994434
Iteration :  46   Loss :  16088.356652
Iteration :  47   Loss :  49020.4536112
Iteration :  48   Loss :  5922.30819884
Iteration :  49   Loss :  35870.1985215
Iteration :  50   Loss :  5064.76860906
Iteration :  51   Loss :  26734.4945266
Iteration :  52   Loss :  9624.71839583
Iteration :  53   Loss :  35408.5688527
Iteration :  54   Loss :  7662.35456211
Iteration :  55   Loss :  27915.013013
Iteration :  56   Loss :  1128.37683972
Iteration :  57   Loss :  43501.0045001
Iteration :  58   Loss :  50622.5133519
Iteration :  59   Loss :  55420.1826962
Iteration :  60   Loss :  24182.7768386
Iteration :  61   Loss :  34181.6274393
Iteration :  62   Loss :  14027.3515889
Iteration :  63   Loss :  22712.1678952
Iteration :  64   Loss :  30854.2370683
Iteration :  65   Loss :  55266.5284683
Iteration :  66   Loss :  82058.8121764
Iteration :  67   Loss :  20256.2550751
Iteration :  68   Loss :  34257.3598851
Iteration :  69   Loss :  17107.0853273
Iteration :  70   Loss :  32903.3165032
Iteration :  71   Loss :  13823.6263404
Iteration :  72   Loss :  62110.8348114
Iteration :  73   Loss :  18599.5975783
Iteration :  74   Loss :  33797.5643505
Iteration :  75   Loss :  15792.5953195
Iteration :  76   Loss :  39993.9256704
Iteration :  77   Loss :  33013.5935827
Iteration :  78   Loss :  36187.0146279
Iteration :  79   Loss :  50648.3742841
Iteration :  80   Loss :  30315.534734
Iteration :  81   Loss :  16124.1427487
Iteration :  82   Loss :  22617.5874553
Iteration :  83   Loss :  30777.6964542
Iteration :  84   Loss :  55230.2933622
Iteration :  85   Loss :  82073.4208563
Iteration :  86   Loss :  20256.1190096
Iteration :  87   Loss :  34257.3436511
Iteration :  88   Loss :  17107.0863669
Iteration :  89   Loss :  32903.3166746
Iteration :  90   Loss :  13823.6263591
Iteration :  91   Loss :  62110.8348151
Iteration :  92   Loss :  18599.5975777
Iteration :  93   Loss :  33797.5643503
Iteration :  94   Loss :  15792.5953195
Iteration :  95   Loss :  39993.9256704
Iteration :  96   Loss :  33013.5935827
Iteration :  97   Loss :  36187.0146279
Iteration :  98   Loss :  50648.3742841
Iteration :  99   Loss :  30315.534734
[-0.92532884 -1.06012982 -0.25922504 ...,  0.4161888  -0.42855353
  0.03398972]
CROSS VALIDATION 8
Iteration :  0   Loss :  37100.9832907
Iteration :  1   Loss :  24572.8330324
Iteration :  2   Loss :  52306.9395921
Iteration :  3   Loss :  16345.80364
Iteration :  4   Loss :  32537.1030103
Iteration :  5   Loss :  22792.2030332
Iteration :  6   Loss :  36588.8553808
Iteration :  7   Loss :  35575.3563255
Iteration :  8   Loss :  67107.1410518
Iteration :  9   Loss :  3626.79115323
Iteration :  10   Loss :  47697.4986537
Iteration :  11   Loss :  31935.8291623
Iteration :  12   Loss :  26903.5207186
Iteration :  13   Loss :  28860.2141569
Iteration :  14   Loss :  36855.1994818
Iteration :  15   Loss :  11166.9121033
Iteration :  16   Loss :  22533.4758507
Iteration :  17   Loss :  18166.9560407
Iteration :  18   Loss :  35630.0678484
Iteration :  19   Loss :  20614.6794987
Iteration :  20   Loss :  42652.6527967
Iteration :  21   Loss :  11632.0190109
Iteration :  22   Loss :  44097.0015953
Iteration :  23   Loss :  29430.4619483
Iteration :  24   Loss :  62209.4100736
Iteration :  25   Loss :  28855.6112548
Iteration :  26   Loss :  23243.0602058
Iteration :  27   Loss :  33189.9509772
Iteration :  28   Loss :  6727.51001411
Iteration :  29   Loss :  31050.9739209
Iteration :  30   Loss :  47329.9804406
Iteration :  31   Loss :  41598.7642821
Iteration :  32   Loss :  41663.1713136
Iteration :  33   Loss :  31050.157305
Iteration :  34   Loss :  25537.5136184
Iteration :  35   Loss :  43532.4419848
Iteration :  36   Loss :  44350.6763783
Iteration :  37   Loss :  34958.044692
Iteration :  38   Loss :  15047.4062322
Iteration :  39   Loss :  49413.8931605
Iteration :  40   Loss :  30110.4127397
Iteration :  41   Loss :  18101.3741724
Iteration :  42   Loss :  22596.0017097
Iteration :  43   Loss :  22354.9024473
Iteration :  44   Loss :  25239.1879107
Iteration :  45   Loss :  23448.5550806
Iteration :  46   Loss :  34479.1843636
Iteration :  47   Loss :  25372.0942742
Iteration :  48   Loss :  57232.084079
Iteration :  49   Loss :  21584.0445999
Iteration :  50   Loss :  45814.1838996
Iteration :  51   Loss :  26762.1098043
Iteration :  52   Loss :  28724.7812322
Iteration :  53   Loss :  36199.1479372
Iteration :  54   Loss :  55273.3432932
Iteration :  55   Loss :  29187.7595697
Iteration :  56   Loss :  14786.9675508
Iteration :  57   Loss :  17789.1072865
Iteration :  58   Loss :  42296.7466382
Iteration :  59   Loss :  28160.0615996
Iteration :  60   Loss :  70000.2582282
Iteration :  61   Loss :  22240.3495686
Iteration :  62   Loss :  15700.49829
Iteration :  63   Loss :  27301.4464032
Iteration :  64   Loss :  26426.0377931
Iteration :  65   Loss :  50571.8817375
Iteration :  66   Loss :  29310.4019287
Iteration :  67   Loss :  29296.9224369
Iteration :  68   Loss :  14501.5488394
Iteration :  69   Loss :  13352.5788441
Iteration :  70   Loss :  44332.9746547
Iteration :  71   Loss :  19248.0724288
Iteration :  72   Loss :  42597.0300537
Iteration :  73   Loss :  72829.5207903
Iteration :  74   Loss :  30929.8674929
Iteration :  75   Loss :  78667.597089
Iteration :  76   Loss :  4779.49782296
Iteration :  77   Loss :  63.115055772
Iteration :  78   Loss :  0.833457909734
Iteration :  79   Loss :  41646.5550708
Iteration :  80   Loss :  17427.4119035
Iteration :  81   Loss :  45894.32574
Iteration :  82   Loss :  27806.3017567
Iteration :  83   Loss :  35853.5449975
Iteration :  84   Loss :  33784.7406456
Iteration :  85   Loss :  45462.6454523
Iteration :  86   Loss :  42666.6389096
Iteration :  87   Loss :  21617.4306293
Iteration :  88   Loss :  32420.5155015
Iteration :  89   Loss :  43621.0058838
Iteration :  90   Loss :  26055.8013982
Iteration :  91   Loss :  16079.1404703
Iteration :  92   Loss :  65181.3489052
Iteration :  93   Loss :  39667.8262484
Iteration :  94   Loss :  27974.1834014
Iteration :  95   Loss :  52266.6650287
Iteration :  96   Loss :  28743.6977376
Iteration :  97   Loss :  33661.1641882
Iteration :  98   Loss :  36217.9508592
Iteration :  99   Loss :  58486.327872
[-0.90435201 -0.51928394 -0.04182479 ...,  0.0385124   0.46498422
  0.00168035]
CROSS VALIDATION 9
Iteration :  0   Loss :  34878.2767598
Iteration :  1   Loss :  19109.3270286
Iteration :  2   Loss :  24385.2654246
Iteration :  3   Loss :  25477.5572105
Iteration :  4   Loss :  40532.7002302
Iteration :  5   Loss :  70130.081741
Iteration :  6   Loss :  23951.6643329
Iteration :  7   Loss :  30809.5424184
Iteration :  8   Loss :  39553.9805872
Iteration :  9   Loss :  53875.2299284
Iteration :  10   Loss :  32559.3246808
Iteration :  11   Loss :  5840.29779469
Iteration :  12   Loss :  28046.4809673
Iteration :  13   Loss :  45479.271091
Iteration :  14   Loss :  32685.4826582
Iteration :  15   Loss :  20041.833271
Iteration :  16   Loss :  41038.2945367
Iteration :  17   Loss :  17167.5198612
Iteration :  18   Loss :  23246.685922
Iteration :  19   Loss :  49768.0177749
Iteration :  20   Loss :  25359.3304327
Iteration :  21   Loss :  18363.0119871
Iteration :  22   Loss :  36931.7499036
Iteration :  23   Loss :  14421.819985
Iteration :  24   Loss :  32985.0977349
Iteration :  25   Loss :  17440.2290998
Iteration :  26   Loss :  36207.3335505
Iteration :  27   Loss :  6904.31057647
Iteration :  28   Loss :  44170.0515219
Iteration :  29   Loss :  8478.70179247
Iteration :  30   Loss :  22797.1257228
Iteration :  31   Loss :  12567.9839063
Iteration :  32   Loss :  46344.0575104
Iteration :  33   Loss :  18533.1428202
Iteration :  34   Loss :  37069.25742
Iteration :  35   Loss :  5496.99954258
Iteration :  36   Loss :  35158.2647876
Iteration :  37   Loss :  25232.8335235
Iteration :  38   Loss :  24529.0556162
Iteration :  39   Loss :  36607.0900724
Iteration :  40   Loss :  30612.4322329
Iteration :  41   Loss :  46224.645212
Iteration :  42   Loss :  21082.7729323
Iteration :  43   Loss :  27941.3019494
Iteration :  44   Loss :  37752.9576798
Iteration :  45   Loss :  14597.3921094
Iteration :  46   Loss :  40884.1873388
Iteration :  47   Loss :  13289.0856996
Iteration :  48   Loss :  37360.5641239
Iteration :  49   Loss :  24631.1288088
Iteration :  50   Loss :  24539.3553572
Iteration :  51   Loss :  32238.7973541
Iteration :  52   Loss :  27930.1879082
Iteration :  53   Loss :  10063.7868469
Iteration :  54   Loss :  67376.6620814
Iteration :  55   Loss :  17719.2905078
Iteration :  56   Loss :  37826.4816313
Iteration :  57   Loss :  10243.9419547
Iteration :  58   Loss :  33072.5738193
Iteration :  59   Loss :  6598.41276683
Iteration :  60   Loss :  50671.2733808
Iteration :  61   Loss :  14251.2084967
Iteration :  62   Loss :  37383.6408899
Iteration :  63   Loss :  47690.3878134
Iteration :  64   Loss :  14857.4595516
Iteration :  65   Loss :  21747.6950863
Iteration :  66   Loss :  21983.781463
Iteration :  67   Loss :  53277.1148986
Iteration :  68   Loss :  29865.3839708
Iteration :  69   Loss :  8933.81781914
Iteration :  70   Loss :  34370.9856407
Iteration :  71   Loss :  10237.0321564
Iteration :  72   Loss :  53017.1002527
Iteration :  73   Loss :  31983.151207
Iteration :  74   Loss :  35415.1277445
Iteration :  75   Loss :  48785.1986904
Iteration :  76   Loss :  18887.6775431
Iteration :  77   Loss :  34201.3962651
Iteration :  78   Loss :  44993.3858162
Iteration :  79   Loss :  18653.5890552
Iteration :  80   Loss :  24571.0402432
Iteration :  81   Loss :  21628.5509362
Iteration :  82   Loss :  36704.0220121
Iteration :  83   Loss :  1011.98517537
Iteration :  84   Loss :  52886.9252065
Iteration :  85   Loss :  31121.7060313
Iteration :  86   Loss :  27827.8304069
Iteration :  87   Loss :  68803.9673422
Iteration :  88   Loss :  45638.297735
Iteration :  89   Loss :  24060.2866663
Iteration :  90   Loss :  18452.2797949
Iteration :  91   Loss :  53726.2906064
Iteration :  92   Loss :  15199.4133716
Iteration :  93   Loss :  21762.9097724
Iteration :  94   Loss :  21990.2337408
Iteration :  95   Loss :  53282.1845118
Iteration :  96   Loss :  29865.1962543
Iteration :  97   Loss :  8933.82260832
Iteration :  98   Loss :  34370.985253
Iteration :  99   Loss :  10237.0323649
[-0.57159611 -0.04415171  0.3649206  ...,  0.1871135   0.15847213
  0.0556333 ]
CROSS VALIDATION 10
Iteration :  0   Loss :  27145.4514821
Iteration :  1   Loss :  18350.6340877
Iteration :  2   Loss :  41799.8450672
Iteration :  3   Loss :  19729.2033644
Iteration :  4   Loss :  46064.8657454
Iteration :  5   Loss :  26692.1836092
Iteration :  6   Loss :  51949.6594752
Iteration :  7   Loss :  42200.5394842
Iteration :  8   Loss :  24027.4085371
Iteration :  9   Loss :  44605.1909253
Iteration :  10   Loss :  12064.3708598
Iteration :  11   Loss :  37900.4659534
Iteration :  12   Loss :  7351.1297389
Iteration :  13   Loss :  64141.4578741
Iteration :  14   Loss :  26260.1411578
Iteration :  15   Loss :  23414.1638948
Iteration :  16   Loss :  51191.2053527
Iteration :  17   Loss :  38141.1542667
Iteration :  18   Loss :  34151.8704124
Iteration :  19   Loss :  78136.9682955
Iteration :  20   Loss :  21367.9807313
Iteration :  21   Loss :  66384.9158095
Iteration :  22   Loss :  15285.1191817
Iteration :  23   Loss :  30265.0498384
Iteration :  24   Loss :  16069.6123242
Iteration :  25   Loss :  41833.3297293
Iteration :  26   Loss :  47894.4541485
Iteration :  27   Loss :  43400.1242095
Iteration :  28   Loss :  51513.2266403
Iteration :  29   Loss :  42149.4296408
Iteration :  30   Loss :  47681.5907155
Iteration :  31   Loss :  7889.59569574
Iteration :  32   Loss :  34152.4746157
Iteration :  33   Loss :  24323.0546114
Iteration :  34   Loss :  33604.4499301
Iteration :  35   Loss :  17950.7249534
Iteration :  36   Loss :  8364.57880069
Iteration :  37   Loss :  28281.918319
Iteration :  38   Loss :  35623.8752892
Iteration :  39   Loss :  20386.3340512
Iteration :  40   Loss :  42316.3217092
Iteration :  41   Loss :  25602.009778
Iteration :  42   Loss :  18317.1374208
Iteration :  43   Loss :  33208.2751054
Iteration :  44   Loss :  22090.4804523
Iteration :  45   Loss :  39558.6333415
Iteration :  46   Loss :  10018.9161654
Iteration :  47   Loss :  49157.4843425
Iteration :  48   Loss :  37092.9484202
Iteration :  49   Loss :  16536.3388036
Iteration :  50   Loss :  51843.5696861
Iteration :  51   Loss :  11233.9114186
Iteration :  52   Loss :  54197.1813066
Iteration :  53   Loss :  15376.0113714
Iteration :  54   Loss :  42813.3333118
Iteration :  55   Loss :  17087.9727527
Iteration :  56   Loss :  30179.9111571
Iteration :  57   Loss :  16474.2446383
Iteration :  58   Loss :  50699.5014589
Iteration :  59   Loss :  19787.2466489
Iteration :  60   Loss :  59294.2324665
Iteration :  61   Loss :  30410.0260635
Iteration :  62   Loss :  35745.4349946
Iteration :  63   Loss :  49859.0037915
Iteration :  64   Loss :  34632.5893525
Iteration :  65   Loss :  44005.6291997
Iteration :  66   Loss :  22281.7862422
Iteration :  67   Loss :  37543.1529843
Iteration :  68   Loss :  38127.3521971
Iteration :  69   Loss :  38182.0702592
Iteration :  70   Loss :  25868.9757326
Iteration :  71   Loss :  37104.1864406
Iteration :  72   Loss :  27852.0422103
Iteration :  73   Loss :  24444.7447698
Iteration :  74   Loss :  39745.2185033
Iteration :  75   Loss :  31851.2439716
Iteration :  76   Loss :  32457.3558668
Iteration :  77   Loss :  44601.0998796
Iteration :  78   Loss :  42655.0973179
Iteration :  79   Loss :  42780.4478371
Iteration :  80   Loss :  15831.1750929
Iteration :  81   Loss :  36860.7750884
Iteration :  82   Loss :  57453.470474
Iteration :  83   Loss :  16090.7984924
Iteration :  84   Loss :  35525.9186909
Iteration :  85   Loss :  6113.77171739
Iteration :  86   Loss :  34468.431076
Iteration :  87   Loss :  3949.58221195
Iteration :  88   Loss :  73053.4610647
Iteration :  89   Loss :  22235.5920905
Iteration :  90   Loss :  25223.4639129
Iteration :  91   Loss :  6239.33703562
Iteration :  92   Loss :  50227.0748208
Iteration :  93   Loss :  20850.7838644
Iteration :  94   Loss :  21873.5661705
Iteration :  95   Loss :  28153.1164468
Iteration :  96   Loss :  47955.4757624
Iteration :  97   Loss :  56907.207608
Iteration :  98   Loss :  27982.5691396
Iteration :  99   Loss :  22824.802232
[ -2.78772049e-01   1.84340679e-04   7.24515876e-01 ...,   3.38269877e-01
   1.67540918e-01   1.11660322e-01]
CROSS VALIDATION 11
Iteration :  0   Loss :  37775.2554268
Iteration :  1   Loss :  16295.7186693
Iteration :  2   Loss :  28526.1049714
Iteration :  3   Loss :  42153.2021915
Iteration :  4   Loss :  9390.73076125
Iteration :  5   Loss :  39780.2360797
Iteration :  6   Loss :  1968.64479843
Iteration :  7   Loss :  42755.5226253
Iteration :  8   Loss :  18110.3774888
Iteration :  9   Loss :  27943.5947591
Iteration :  10   Loss :  10198.4442233
Iteration :  11   Loss :  24259.6594208
Iteration :  12   Loss :  20948.2163396
Iteration :  13   Loss :  45735.2272368
Iteration :  14   Loss :  9568.19960642
Iteration :  15   Loss :  56103.8744007
Iteration :  16   Loss :  74199.6947132
Iteration :  17   Loss :  25287.9338918
Iteration :  18   Loss :  18590.8231501
Iteration :  19   Loss :  32448.6607916
Iteration :  20   Loss :  16653.9873163
Iteration :  21   Loss :  24420.4134277
Iteration :  22   Loss :  44598.1617342
Iteration :  23   Loss :  64197.1926596
Iteration :  24   Loss :  18158.7619095
Iteration :  25   Loss :  7319.86170252
Iteration :  26   Loss :  31767.9755281
Iteration :  27   Loss :  23143.2553506
Iteration :  28   Loss :  41103.2429044
Iteration :  29   Loss :  18195.2990451
Iteration :  30   Loss :  33448.0862449
Iteration :  31   Loss :  71731.1014729
Iteration :  32   Loss :  17474.0243491
Iteration :  33   Loss :  53043.083929
Iteration :  34   Loss :  11400.8735661
Iteration :  35   Loss :  25739.2712746
Iteration :  36   Loss :  37654.7223979
Iteration :  37   Loss :  49946.0870098
Iteration :  38   Loss :  13355.4014827
Iteration :  39   Loss :  33981.5546802
Iteration :  40   Loss :  16453.8606328
Iteration :  41   Loss :  19578.973299
Iteration :  42   Loss :  16051.8503814
Iteration :  43   Loss :  23355.0964414
Iteration :  44   Loss :  8545.29578247
Iteration :  45   Loss :  15126.8101723
Iteration :  46   Loss :  18725.2811602
Iteration :  47   Loss :  24929.6233851
Iteration :  48   Loss :  60304.3028356
Iteration :  49   Loss :  39683.7150419
Iteration :  50   Loss :  19924.8990379
Iteration :  51   Loss :  39219.5396094
Iteration :  52   Loss :  47553.3685166
Iteration :  53   Loss :  43835.5922879
Iteration :  54   Loss :  5175.32951613
Iteration :  55   Loss :  24634.636875
Iteration :  56   Loss :  40630.5264238
Iteration :  57   Loss :  50155.699641
Iteration :  58   Loss :  11162.7691541
Iteration :  59   Loss :  43091.7648714
Iteration :  60   Loss :  17310.2939316
Iteration :  61   Loss :  41323.0337241
Iteration :  62   Loss :  21672.3868391
Iteration :  63   Loss :  27487.7752618
Iteration :  64   Loss :  37791.4764266
Iteration :  65   Loss :  81529.1908759
Iteration :  66   Loss :  40995.3826256
Iteration :  67   Loss :  48249.675177
Iteration :  68   Loss :  22120.7216304
Iteration :  69   Loss :  33383.5156143
Iteration :  70   Loss :  29304.4959116
Iteration :  71   Loss :  29816.5624466
Iteration :  72   Loss :  16725.3636806
Iteration :  73   Loss :  23415.8099434
Iteration :  74   Loss :  43719.5556367
Iteration :  75   Loss :  48294.014642
Iteration :  76   Loss :  13857.3273486
Iteration :  77   Loss :  39753.2514886
Iteration :  78   Loss :  8441.97517664
Iteration :  79   Loss :  42644.1382102
Iteration :  80   Loss :  17074.1708602
Iteration :  81   Loss :  42412.2764854
Iteration :  82   Loss :  9033.12761105
Iteration :  83   Loss :  24844.1703494
Iteration :  84   Loss :  32081.2458187
Iteration :  85   Loss :  43066.1365989
Iteration :  86   Loss :  6719.48632589
Iteration :  87   Loss :  25065.9869178
Iteration :  88   Loss :  19515.8142155
Iteration :  89   Loss :  56256.2830703
Iteration :  90   Loss :  4809.53384968
Iteration :  91   Loss :  25722.8212102
Iteration :  92   Loss :  38826.6283727
Iteration :  93   Loss :  68370.877755
Iteration :  94   Loss :  37670.1983937
Iteration :  95   Loss :  13835.1512019
Iteration :  96   Loss :  33947.0452698
Iteration :  97   Loss :  11728.2277422
Iteration :  98   Loss :  34020.7049419
Iteration :  99   Loss :  46750.8961421
[-0.68246908 -0.89060998 -0.11751472 ...,  0.42247843 -0.38658735
  0.00731048]
CROSS VALIDATION 12
Iteration :  0   Loss :  22685.836121
Iteration :  1   Loss :  27390.3670304
Iteration :  2   Loss :  19505.7207468
Iteration :  3   Loss :  20750.5623796
Iteration :  4   Loss :  22337.3200949
Iteration :  5   Loss :  11840.7271783
Iteration :  6   Loss :  33490.1745434
Iteration :  7   Loss :  50622.554851
Iteration :  8   Loss :  44233.9368542
Iteration :  9   Loss :  39230.226063
Iteration :  10   Loss :  16579.2264681
Iteration :  11   Loss :  48441.7445906
Iteration :  12   Loss :  35100.138568
Iteration :  13   Loss :  21839.4224687
Iteration :  14   Loss :  56021.5546158
Iteration :  15   Loss :  40548.393828
Iteration :  16   Loss :  43136.9266826
Iteration :  17   Loss :  28605.1771682
Iteration :  18   Loss :  37359.1140292
Iteration :  19   Loss :  9821.3721126
Iteration :  20   Loss :  24523.0226505
Iteration :  21   Loss :  21540.8113329
Iteration :  22   Loss :  51965.0134712
Iteration :  23   Loss :  31445.4352574
Iteration :  24   Loss :  28225.1384634
Iteration :  25   Loss :  55121.7888205
Iteration :  26   Loss :  36867.9262637
Iteration :  27   Loss :  19375.6615794
Iteration :  28   Loss :  29363.5057436
Iteration :  29   Loss :  14328.4464249
Iteration :  30   Loss :  47570.7045305
Iteration :  31   Loss :  31552.8082885
Iteration :  32   Loss :  54794.3917626
Iteration :  33   Loss :  57534.7470605
Iteration :  34   Loss :  27978.7446541
Iteration :  35   Loss :  69066.7708416
Iteration :  36   Loss :  38234.2069475
Iteration :  37   Loss :  25069.134656
Iteration :  38   Loss :  18010.3271412
Iteration :  39   Loss :  33211.8271739
Iteration :  40   Loss :  7333.9343984
Iteration :  41   Loss :  26066.8964704
Iteration :  42   Loss :  27582.0259944
Iteration :  43   Loss :  31380.2467197
Iteration :  44   Loss :  70315.5026352
Iteration :  45   Loss :  5083.79397351
Iteration :  46   Loss :  63468.4666568
Iteration :  47   Loss :  32426.4431921
Iteration :  48   Loss :  19891.8077151
Iteration :  49   Loss :  15988.1436086
Iteration :  50   Loss :  32885.2907555
Iteration :  51   Loss :  5287.13496636
Iteration :  52   Loss :  24875.646044
Iteration :  53   Loss :  21919.1098516
Iteration :  54   Loss :  37016.2793338
Iteration :  55   Loss :  21507.2178737
Iteration :  56   Loss :  26715.233743
Iteration :  57   Loss :  38438.7534977
Iteration :  58   Loss :  17632.2470699
Iteration :  59   Loss :  5588.80477483
Iteration :  60   Loss :  45188.7674082
Iteration :  61   Loss :  23329.4707091
Iteration :  62   Loss :  45105.3335715
Iteration :  63   Loss :  10889.6927763
Iteration :  64   Loss :  13781.1765369
Iteration :  65   Loss :  22399.7817765
Iteration :  66   Loss :  45355.9874533
Iteration :  67   Loss :  35613.2267214
Iteration :  68   Loss :  29721.5543349
Iteration :  69   Loss :  42418.5498106
Iteration :  70   Loss :  24009.994193
Iteration :  71   Loss :  18828.6441002
Iteration :  72   Loss :  31382.6301657
Iteration :  73   Loss :  20841.7300652
Iteration :  74   Loss :  30461.6503722
Iteration :  75   Loss :  40824.0929855
Iteration :  76   Loss :  38932.2164412
Iteration :  77   Loss :  57335.2649358
Iteration :  78   Loss :  24715.4809777
Iteration :  79   Loss :  43378.2887836
Iteration :  80   Loss :  27757.7972529
Iteration :  81   Loss :  75902.1968473
Iteration :  82   Loss :  74039.9870991
Iteration :  83   Loss :  9296.8926763
Iteration :  84   Loss :  30127.5806785
Iteration :  85   Loss :  5788.19985158
Iteration :  86   Loss :  31204.7234725
Iteration :  87   Loss :  71288.2600696
Iteration :  88   Loss :  53870.3354158
Iteration :  89   Loss :  23433.1606337
Iteration :  90   Loss :  33547.666939
Iteration :  91   Loss :  27180.9929086
Iteration :  92   Loss :  28154.042276
Iteration :  93   Loss :  32630.5929438
Iteration :  94   Loss :  25535.6542853
Iteration :  95   Loss :  15667.7557926
Iteration :  96   Loss :  32231.9911452
Iteration :  97   Loss :  11978.8306772
Iteration :  98   Loss :  26589.7017867
Iteration :  99   Loss :  23984.5603726
[-1.13707159 -0.23166611  0.05584079 ...,  0.55672324  0.21424346
  0.02941335]
CROSS VALIDATION 13
Iteration :  0   Loss :  45622.0996731
Iteration :  1   Loss :  10136.7116822
Iteration :  2   Loss :  37522.1380572
Iteration :  3   Loss :  12963.6656692
Iteration :  4   Loss :  23535.4928729
Iteration :  5   Loss :  16770.8677005
Iteration :  6   Loss :  45459.680957
Iteration :  7   Loss :  10573.7551448
Iteration :  8   Loss :  22891.2584486
Iteration :  9   Loss :  11538.5669128
Iteration :  10   Loss :  36789.5241411
Iteration :  11   Loss :  46598.7529486
Iteration :  12   Loss :  18903.6861368
Iteration :  13   Loss :  25205.2635324
Iteration :  14   Loss :  22811.7215462
Iteration :  15   Loss :  27425.6374797
Iteration :  16   Loss :  25365.2113405
Iteration :  17   Loss :  28223.1122122
Iteration :  18   Loss :  32621.1454126
Iteration :  19   Loss :  31053.8472441
Iteration :  20   Loss :  19134.9062374
Iteration :  21   Loss :  68791.8378941
Iteration :  22   Loss :  13849.8082018
Iteration :  23   Loss :  41598.0837153
Iteration :  24   Loss :  22173.0087009
Iteration :  25   Loss :  25738.5652586
Iteration :  26   Loss :  37266.3406758
Iteration :  27   Loss :  30384.723642
Iteration :  28   Loss :  36927.5563148
Iteration :  29   Loss :  35615.3799819
Iteration :  30   Loss :  14780.1836057
Iteration :  31   Loss :  34395.285255
Iteration :  32   Loss :  20564.3777269
Iteration :  33   Loss :  39700.1240164
Iteration :  34   Loss :  43208.5763306
Iteration :  35   Loss :  42936.3897723
Iteration :  36   Loss :  9325.7666726
Iteration :  37   Loss :  38395.6834274
Iteration :  38   Loss :  94765.1087252
Iteration :  39   Loss :  43981.3285818
Iteration :  40   Loss :  24889.4626626
Iteration :  41   Loss :  40688.7114067
Iteration :  42   Loss :  28986.7529995
Iteration :  43   Loss :  27540.2702154
Iteration :  44   Loss :  48586.7611874
Iteration :  45   Loss :  19186.5042138
Iteration :  46   Loss :  21509.5588403
Iteration :  47   Loss :  34633.4805281
Iteration :  48   Loss :  26917.1901743
Iteration :  49   Loss :  43938.6775888
Iteration :  50   Loss :  21532.9939431
Iteration :  51   Loss :  30573.5520082
Iteration :  52   Loss :  30888.1896498
Iteration :  53   Loss :  31283.8409139
Iteration :  54   Loss :  23443.3269063
Iteration :  55   Loss :  30787.6248713
Iteration :  56   Loss :  26216.4094546
Iteration :  57   Loss :  43989.6896661
Iteration :  58   Loss :  30800.4611407
Iteration :  59   Loss :  43607.6897321
Iteration :  60   Loss :  15503.0526803
Iteration :  61   Loss :  31951.3656588
Iteration :  62   Loss :  19795.7589147
Iteration :  63   Loss :  34661.1319424
Iteration :  64   Loss :  21473.0784102
Iteration :  65   Loss :  19675.5192269
Iteration :  66   Loss :  39379.7079819
Iteration :  67   Loss :  16605.4071146
Iteration :  68   Loss :  44236.126946
Iteration :  69   Loss :  22892.7763633
Iteration :  70   Loss :  30294.6736662
Iteration :  71   Loss :  13780.7989831
Iteration :  72   Loss :  23529.2487576
Iteration :  73   Loss :  30053.8215703
Iteration :  74   Loss :  55952.0926257
Iteration :  75   Loss :  24800.8712517
Iteration :  76   Loss :  40893.6954609
Iteration :  77   Loss :  11318.1946903
Iteration :  78   Loss :  38880.1725851
Iteration :  79   Loss :  37486.7643685
Iteration :  80   Loss :  37730.8222213
Iteration :  81   Loss :  41903.7428009
Iteration :  82   Loss :  54305.4749772
Iteration :  83   Loss :  29735.3070923
Iteration :  84   Loss :  42386.474011
Iteration :  85   Loss :  28754.6081856
Iteration :  86   Loss :  10161.5141822
Iteration :  87   Loss :  29186.8001666
Iteration :  88   Loss :  33517.4375893
Iteration :  89   Loss :  38026.0228206
Iteration :  90   Loss :  27894.9700654
Iteration :  91   Loss :  12556.2956252
Iteration :  92   Loss :  50462.4779018
Iteration :  93   Loss :  16852.1144255
Iteration :  94   Loss :  34495.8959137
Iteration :  95   Loss :  13846.9700062
Iteration :  96   Loss :  53550.8946008
Iteration :  97   Loss :  51960.864477
Iteration :  98   Loss :  17259.786309
Iteration :  99   Loss :  30638.4899288
[-0.10267653  0.05155621 -0.04007173 ...,  0.11422658  0.36976702
  0.13039248]
CROSS VALIDATION 14
Iteration :  0   Loss :  34530.3121444
Iteration :  1   Loss :  15543.9686284
Iteration :  2   Loss :  49178.2218482
Iteration :  3   Loss :  36774.2799772
Iteration :  4   Loss :  26177.3138799
Iteration :  5   Loss :  36457.5207866
Iteration :  6   Loss :  18224.7710366
Iteration :  7   Loss :  27359.3138891
Iteration :  8   Loss :  72937.8007775
Iteration :  9   Loss :  43403.9786209
Iteration :  10   Loss :  28475.6554506
Iteration :  11   Loss :  50766.8951407
Iteration :  12   Loss :  3282.08917514
Iteration :  13   Loss :  53734.3210463
Iteration :  14   Loss :  28239.0593635
Iteration :  15   Loss :  39816.8916043
Iteration :  16   Loss :  14715.2549416
Iteration :  17   Loss :  45270.1138267
Iteration :  18   Loss :  59345.3180456
Iteration :  19   Loss :  47184.6205616
Iteration :  20   Loss :  29461.9399073
Iteration :  21   Loss :  27281.0205188
Iteration :  22   Loss :  33248.1183366
Iteration :  23   Loss :  9412.73345284
Iteration :  24   Loss :  12303.829198
Iteration :  25   Loss :  40802.1626394
Iteration :  26   Loss :  2985.82758468
Iteration :  27   Loss :  30009.711535
Iteration :  28   Loss :  23810.3145879
Iteration :  29   Loss :  30355.8349436
Iteration :  30   Loss :  49863.7946762
Iteration :  31   Loss :  30450.5091591
Iteration :  32   Loss :  41043.2109139
Iteration :  33   Loss :  8540.519456
Iteration :  34   Loss :  25356.4802966
Iteration :  35   Loss :  32054.0010865
Iteration :  36   Loss :  32885.7586099
Iteration :  37   Loss :  5794.3578339
Iteration :  38   Loss :  29854.628455
Iteration :  39   Loss :  15128.9547954
Iteration :  40   Loss :  1155.88867548
Iteration :  41   Loss :  26752.6895257
Iteration :  42   Loss :  8477.51943298
Iteration :  43   Loss :  37693.2703099
Iteration :  44   Loss :  27321.9400268
Iteration :  45   Loss :  23073.6240345
Iteration :  46   Loss :  17013.3881619
Iteration :  47   Loss :  21817.860039
Iteration :  48   Loss :  23827.8064533
Iteration :  49   Loss :  37708.1745772
Iteration :  50   Loss :  25948.9001323
Iteration :  51   Loss :  25160.8627311
Iteration :  52   Loss :  33992.0135985
Iteration :  53   Loss :  14020.0923899
Iteration :  54   Loss :  26318.7909991
Iteration :  55   Loss :  1373.30981958
Iteration :  56   Loss :  22174.1196827
Iteration :  57   Loss :  17618.4765254
Iteration :  58   Loss :  75334.847452
Iteration :  59   Loss :  20485.8468317
Iteration :  60   Loss :  25345.8676866
Iteration :  61   Loss :  14573.3049043
Iteration :  62   Loss :  55482.7814655
Iteration :  63   Loss :  33463.2014973
Iteration :  64   Loss :  34829.5508982
Iteration :  65   Loss :  35451.9925327
Iteration :  66   Loss :  30941.3315276
Iteration :  67   Loss :  22021.6444731
Iteration :  68   Loss :  51636.5236835
Iteration :  69   Loss :  27728.1585936
Iteration :  70   Loss :  33331.6822954
Iteration :  71   Loss :  17787.779805
Iteration :  72   Loss :  29428.5697466
Iteration :  73   Loss :  42325.826548
Iteration :  74   Loss :  59702.7443262
Iteration :  75   Loss :  44336.2874496
Iteration :  76   Loss :  46099.5977653
Iteration :  77   Loss :  31976.6058454
Iteration :  78   Loss :  17568.3070478
Iteration :  79   Loss :  39629.5704215
Iteration :  80   Loss :  44238.2254403
Iteration :  81   Loss :  44600.4322486
Iteration :  82   Loss :  11288.2295813
Iteration :  83   Loss :  40996.7972066
Iteration :  84   Loss :  56853.7711964
Iteration :  85   Loss :  15424.4654508
Iteration :  86   Loss :  23724.7665465
Iteration :  87   Loss :  27098.9429053
Iteration :  88   Loss :  20204.7391802
Iteration :  89   Loss :  49483.6883237
Iteration :  90   Loss :  42121.4214052
Iteration :  91   Loss :  28978.8595612
Iteration :  92   Loss :  6748.05557545
Iteration :  93   Loss :  21107.8173
Iteration :  94   Loss :  16874.8278185
Iteration :  95   Loss :  24364.2937101
Iteration :  96   Loss :  63661.5972617
Iteration :  97   Loss :  18162.5282473
Iteration :  98   Loss :  19795.3069978
Iteration :  99   Loss :  30137.0626502
[-0.42325796 -0.93967988 -0.41510821 ...,  0.4578049  -0.24295431
 -0.03925407]
CROSS VALIDATION 15
Iteration :  0   Loss :  37763.9606433
Iteration :  1   Loss :  18774.7834204
Iteration :  2   Loss :  51858.4133001
Iteration :  3   Loss :  3666.83632767
Iteration :  4   Loss :  38132.263584
Iteration :  5   Loss :  36366.6022856
Iteration :  6   Loss :  15603.4446251
Iteration :  7   Loss :  42233.9870532
Iteration :  8   Loss :  29545.7633711
Iteration :  9   Loss :  20562.0069191
Iteration :  10   Loss :  35041.5432428
Iteration :  11   Loss :  51129.6934923
Iteration :  12   Loss :  24204.6377958
Iteration :  13   Loss :  32421.7510983
Iteration :  14   Loss :  20633.5212671
Iteration :  15   Loss :  22693.284501
Iteration :  16   Loss :  51952.7459065
Iteration :  17   Loss :  28432.4913196
Iteration :  18   Loss :  14154.0628664
Iteration :  19   Loss :  20300.3261683
Iteration :  20   Loss :  56486.8631398
Iteration :  21   Loss :  17129.0041204
Iteration :  22   Loss :  13122.6626244
Iteration :  23   Loss :  49073.7255386
Iteration :  24   Loss :  21465.5550418
Iteration :  25   Loss :  32416.7351715
Iteration :  26   Loss :  14505.245401
Iteration :  27   Loss :  37304.0347542
Iteration :  28   Loss :  24188.9154207
Iteration :  29   Loss :  45344.0803794
Iteration :  30   Loss :  14162.4165068
Iteration :  31   Loss :  37578.3596291
Iteration :  32   Loss :  18350.8528066
Iteration :  33   Loss :  38394.2531845
Iteration :  34   Loss :  38903.5966577
Iteration :  35   Loss :  73038.6632176
Iteration :  36   Loss :  31535.9007639
Iteration :  37   Loss :  50909.4691671
Iteration :  38   Loss :  12107.2019791
Iteration :  39   Loss :  54249.9355472
Iteration :  40   Loss :  34083.892029
Iteration :  41   Loss :  20949.938759
Iteration :  42   Loss :  27896.0347287
Iteration :  43   Loss :  46328.3596735
Iteration :  44   Loss :  25048.9795094
Iteration :  45   Loss :  27018.9775775
Iteration :  46   Loss :  53772.8027293
Iteration :  47   Loss :  15043.0103026
Iteration :  48   Loss :  15311.8827073
Iteration :  49   Loss :  52971.569245
Iteration :  50   Loss :  66788.1195908
Iteration :  51   Loss :  32731.262905
Iteration :  52   Loss :  26294.758264
Iteration :  53   Loss :  71167.8085658
Iteration :  54   Loss :  68501.5952248
Iteration :  55   Loss :  30205.8636184
Iteration :  56   Loss :  10591.8280491
Iteration :  57   Loss :  27173.7726446
Iteration :  58   Loss :  23937.0080138
Iteration :  59   Loss :  20390.0867265
Iteration :  60   Loss :  24242.8912869
Iteration :  61   Loss :  30080.2252597
Iteration :  62   Loss :  45554.98933
Iteration :  63   Loss :  30635.20943
Iteration :  64   Loss :  7112.56487158
Iteration :  65   Loss :  28606.2572922
Iteration :  66   Loss :  66688.0790242
Iteration :  67   Loss :  82305.2969413
Iteration :  68   Loss :  24286.5531901
Iteration :  69   Loss :  53125.0395357
Iteration :  70   Loss :  62144.0370882
Iteration :  71   Loss :  30576.9777476
Iteration :  72   Loss :  30220.5297449
Iteration :  73   Loss :  19631.2970768
Iteration :  74   Loss :  22694.6102042
Iteration :  75   Loss :  51940.4422952
Iteration :  76   Loss :  28432.3202112
Iteration :  77   Loss :  14154.055593
Iteration :  78   Loss :  20300.3257255
Iteration :  79   Loss :  56486.8624646
Iteration :  80   Loss :  17129.0041151
Iteration :  81   Loss :  13122.6625962
Iteration :  82   Loss :  49073.7255388
Iteration :  83   Loss :  21465.5550418
Iteration :  84   Loss :  32416.7351715
Iteration :  85   Loss :  14505.245401
Iteration :  86   Loss :  37304.0347542
Iteration :  87   Loss :  24188.9154207
Iteration :  88   Loss :  45344.0803794
Iteration :  89   Loss :  14162.4165068
Iteration :  90   Loss :  37578.3596291
Iteration :  91   Loss :  18350.8528066
Iteration :  92   Loss :  38394.2531845
Iteration :  93   Loss :  38903.5966577
Iteration :  94   Loss :  73038.6632176
Iteration :  95   Loss :  31535.9007639
Iteration :  96   Loss :  50909.4691671
Iteration :  97   Loss :  12107.2019791
Iteration :  98   Loss :  54249.9355472
Iteration :  99   Loss :  34083.892029
[ 0.17820551  0.00770169 -0.19586951 ...,  0.14483099  0.27061643
  0.07894654]
CROSS VALIDATION 16
Iteration :  0   Loss :  37763.9606433
Iteration :  1   Loss :  47426.6439098
Iteration :  2   Loss :  46673.2128798
Iteration :  3   Loss :  25688.8749199
Iteration :  4   Loss :  28685.8583561
Iteration :  5   Loss :  56511.6228924
Iteration :  6   Loss :  15591.0344025
Iteration :  7   Loss :  21185.4507877
Iteration :  8   Loss :  45019.274897
Iteration :  9   Loss :  41447.3659955
Iteration :  10   Loss :  17663.5302652
Iteration :  11   Loss :  35879.522364
Iteration :  12   Loss :  23830.0405139
Iteration :  13   Loss :  47042.5972515
Iteration :  14   Loss :  28241.0282909
Iteration :  15   Loss :  39250.5305764
Iteration :  16   Loss :  5924.87018867
Iteration :  17   Loss :  30683.916
Iteration :  18   Loss :  4572.85939865
Iteration :  19   Loss :  48614.1896611
Iteration :  20   Loss :  29578.9138952
Iteration :  21   Loss :  38226.5489826
Iteration :  22   Loss :  33840.2588723
Iteration :  23   Loss :  21979.3600745
Iteration :  24   Loss :  35491.7189524
Iteration :  25   Loss :  72080.1347172
Iteration :  26   Loss :  18361.7762392
Iteration :  27   Loss :  44847.7124558
Iteration :  28   Loss :  21623.8267247
Iteration :  29   Loss :  50326.8543622
Iteration :  30   Loss :  39265.4493279
Iteration :  31   Loss :  52843.808525
Iteration :  32   Loss :  64651.8957801
Iteration :  33   Loss :  3727.99929852
Iteration :  34   Loss :  47801.1329078
Iteration :  35   Loss :  38239.9744496
Iteration :  36   Loss :  31409.9916331
Iteration :  37   Loss :  6765.64135115
Iteration :  38   Loss :  38881.4122874
Iteration :  39   Loss :  20205.0390255
Iteration :  40   Loss :  56147.1245879
Iteration :  41   Loss :  4101.15146688
Iteration :  42   Loss :  28684.9721129
Iteration :  43   Loss :  33827.1800434
Iteration :  44   Loss :  24297.0676093
Iteration :  45   Loss :  9375.86256321
Iteration :  46   Loss :  37885.383243
Iteration :  47   Loss :  26348.217494
Iteration :  48   Loss :  40794.3888477
Iteration :  49   Loss :  10364.0749418
Iteration :  50   Loss :  29432.6827051
Iteration :  51   Loss :  3823.02104559
Iteration :  52   Loss :  43480.9116643
Iteration :  53   Loss :  22695.1878885
Iteration :  54   Loss :  18358.2885491
Iteration :  55   Loss :  50834.9432607
Iteration :  56   Loss :  33319.2346277
Iteration :  57   Loss :  25235.2539028
Iteration :  58   Loss :  81944.3438796
Iteration :  59   Loss :  4737.9463912
Iteration :  60   Loss :  62.5663535798
Iteration :  61   Loss :  0.82621209213
Iteration :  62   Loss :  0.328831676861
Iteration :  63   Loss :  49184.3693339
Iteration :  64   Loss :  20581.3550333
Iteration :  65   Loss :  44401.0283228
Iteration :  66   Loss :  39718.9955682
Iteration :  67   Loss :  42550.8670563
Iteration :  68   Loss :  22686.6391506
Iteration :  69   Loss :  36649.6202171
Iteration :  70   Loss :  24266.3900453
Iteration :  71   Loss :  26180.3691263
Iteration :  72   Loss :  60161.0278795
Iteration :  73   Loss :  29919.6567783
Iteration :  74   Loss :  55053.8573106
Iteration :  75   Loss :  27182.0273279
Iteration :  76   Loss :  65002.9797212
Iteration :  77   Loss :  47549.6146256
Iteration :  78   Loss :  17345.3816444
Iteration :  79   Loss :  52119.1546901
Iteration :  80   Loss :  24187.3679926
Iteration :  81   Loss :  27699.8774756
Iteration :  82   Loss :  12318.882064
Iteration :  83   Loss :  44757.7761409
Iteration :  84   Loss :  25274.7408597
Iteration :  85   Loss :  32021.0888217
Iteration :  86   Loss :  27368.0662924
Iteration :  87   Loss :  26102.6204904
Iteration :  88   Loss :  56523.5701409
Iteration :  89   Loss :  42239.8908802
Iteration :  90   Loss :  47295.4978927
Iteration :  91   Loss :  58720.9351286
Iteration :  92   Loss :  6087.39741113
Iteration :  93   Loss :  47167.6572999
Iteration :  94   Loss :  36565.6436541
Iteration :  95   Loss :  65220.8901513
Iteration :  96   Loss :  18659.9011565
Iteration :  97   Loss :  50568.3687903
Iteration :  98   Loss :  7404.96437895
Iteration :  99   Loss :  16125.8415569
[-0.04598164 -0.76840125 -0.09014303 ...,  0.41495069 -0.33276529
 -0.01239141]
CROSS VALIDATION 17
Iteration :  0   Loss :  37763.9606433
Iteration :  1   Loss :  19140.6324703
Iteration :  2   Loss :  51875.7585209
Iteration :  3   Loss :  3756.21121706
Iteration :  4   Loss :  38130.7611658
Iteration :  5   Loss :  36373.5195967
Iteration :  6   Loss :  14458.5771484
Iteration :  7   Loss :  18216.4210378
Iteration :  8   Loss :  32570.6327813
Iteration :  9   Loss :  54986.9779315
Iteration :  10   Loss :  27007.940849
Iteration :  11   Loss :  30629.5330145
Iteration :  12   Loss :  41108.1043785
Iteration :  13   Loss :  71891.9679922
Iteration :  14   Loss :  28963.6534499
Iteration :  15   Loss :  18450.5663604
Iteration :  16   Loss :  29587.233156
Iteration :  17   Loss :  6124.85851844
Iteration :  18   Loss :  20940.1684413
Iteration :  19   Loss :  9549.36666126
Iteration :  20   Loss :  41026.6364893
Iteration :  21   Loss :  29511.9551409
Iteration :  22   Loss :  37103.3397154
Iteration :  23   Loss :  37093.8336337
Iteration :  24   Loss :  23371.7050975
Iteration :  25   Loss :  5942.77949971
Iteration :  26   Loss :  32669.639719
Iteration :  27   Loss :  21209.1464164
Iteration :  28   Loss :  27900.3854425
Iteration :  29   Loss :  34362.6282593
Iteration :  30   Loss :  20573.1995419
Iteration :  31   Loss :  34647.9955805
Iteration :  32   Loss :  23248.6827539
Iteration :  33   Loss :  23125.6745387
Iteration :  34   Loss :  35378.5527102
Iteration :  35   Loss :  54274.4717577
Iteration :  36   Loss :  52959.0506924
Iteration :  37   Loss :  10218.3341441
Iteration :  38   Loss :  34473.4569537
Iteration :  39   Loss :  39748.3367664
Iteration :  40   Loss :  35939.3677075
Iteration :  41   Loss :  25557.0740473
Iteration :  42   Loss :  39902.6118258
Iteration :  43   Loss :  9592.93593826
Iteration :  44   Loss :  51556.6220016
Iteration :  45   Loss :  15649.4867619
Iteration :  46   Loss :  44391.9737295
Iteration :  47   Loss :  26974.7934538
Iteration :  48   Loss :  24305.4398132
Iteration :  49   Loss :  17881.2406943
Iteration :  50   Loss :  435.127164244
Iteration :  51   Loss :  38260.029187
Iteration :  52   Loss :  31240.6370384
Iteration :  53   Loss :  22580.0037789
Iteration :  54   Loss :  47989.9348017
Iteration :  55   Loss :  20429.9409535
Iteration :  56   Loss :  23019.5421547
Iteration :  57   Loss :  42995.7934048
Iteration :  58   Loss :  58649.8889969
Iteration :  59   Loss :  14144.7936346
Iteration :  60   Loss :  36557.8134138
Iteration :  61   Loss :  45189.900338
Iteration :  62   Loss :  37406.5053314
Iteration :  63   Loss :  16633.1054194
Iteration :  64   Loss :  22689.8478509
Iteration :  65   Loss :  34528.1102961
Iteration :  66   Loss :  11240.554401
Iteration :  67   Loss :  33001.2874826
Iteration :  68   Loss :  17799.9844002
Iteration :  69   Loss :  40624.776165
Iteration :  70   Loss :  42316.6667041
Iteration :  71   Loss :  42926.9727502
Iteration :  72   Loss :  57897.0762393
Iteration :  73   Loss :  35665.0154043
Iteration :  74   Loss :  34261.8179001
Iteration :  75   Loss :  8295.64820839
Iteration :  76   Loss :  39375.5465534
Iteration :  77   Loss :  57713.9279384
Iteration :  78   Loss :  38203.1320277
Iteration :  79   Loss :  5931.77719624
Iteration :  80   Loss :  36823.109965
Iteration :  81   Loss :  28276.2154504
Iteration :  82   Loss :  35119.5753319
Iteration :  83   Loss :  77520.7951336
Iteration :  84   Loss :  49868.7444972
Iteration :  85   Loss :  31185.8181319
Iteration :  86   Loss :  32359.418655
Iteration :  87   Loss :  51660.6952083
Iteration :  88   Loss :  12279.2494193
Iteration :  89   Loss :  28677.1689679
Iteration :  90   Loss :  10450.3436528
Iteration :  91   Loss :  32249.5124734
Iteration :  92   Loss :  57156.5388618
Iteration :  93   Loss :  41821.4982461
Iteration :  94   Loss :  2694.00381181
Iteration :  95   Loss :  35.5753276036
Iteration :  96   Loss :  109307.734145
Iteration :  97   Loss :  48276.4947927
Iteration :  98   Loss :  26768.7445747
Iteration :  99   Loss :  7604.13347288
[-0.2594887  -0.00897555  0.33282173 ...,  0.22609898  0.29791837
 -0.01641057]
CROSS VALIDATION 18
Iteration :  0   Loss :  29828.6571841
Iteration :  1   Loss :  26943.9218343
Iteration :  2   Loss :  55462.6674915
Iteration :  3   Loss :  31793.0402601
Iteration :  4   Loss :  17247.1514723
Iteration :  5   Loss :  12100.2712283
Iteration :  6   Loss :  33119.3427931
Iteration :  7   Loss :  43784.4065608
Iteration :  8   Loss :  28896.7264146
Iteration :  9   Loss :  16596.8814222
Iteration :  10   Loss :  17820.2922263
Iteration :  11   Loss :  17103.8741683
Iteration :  12   Loss :  24386.7181493
Iteration :  13   Loss :  33386.0408068
Iteration :  14   Loss :  24260.4126329
Iteration :  15   Loss :  19542.5185008
Iteration :  16   Loss :  31627.6674611
Iteration :  17   Loss :  15182.5355798
Iteration :  18   Loss :  34882.9454783
Iteration :  19   Loss :  47996.2592087
Iteration :  20   Loss :  26398.7983816
Iteration :  21   Loss :  17270.9090112
Iteration :  22   Loss :  29940.6729889
Iteration :  23   Loss :  22445.7539741
Iteration :  24   Loss :  36063.4973664
Iteration :  25   Loss :  21465.9771143
Iteration :  26   Loss :  47515.6986626
Iteration :  27   Loss :  13617.9776193
Iteration :  28   Loss :  51232.9535339
Iteration :  29   Loss :  16785.2132689
Iteration :  30   Loss :  27915.7545561
Iteration :  31   Loss :  65652.5811432
Iteration :  32   Loss :  24715.388385
Iteration :  33   Loss :  61622.559575
Iteration :  34   Loss :  9730.50889574
Iteration :  35   Loss :  41381.2887916
Iteration :  36   Loss :  13921.5282536
Iteration :  37   Loss :  22241.5206863
Iteration :  38   Loss :  44593.2738502
Iteration :  39   Loss :  68282.6807772
Iteration :  40   Loss :  5491.71452696
Iteration :  41   Loss :  43145.1305008
Iteration :  42   Loss :  25241.5617326
Iteration :  43   Loss :  20767.3387242
Iteration :  44   Loss :  32040.3875594
Iteration :  45   Loss :  24182.5414819
Iteration :  46   Loss :  19642.5699736
Iteration :  47   Loss :  41520.4804002
Iteration :  48   Loss :  49212.5667386
Iteration :  49   Loss :  34145.6839947
Iteration :  50   Loss :  14778.4702598
Iteration :  51   Loss :  39404.917017
Iteration :  52   Loss :  41059.16264
Iteration :  53   Loss :  335.165757826
Iteration :  54   Loss :  4.42598914816
Iteration :  55   Loss :  0.646283126327
Iteration :  56   Loss :  43740.9060391
Iteration :  57   Loss :  16594.6001738
Iteration :  58   Loss :  46713.8478394
Iteration :  59   Loss :  44768.8660288
Iteration :  60   Loss :  23398.3504222
Iteration :  61   Loss :  27437.084335
Iteration :  62   Loss :  38871.9664697
Iteration :  63   Loss :  16467.6776898
Iteration :  64   Loss :  24764.3638541
Iteration :  65   Loss :  18364.492462
Iteration :  66   Loss :  33770.6023266
Iteration :  67   Loss :  21832.2290174
Iteration :  68   Loss :  31151.4360556
Iteration :  69   Loss :  20539.7938658
Iteration :  70   Loss :  47122.9551527
Iteration :  71   Loss :  5389.83263913
Iteration :  72   Loss :  48510.1059299
Iteration :  73   Loss :  19913.447685
Iteration :  74   Loss :  32081.2894332
Iteration :  75   Loss :  19114.943566
Iteration :  76   Loss :  28125.5109386
Iteration :  77   Loss :  19515.0381643
Iteration :  78   Loss :  37325.603886
Iteration :  79   Loss :  9027.76636033
Iteration :  80   Loss :  34871.50859
Iteration :  81   Loss :  22236.0401788
Iteration :  82   Loss :  3550.56603386
Iteration :  83   Loss :  46.8865520081
Iteration :  84   Loss :  0.619154449812
Iteration :  85   Loss :  0.618518534662
Iteration :  86   Loss :  40986.9218829
Iteration :  87   Loss :  5708.67961514
Iteration :  88   Loss :  27309.0220886
Iteration :  89   Loss :  49372.9012538
Iteration :  90   Loss :  20338.3981324
Iteration :  91   Loss :  18428.7130277
Iteration :  92   Loss :  30910.805943
Iteration :  93   Loss :  39225.5324784
Iteration :  94   Loss :  65535.2174695
Iteration :  95   Loss :  22675.4460965
Iteration :  96   Loss :  34638.1014808
Iteration :  97   Loss :  24415.7983583
Iteration :  98   Loss :  24644.8294543
Iteration :  99   Loss :  9511.356401
[-0.68473766 -0.16388652  0.34569603 ...,  0.674496    0.21826733
  0.01846984]
CROSS VALIDATION 19
Iteration :  0   Loss :  23728.5825667
Iteration :  1   Loss :  6780.31225228
Iteration :  2   Loss :  28637.0701327
Iteration :  3   Loss :  17565.2762257
Iteration :  4   Loss :  29810.0258909
Iteration :  5   Loss :  84728.182022
Iteration :  6   Loss :  45273.6462945
Iteration :  7   Loss :  9970.36833378
Iteration :  8   Loss :  26217.8240611
Iteration :  9   Loss :  40422.4984469
Iteration :  10   Loss :  15616.935992
Iteration :  11   Loss :  40576.8944016
Iteration :  12   Loss :  10229.1788371
Iteration :  13   Loss :  33786.468642
Iteration :  14   Loss :  12085.5694936
Iteration :  15   Loss :  29884.6861108
Iteration :  16   Loss :  20353.7093705
Iteration :  17   Loss :  36691.5517594
Iteration :  18   Loss :  18532.1496177
Iteration :  19   Loss :  20172.8019021
Iteration :  20   Loss :  7309.27244187
Iteration :  21   Loss :  19606.1937673
Iteration :  22   Loss :  30069.8738409
Iteration :  23   Loss :  17886.2100627
Iteration :  24   Loss :  34021.6798096
Iteration :  25   Loss :  38840.9224903
Iteration :  26   Loss :  12949.4425683
Iteration :  27   Loss :  33882.0770853
Iteration :  28   Loss :  22152.1645088
Iteration :  29   Loss :  32112.8948176
Iteration :  30   Loss :  43653.557532
Iteration :  31   Loss :  17563.2675257
Iteration :  32   Loss :  25065.9364438
Iteration :  33   Loss :  13935.3312429
Iteration :  34   Loss :  49121.2884901
Iteration :  35   Loss :  35791.1127352
Iteration :  36   Loss :  42340.9918893
Iteration :  37   Loss :  21482.2679778
Iteration :  38   Loss :  40202.5957076
Iteration :  39   Loss :  10021.3622543
Iteration :  40   Loss :  38998.0234047
Iteration :  41   Loss :  79512.7562708
Iteration :  42   Loss :  12373.6016625
Iteration :  43   Loss :  26783.1864958
Iteration :  44   Loss :  15957.3647291
Iteration :  45   Loss :  41226.2813863
Iteration :  46   Loss :  44996.5933221
Iteration :  47   Loss :  27201.9044634
Iteration :  48   Loss :  35401.8759048
Iteration :  49   Loss :  27550.2653945
Iteration :  50   Loss :  37524.9696956
Iteration :  51   Loss :  32964.8978092
Iteration :  52   Loss :  8887.91813526
Iteration :  53   Loss :  30193.4694669
Iteration :  54   Loss :  13352.6792766
Iteration :  55   Loss :  48404.2962721
Iteration :  56   Loss :  20090.2327969
Iteration :  57   Loss :  27737.1525385
Iteration :  58   Loss :  33186.1482118
Iteration :  59   Loss :  38295.5205245
Iteration :  60   Loss :  22478.9665355
Iteration :  61   Loss :  20135.5153682
Iteration :  62   Loss :  38495.2733471
Iteration :  63   Loss :  20058.1082733
Iteration :  64   Loss :  45631.5470345
Iteration :  65   Loss :  5452.91377516
Iteration :  66   Loss :  25252.6645053
Iteration :  67   Loss :  38085.3809585
Iteration :  68   Loss :  28967.5629069
Iteration :  69   Loss :  40176.9442763
Iteration :  70   Loss :  15929.2315496
Iteration :  71   Loss :  16396.532048
Iteration :  72   Loss :  25616.0208545
Iteration :  73   Loss :  12488.3834525
Iteration :  74   Loss :  53196.160358
Iteration :  75   Loss :  29383.3846912
Iteration :  76   Loss :  31188.8838745
Iteration :  77   Loss :  33366.1795028
Iteration :  78   Loss :  20092.9812498
Iteration :  79   Loss :  30881.0749424
Iteration :  80   Loss :  8608.7866736
Iteration :  81   Loss :  44612.1468292
Iteration :  82   Loss :  29428.3259184
Iteration :  83   Loss :  31544.3560006
Iteration :  84   Loss :  33407.4733073
Iteration :  85   Loss :  39722.449621
Iteration :  86   Loss :  37367.913814
Iteration :  87   Loss :  42833.3717384
Iteration :  88   Loss :  21448.1717453
Iteration :  89   Loss :  61350.2277353
Iteration :  90   Loss :  30444.6573799
Iteration :  91   Loss :  28586.2176523
Iteration :  92   Loss :  33084.840288
Iteration :  93   Loss :  19515.2418093
Iteration :  94   Loss :  28155.7078129
Iteration :  95   Loss :  7666.97032417
Iteration :  96   Loss :  25529.4838415
Iteration :  97   Loss :  23129.1289028
Iteration :  98   Loss :  33498.9476928
Iteration :  99   Loss :  69154.308227
[-0.23783968  0.2235248  -0.1003214  ...,  0.29961027  0.26280725
  0.21055814]
Accuracy (Hinge Loss):	0.65
lmda : 0.1  eta : 0.1
Logistic Loss
CROSS VALIDATION 0
Iteration :  0   Loss :  21903.2048031
Iteration :  1   Loss :  3948.16783656
Iteration :  2   Loss :  5645.01123795
Iteration :  3   Loss :  340.64237968
Iteration :  4   Loss :  40.0186746483
Iteration :  5   Loss :  4.70139482383
Iteration :  6   Loss :  0.552393499368
Iteration :  7   Loss :  53749.7355428
Iteration :  8   Loss :  13506.0833226
Iteration :  9   Loss :  2848.08296688
Iteration :  10   Loss :  19364.7357178
Iteration :  11   Loss :  24902.1919256
Iteration :  12   Loss :  6996.78720542
Iteration :  13   Loss :  15122.4787395
Iteration :  14   Loss :  42647.0418841
Iteration :  15   Loss :  7794.31815902
Iteration :  16   Loss :  6868.78872401
Iteration :  17   Loss :  18908.4348481
Iteration :  18   Loss :  1473.59523363
Iteration :  19   Loss :  173.11800215
Iteration :  20   Loss :  20.337906899
Iteration :  21   Loss :  2.3892977888
Iteration :  22   Loss :  0.280734112325
Iteration :  23   Loss :  0.0639341043497
Iteration :  24   Loss :  31414.2539894
Iteration :  25   Loss :  20405.7427581
Iteration :  26   Loss :  7980.66039014
Iteration :  27   Loss :  12997.4299039
Iteration :  28   Loss :  23187.8712142
Iteration :  29   Loss :  29400.7537734
Iteration :  30   Loss :  9015.20284566
Iteration :  31   Loss :  21214.2110075
Iteration :  32   Loss :  22405.6232335
Iteration :  33   Loss :  26220.825409
Iteration :  34   Loss :  6980.13510531
Iteration :  35   Loss :  1570.98723502
Iteration :  36   Loss :  38767.8184152
Iteration :  37   Loss :  29554.6709855
Iteration :  38   Loss :  16833.6733083
Iteration :  39   Loss :  3553.262716
Iteration :  40   Loss :  34117.3800462
Iteration :  41   Loss :  7117.9996167
Iteration :  42   Loss :  18613.893292
Iteration :  43   Loss :  26310.771019
Iteration :  44   Loss :  5597.62254554
Iteration :  45   Loss :  13583.8306311
Iteration :  46   Loss :  27804.2867627
Iteration :  47   Loss :  7600.78153632
Iteration :  48   Loss :  20903.0978104
Iteration :  49   Loss :  4956.53991968
Iteration :  50   Loss :  11365.7952296
Iteration :  51   Loss :  13555.2725156
Iteration :  52   Loss :  18943.0237703
Iteration :  53   Loss :  5623.32655569
Iteration :  54   Loss :  26769.6228684
Iteration :  55   Loss :  33508.9135038
Iteration :  56   Loss :  13672.4389834
Iteration :  57   Loss :  34054.7250839
Iteration :  58   Loss :  13809.0592266
Iteration :  59   Loss :  38393.0223359
Iteration :  60   Loss :  2508.59718046
Iteration :  61   Loss :  17255.9373452
Iteration :  62   Loss :  41013.4971471
Iteration :  63   Loss :  20514.3012251
Iteration :  64   Loss :  5476.88813224
Iteration :  65   Loss :  24818.2385099
Iteration :  66   Loss :  38779.6375803
Iteration :  67   Loss :  4333.47962132
Iteration :  68   Loss :  35610.8295262
Iteration :  69   Loss :  5106.5144259
Iteration :  70   Loss :  23283.1945634
Iteration :  71   Loss :  5185.19094697
Iteration :  72   Loss :  10405.1711964
Iteration :  73   Loss :  1377.24787589
Iteration :  74   Loss :  30877.3043189
Iteration :  75   Loss :  17041.2827689
Iteration :  76   Loss :  27065.5361844
Iteration :  77   Loss :  14120.4666211
Iteration :  78   Loss :  19671.6032486
Iteration :  79   Loss :  23686.8725936
Iteration :  80   Loss :  9014.06562165
Iteration :  81   Loss :  13825.8877753
Iteration :  82   Loss :  37907.7163672
Iteration :  83   Loss :  30864.6475697
Iteration :  84   Loss :  25249.2189722
Iteration :  85   Loss :  30416.9008896
Iteration :  86   Loss :  4888.61919648
Iteration :  87   Loss :  9947.09377798
Iteration :  88   Loss :  26410.1129072
Iteration :  89   Loss :  10472.7286633
Iteration :  90   Loss :  24445.8963697
Iteration :  91   Loss :  21187.4702265
Iteration :  92   Loss :  22959.5653218
Iteration :  93   Loss :  13238.5869456
Iteration :  94   Loss :  32952.3487786
Iteration :  95   Loss :  12059.1562841
Iteration :  96   Loss :  18582.3890531
Iteration :  97   Loss :  22936.9945369
Iteration :  98   Loss :  32339.3181872
Iteration :  99   Loss :  21241.5011514
[-0.32580029 -0.02868749  0.20528355 ...,  0.11413497  0.06986788
 -0.12600736]
CROSS VALIDATION 1
Iteration :  0   Loss :  27361.1104953
Iteration :  1   Loss :  11998.6409383
Iteration :  2   Loss :  4744.23782992
Iteration :  3   Loss :  42653.1936036
Iteration :  4   Loss :  14388.8160878
Iteration :  5   Loss :  7401.21030452
Iteration :  6   Loss :  31302.666742
Iteration :  7   Loss :  23977.2989861
Iteration :  8   Loss :  3641.66735094
Iteration :  9   Loss :  21648.4237758
Iteration :  10   Loss :  3745.25736836
Iteration :  11   Loss :  439.992922311
Iteration :  12   Loss :  51.6903786958
Iteration :  13   Loss :  6.07259674732
Iteration :  14   Loss :  0.77568959897
Iteration :  15   Loss :  0.0921498760713
Iteration :  16   Loss :  2.81978304033
Iteration :  17   Loss :  29580.0406511
Iteration :  18   Loss :  17820.8704279
Iteration :  19   Loss :  46493.7036606
Iteration :  20   Loss :  9737.12305806
Iteration :  21   Loss :  22084.0478654
Iteration :  22   Loss :  22589.4029871
Iteration :  23   Loss :  11895.2295091
Iteration :  24   Loss :  14289.1671158
Iteration :  25   Loss :  40388.3366035
Iteration :  26   Loss :  16437.8608729
Iteration :  27   Loss :  4485.95035642
Iteration :  28   Loss :  5052.01514554
Iteration :  29   Loss :  24623.645136
Iteration :  30   Loss :  24473.4091988
Iteration :  31   Loss :  2692.9561822
Iteration :  32   Loss :  20656.9655372
Iteration :  33   Loss :  12095.788299
Iteration :  34   Loss :  7554.28321603
Iteration :  35   Loss :  28058.3981709
Iteration :  36   Loss :  5430.02945878
Iteration :  37   Loss :  637.919986484
Iteration :  38   Loss :  74.9428547755
Iteration :  39   Loss :  8.80428831342
Iteration :  40   Loss :  1.03439330967
Iteration :  41   Loss :  0.122790597122
Iteration :  42   Loss :  124172.749918
Iteration :  43   Loss :  41084.4464274
Iteration :  44   Loss :  33517.5636268
Iteration :  45   Loss :  8040.69894546
Iteration :  46   Loss :  2105.96583806
Iteration :  47   Loss :  247.408915393
Iteration :  48   Loss :  29.0656051061
Iteration :  49   Loss :  3.41462796054
Iteration :  50   Loss :  0.401242784608
Iteration :  51   Loss :  0.0506221005196
Iteration :  52   Loss :  98380.003458
Iteration :  53   Loss :  21793.3870391
Iteration :  54   Loss :  5097.0115705
Iteration :  55   Loss :  598.797037262
Iteration :  56   Loss :  70.3466897954
Iteration :  57   Loss :  8.26433074519
Iteration :  58   Loss :  0.970893883995
Iteration :  59   Loss :  0.114643748025
Iteration :  60   Loss :  0.0310394623038
Iteration :  61   Loss :  16368.8910045
Iteration :  62   Loss :  7888.34252234
Iteration :  63   Loss :  16277.3068705
Iteration :  64   Loss :  67453.1952517
Iteration :  65   Loss :  3311.54969561
Iteration :  66   Loss :  389.040934879
Iteration :  67   Loss :  45.7045380331
Iteration :  68   Loss :  5.36937225987
Iteration :  69   Loss :  11.8105907143
Iteration :  70   Loss :  28974.6121556
Iteration :  71   Loss :  17616.3505529
Iteration :  72   Loss :  40854.4245719
Iteration :  73   Loss :  4712.38193139
Iteration :  74   Loss :  553.610738358
Iteration :  75   Loss :  65.0382024464
Iteration :  76   Loss :  7.64126641432
Iteration :  77   Loss :  0.897730174734
Iteration :  78   Loss :  0.120160171251
Iteration :  79   Loss :  36339.0155894
Iteration :  80   Loss :  10687.1474109
Iteration :  81   Loss :  1298.86938293
Iteration :  82   Loss :  152.591938665
Iteration :  83   Loss :  17.9348586371
Iteration :  84   Loss :  2.10698760775
Iteration :  85   Loss :  0.24752944788
Iteration :  86   Loss :  16.1454486243
Iteration :  87   Loss :  27726.9097224
Iteration :  88   Loss :  7830.47849068
Iteration :  89   Loss :  2327.03897353
Iteration :  90   Loss :  11943.6052073
Iteration :  91   Loss :  33672.4836118
Iteration :  92   Loss :  5934.17942884
Iteration :  93   Loss :  697.147536634
Iteration :  94   Loss :  81.9009087378
Iteration :  95   Loss :  9.62172067262
Iteration :  96   Loss :  1.1305036617
Iteration :  97   Loss :  69199.4864661
Iteration :  98   Loss :  20336.3295041
Iteration :  99   Loss :  17957.4320228
[-0.77883448 -0.36918599  0.41686376 ...,  0.11852862  0.11809526
  0.01691384]
CROSS VALIDATION 2
Iteration :  0   Loss :  20143.9402097
Iteration :  1   Loss :  10018.8057925
Iteration :  2   Loss :  10478.4799712
Iteration :  3   Loss :  20543.4586046
Iteration :  4   Loss :  19094.3613361
Iteration :  5   Loss :  1081.59148473
Iteration :  6   Loss :  7627.57993813
Iteration :  7   Loss :  51173.0781854
Iteration :  8   Loss :  52959.0850066
Iteration :  9   Loss :  16440.0432065
Iteration :  10   Loss :  17267.2508957
Iteration :  11   Loss :  12405.8038262
Iteration :  12   Loss :  17200.8073946
Iteration :  13   Loss :  15243.7764883
Iteration :  14   Loss :  11823.8908891
Iteration :  15   Loss :  12095.6653747
Iteration :  16   Loss :  1901.91881613
Iteration :  17   Loss :  36569.3679199
Iteration :  18   Loss :  41014.6866865
Iteration :  19   Loss :  32178.318738
Iteration :  20   Loss :  2920.77510496
Iteration :  21   Loss :  21319.415531
Iteration :  22   Loss :  16536.7029445
Iteration :  23   Loss :  4840.37877578
Iteration :  24   Loss :  568.647810599
Iteration :  25   Loss :  66.8047579494
Iteration :  26   Loss :  7.84822450995
Iteration :  27   Loss :  0.922048768487
Iteration :  28   Loss :  0.111654290154
Iteration :  29   Loss :  4355.17297295
Iteration :  30   Loss :  22800.12811
Iteration :  31   Loss :  21754.6216346
Iteration :  32   Loss :  14431.584643
Iteration :  33   Loss :  6882.42144077
Iteration :  34   Loss :  1244.40487558
Iteration :  35   Loss :  84858.9511378
Iteration :  36   Loss :  45484.9680188
Iteration :  37   Loss :  10627.9258618
Iteration :  38   Loss :  23349.1196185
Iteration :  39   Loss :  31135.2784861
Iteration :  40   Loss :  4024.63074519
Iteration :  41   Loss :  23325.7373438
Iteration :  42   Loss :  34035.453962
Iteration :  43   Loss :  3313.15254653
Iteration :  44   Loss :  389.229237842
Iteration :  45   Loss :  45.7266598696
Iteration :  46   Loss :  5.37196905973
Iteration :  47   Loss :  0.631127782066
Iteration :  48   Loss :  101335.970178
Iteration :  49   Loss :  37026.6748406
Iteration :  50   Loss :  2377.94253952
Iteration :  51   Loss :  279.360744576
Iteration :  52   Loss :  32.819306738
Iteration :  53   Loss :  3.85561291512
Iteration :  54   Loss :  0.453132114517
Iteration :  55   Loss :  49.9641230468
Iteration :  56   Loss :  35131.9845942
Iteration :  57   Loss :  7165.33216302
Iteration :  58   Loss :  3868.07872126
Iteration :  59   Loss :  33721.7681133
Iteration :  60   Loss :  18534.9425782
Iteration :  61   Loss :  12734.8967708
Iteration :  62   Loss :  23699.5988147
Iteration :  63   Loss :  3409.2583578
Iteration :  64   Loss :  21016.5863479
Iteration :  65   Loss :  32133.1337781
Iteration :  66   Loss :  9350.10654926
Iteration :  67   Loss :  908.293510084
Iteration :  68   Loss :  106.706342586
Iteration :  69   Loss :  12.5367284139
Iteration :  70   Loss :  1.47577834762
Iteration :  71   Loss :  0.173466764676
Iteration :  72   Loss :  98016.0186304
Iteration :  73   Loss :  20087.7165471
Iteration :  74   Loss :  15737.5893729
Iteration :  75   Loss :  1612.13202092
Iteration :  76   Loss :  22739.8260515
Iteration :  77   Loss :  35734.3218049
Iteration :  78   Loss :  18830.9210143
Iteration :  79   Loss :  5221.94749237
Iteration :  80   Loss :  23137.4552265
Iteration :  81   Loss :  15299.3874365
Iteration :  82   Loss :  13809.0354858
Iteration :  83   Loss :  34624.4609741
Iteration :  84   Loss :  2338.60286476
Iteration :  85   Loss :  274.739195188
Iteration :  86   Loss :  32.2812470418
Iteration :  87   Loss :  3.79240164955
Iteration :  88   Loss :  0.445556495324
Iteration :  89   Loss :  97393.5517744
Iteration :  90   Loss :  19979.3500741
Iteration :  91   Loss :  15802.4344875
Iteration :  92   Loss :  1610.15560146
Iteration :  93   Loss :  22733.4689705
Iteration :  94   Loss :  35728.3059543
Iteration :  95   Loss :  18802.2816647
Iteration :  96   Loss :  5219.48484022
Iteration :  97   Loss :  23124.9145004
Iteration :  98   Loss :  15298.5387056
Iteration :  99   Loss :  13808.9851987
[-0.47295908 -0.37998241  0.46932359 ...,  0.16779999  0.02170876
  0.14521541]
CROSS VALIDATION 3
Iteration :  0   Loss :  27407.4260099
Iteration :  1   Loss :  17018.6386903
Iteration :  2   Loss :  6618.83740939
Iteration :  3   Loss :  16291.5595006
Iteration :  4   Loss :  17018.9725273
Iteration :  5   Loss :  5052.24624167
Iteration :  6   Loss :  40580.4712612
Iteration :  7   Loss :  11969.7093113
Iteration :  8   Loss :  25125.6665705
Iteration :  9   Loss :  13532.2295981
Iteration :  10   Loss :  23817.9277899
Iteration :  11   Loss :  21477.8462572
Iteration :  12   Loss :  29914.1856818
Iteration :  13   Loss :  7892.22671487
Iteration :  14   Loss :  25412.9447126
Iteration :  15   Loss :  28370.5165942
Iteration :  16   Loss :  21977.3003007
Iteration :  17   Loss :  1240.12331478
Iteration :  18   Loss :  145.689715721
Iteration :  19   Loss :  17.1156311746
Iteration :  20   Loss :  2.01074482784
Iteration :  21   Loss :  0.241627063235
Iteration :  22   Loss :  36625.3653124
Iteration :  23   Loss :  10726.9019416
Iteration :  24   Loss :  1298.73818875
Iteration :  25   Loss :  152.577520836
Iteration :  26   Loss :  17.9541007436
Iteration :  27   Loss :  2.10924817086
Iteration :  28   Loss :  0.247794871608
Iteration :  29   Loss :  0.0903446050579
Iteration :  30   Loss :  37768.0310812
Iteration :  31   Loss :  19340.6068067
Iteration :  32   Loss :  3450.79528328
Iteration :  33   Loss :  50069.7940267
Iteration :  34   Loss :  13156.8587758
Iteration :  35   Loss :  16237.5341594
Iteration :  36   Loss :  20403.0483097
Iteration :  37   Loss :  30777.5437071
Iteration :  38   Loss :  3947.59056483
Iteration :  39   Loss :  28696.2974164
Iteration :  40   Loss :  17543.397994
Iteration :  41   Loss :  1190.20855162
Iteration :  42   Loss :  18593.0669126
Iteration :  43   Loss :  18127.7611749
Iteration :  44   Loss :  19523.9344381
Iteration :  45   Loss :  33230.3236532
Iteration :  46   Loss :  3463.89511255
Iteration :  47   Loss :  1387.81239543
Iteration :  48   Loss :  48576.6113051
Iteration :  49   Loss :  19128.7534549
Iteration :  50   Loss :  43075.6502253
Iteration :  51   Loss :  5822.57174143
Iteration :  52   Loss :  31290.9661217
Iteration :  53   Loss :  6454.37563841
Iteration :  54   Loss :  25362.5307859
Iteration :  55   Loss :  48579.9003642
Iteration :  56   Loss :  7959.93756679
Iteration :  57   Loss :  8845.9658034
Iteration :  58   Loss :  9478.39439183
Iteration :  59   Loss :  5840.85607426
Iteration :  60   Loss :  6401.07810342
Iteration :  61   Loss :  20200.5442256
Iteration :  62   Loss :  8313.54829288
Iteration :  63   Loss :  37978.8547545
Iteration :  64   Loss :  55228.0437382
Iteration :  65   Loss :  12295.467929
Iteration :  66   Loss :  14121.6282141
Iteration :  67   Loss :  4329.5077196
Iteration :  68   Loss :  20999.739721
Iteration :  69   Loss :  47599.2580634
Iteration :  70   Loss :  6215.44698788
Iteration :  71   Loss :  6859.67002577
Iteration :  72   Loss :  25826.5752576
Iteration :  73   Loss :  8909.0114512
Iteration :  74   Loss :  26382.8857781
Iteration :  75   Loss :  37923.0060941
Iteration :  76   Loss :  26784.9660515
Iteration :  77   Loss :  14984.6909511
Iteration :  78   Loss :  1732.5836682
Iteration :  79   Loss :  67051.5389911
Iteration :  80   Loss :  13256.7620755
Iteration :  81   Loss :  6758.89120618
Iteration :  82   Loss :  25163.9800881
Iteration :  83   Loss :  6212.97015447
Iteration :  84   Loss :  28815.6359532
Iteration :  85   Loss :  18186.3729913
Iteration :  86   Loss :  1961.1117922
Iteration :  87   Loss :  230.391458734
Iteration :  88   Loss :  27.066393904
Iteration :  89   Loss :  3.17979023181
Iteration :  90   Loss :  0.373737419709
Iteration :  91   Loss :  0.0455245738629
Iteration :  92   Loss :  33589.7547154
Iteration :  93   Loss :  24759.4737688
Iteration :  94   Loss :  3827.95456631
Iteration :  95   Loss :  33744.7861153
Iteration :  96   Loss :  14222.4893338
Iteration :  97   Loss :  4372.25339299
Iteration :  98   Loss :  38109.1776211
Iteration :  99   Loss :  20551.3495437
[-0.26359922  0.22257267 -0.52909713 ...,  0.58177967  0.13502155
  0.17862518]
CROSS VALIDATION 4
Iteration :  0   Loss :  27407.4260099
Iteration :  1   Loss :  17018.6386903
Iteration :  2   Loss :  6245.02021553
Iteration :  3   Loss :  23669.2149392
Iteration :  4   Loss :  29720.4312507
Iteration :  5   Loss :  14759.6329849
Iteration :  6   Loss :  21217.7174974
Iteration :  7   Loss :  15263.1804948
Iteration :  8   Loss :  5033.0520396
Iteration :  9   Loss :  16317.593847
Iteration :  10   Loss :  15551.2058216
Iteration :  11   Loss :  10247.7467632
Iteration :  12   Loss :  13960.4910142
Iteration :  13   Loss :  9066.73254027
Iteration :  14   Loss :  19997.0200742
Iteration :  15   Loss :  30306.8064069
Iteration :  16   Loss :  10202.9298661
Iteration :  17   Loss :  34502.9450788
Iteration :  18   Loss :  25416.4727157
Iteration :  19   Loss :  12213.1814242
Iteration :  20   Loss :  14448.9660655
Iteration :  21   Loss :  40497.8464229
Iteration :  22   Loss :  3506.54929153
Iteration :  23   Loss :  16721.8892329
Iteration :  24   Loss :  51208.6188816
Iteration :  25   Loss :  21872.7063658
Iteration :  26   Loss :  4394.11386935
Iteration :  27   Loss :  28876.0216073
Iteration :  28   Loss :  25409.232835
Iteration :  29   Loss :  3822.44980907
Iteration :  30   Loss :  14792.7141973
Iteration :  31   Loss :  22683.3619229
Iteration :  32   Loss :  11116.0781979
Iteration :  33   Loss :  14527.2930689
Iteration :  34   Loss :  2403.06067193
Iteration :  35   Loss :  42118.4565822
Iteration :  36   Loss :  37351.6134921
Iteration :  37   Loss :  7917.34377946
Iteration :  38   Loss :  19986.7563164
Iteration :  39   Loss :  21969.2524132
Iteration :  40   Loss :  3594.88963111
Iteration :  41   Loss :  422.327717059
Iteration :  42   Loss :  49.6150699739
Iteration :  43   Loss :  5.82877956914
Iteration :  44   Loss :  0.68534747236
Iteration :  45   Loss :  0.0822746471645
Iteration :  46   Loss :  0.0119259055893
Iteration :  47   Loss :  19124.5021098
Iteration :  48   Loss :  33062.0607772
Iteration :  49   Loss :  11767.4276623
Iteration :  50   Loss :  28699.7873982
Iteration :  51   Loss :  4704.25671598
Iteration :  52   Loss :  9990.84213546
Iteration :  53   Loss :  9897.3473878
Iteration :  54   Loss :  3586.6614427
Iteration :  55   Loss :  7933.51785438
Iteration :  56   Loss :  20819.2805134
Iteration :  57   Loss :  3485.12563919
Iteration :  58   Loss :  15315.5175709
Iteration :  59   Loss :  27598.1223
Iteration :  60   Loss :  4967.64934019
Iteration :  61   Loss :  583.599559451
Iteration :  62   Loss :  68.5612897505
Iteration :  63   Loss :  8.05458192203
Iteration :  64   Loss :  0.946358502506
Iteration :  65   Loss :  12.9351235292
Iteration :  66   Loss :  30239.2095617
Iteration :  67   Loss :  16557.8685965
Iteration :  68   Loss :  24667.4856083
Iteration :  69   Loss :  2521.74481992
Iteration :  70   Loss :  5335.11845711
Iteration :  71   Loss :  57790.6551271
Iteration :  72   Loss :  5704.91493002
Iteration :  73   Loss :  670.213538008
Iteration :  74   Loss :  78.7367019559
Iteration :  75   Loss :  9.24998956868
Iteration :  76   Loss :  1.08671810109
Iteration :  77   Loss :  105595.682809
Iteration :  78   Loss :  37080.850773
Iteration :  79   Loss :  2378.53262215
Iteration :  80   Loss :  279.430067496
Iteration :  81   Loss :  32.8274507963
Iteration :  82   Loss :  3.85656967927
Iteration :  83   Loss :  0.453171782606
Iteration :  84   Loss :  33.5119177701
Iteration :  85   Loss :  32024.415529
Iteration :  86   Loss :  12362.6535775
Iteration :  87   Loss :  20523.5164221
Iteration :  88   Loss :  18567.8710736
Iteration :  89   Loss :  4067.32622132
Iteration :  90   Loss :  6472.12611321
Iteration :  91   Loss :  942.68153279
Iteration :  92   Loss :  30426.8877591
Iteration :  93   Loss :  4396.15318232
Iteration :  94   Loss :  516.460177599
Iteration :  95   Loss :  60.6737536167
Iteration :  96   Loss :  7.12795397906
Iteration :  97   Loss :  0.837392946975
Iteration :  98   Loss :  0.11991091939
Iteration :  99   Loss :  29486.1321131
[-1.3884826   0.06340174 -0.37011376 ...,  0.10605739 -0.54691191
  0.10520479]
CROSS VALIDATION 5
Iteration :  0   Loss :  23826.4791247
Iteration :  1   Loss :  16527.5980148
Iteration :  2   Loss :  2354.96729682
Iteration :  3   Loss :  276.661612531
Iteration :  4   Loss :  32.5022126429
Iteration :  5   Loss :  3.8183606935
Iteration :  6   Loss :  0.448744858026
Iteration :  7   Loss :  0.0542112258361
Iteration :  8   Loss :  136925.443812
Iteration :  9   Loss :  78770.6872025
Iteration :  10   Loss :  18436.2864505
Iteration :  11   Loss :  8015.69615897
Iteration :  12   Loss :  15617.233567
Iteration :  13   Loss :  13710.0555696
Iteration :  14   Loss :  13378.2288226
Iteration :  15   Loss :  57113.7102559
Iteration :  16   Loss :  22137.1985207
Iteration :  17   Loss :  18675.6034987
Iteration :  18   Loss :  19534.4438424
Iteration :  19   Loss :  10936.0786064
Iteration :  20   Loss :  32870.5663713
Iteration :  21   Loss :  16415.4255736
Iteration :  22   Loss :  41391.3459653
Iteration :  23   Loss :  6833.93881903
Iteration :  24   Loss :  27456.1474504
Iteration :  25   Loss :  9255.88051227
Iteration :  26   Loss :  14686.6159425
Iteration :  27   Loss :  35005.1692928
Iteration :  28   Loss :  41326.5101837
Iteration :  29   Loss :  2562.27992582
Iteration :  30   Loss :  301.016704984
Iteration :  31   Loss :  35.3634494679
Iteration :  32   Loss :  4.15450415606
Iteration :  33   Loss :  0.491198869687
Iteration :  34   Loss :  41534.0403513
Iteration :  35   Loss :  7242.13893251
Iteration :  36   Loss :  26740.8552315
Iteration :  37   Loss :  52469.6559488
Iteration :  38   Loss :  11995.9905335
Iteration :  39   Loss :  13802.863212
Iteration :  40   Loss :  13244.3233163
Iteration :  41   Loss :  5374.92750236
Iteration :  42   Loss :  12887.3928028
Iteration :  43   Loss :  8547.06879286
Iteration :  44   Loss :  6771.52669535
Iteration :  45   Loss :  3222.90852874
Iteration :  46   Loss :  33266.767045
Iteration :  47   Loss :  27756.5051639
Iteration :  48   Loss :  14777.1601853
Iteration :  49   Loss :  8975.67132247
Iteration :  50   Loss :  35678.9773523
Iteration :  51   Loss :  19288.8877656
Iteration :  52   Loss :  21754.1541143
Iteration :  53   Loss :  12315.031132
Iteration :  54   Loss :  8081.34635655
Iteration :  55   Loss :  18262.5254355
Iteration :  56   Loss :  22928.4403974
Iteration :  57   Loss :  15863.276625
Iteration :  58   Loss :  15029.7424248
Iteration :  59   Loss :  9565.66925378
Iteration :  60   Loss :  15994.9747466
Iteration :  61   Loss :  4915.54313828
Iteration :  62   Loss :  577.478121646
Iteration :  63   Loss :  67.8421430955
Iteration :  64   Loss :  7.97009654092
Iteration :  65   Loss :  0.936327126081
Iteration :  66   Loss :  0.110157357179
Iteration :  67   Loss :  137969.104415
Iteration :  68   Loss :  78858.4676232
Iteration :  69   Loss :  18457.8856361
Iteration :  70   Loss :  8016.27696937
Iteration :  71   Loss :  15619.009962
Iteration :  72   Loss :  13709.6738117
Iteration :  73   Loss :  13377.876273
Iteration :  74   Loss :  57114.0376425
Iteration :  75   Loss :  22137.1823201
Iteration :  76   Loss :  18675.5892929
Iteration :  77   Loss :  19534.4427429
Iteration :  78   Loss :  10936.0826615
Iteration :  79   Loss :  32870.5664999
Iteration :  80   Loss :  16415.4255177
Iteration :  81   Loss :  41391.3459141
Iteration :  82   Loss :  6833.93880427
Iteration :  83   Loss :  27456.147459
Iteration :  84   Loss :  9255.88051425
Iteration :  85   Loss :  14686.6159435
Iteration :  86   Loss :  35005.1692924
Iteration :  87   Loss :  41326.5101836
Iteration :  88   Loss :  2562.27992582
Iteration :  89   Loss :  301.016704984
Iteration :  90   Loss :  35.3634494679
Iteration :  91   Loss :  4.15450415606
Iteration :  92   Loss :  0.491198869687
Iteration :  93   Loss :  41534.0403499
Iteration :  94   Loss :  7242.13893292
Iteration :  95   Loss :  26740.855232
Iteration :  96   Loss :  52469.6559486
Iteration :  97   Loss :  11995.9905335
Iteration :  98   Loss :  13802.863212
Iteration :  99   Loss :  13244.3233163
[-0.79224076 -0.35947002 -1.05929347 ...,  0.49862977 -0.27113056
  0.26840643]
CROSS VALIDATION 6
Iteration :  0   Loss :  27407.4260099
Iteration :  1   Loss :  16915.7400903
Iteration :  2   Loss :  6580.02583276
Iteration :  3   Loss :  23898.9849916
Iteration :  4   Loss :  31678.8923095
Iteration :  5   Loss :  5407.73657525
Iteration :  6   Loss :  635.301018012
Iteration :  7   Loss :  74.6351782987
Iteration :  8   Loss :  8.76814248664
Iteration :  9   Loss :  1.03204342683
Iteration :  10   Loss :  17671.8219878
Iteration :  11   Loss :  32880.0517387
Iteration :  12   Loss :  4222.00705116
Iteration :  13   Loss :  27042.7070836
Iteration :  14   Loss :  21438.6200003
Iteration :  15   Loss :  18683.2026282
Iteration :  16   Loss :  9798.22311914
Iteration :  17   Loss :  14120.3812407
Iteration :  18   Loss :  6924.73766526
Iteration :  19   Loss :  43391.4874485
Iteration :  20   Loss :  24261.765662
Iteration :  21   Loss :  11474.5301706
Iteration :  22   Loss :  19189.8359448
Iteration :  23   Loss :  13414.3285598
Iteration :  24   Loss :  15790.1709595
Iteration :  25   Loss :  8828.52668711
Iteration :  26   Loss :  33677.476259
Iteration :  27   Loss :  39297.1443743
Iteration :  28   Loss :  3713.4193366
Iteration :  29   Loss :  436.252589604
Iteration :  30   Loss :  51.2509643228
Iteration :  31   Loss :  6.02096447474
Iteration :  32   Loss :  0.707383942075
Iteration :  33   Loss :  103704.643191
Iteration :  34   Loss :  33888.7554829
Iteration :  35   Loss :  23165.3229997
Iteration :  36   Loss :  8513.90622772
Iteration :  37   Loss :  34092.9770972
Iteration :  38   Loss :  6650.80369132
Iteration :  39   Loss :  5319.25833653
Iteration :  40   Loss :  31372.5577441
Iteration :  41   Loss :  8917.69705492
Iteration :  42   Loss :  33620.4233138
Iteration :  43   Loss :  13846.8594952
Iteration :  44   Loss :  70426.7723838
Iteration :  45   Loss :  25732.562143
Iteration :  46   Loss :  15405.1643274
Iteration :  47   Loss :  14273.8207217
Iteration :  48   Loss :  9034.26568022
Iteration :  49   Loss :  37818.4121424
Iteration :  50   Loss :  57859.9840717
Iteration :  51   Loss :  4156.45731898
Iteration :  52   Loss :  488.301346738
Iteration :  53   Loss :  57.3708433939
Iteration :  54   Loss :  6.73992800964
Iteration :  55   Loss :  0.791808734393
Iteration :  56   Loss :  0.103786068861
Iteration :  57   Loss :  31336.565315
Iteration :  58   Loss :  5765.23225306
Iteration :  59   Loss :  7667.09250511
Iteration :  60   Loss :  19828.064623
Iteration :  61   Loss :  31776.8524142
Iteration :  62   Loss :  13924.9335567
Iteration :  63   Loss :  45363.8011688
Iteration :  64   Loss :  19489.8386256
Iteration :  65   Loss :  33882.498584
Iteration :  66   Loss :  9306.44058409
Iteration :  67   Loss :  477.496055022
Iteration :  68   Loss :  56.0962475947
Iteration :  69   Loss :  6.59018846566
Iteration :  70   Loss :  0.774215558891
Iteration :  71   Loss :  10.5466535401
Iteration :  72   Loss :  34627.2915917
Iteration :  73   Loss :  8112.2962076
Iteration :  74   Loss :  4425.1639578
Iteration :  75   Loss :  1723.24694678
Iteration :  76   Loss :  24725.1901624
Iteration :  77   Loss :  21129.9304795
Iteration :  78   Loss :  6317.83288026
Iteration :  79   Loss :  7456.44608678
Iteration :  80   Loss :  28998.0196199
Iteration :  81   Loss :  2266.05873568
Iteration :  82   Loss :  35028.5118448
Iteration :  83   Loss :  7646.52363905
Iteration :  84   Loss :  898.313774078
Iteration :  85   Loss :  105.533922968
Iteration :  86   Loss :  12.3981277141
Iteration :  87   Loss :  1.45751561014
Iteration :  88   Loss :  51157.0976883
Iteration :  89   Loss :  17102.9669066
Iteration :  90   Loss :  5719.69867472
Iteration :  91   Loss :  14094.9198271
Iteration :  92   Loss :  27614.4561501
Iteration :  93   Loss :  22107.9122018
Iteration :  94   Loss :  21884.9394287
Iteration :  95   Loss :  15064.6889228
Iteration :  96   Loss :  29336.8214335
Iteration :  97   Loss :  9912.670275
Iteration :  98   Loss :  4726.45932406
Iteration :  99   Loss :  33163.9193611
[-1.15992492 -0.4923409  -1.22028558 ...,  0.61728672 -0.70701219
  0.12267738]
CROSS VALIDATION 7
Iteration :  0   Loss :  27407.4260099
Iteration :  1   Loss :  16915.7400903
Iteration :  2   Loss :  6580.02583276
Iteration :  3   Loss :  16370.1190211
Iteration :  4   Loss :  16931.8423116
Iteration :  5   Loss :  5040.51343827
Iteration :  6   Loss :  40523.4144995
Iteration :  7   Loss :  11961.7414854
Iteration :  8   Loss :  25148.2522064
Iteration :  9   Loss :  13388.9240332
Iteration :  10   Loss :  22402.3615427
Iteration :  11   Loss :  16061.9557334
Iteration :  12   Loss :  25406.4763293
Iteration :  13   Loss :  9628.5448707
Iteration :  14   Loss :  62989.3200459
Iteration :  15   Loss :  20038.8301106
Iteration :  16   Loss :  20143.8242446
Iteration :  17   Loss :  18153.3052288
Iteration :  18   Loss :  3054.06548732
Iteration :  19   Loss :  2585.16229843
Iteration :  20   Loss :  40709.7672496
Iteration :  21   Loss :  41400.120323
Iteration :  22   Loss :  15893.4094362
Iteration :  23   Loss :  18352.3331188
Iteration :  24   Loss :  4596.62949344
Iteration :  25   Loss :  17875.6021845
Iteration :  26   Loss :  43881.1426932
Iteration :  27   Loss :  7610.0475826
Iteration :  28   Loss :  36727.9062175
Iteration :  29   Loss :  24642.1007667
Iteration :  30   Loss :  2994.12705896
Iteration :  31   Loss :  23531.5927171
Iteration :  32   Loss :  17244.3387215
Iteration :  33   Loss :  9324.09821034
Iteration :  34   Loss :  11452.0548009
Iteration :  35   Loss :  13615.5215571
Iteration :  36   Loss :  10512.8277612
Iteration :  37   Loss :  3892.47219913
Iteration :  38   Loss :  30236.1436712
Iteration :  39   Loss :  28976.8706776
Iteration :  40   Loss :  8045.70665558
Iteration :  41   Loss :  32723.3332957
Iteration :  42   Loss :  25317.346329
Iteration :  43   Loss :  8597.22348337
Iteration :  44   Loss :  9900.82290717
Iteration :  45   Loss :  18356.4843784
Iteration :  46   Loss :  8931.0096329
Iteration :  47   Loss :  30977.8766905
Iteration :  48   Loss :  14675.886049
Iteration :  49   Loss :  2988.66333912
Iteration :  50   Loss :  22119.1828842
Iteration :  51   Loss :  15566.8118114
Iteration :  52   Loss :  14972.2548163
Iteration :  53   Loss :  27773.9345359
Iteration :  54   Loss :  4503.60233571
Iteration :  55   Loss :  2914.81149151
Iteration :  56   Loss :  342.432121479
Iteration :  57   Loss :  40.2289335563
Iteration :  58   Loss :  4.72609604521
Iteration :  59   Loss :  0.55525428752
Iteration :  60   Loss :  91213.4058519
Iteration :  61   Loss :  56664.7225208
Iteration :  62   Loss :  21636.8149854
Iteration :  63   Loss :  1985.73181917
Iteration :  64   Loss :  19072.8767129
Iteration :  65   Loss :  14550.2368337
Iteration :  66   Loss :  14695.1072092
Iteration :  67   Loss :  1694.73178634
Iteration :  68   Loss :  199.09712948
Iteration :  69   Loss :  23.389935379
Iteration :  70   Loss :  2.74792844618
Iteration :  71   Loss :  0.323172546646
Iteration :  72   Loss :  0.201822616332
Iteration :  73   Loss :  40886.0345132
Iteration :  74   Loss :  2910.75769743
Iteration :  75   Loss :  11478.2700097
Iteration :  76   Loss :  25675.7908078
Iteration :  77   Loss :  18233.027684
Iteration :  78   Loss :  8466.87572482
Iteration :  79   Loss :  9833.78702501
Iteration :  80   Loss :  4204.51637174
Iteration :  81   Loss :  22416.333127
Iteration :  82   Loss :  60278.7288701
Iteration :  83   Loss :  10125.9078555
Iteration :  84   Loss :  9906.3926394
Iteration :  85   Loss :  17853.6735898
Iteration :  86   Loss :  2382.88486176
Iteration :  87   Loss :  19375.3268738
Iteration :  88   Loss :  24784.165413
Iteration :  89   Loss :  5955.20142198
Iteration :  90   Loss :  24680.5641968
Iteration :  91   Loss :  22113.0822278
Iteration :  92   Loss :  10205.7512357
Iteration :  93   Loss :  20864.366168
Iteration :  94   Loss :  8042.47177863
Iteration :  95   Loss :  2683.58513195
Iteration :  96   Loss :  315.267643408
Iteration :  97   Loss :  37.0376500438
Iteration :  98   Loss :  4.35118398649
Iteration :  99   Loss :  0.511669580536
[-0.00853684 -0.00608484 -0.00025368 ...,  0.00516658  0.00225367
 -0.00079081]
CROSS VALIDATION 8
Iteration :  0   Loss :  27407.4260099
Iteration :  1   Loss :  16915.7400903
Iteration :  2   Loss :  6580.02583276
Iteration :  3   Loss :  16370.1190211
Iteration :  4   Loss :  16931.8423116
Iteration :  5   Loss :  5040.51343827
Iteration :  6   Loss :  40523.4144995
Iteration :  7   Loss :  11961.7414854
Iteration :  8   Loss :  25148.2522064
Iteration :  9   Loss :  13388.9240332
Iteration :  10   Loss :  23699.2264204
Iteration :  11   Loss :  16061.9704643
Iteration :  12   Loss :  26274.3108391
Iteration :  13   Loss :  9628.54922149
Iteration :  14   Loss :  60054.7078849
Iteration :  15   Loss :  16271.4825824
Iteration :  16   Loss :  10421.3719951
Iteration :  17   Loss :  11419.816
Iteration :  18   Loss :  13035.9245724
Iteration :  19   Loss :  3997.47069708
Iteration :  20   Loss :  4047.14998662
Iteration :  21   Loss :  29964.4718256
Iteration :  22   Loss :  28258.074521
Iteration :  23   Loss :  2365.7330083
Iteration :  24   Loss :  25360.6813137
Iteration :  25   Loss :  3649.38088427
Iteration :  26   Loss :  428.729350742
Iteration :  27   Loss :  50.3671340472
Iteration :  28   Loss :  5.91713207351
Iteration :  29   Loss :  0.69526563185
Iteration :  30   Loss :  62.2947644781
Iteration :  31   Loss :  31336.0853982
Iteration :  32   Loss :  5784.57084508
Iteration :  33   Loss :  7657.89360133
Iteration :  34   Loss :  19848.5432634
Iteration :  35   Loss :  31778.8293158
Iteration :  36   Loss :  13931.3502769
Iteration :  37   Loss :  44747.8624306
Iteration :  38   Loss :  19488.4545959
Iteration :  39   Loss :  34585.012865
Iteration :  40   Loss :  9306.40266118
Iteration :  41   Loss :  477.495958921
Iteration :  42   Loss :  56.0962363047
Iteration :  43   Loss :  6.59018713931
Iteration :  44   Loss :  0.774215403077
Iteration :  45   Loss :  10.5474893784
Iteration :  46   Loss :  34627.2997204
Iteration :  47   Loss :  23764.3231528
Iteration :  48   Loss :  9642.63899528
Iteration :  49   Loss :  21814.4779248
Iteration :  50   Loss :  11883.2059731
Iteration :  51   Loss :  5384.31735946
Iteration :  52   Loss :  6985.12702665
Iteration :  53   Loss :  27079.7981191
Iteration :  54   Loss :  38697.3756607
Iteration :  55   Loss :  21013.342298
Iteration :  56   Loss :  9002.4522376
Iteration :  57   Loss :  11135.7313057
Iteration :  58   Loss :  11279.4995028
Iteration :  59   Loss :  10617.0693698
Iteration :  60   Loss :  30009.8074616
Iteration :  61   Loss :  8944.24920487
Iteration :  62   Loss :  16613.7561456
Iteration :  63   Loss :  14612.5920766
Iteration :  64   Loss :  19258.6797826
Iteration :  65   Loss :  21139.1435557
Iteration :  66   Loss :  33157.6442767
Iteration :  67   Loss :  11292.6196427
Iteration :  68   Loss :  1585.56231459
Iteration :  69   Loss :  64455.5133849
Iteration :  70   Loss :  14640.7183791
Iteration :  71   Loss :  18019.1182416
Iteration :  72   Loss :  19875.0121316
Iteration :  73   Loss :  11061.6922917
Iteration :  74   Loss :  13976.8760649
Iteration :  75   Loss :  64342.2898145
Iteration :  76   Loss :  21984.648493
Iteration :  77   Loss :  21489.4591305
Iteration :  78   Loss :  24378.8006534
Iteration :  79   Loss :  21574.1762734
Iteration :  80   Loss :  19516.6028817
Iteration :  81   Loss :  20118.908186
Iteration :  82   Loss :  36879.3542908
Iteration :  83   Loss :  18042.1454415
Iteration :  84   Loss :  47584.1958164
Iteration :  85   Loss :  15571.0241745
Iteration :  86   Loss :  2902.83383128
Iteration :  87   Loss :  50688.9384181
Iteration :  88   Loss :  11023.8152114
Iteration :  89   Loss :  24510.8874524
Iteration :  90   Loss :  42427.2763661
Iteration :  91   Loss :  3856.20719083
Iteration :  92   Loss :  24730.5489318
Iteration :  93   Loss :  73891.152795
Iteration :  94   Loss :  4338.64314907
Iteration :  95   Loss :  30818.1258783
Iteration :  96   Loss :  12983.2906545
Iteration :  97   Loss :  14469.7080296
Iteration :  98   Loss :  14347.4438522
Iteration :  99   Loss :  4050.21649453
[-0.32771562 -0.52068423  0.01635836 ..., -0.04009854 -0.01402441
  0.02235863]
CROSS VALIDATION 9
Iteration :  0   Loss :  34968.5690306
Iteration :  1   Loss :  5352.85106136
Iteration :  2   Loss :  628.85306657
Iteration :  3   Loss :  73.8776728141
Iteration :  4   Loss :  8.67916342444
Iteration :  5   Loss :  1.0277018911
Iteration :  6   Loss :  20718.3602936
Iteration :  7   Loss :  18649.8876201
Iteration :  8   Loss :  7210.65810264
Iteration :  9   Loss :  2037.07293777
Iteration :  10   Loss :  239.315375879
Iteration :  11   Loss :  28.1147879197
Iteration :  12   Loss :  3.3060649782
Iteration :  13   Loss :  0.388482540201
Iteration :  14   Loss :  70630.407418
Iteration :  15   Loss :  21763.200424
Iteration :  16   Loss :  1123.49746682
Iteration :  17   Loss :  131.988508403
Iteration :  18   Loss :  15.5060513999
Iteration :  19   Loss :  1.82931642349
Iteration :  20   Loss :  0.21560251047
Iteration :  21   Loss :  67272.4386086
Iteration :  22   Loss :  10554.7090344
Iteration :  23   Loss :  25906.2023832
Iteration :  24   Loss :  16061.2798046
Iteration :  25   Loss :  14818.3301295
Iteration :  26   Loss :  6077.94610217
Iteration :  27   Loss :  23228.6968606
Iteration :  28   Loss :  19301.2886846
Iteration :  29   Loss :  40950.8099563
Iteration :  30   Loss :  6311.33841916
Iteration :  31   Loss :  1490.10348354
Iteration :  32   Loss :  175.057391731
Iteration :  33   Loss :  20.565746432
Iteration :  34   Loss :  2.41618130889
Iteration :  35   Loss :  127395.869883
Iteration :  36   Loss :  51272.8403006
Iteration :  37   Loss :  14694.6293155
Iteration :  38   Loss :  28515.8273741
Iteration :  39   Loss :  2909.38813795
Iteration :  40   Loss :  24861.7410614
Iteration :  41   Loss :  6270.47250418
Iteration :  42   Loss :  22821.7015786
Iteration :  43   Loss :  11276.7267991
Iteration :  44   Loss :  10761.6645416
Iteration :  45   Loss :  33271.9536761
Iteration :  46   Loss :  15650.3210191
Iteration :  47   Loss :  1402.54165197
Iteration :  48   Loss :  164.770625732
Iteration :  49   Loss :  19.3572569243
Iteration :  50   Loss :  2.27410215996
Iteration :  51   Loss :  88204.4221577
Iteration :  52   Loss :  70729.2297512
Iteration :  53   Loss :  4209.46918793
Iteration :  54   Loss :  494.528537617
Iteration :  55   Loss :  58.0972241401
Iteration :  56   Loss :  6.82915618715
Iteration :  57   Loss :  0.805903979318
Iteration :  58   Loss :  0.0948922306474
Iteration :  59   Loss :  34401.5726361
Iteration :  60   Loss :  33214.7810588
Iteration :  61   Loss :  23456.457527
Iteration :  62   Loss :  33065.6891561
Iteration :  63   Loss :  18701.5841195
Iteration :  64   Loss :  5840.87286179
Iteration :  65   Loss :  12857.7817656
Iteration :  66   Loss :  30523.7145833
Iteration :  67   Loss :  30383.6568931
Iteration :  68   Loss :  3719.79171897
Iteration :  69   Loss :  27725.0120285
Iteration :  70   Loss :  12263.4704071
Iteration :  71   Loss :  33925.246085
Iteration :  72   Loss :  13639.6494229
Iteration :  73   Loss :  25624.2027902
Iteration :  74   Loss :  42931.3483653
Iteration :  75   Loss :  12742.6741216
Iteration :  76   Loss :  6411.61403606
Iteration :  77   Loss :  15465.7740266
Iteration :  78   Loss :  18831.3932824
Iteration :  79   Loss :  16650.222467
Iteration :  80   Loss :  57226.0401526
Iteration :  81   Loss :  3403.41370467
Iteration :  82   Loss :  35670.6775261
Iteration :  83   Loss :  13838.1144579
Iteration :  84   Loss :  7447.24371014
Iteration :  85   Loss :  8670.40261874
Iteration :  86   Loss :  13917.1334622
Iteration :  87   Loss :  16989.6911258
Iteration :  88   Loss :  38740.5452582
Iteration :  89   Loss :  5661.0017463
Iteration :  90   Loss :  35664.1444408
Iteration :  91   Loss :  3550.52880415
Iteration :  92   Loss :  4834.88323897
Iteration :  93   Loss :  34371.6933848
Iteration :  94   Loss :  19955.1919859
Iteration :  95   Loss :  5618.68185135
Iteration :  96   Loss :  73156.591272
Iteration :  97   Loss :  31671.6138045
Iteration :  98   Loss :  26236.1528746
Iteration :  99   Loss :  11980.4756122
[-1.05404258 -0.85629305 -0.49249582 ...,  0.41310739  0.04345014
 -0.02153756]
CROSS VALIDATION 10
Iteration :  0   Loss :  27396.3021008
Iteration :  1   Loss :  10877.6633329
Iteration :  2   Loss :  875.974473805
Iteration :  3   Loss :  102.909501455
Iteration :  4   Loss :  12.0898106126
Iteration :  5   Loss :  1.42031123056
Iteration :  6   Loss :  0.166907266672
Iteration :  7   Loss :  98074.7687416
Iteration :  8   Loss :  24946.1832339
Iteration :  9   Loss :  4535.81275654
Iteration :  10   Loss :  6825.1094309
Iteration :  11   Loss :  22500.6143968
Iteration :  12   Loss :  19326.3237676
Iteration :  13   Loss :  4033.10135673
Iteration :  14   Loss :  473.808840726
Iteration :  15   Loss :  55.6630735739
Iteration :  16   Loss :  6.53929928193
Iteration :  17   Loss :  0.78242365729
Iteration :  18   Loss :  0.0926765424071
Iteration :  19   Loss :  61357.5020966
Iteration :  20   Loss :  27303.5167166
Iteration :  21   Loss :  4475.0238367
Iteration :  22   Loss :  25555.3522673
Iteration :  23   Loss :  20108.3052873
Iteration :  24   Loss :  16925.6613319
Iteration :  25   Loss :  34268.1438363
Iteration :  26   Loss :  3841.44240298
Iteration :  27   Loss :  37844.0997549
Iteration :  28   Loss :  17233.2191687
Iteration :  29   Loss :  1846.83387789
Iteration :  30   Loss :  216.966086716
Iteration :  31   Loss :  25.4891808886
Iteration :  32   Loss :  2.99446964486
Iteration :  33   Loss :  0.355274467703
Iteration :  34   Loss :  46039.0283088
Iteration :  35   Loss :  17243.5754124
Iteration :  36   Loss :  3338.8702728
Iteration :  37   Loss :  20808.4573944
Iteration :  38   Loss :  16727.6280911
Iteration :  39   Loss :  16353.2306848
Iteration :  40   Loss :  7623.86270325
Iteration :  41   Loss :  32525.0492922
Iteration :  42   Loss :  30638.4707354
Iteration :  43   Loss :  24763.9359752
Iteration :  44   Loss :  21957.0153977
Iteration :  45   Loss :  20235.0357571
Iteration :  46   Loss :  18007.1959468
Iteration :  47   Loss :  15173.1155416
Iteration :  48   Loss :  971.162861174
Iteration :  49   Loss :  37107.9278109
Iteration :  50   Loss :  19632.1160031
Iteration :  51   Loss :  7618.99191409
Iteration :  52   Loss :  5917.80098917
Iteration :  53   Loss :  13128.8228918
Iteration :  54   Loss :  53096.7888916
Iteration :  55   Loss :  3314.66448349
Iteration :  56   Loss :  33601.78208
Iteration :  57   Loss :  6706.69331112
Iteration :  58   Loss :  18527.6168153
Iteration :  59   Loss :  21931.3456115
Iteration :  60   Loss :  80722.025047
Iteration :  61   Loss :  13600.1485072
Iteration :  62   Loss :  6423.42283631
Iteration :  63   Loss :  23940.9660228
Iteration :  64   Loss :  10311.5596934
Iteration :  65   Loss :  21881.3553296
Iteration :  66   Loss :  51182.3831505
Iteration :  67   Loss :  8640.76300877
Iteration :  68   Loss :  26980.3048943
Iteration :  69   Loss :  54245.9737529
Iteration :  70   Loss :  3922.77366979
Iteration :  71   Loss :  5171.31819697
Iteration :  72   Loss :  13718.46716
Iteration :  73   Loss :  54644.9089672
Iteration :  74   Loss :  3216.22834775
Iteration :  75   Loss :  753.201381465
Iteration :  76   Loss :  88.4861157258
Iteration :  77   Loss :  10.3953509234
Iteration :  78   Loss :  1.22127381855
Iteration :  79   Loss :  109.874144417
Iteration :  80   Loss :  33637.4849378
Iteration :  81   Loss :  21798.9437806
Iteration :  82   Loss :  10785.4482193
Iteration :  83   Loss :  75092.5278178
Iteration :  84   Loss :  3863.51312034
Iteration :  85   Loss :  22391.8085903
Iteration :  86   Loss :  24481.900854
Iteration :  87   Loss :  5489.44572642
Iteration :  88   Loss :  7223.64382612
Iteration :  89   Loss :  26370.7793363
Iteration :  90   Loss :  23831.1800725
Iteration :  91   Loss :  25027.5680077
Iteration :  92   Loss :  24986.1297662
Iteration :  93   Loss :  21345.9610228
Iteration :  94   Loss :  4901.69857687
Iteration :  95   Loss :  22711.1881942
Iteration :  96   Loss :  15949.9472619
Iteration :  97   Loss :  17277.0842743
Iteration :  98   Loss :  17163.6863448
Iteration :  99   Loss :  34928.3533962
[-0.24553863 -0.53486867 -1.14361237 ...,  0.58100564 -0.09123346
  0.26736217]
CROSS VALIDATION 11
Iteration :  0   Loss :  27396.3021008
Iteration :  1   Loss :  14169.0163029
Iteration :  2   Loss :  4430.06568583
Iteration :  3   Loss :  16351.1121299
Iteration :  4   Loss :  16783.37897
Iteration :  5   Loss :  5032.57339908
Iteration :  6   Loss :  29975.7303817
Iteration :  7   Loss :  17809.1228398
Iteration :  8   Loss :  8888.48195723
Iteration :  9   Loss :  18499.5985476
Iteration :  10   Loss :  10475.0526539
Iteration :  11   Loss :  14849.4895202
Iteration :  12   Loss :  35247.5310446
Iteration :  13   Loss :  2896.07663658
Iteration :  14   Loss :  340.231150289
Iteration :  15   Loss :  39.9703634099
Iteration :  16   Loss :  4.69571921842
Iteration :  17   Loss :  0.551660883825
Iteration :  18   Loss :  0.0738587116478
Iteration :  19   Loss :  20193.4152405
Iteration :  20   Loss :  17277.2712311
Iteration :  21   Loss :  26975.1132653
Iteration :  22   Loss :  31362.4209461
Iteration :  23   Loss :  14103.0009434
Iteration :  24   Loss :  1488.99681252
Iteration :  25   Loss :  18565.9117373
Iteration :  26   Loss :  29651.8704942
Iteration :  27   Loss :  10212.5556608
Iteration :  28   Loss :  35765.1134062
Iteration :  29   Loss :  13111.9583923
Iteration :  30   Loss :  23141.2377432
Iteration :  31   Loss :  15434.7793765
Iteration :  32   Loss :  12561.8749034
Iteration :  33   Loss :  24568.4378008
Iteration :  34   Loss :  1671.88625932
Iteration :  35   Loss :  26932.6552296
Iteration :  36   Loss :  7404.07408258
Iteration :  37   Loss :  20468.7766293
Iteration :  38   Loss :  6957.40226837
Iteration :  39   Loss :  20494.2332984
Iteration :  40   Loss :  40653.6793249
Iteration :  41   Loss :  7852.087193
Iteration :  42   Loss :  11034.6165345
Iteration :  43   Loss :  24639.6687013
Iteration :  44   Loss :  8617.30130912
Iteration :  45   Loss :  24012.2696452
Iteration :  46   Loss :  14619.8001628
Iteration :  47   Loss :  8682.08458974
Iteration :  48   Loss :  6658.65670563
Iteration :  49   Loss :  28086.5382161
Iteration :  50   Loss :  24642.7130002
Iteration :  51   Loss :  1150.6995264
Iteration :  52   Loss :  135.184206992
Iteration :  53   Loss :  15.881443766
Iteration :  54   Loss :  1.86575238158
Iteration :  55   Loss :  0.220013320951
Iteration :  56   Loss :  52858.7304415
Iteration :  57   Loss :  14862.144282
Iteration :  58   Loss :  8796.07706343
Iteration :  59   Loss :  9412.17342491
Iteration :  60   Loss :  35891.2268853
Iteration :  61   Loss :  24762.2938538
Iteration :  62   Loss :  15438.498921
Iteration :  63   Loss :  7830.17327267
Iteration :  64   Loss :  66434.7939243
Iteration :  65   Loss :  50936.9028046
Iteration :  66   Loss :  5242.29159201
Iteration :  67   Loss :  20352.0105453
Iteration :  68   Loss :  10537.5434362
Iteration :  69   Loss :  8008.62420606
Iteration :  70   Loss :  11317.5048021
Iteration :  71   Loss :  35488.1499464
Iteration :  72   Loss :  22888.9368762
Iteration :  73   Loss :  15515.8929747
Iteration :  74   Loss :  10735.2351379
Iteration :  75   Loss :  21769.3421361
Iteration :  76   Loss :  24464.4439083
Iteration :  77   Loss :  17591.8148958
Iteration :  78   Loss :  50987.3855648
Iteration :  79   Loss :  17775.2308937
Iteration :  80   Loss :  12950.0185297
Iteration :  81   Loss :  32471.6451262
Iteration :  82   Loss :  29465.8658461
Iteration :  83   Loss :  21257.1388857
Iteration :  84   Loss :  7920.49060502
Iteration :  85   Loss :  17776.7854885
Iteration :  86   Loss :  9076.62177775
Iteration :  87   Loss :  3115.58056901
Iteration :  88   Loss :  366.018477351
Iteration :  89   Loss :  42.9998591899
Iteration :  90   Loss :  5.05162445277
Iteration :  91   Loss :  0.593530175988
Iteration :  92   Loss :  105749.501507
Iteration :  93   Loss :  26944.3031385
Iteration :  94   Loss :  5904.11618926
Iteration :  95   Loss :  6929.63891639
Iteration :  96   Loss :  36613.203412
Iteration :  97   Loss :  19366.9643982
Iteration :  98   Loss :  1296.05329644
Iteration :  99   Loss :  152.26037126
[-0.19694469 -0.14638977 -0.01077358 ...,  0.05406069  0.0056074
 -0.00588313]
CROSS VALIDATION 12
Iteration :  0   Loss :  28534.8756222
Iteration :  1   Loss :  8381.88190035
Iteration :  2   Loss :  10220.6485833
Iteration :  3   Loss :  12048.7214261
Iteration :  4   Loss :  12085.8421679
Iteration :  5   Loss :  15199.3716995
Iteration :  6   Loss :  4316.90471685
Iteration :  7   Loss :  507.150066041
Iteration :  8   Loss :  59.5800242572
Iteration :  9   Loss :  7.08162816509
Iteration :  10   Loss :  0.83197593207
Iteration :  11   Loss :  0.0978186924008
Iteration :  12   Loss :  0.0578347746328
Iteration :  13   Loss :  24305.95252
Iteration :  14   Loss :  4645.13247799
Iteration :  15   Loss :  545.710271017
Iteration :  16   Loss :  64.1100552684
Iteration :  17   Loss :  7.53165077663
Iteration :  18   Loss :  0.884819111334
Iteration :  19   Loss :  0.107387259605
Iteration :  20   Loss :  33947.9212655
Iteration :  21   Loss :  17230.0750886
Iteration :  22   Loss :  68982.3649572
Iteration :  23   Loss :  14226.8226147
Iteration :  24   Loss :  4914.19119953
Iteration :  25   Loss :  31327.2517322
Iteration :  26   Loss :  19018.1633928
Iteration :  27   Loss :  4732.94682452
Iteration :  28   Loss :  28150.1957862
Iteration :  29   Loss :  21012.0202963
Iteration :  30   Loss :  13954.2187992
Iteration :  31   Loss :  11901.6576241
Iteration :  32   Loss :  61922.9212746
Iteration :  33   Loss :  15376.9730141
Iteration :  34   Loss :  8606.52114879
Iteration :  35   Loss :  9770.08023091
Iteration :  36   Loss :  26777.8888263
Iteration :  37   Loss :  19510.7772321
Iteration :  38   Loss :  14309.592193
Iteration :  39   Loss :  36455.3828313
Iteration :  40   Loss :  4872.44716051
Iteration :  41   Loss :  26258.7443812
Iteration :  42   Loss :  41243.8785231
Iteration :  43   Loss :  1754.23518828
Iteration :  44   Loss :  206.087590517
Iteration :  45   Loss :  24.2111749034
Iteration :  46   Loss :  2.84432939718
Iteration :  47   Loss :  0.335940639735
Iteration :  48   Loss :  49165.2803478
Iteration :  49   Loss :  24832.0298736
Iteration :  50   Loss :  3258.12741682
Iteration :  51   Loss :  382.765484746
Iteration :  52   Loss :  44.9688943432
Iteration :  53   Loss :  5.30077706297
Iteration :  54   Loss :  0.623020113227
Iteration :  55   Loss :  0.0752006879338
Iteration :  56   Loss :  0.0206928618393
Iteration :  57   Loss :  25539.6428524
Iteration :  58   Loss :  7684.20457732
Iteration :  59   Loss :  8877.86274917
Iteration :  60   Loss :  34207.0734167
Iteration :  61   Loss :  28347.3485428
Iteration :  62   Loss :  7191.29350398
Iteration :  63   Loss :  64766.7433821
Iteration :  64   Loss :  26663.9420074
Iteration :  65   Loss :  3000.24946117
Iteration :  66   Loss :  29907.4035988
Iteration :  67   Loss :  6120.95756017
Iteration :  68   Loss :  15064.5287899
Iteration :  69   Loss :  32942.3732957
Iteration :  70   Loss :  13538.9892076
Iteration :  71   Loss :  19278.383034
Iteration :  72   Loss :  22878.7583395
Iteration :  73   Loss :  9137.58646901
Iteration :  74   Loss :  598.318886228
Iteration :  75   Loss :  70.2905261218
Iteration :  76   Loss :  8.25826047276
Iteration :  77   Loss :  0.970180631461
Iteration :  78   Loss :  0.113993178846
Iteration :  79   Loss :  0.0174295083609
Iteration :  80   Loss :  22442.8836703
Iteration :  81   Loss :  35030.5825857
Iteration :  82   Loss :  9043.99018482
Iteration :  83   Loss :  31722.5451981
Iteration :  84   Loss :  30346.2719869
Iteration :  85   Loss :  22007.0781303
Iteration :  86   Loss :  3865.87009162
Iteration :  87   Loss :  53053.8844367
Iteration :  88   Loss :  18326.5240908
Iteration :  89   Loss :  11919.8193179
Iteration :  90   Loss :  2083.75888164
Iteration :  91   Loss :  244.80004164
Iteration :  92   Loss :  28.7591145572
Iteration :  93   Loss :  3.37862164782
Iteration :  94   Loss :  0.398987618798
Iteration :  95   Loss :  68932.5594495
Iteration :  96   Loss :  12447.292994
Iteration :  97   Loss :  4269.7472682
Iteration :  98   Loss :  501.610007883
Iteration :  99   Loss :  58.9291553348
[-0.10165875  0.04459065 -0.07697009 ...,  0.07631969  0.00020931
  0.01010691]
CROSS VALIDATION 13
Iteration :  0   Loss :  42095.7436006
Iteration :  1   Loss :  7104.16127695
Iteration :  2   Loss :  19637.6565677
Iteration :  3   Loss :  19432.0391478
Iteration :  4   Loss :  17395.7352396
Iteration :  5   Loss :  5670.13890611
Iteration :  6   Loss :  13497.8833585
Iteration :  7   Loss :  33612.7651852
Iteration :  8   Loss :  23288.2977635
Iteration :  9   Loss :  14327.9168237
Iteration :  10   Loss :  27841.8875641
Iteration :  11   Loss :  10604.9624577
Iteration :  12   Loss :  13268.1990145
Iteration :  13   Loss :  15312.6339503
Iteration :  14   Loss :  10636.4128746
Iteration :  15   Loss :  17857.1406233
Iteration :  16   Loss :  2964.56753052
Iteration :  17   Loss :  19437.1713754
Iteration :  18   Loss :  3906.1810358
Iteration :  19   Loss :  41763.5282702
Iteration :  20   Loss :  16413.3091295
Iteration :  21   Loss :  17889.2765175
Iteration :  22   Loss :  30218.918951
Iteration :  23   Loss :  1170.49206505
Iteration :  24   Loss :  137.509434891
Iteration :  25   Loss :  16.1546348786
Iteration :  26   Loss :  90.494274655
Iteration :  27   Loss :  66523.6197668
Iteration :  28   Loss :  12740.3413014
Iteration :  29   Loss :  6722.66059869
Iteration :  30   Loss :  12712.8434341
Iteration :  31   Loss :  14101.599035
Iteration :  32   Loss :  8097.94953277
Iteration :  33   Loss :  30568.7880867
Iteration :  34   Loss :  11458.3063432
Iteration :  35   Loss :  13423.1062279
Iteration :  36   Loss :  26492.1274798
Iteration :  37   Loss :  46959.2565675
Iteration :  38   Loss :  27093.4926844
Iteration :  39   Loss :  12525.451916
Iteration :  40   Loss :  55849.3217763
Iteration :  41   Loss :  1845.63739147
Iteration :  42   Loss :  216.825523463
Iteration :  43   Loss :  25.4726901109
Iteration :  44   Loss :  2.99850789805
Iteration :  45   Loss :  0.352705896169
Iteration :  46   Loss :  0.0428809388944
Iteration :  47   Loss :  68874.2361873
Iteration :  48   Loss :  13115.2926018
Iteration :  49   Loss :  18165.4129446
Iteration :  50   Loss :  23248.6213642
Iteration :  51   Loss :  9971.06502362
Iteration :  52   Loss :  32716.1971405
Iteration :  53   Loss :  50491.8472107
Iteration :  54   Loss :  11318.1047144
Iteration :  55   Loss :  23754.0360248
Iteration :  56   Loss :  19345.5753496
Iteration :  57   Loss :  36378.6958971
Iteration :  58   Loss :  18206.2225522
Iteration :  59   Loss :  18739.7177465
Iteration :  60   Loss :  9340.051033
Iteration :  61   Loss :  41302.1644415
Iteration :  62   Loss :  37821.5877262
Iteration :  63   Loss :  19065.2137467
Iteration :  64   Loss :  21357.2026745
Iteration :  65   Loss :  20405.3184936
Iteration :  66   Loss :  21114.3463161
Iteration :  67   Loss :  8822.97285215
Iteration :  68   Loss :  25828.6967495
Iteration :  69   Loss :  21707.2484288
Iteration :  70   Loss :  10209.6052352
Iteration :  71   Loss :  16320.1085397
Iteration :  72   Loss :  20812.0176569
Iteration :  73   Loss :  21785.2317889
Iteration :  74   Loss :  13821.7699029
Iteration :  75   Loss :  7207.53952845
Iteration :  76   Loss :  15087.3468881
Iteration :  77   Loss :  23081.4473308
Iteration :  78   Loss :  14757.3660886
Iteration :  79   Loss :  16554.8834759
Iteration :  80   Loss :  5269.39694234
Iteration :  81   Loss :  37302.2601163
Iteration :  82   Loss :  15925.0243049
Iteration :  83   Loss :  4126.99762631
Iteration :  84   Loss :  484.83977665
Iteration :  85   Loss :  56.9589881814
Iteration :  86   Loss :  6.69154324975
Iteration :  87   Loss :  0.786122783228
Iteration :  88   Loss :  0.094176201517
Iteration :  89   Loss :  55043.34914
Iteration :  90   Loss :  24667.4926365
Iteration :  91   Loss :  36732.9147537
Iteration :  92   Loss :  8323.20185925
Iteration :  93   Loss :  40221.8768635
Iteration :  94   Loss :  20592.8882717
Iteration :  95   Loss :  16103.7187653
Iteration :  96   Loss :  27915.9331908
Iteration :  97   Loss :  6859.55589404
Iteration :  98   Loss :  15881.7484429
Iteration :  99   Loss :  35426.7652226
[-1.40751488 -1.08589333 -0.61657973 ...,  0.64455991  0.04563254
  0.1274558 ]
CROSS VALIDATION 14
Iteration :  0   Loss :  27391.1406951
Iteration :  1   Loss :  17030.7918966
Iteration :  2   Loss :  6613.49308328
Iteration :  3   Loss :  15816.6169269
Iteration :  4   Loss :  11854.6868028
Iteration :  5   Loss :  16905.6734204
Iteration :  6   Loss :  16396.2377142
Iteration :  7   Loss :  16510.4804258
Iteration :  8   Loss :  28486.6586976
Iteration :  9   Loss :  17384.0260995
Iteration :  10   Loss :  34195.0654805
Iteration :  11   Loss :  24247.1732379
Iteration :  12   Loss :  23119.6417274
Iteration :  13   Loss :  6282.06996098
Iteration :  14   Loss :  23006.2162357
Iteration :  15   Loss :  3411.33702852
Iteration :  16   Loss :  25916.164131
Iteration :  17   Loss :  29893.5338808
Iteration :  18   Loss :  3480.41723135
Iteration :  19   Loss :  16677.892494
Iteration :  20   Loss :  15708.2550508
Iteration :  21   Loss :  20808.7026442
Iteration :  22   Loss :  6052.82646164
Iteration :  23   Loss :  2483.7059641
Iteration :  24   Loss :  291.785834142
Iteration :  25   Loss :  34.2790065478
Iteration :  26   Loss :  4.02709848255
Iteration :  27   Loss :  0.473105831602
Iteration :  28   Loss :  0.0597428644027
Iteration :  29   Loss :  32225.2087382
Iteration :  30   Loss :  7953.60554253
Iteration :  31   Loss :  3150.23785054
Iteration :  32   Loss :  370.090015588
Iteration :  33   Loss :  43.4781836958
Iteration :  34   Loss :  5.11094363257
Iteration :  35   Loss :  8.32634877689
Iteration :  36   Loss :  43964.7436683
Iteration :  37   Loss :  67878.186068
Iteration :  38   Loss :  2777.68721346
Iteration :  39   Loss :  24633.8720426
Iteration :  40   Loss :  17403.0979765
Iteration :  41   Loss :  1644.16667212
Iteration :  42   Loss :  20068.5662465
Iteration :  43   Loss :  34161.4388285
Iteration :  44   Loss :  1581.91685838
Iteration :  45   Loss :  185.843629133
Iteration :  46   Loss :  21.8329138515
Iteration :  47   Loss :  2.56493122455
Iteration :  48   Loss :  0.30132834874
Iteration :  49   Loss :  0.0402719967605
Iteration :  50   Loss :  37702.0326196
Iteration :  51   Loss :  20741.1258504
Iteration :  52   Loss :  1347.4709604
Iteration :  53   Loss :  158.300919612
Iteration :  54   Loss :  18.5971956996
Iteration :  55   Loss :  2.18479898118
Iteration :  56   Loss :  0.256671617178
Iteration :  57   Loss :  0.0350729057909
Iteration :  58   Loss :  32214.8465726
Iteration :  59   Loss :  7923.68987863
Iteration :  60   Loss :  3150.21638316
Iteration :  61   Loss :  370.087493599
Iteration :  62   Loss :  43.477887449
Iteration :  63   Loss :  5.11103674019
Iteration :  64   Loss :  8.52195685349
Iteration :  65   Loss :  43963.858493
Iteration :  66   Loss :  67877.8435409
Iteration :  67   Loss :  2776.71087253
Iteration :  68   Loss :  24633.87672
Iteration :  69   Loss :  17403.013018
Iteration :  70   Loss :  1644.16792924
Iteration :  71   Loss :  20068.5680424
Iteration :  72   Loss :  34161.4351888
Iteration :  73   Loss :  1581.91685935
Iteration :  74   Loss :  185.843629247
Iteration :  75   Loss :  21.8329138649
Iteration :  76   Loss :  2.56493122612
Iteration :  77   Loss :  0.301328348923
Iteration :  78   Loss :  0.0402719904919
Iteration :  79   Loss :  37702.03283
Iteration :  80   Loss :  20741.1252664
Iteration :  81   Loss :  1347.47096205
Iteration :  82   Loss :  158.300919806
Iteration :  83   Loss :  18.5971957223
Iteration :  84   Loss :  2.18479898385
Iteration :  85   Loss :  0.256671617492
Iteration :  86   Loss :  0.0350729055681
Iteration :  87   Loss :  32214.8465711
Iteration :  88   Loss :  7923.68987848
Iteration :  89   Loss :  3150.21638316
Iteration :  90   Loss :  370.087493599
Iteration :  91   Loss :  43.477887449
Iteration :  92   Loss :  5.11103674016
Iteration :  93   Loss :  8.52195672732
Iteration :  94   Loss :  43963.8584939
Iteration :  95   Loss :  67877.8435412
Iteration :  96   Loss :  2776.7108728
Iteration :  97   Loss :  24633.87672
Iteration :  98   Loss :  17403.013018
Iteration :  99   Loss :  1644.16792924
[-0.70792046 -0.26396635 -0.40150026 ...,  0.1812147   0.1022395
 -0.03396788]
CROSS VALIDATION 15
Iteration :  0   Loss :  27391.1406951
Iteration :  1   Loss :  17030.7918966
Iteration :  2   Loss :  6613.49308328
Iteration :  3   Loss :  16302.1562928
Iteration :  4   Loss :  16853.5325242
Iteration :  5   Loss :  4766.08447974
Iteration :  6   Loss :  21554.0065958
Iteration :  7   Loss :  23097.1615962
Iteration :  8   Loss :  26867.4553653
Iteration :  9   Loss :  31294.1628776
Iteration :  10   Loss :  5630.84736191
Iteration :  11   Loss :  21516.4504033
Iteration :  12   Loss :  63766.0788713
Iteration :  13   Loss :  6740.77853247
Iteration :  14   Loss :  19377.4463968
Iteration :  15   Loss :  2377.61190575
Iteration :  16   Loss :  279.321901862
Iteration :  17   Loss :  32.8161273688
Iteration :  18   Loss :  3.85523940454
Iteration :  19   Loss :  0.453117440197
Iteration :  20   Loss :  110136.713323
Iteration :  21   Loss :  75815.3213851
Iteration :  22   Loss :  11237.4818742
Iteration :  23   Loss :  2132.07386803
Iteration :  24   Loss :  16464.934336
Iteration :  25   Loss :  41400.1594768
Iteration :  26   Loss :  34335.7030807
Iteration :  27   Loss :  3970.20058542
Iteration :  28   Loss :  3828.7397549
Iteration :  29   Loss :  8031.03761286
Iteration :  30   Loss :  56436.8377584
Iteration :  31   Loss :  22411.8823678
Iteration :  32   Loss :  4006.73133333
Iteration :  33   Loss :  3737.89901407
Iteration :  34   Loss :  30358.5844623
Iteration :  35   Loss :  9901.91830631
Iteration :  36   Loss :  10718.0691026
Iteration :  37   Loss :  16128.2277637
Iteration :  38   Loss :  15136.4581319
Iteration :  39   Loss :  25785.7450551
Iteration :  40   Loss :  3104.20881709
Iteration :  41   Loss :  364.682523673
Iteration :  42   Loss :  42.8429113209
Iteration :  43   Loss :  5.03318621349
Iteration :  44   Loss :  0.591298843772
Iteration :  45   Loss :  0.137925176349
Iteration :  46   Loss :  32339.2057562
Iteration :  47   Loss :  3444.4872936
Iteration :  48   Loss :  404.658446968
Iteration :  49   Loss :  47.5392837148
Iteration :  50   Loss :  5.58491615101
Iteration :  51   Loss :  0.656167764557
Iteration :  52   Loss :  0.0774526292388
Iteration :  53   Loss :  48475.9674211
Iteration :  54   Loss :  14767.6796261
Iteration :  55   Loss :  21182.0145679
Iteration :  56   Loss :  44291.4430445
Iteration :  57   Loss :  16565.4894848
Iteration :  58   Loss :  7468.53512558
Iteration :  59   Loss :  54782.3436386
Iteration :  60   Loss :  18423.7853272
Iteration :  61   Loss :  11128.2418986
Iteration :  62   Loss :  23591.384474
Iteration :  63   Loss :  64436.569543
Iteration :  64   Loss :  34225.8751411
Iteration :  65   Loss :  14430.0489584
Iteration :  66   Loss :  21852.9093564
Iteration :  67   Loss :  13966.2191846
Iteration :  68   Loss :  17879.0103633
Iteration :  69   Loss :  37631.0976067
Iteration :  70   Loss :  7319.76534852
Iteration :  71   Loss :  18996.8445643
Iteration :  72   Loss :  18721.9978815
Iteration :  73   Loss :  12595.0086604
Iteration :  74   Loss :  24638.6014187
Iteration :  75   Loss :  23305.8501647
Iteration :  76   Loss :  53033.0180645
Iteration :  77   Loss :  7815.15444199
Iteration :  78   Loss :  32678.4319866
Iteration :  79   Loss :  21300.7664748
Iteration :  80   Loss :  8362.82562295
Iteration :  81   Loss :  2792.41974032
Iteration :  82   Loss :  12135.2220505
Iteration :  83   Loss :  20352.0498625
Iteration :  84   Loss :  2693.72222407
Iteration :  85   Loss :  11078.1255827
Iteration :  86   Loss :  16893.0187677
Iteration :  87   Loss :  9374.95010413
Iteration :  88   Loss :  11521.687165
Iteration :  89   Loss :  11994.6439464
Iteration :  90   Loss :  19959.3347285
Iteration :  91   Loss :  9551.05273606
Iteration :  92   Loss :  11989.132057
Iteration :  93   Loss :  1674.49337393
Iteration :  94   Loss :  196.719520322
Iteration :  95   Loss :  23.1106138005
Iteration :  96   Loss :  2.71567938522
Iteration :  97   Loss :  95861.7024402
Iteration :  98   Loss :  39975.2878502
Iteration :  99   Loss :  9874.83212346
[-0.51698375 -0.29066737 -0.04820444 ...,  0.45852994  0.33523113
  0.10035771]
CROSS VALIDATION 16
Iteration :  0   Loss :  27391.1406951
Iteration :  1   Loss :  17030.7918966
Iteration :  2   Loss :  771.520559378
Iteration :  3   Loss :  90.638253171
Iteration :  4   Loss :  10.6481840802
Iteration :  5   Loss :  1.25124092921
Iteration :  6   Loss :  100558.483732
Iteration :  7   Loss :  37365.4768995
Iteration :  8   Loss :  5231.0723512
Iteration :  9   Loss :  13763.9675006
Iteration :  10   Loss :  28989.3062948
Iteration :  11   Loss :  4908.32638727
Iteration :  12   Loss :  7708.79624334
Iteration :  13   Loss :  24648.479393
Iteration :  14   Loss :  4718.99306411
Iteration :  15   Loss :  15837.3423634
Iteration :  16   Loss :  36175.6779145
Iteration :  17   Loss :  6331.91260136
Iteration :  18   Loss :  37315.027779
Iteration :  19   Loss :  40061.1516599
Iteration :  20   Loss :  8150.73135435
Iteration :  21   Loss :  8354.9868582
Iteration :  22   Loss :  28421.7978904
Iteration :  23   Loss :  7957.47426947
Iteration :  24   Loss :  21547.0573351
Iteration :  25   Loss :  24834.2872867
Iteration :  26   Loss :  48124.9757341
Iteration :  27   Loss :  3619.42564094
Iteration :  28   Loss :  610.116615636
Iteration :  29   Loss :  71.6765142879
Iteration :  30   Loss :  8.42080959448
Iteration :  31   Loss :  0.989587439421
Iteration :  32   Loss :  0.121962671476
Iteration :  33   Loss :  57308.5994118
Iteration :  34   Loss :  23167.5528552
Iteration :  35   Loss :  6263.74575862
Iteration :  36   Loss :  2367.45687462
Iteration :  37   Loss :  278.128888421
Iteration :  38   Loss :  32.6745882511
Iteration :  39   Loss :  3.83973715753
Iteration :  40   Loss :  72878.2782661
Iteration :  41   Loss :  28324.269568
Iteration :  42   Loss :  5035.4024314
Iteration :  43   Loss :  26564.4799254
Iteration :  44   Loss :  2788.7462548
Iteration :  45   Loss :  17427.6100268
Iteration :  46   Loss :  37436.7971222
Iteration :  47   Loss :  1708.34094057
Iteration :  48   Loss :  200.695933234
Iteration :  49   Loss :  23.5777628824
Iteration :  50   Loss :  2.77085877526
Iteration :  51   Loss :  0.326261638206
Iteration :  52   Loss :  61733.2184199
Iteration :  53   Loss :  15166.1010075
Iteration :  54   Loss :  3605.73639728
Iteration :  55   Loss :  9296.73908947
Iteration :  56   Loss :  40638.4023163
Iteration :  57   Loss :  9320.95398112
Iteration :  58   Loss :  3564.18590833
Iteration :  59   Loss :  17292.4632027
Iteration :  60   Loss :  18001.2886671
Iteration :  61   Loss :  3326.66144318
Iteration :  62   Loss :  19013.8155625
Iteration :  63   Loss :  34267.646574
Iteration :  64   Loss :  14834.1598804
Iteration :  65   Loss :  8873.43307606
Iteration :  66   Loss :  27008.0676263
Iteration :  67   Loss :  27429.132624
Iteration :  68   Loss :  3924.3461342
Iteration :  69   Loss :  13238.1163921
Iteration :  70   Loss :  20464.8862525
Iteration :  71   Loss :  4685.13827021
Iteration :  72   Loss :  17848.340541
Iteration :  73   Loss :  29811.743468
Iteration :  74   Loss :  21475.0659379
Iteration :  75   Loss :  5168.88050941
Iteration :  76   Loss :  29754.7702722
Iteration :  77   Loss :  28287.4971904
Iteration :  78   Loss :  34691.9219165
Iteration :  79   Loss :  5177.96130083
Iteration :  80   Loss :  8896.34971754
Iteration :  81   Loss :  27084.9493579
Iteration :  82   Loss :  65251.9301179
Iteration :  83   Loss :  8572.24127467
Iteration :  84   Loss :  21933.0250477
Iteration :  85   Loss :  17228.6704473
Iteration :  86   Loss :  15303.0958208
Iteration :  87   Loss :  12516.2912778
Iteration :  88   Loss :  12292.480905
Iteration :  89   Loss :  702.138258507
Iteration :  90   Loss :  82.487218859
Iteration :  91   Loss :  9.69060038055
Iteration :  92   Loss :  1.13845928052
Iteration :  93   Loss :  0.135691122298
Iteration :  94   Loss :  106412.914742
Iteration :  95   Loss :  37380.3872147
Iteration :  96   Loss :  5241.54405627
Iteration :  97   Loss :  13756.7963642
Iteration :  98   Loss :  29006.1461944
Iteration :  99   Loss :  4910.15161786
[-0.38459657 -0.24287248  0.36036077 ...,  0.68391409  0.19616834
 -0.06343638]
CROSS VALIDATION 17
Iteration :  0   Loss :  27391.1406951
Iteration :  1   Loss :  17030.7918966
Iteration :  2   Loss :  6836.0267989
Iteration :  3   Loss :  16339.2441797
Iteration :  4   Loss :  16876.8622307
Iteration :  5   Loss :  5022.86319086
Iteration :  6   Loss :  40226.7768975
Iteration :  7   Loss :  7242.57814213
Iteration :  8   Loss :  20373.5483779
Iteration :  9   Loss :  30240.491234
Iteration :  10   Loss :  6882.57286883
Iteration :  11   Loss :  13050.3213003
Iteration :  12   Loss :  19135.2374114
Iteration :  13   Loss :  37576.4399068
Iteration :  14   Loss :  8053.60824287
Iteration :  15   Loss :  17241.2276435
Iteration :  16   Loss :  11570.4452783
Iteration :  17   Loss :  15479.7489358
Iteration :  18   Loss :  20362.6673333
Iteration :  19   Loss :  2861.99628164
Iteration :  20   Loss :  336.227389403
Iteration :  21   Loss :  39.5000014883
Iteration :  22   Loss :  4.64069716184
Iteration :  23   Loss :  0.554309420061
Iteration :  24   Loss :  47505.1944058
Iteration :  25   Loss :  26857.1036311
Iteration :  26   Loss :  10627.302384
Iteration :  27   Loss :  12674.0553412
Iteration :  28   Loss :  18914.3570019
Iteration :  29   Loss :  44131.9814519
Iteration :  30   Loss :  10633.42896
Iteration :  31   Loss :  3541.29962604
Iteration :  32   Loss :  3893.49444883
Iteration :  33   Loss :  40048.508365
Iteration :  34   Loss :  23360.670232
Iteration :  35   Loss :  2639.47058562
Iteration :  36   Loss :  310.085065483
Iteration :  37   Loss :  36.4288006693
Iteration :  38   Loss :  4.2796563457
Iteration :  39   Loss :  0.502899596079
Iteration :  40   Loss :  0.0598343614017
Iteration :  41   Loss :  75130.020259
Iteration :  42   Loss :  28435.6069887
Iteration :  43   Loss :  43743.11731
Iteration :  44   Loss :  6539.53970276
Iteration :  45   Loss :  5748.31432175
Iteration :  46   Loss :  33640.0975129
Iteration :  47   Loss :  52557.1722321
Iteration :  48   Loss :  5275.01331095
Iteration :  49   Loss :  21509.5787834
Iteration :  50   Loss :  51581.6661782
Iteration :  51   Loss :  7294.36513556
Iteration :  52   Loss :  20663.9330279
Iteration :  53   Loss :  20845.5308054
Iteration :  54   Loss :  16105.9068678
Iteration :  55   Loss :  13630.7826867
Iteration :  56   Loss :  8329.97515159
Iteration :  57   Loss :  24544.8658378
Iteration :  58   Loss :  5524.11518563
Iteration :  59   Loss :  20714.812877
Iteration :  60   Loss :  11595.3758075
Iteration :  61   Loss :  5945.86461946
Iteration :  62   Loss :  20962.7857207
Iteration :  63   Loss :  46057.2796046
Iteration :  64   Loss :  6861.70298794
Iteration :  65   Loss :  16320.6323323
Iteration :  66   Loss :  19050.8741814
Iteration :  67   Loss :  19184.5104091
Iteration :  68   Loss :  26317.9280142
Iteration :  69   Loss :  6833.18069836
Iteration :  70   Loss :  7371.42769273
Iteration :  71   Loss :  18493.2712119
Iteration :  72   Loss :  23887.2828596
Iteration :  73   Loss :  25546.0742497
Iteration :  74   Loss :  12657.8531886
Iteration :  75   Loss :  3771.47539203
Iteration :  76   Loss :  21312.5374382
Iteration :  77   Loss :  18379.8328588
Iteration :  78   Loss :  28871.2886354
Iteration :  79   Loss :  30457.4585503
Iteration :  80   Loss :  4986.57049678
Iteration :  81   Loss :  40440.5832594
Iteration :  82   Loss :  33298.123768
Iteration :  83   Loss :  8884.24529104
Iteration :  84   Loss :  8181.96308202
Iteration :  85   Loss :  15377.1349823
Iteration :  86   Loss :  1566.03493032
Iteration :  87   Loss :  183.977819857
Iteration :  88   Loss :  21.6137185346
Iteration :  89   Loss :  2.53918015364
Iteration :  90   Loss :  0.298302959712
Iteration :  91   Loss :  4.79005177954
Iteration :  92   Loss :  47977.0156367
Iteration :  93   Loss :  23962.0957278
Iteration :  94   Loss :  8874.91607362
Iteration :  95   Loss :  26792.1795945
Iteration :  96   Loss :  8789.87167893
Iteration :  97   Loss :  20307.5893475
Iteration :  98   Loss :  23931.0334666
Iteration :  99   Loss :  7005.87889245
[-0.87935972 -0.28516296 -0.22662746 ...,  0.32312508 -0.05205885
  0.0918771 ]
CROSS VALIDATION 18
Iteration :  0   Loss :  23685.4557517
Iteration :  1   Loss :  4596.25185245
Iteration :  2   Loss :  3330.01107936
Iteration :  3   Loss :  11162.0076112
Iteration :  4   Loss :  10515.8652737
Iteration :  5   Loss :  20975.8043818
Iteration :  6   Loss :  9282.08471225
Iteration :  7   Loss :  6774.38543127
Iteration :  8   Loss :  24181.7114021
Iteration :  9   Loss :  51226.5920437
Iteration :  10   Loss :  5532.82323518
Iteration :  11   Loss :  14634.9423497
Iteration :  12   Loss :  36685.7227179
Iteration :  13   Loss :  7705.31851967
Iteration :  14   Loss :  3091.46592599
Iteration :  15   Loss :  363.185488531
Iteration :  16   Loss :  42.6670395978
Iteration :  17   Loss :  5.01252479939
Iteration :  18   Loss :  0.588871529438
Iteration :  19   Loss :  0.0692259570064
Iteration :  20   Loss :  92220.3292715
Iteration :  21   Loss :  33245.3607313
Iteration :  22   Loss :  46531.9559283
Iteration :  23   Loss :  8495.46213268
Iteration :  24   Loss :  14999.5700967
Iteration :  25   Loss :  1166.88775156
Iteration :  26   Loss :  41211.4248269
Iteration :  27   Loss :  6838.07346416
Iteration :  28   Loss :  2740.88610973
Iteration :  29   Loss :  321.999363603
Iteration :  30   Loss :  37.8284926881
Iteration :  31   Loss :  4.44409219647
Iteration :  32   Loss :  0.522092090154
Iteration :  33   Loss :  0.0619310699315
Iteration :  34   Loss :  77531.6342868
Iteration :  35   Loss :  34511.4861649
Iteration :  36   Loss :  20192.5325006
Iteration :  37   Loss :  2170.46350694
Iteration :  38   Loss :  25638.2811046
Iteration :  39   Loss :  18425.5575739
Iteration :  40   Loss :  11830.8007699
Iteration :  41   Loss :  29020.0303829
Iteration :  42   Loss :  51943.6320363
Iteration :  43   Loss :  20000.2526687
Iteration :  44   Loss :  864.09057283
Iteration :  45   Loss :  101.513380494
Iteration :  46   Loss :  11.9258522321
Iteration :  47   Loss :  196.812127277
Iteration :  48   Loss :  26454.1115501
Iteration :  49   Loss :  12602.5868345
Iteration :  50   Loss :  20871.5728517
Iteration :  51   Loss :  35902.4342308
Iteration :  52   Loss :  19099.4459551
Iteration :  53   Loss :  4210.43674778
Iteration :  54   Loss :  52640.8509328
Iteration :  55   Loss :  2916.9405098
Iteration :  56   Loss :  342.682239249
Iteration :  57   Loss :  40.2584202486
Iteration :  58   Loss :  4.72956104416
Iteration :  59   Loss :  0.555827134442
Iteration :  60   Loss :  84178.0603584
Iteration :  61   Loss :  33298.5935189
Iteration :  62   Loss :  5146.60135753
Iteration :  63   Loss :  13316.6300674
Iteration :  64   Loss :  15921.6383304
Iteration :  65   Loss :  23182.5273115
Iteration :  66   Loss :  2450.98672546
Iteration :  67   Loss :  16842.1860882
Iteration :  68   Loss :  7024.97005869
Iteration :  69   Loss :  14889.1386679
Iteration :  70   Loss :  31637.4961618
Iteration :  71   Loss :  4293.49950909
Iteration :  72   Loss :  48346.1192035
Iteration :  73   Loss :  19151.0040428
Iteration :  74   Loss :  20646.2963379
Iteration :  75   Loss :  20063.5043363
Iteration :  76   Loss :  60977.6188373
Iteration :  77   Loss :  17809.2322949
Iteration :  78   Loss :  27790.8065983
Iteration :  79   Loss :  4305.21068204
Iteration :  80   Loss :  37642.6563897
Iteration :  81   Loss :  3865.49084977
Iteration :  82   Loss :  20727.7915158
Iteration :  83   Loss :  30483.3646639
Iteration :  84   Loss :  16241.8658905
Iteration :  85   Loss :  32906.5315682
Iteration :  86   Loss :  16693.4518244
Iteration :  87   Loss :  7283.49623232
Iteration :  88   Loss :  21902.5023678
Iteration :  89   Loss :  2969.75127777
Iteration :  90   Loss :  348.886448842
Iteration :  91   Loss :  40.9871880842
Iteration :  92   Loss :  4.81517880924
Iteration :  93   Loss :  0.572522857779
Iteration :  94   Loss :  24333.9227528
Iteration :  95   Loss :  13981.150918
Iteration :  96   Loss :  5509.44283474
Iteration :  97   Loss :  19728.7263641
Iteration :  98   Loss :  19993.1807095
Iteration :  99   Loss :  21707.3747049
[ 0.13625699 -0.31914932  0.19164285 ...,  0.22788439 -0.04223641
  0.03726494]
CROSS VALIDATION 19
Iteration :  0   Loss :  27316.9490482
Iteration :  1   Loss :  16998.963985
Iteration :  2   Loss :  4928.00499673
Iteration :  3   Loss :  13662.9888492
Iteration :  4   Loss :  13960.0265998
Iteration :  5   Loss :  10503.9086232
Iteration :  6   Loss :  32503.4894658
Iteration :  7   Loss :  15367.6724912
Iteration :  8   Loss :  3612.52641133
Iteration :  9   Loss :  27453.1526352
Iteration :  10   Loss :  2693.84083379
Iteration :  11   Loss :  18827.5704935
Iteration :  12   Loss :  46337.9635144
Iteration :  13   Loss :  3340.99795989
Iteration :  14   Loss :  392.500517648
Iteration :  15   Loss :  46.110969897
Iteration :  16   Loss :  5.41733250284
Iteration :  17   Loss :  0.639898929697
Iteration :  18   Loss :  58561.6014987
Iteration :  19   Loss :  14932.7267596
Iteration :  20   Loss :  13028.7973808
Iteration :  21   Loss :  8685.5608919
Iteration :  22   Loss :  35639.6778037
Iteration :  23   Loss :  29914.5639715
Iteration :  24   Loss :  4816.63558874
Iteration :  25   Loss :  33124.7954877
Iteration :  26   Loss :  26541.085755
Iteration :  27   Loss :  24244.9353821
Iteration :  28   Loss :  16458.2861633
Iteration :  29   Loss :  39200.1105985
Iteration :  30   Loss :  13236.8486239
Iteration :  31   Loss :  8702.73013912
Iteration :  32   Loss :  26295.3750858
Iteration :  33   Loss :  4308.76053723
Iteration :  34   Loss :  42491.5469716
Iteration :  35   Loss :  23479.3677876
Iteration :  36   Loss :  582.198188747
Iteration :  37   Loss :  68.3971657671
Iteration :  38   Loss :  8.03530061723
Iteration :  39   Loss :  0.94399244009
Iteration :  40   Loss :  0.114708856792
Iteration :  41   Loss :  20109.783553
Iteration :  42   Loss :  27396.5958688
Iteration :  43   Loss :  11898.1044278
Iteration :  44   Loss :  3297.06942196
Iteration :  45   Loss :  27210.7049414
Iteration :  46   Loss :  16802.2070446
Iteration :  47   Loss :  2636.27650387
Iteration :  48   Loss :  24675.1167236
Iteration :  49   Loss :  5951.76638694
Iteration :  50   Loss :  38617.7747327
Iteration :  51   Loss :  56450.9236859
Iteration :  52   Loss :  11285.2970877
Iteration :  53   Loss :  5106.23240099
Iteration :  54   Loss :  23605.8005712
Iteration :  55   Loss :  32538.1030445
Iteration :  56   Loss :  16537.2629007
Iteration :  57   Loss :  5172.58649049
Iteration :  58   Loss :  26064.1305938
Iteration :  59   Loss :  14823.6428139
Iteration :  60   Loss :  18853.5840232
Iteration :  61   Loss :  41989.1042366
Iteration :  62   Loss :  2828.43674363
Iteration :  63   Loss :  28195.8167667
Iteration :  64   Loss :  26479.1245231
Iteration :  65   Loss :  18271.0059421
Iteration :  66   Loss :  622.136587717
Iteration :  67   Loss :  73.0886207231
Iteration :  68   Loss :  8.58645285403
Iteration :  69   Loss :  1.00873669746
Iteration :  70   Loss :  0.122556245499
Iteration :  71   Loss :  66873.7468398
Iteration :  72   Loss :  13468.9295755
Iteration :  73   Loss :  13448.8323966
Iteration :  74   Loss :  23601.9614853
Iteration :  75   Loss :  17632.641436
Iteration :  76   Loss :  27646.820292
Iteration :  77   Loss :  18205.1270937
Iteration :  78   Loss :  24893.1684531
Iteration :  79   Loss :  6470.24160795
Iteration :  80   Loss :  29767.66829
Iteration :  81   Loss :  54641.6890208
Iteration :  82   Loss :  13772.4170094
Iteration :  83   Loss :  26037.7829634
Iteration :  84   Loss :  3468.14013575
Iteration :  85   Loss :  407.437183411
Iteration :  86   Loss :  47.8657297478
Iteration :  87   Loss :  5.62514614966
Iteration :  88   Loss :  0.663525025446
Iteration :  89   Loss :  0.0846603280204
Iteration :  90   Loss :  33578.5793048
Iteration :  91   Loss :  11554.2013968
Iteration :  92   Loss :  5845.75809842
Iteration :  93   Loss :  12219.8221636
Iteration :  94   Loss :  29667.994613
Iteration :  95   Loss :  10369.0437186
Iteration :  96   Loss :  25640.0272855
Iteration :  97   Loss :  28120.3678813
Iteration :  98   Loss :  7530.38409575
Iteration :  99   Loss :  884.669697843
[-0.39997532 -0.02629364 -0.46583586 ...,  0.15745239 -0.03657423
  0.10663795]
Accuracy (Logistic Loss):	0.8
Hinge Loss
CROSS VALIDATION 0
Iteration :  0   Loss :  32180.6579996
Iteration :  1   Loss :  12454.0737345
Iteration :  2   Loss :  20703.3399916
Iteration :  3   Loss :  18656.2351741
Iteration :  4   Loss :  4124.01409547
Iteration :  5   Loss :  6487.37108367
Iteration :  6   Loss :  950.973753737
Iteration :  7   Loss :  31330.5581184
Iteration :  8   Loss :  4397.04270472
Iteration :  9   Loss :  516.564678711
Iteration :  10   Loss :  60.6860304098
Iteration :  11   Loss :  7.12939625699
Iteration :  12   Loss :  0.837561637266
Iteration :  13   Loss :  0.0983967605297
Iteration :  14   Loss :  0.0115596537042
Iteration :  15   Loss :  1.13649019495
Iteration :  16   Loss :  45302.1572624
Iteration :  17   Loss :  68557.238277
Iteration :  18   Loss :  5233.94388167
Iteration :  19   Loss :  8324.84311377
Iteration :  20   Loss :  27670.3114066
Iteration :  21   Loss :  22864.6365795
Iteration :  22   Loss :  3724.88986666
Iteration :  23   Loss :  34633.3507441
Iteration :  24   Loss :  12089.3630817
Iteration :  25   Loss :  15757.3030181
Iteration :  26   Loss :  24662.6115569
Iteration :  27   Loss :  37030.2857997
Iteration :  28   Loss :  10217.0907721
Iteration :  29   Loss :  61023.4769058
Iteration :  30   Loss :  22523.6973424
Iteration :  31   Loss :  6368.35354797
Iteration :  32   Loss :  6845.2581109
Iteration :  33   Loss :  26272.8806731
Iteration :  34   Loss :  9735.36254983
Iteration :  35   Loss :  31966.5024791
Iteration :  36   Loss :  26190.6109244
Iteration :  37   Loss :  7227.96452604
Iteration :  38   Loss :  14612.6337468
Iteration :  39   Loss :  11245.6770408
Iteration :  40   Loss :  9344.50430104
Iteration :  41   Loss :  5540.69915969
Iteration :  42   Loss :  26337.664466
Iteration :  43   Loss :  12012.0235799
Iteration :  44   Loss :  22690.0919882
Iteration :  45   Loss :  46014.3768943
Iteration :  46   Loss :  6357.45328786
Iteration :  47   Loss :  8326.90739769
Iteration :  48   Loss :  11015.005328
Iteration :  49   Loss :  21789.3553388
Iteration :  50   Loss :  1247.20661112
Iteration :  51   Loss :  146.521861538
Iteration :  52   Loss :  17.2133916844
Iteration :  53   Loss :  2.02222965346
Iteration :  54   Loss :  0.237571586488
Iteration :  55   Loss :  0.0279099154788
Iteration :  56   Loss :  109988.479631
Iteration :  57   Loss :  30923.2585184
Iteration :  58   Loss :  30981.9929751
Iteration :  59   Loss :  2202.38350718
Iteration :  60   Loss :  258.736065393
Iteration :  61   Loss :  30.3963189503
Iteration :  62   Loss :  3.57096025374
Iteration :  63   Loss :  0.419516493251
Iteration :  64   Loss :  0.049284807336
Iteration :  65   Loss :  0.367440520041
Iteration :  66   Loss :  47338.5177996
Iteration :  67   Loss :  13541.9084842
Iteration :  68   Loss :  7620.46327372
Iteration :  69   Loss :  30542.4930023
Iteration :  70   Loss :  22503.8112726
Iteration :  71   Loss :  17643.7187078
Iteration :  72   Loss :  13115.8492823
Iteration :  73   Loss :  4099.92639703
Iteration :  74   Loss :  3457.84439002
Iteration :  75   Loss :  36004.7937021
Iteration :  76   Loss :  18414.8584728
Iteration :  77   Loss :  35293.3141625
Iteration :  78   Loss :  6647.63344072
Iteration :  79   Loss :  20648.444367
Iteration :  80   Loss :  8759.79899428
Iteration :  81   Loss :  31474.1797687
Iteration :  82   Loss :  15605.5249744
Iteration :  83   Loss :  8838.99421011
Iteration :  84   Loss :  10590.1460117
Iteration :  85   Loss :  13064.6643417
Iteration :  86   Loss :  2489.8595804
Iteration :  87   Loss :  292.508761128
Iteration :  88   Loss :  34.3639360269
Iteration :  89   Loss :  4.03707599972
Iteration :  90   Loss :  0.488650394682
Iteration :  91   Loss :  30162.5295349
Iteration :  92   Loss :  16609.1180239
Iteration :  93   Loss :  24722.293026
Iteration :  94   Loss :  2096.18867955
Iteration :  95   Loss :  26168.9957532
Iteration :  96   Loss :  22153.1811765
Iteration :  97   Loss :  19486.4926173
Iteration :  98   Loss :  4509.05596683
Iteration :  99   Loss :  489.44490393
[-0.20399905 -0.1650743  -0.07973155 ...,  0.09212611  0.14710891
 -0.0046757 ]
CROSS VALIDATION 1
Iteration :  0   Loss :  18910.7918062
Iteration :  1   Loss :  15538.8666273
Iteration :  2   Loss :  32341.666184
Iteration :  3   Loss :  4469.13813678
Iteration :  4   Loss :  26520.3195444
Iteration :  5   Loss :  28756.0094882
Iteration :  6   Loss :  15268.836043
Iteration :  7   Loss :  14202.6942343
Iteration :  8   Loss :  5700.10279686
Iteration :  9   Loss :  23474.8222177
Iteration :  10   Loss :  44489.9998406
Iteration :  11   Loss :  5879.49401581
Iteration :  12   Loss :  25217.4429145
Iteration :  13   Loss :  3493.20585979
Iteration :  14   Loss :  3083.44429121
Iteration :  15   Loss :  34396.0033113
Iteration :  16   Loss :  17444.7722793
Iteration :  17   Loss :  20165.5900422
Iteration :  18   Loss :  9505.30418987
Iteration :  19   Loss :  30182.5739661
Iteration :  20   Loss :  44016.1807617
Iteration :  21   Loss :  23092.3710914
Iteration :  22   Loss :  11111.860347
Iteration :  23   Loss :  21847.2865489
Iteration :  24   Loss :  18286.9167314
Iteration :  25   Loss :  22199.2893019
Iteration :  26   Loss :  12155.8375944
Iteration :  27   Loss :  28948.8373109
Iteration :  28   Loss :  15726.208759
Iteration :  29   Loss :  29318.284289
Iteration :  30   Loss :  6687.25161347
Iteration :  31   Loss :  1882.98256748
Iteration :  32   Loss :  221.212835605
Iteration :  33   Loss :  25.9880890464
Iteration :  34   Loss :  3.05308130261
Iteration :  35   Loss :  0.358676062088
Iteration :  36   Loss :  0.646418691185
Iteration :  37   Loss :  18832.6225284
Iteration :  38   Loss :  15519.8404686
Iteration :  39   Loss :  32328.8919717
Iteration :  40   Loss :  4468.99479451
Iteration :  41   Loss :  26519.4576201
Iteration :  42   Loss :  28755.2230066
Iteration :  43   Loss :  15268.6236425
Iteration :  44   Loss :  14202.5622325
Iteration :  45   Loss :  5700.02942049
Iteration :  46   Loss :  23474.811429
Iteration :  47   Loss :  44489.9982799
Iteration :  48   Loss :  5879.49402468
Iteration :  49   Loss :  25217.4426686
Iteration :  50   Loss :  3493.20577281
Iteration :  51   Loss :  3083.44428977
Iteration :  52   Loss :  34396.0032947
Iteration :  53   Loss :  17444.7722772
Iteration :  54   Loss :  20165.5900401
Iteration :  55   Loss :  9505.30418937
Iteration :  56   Loss :  30182.5739659
Iteration :  57   Loss :  44016.1807616
Iteration :  58   Loss :  23092.3710914
Iteration :  59   Loss :  11111.860347
Iteration :  60   Loss :  21847.2865489
Iteration :  61   Loss :  18286.9167314
Iteration :  62   Loss :  22199.2893019
Iteration :  63   Loss :  12155.8375944
Iteration :  64   Loss :  28948.8373109
Iteration :  65   Loss :  15726.208759
Iteration :  66   Loss :  29318.284289
Iteration :  67   Loss :  6687.25161347
Iteration :  68   Loss :  1882.98256748
Iteration :  69   Loss :  221.212835605
Iteration :  70   Loss :  25.9880890464
Iteration :  71   Loss :  3.05308130261
Iteration :  72   Loss :  0.358676062088
Iteration :  73   Loss :  0.646418691185
Iteration :  74   Loss :  18832.6225284
Iteration :  75   Loss :  15519.8404686
Iteration :  76   Loss :  32328.8919717
Iteration :  77   Loss :  4468.99479451
Iteration :  78   Loss :  26519.4576201
Iteration :  79   Loss :  28755.2230066
Iteration :  80   Loss :  15268.6236425
Iteration :  81   Loss :  14202.5622325
Iteration :  82   Loss :  5700.02942049
Iteration :  83   Loss :  23474.811429
Iteration :  84   Loss :  44489.9982799
Iteration :  85   Loss :  5879.49402468
Iteration :  86   Loss :  25217.4426686
Iteration :  87   Loss :  3493.20577281
Iteration :  88   Loss :  3083.44428977
Iteration :  89   Loss :  34396.0032947
Iteration :  90   Loss :  17444.7722772
Iteration :  91   Loss :  20165.5900401
Iteration :  92   Loss :  9505.30418937
Iteration :  93   Loss :  30182.5739659
Iteration :  94   Loss :  44016.1807616
Iteration :  95   Loss :  23092.3710914
Iteration :  96   Loss :  11111.860347
Iteration :  97   Loss :  21847.2865489
Iteration :  98   Loss :  18286.9167314
Iteration :  99   Loss :  22199.2893019
[-0.47476648 -0.05935984 -0.31233754 ...,  0.14794864  0.35651959
  0.04476747]
CROSS VALIDATION 2
Iteration :  0   Loss :  40163.7189609
Iteration :  1   Loss :  20617.3248921
Iteration :  2   Loss :  43630.7606795
Iteration :  3   Loss :  4557.66761128
Iteration :  4   Loss :  535.43489645
Iteration :  5   Loss :  62.9029040265
Iteration :  6   Loss :  7.38983462079
Iteration :  7   Loss :  0.868157942273
Iteration :  8   Loss :  0.10199121515
Iteration :  9   Loss :  0.0119819303162
Iteration :  10   Loss :  36653.4979278
Iteration :  11   Loss :  10737.2583691
Iteration :  12   Loss :  1298.53083094
Iteration :  13   Loss :  152.55143207
Iteration :  14   Loss :  17.9217457701
Iteration :  15   Loss :  2.29037481972
Iteration :  16   Loss :  53542.1338707
Iteration :  17   Loss :  4967.43216199
Iteration :  18   Loss :  18428.8412634
Iteration :  19   Loss :  25896.3679856
Iteration :  20   Loss :  13439.775008
Iteration :  21   Loss :  12960.0511109
Iteration :  22   Loss :  10412.7309475
Iteration :  23   Loss :  19107.6611475
Iteration :  24   Loss :  37992.4458911
Iteration :  25   Loss :  8432.06062331
Iteration :  26   Loss :  990.598677169
Iteration :  27   Loss :  116.375555519
Iteration :  28   Loss :  13.6718029557
Iteration :  29   Loss :  1.60616372764
Iteration :  30   Loss :  0.188692151894
Iteration :  31   Loss :  0.745478952388
Iteration :  32   Loss :  36100.5916382
Iteration :  33   Loss :  23911.8387344
Iteration :  34   Loss :  3151.76966524
Iteration :  35   Loss :  15312.3473238
Iteration :  36   Loss :  27120.8533319
Iteration :  37   Loss :  5994.18586
Iteration :  38   Loss :  15980.2753067
Iteration :  39   Loss :  24752.9835355
Iteration :  40   Loss :  2866.04642305
Iteration :  41   Loss :  22690.0719891
Iteration :  42   Loss :  10938.3894015
Iteration :  43   Loss :  14184.8294904
Iteration :  44   Loss :  23156.6631463
Iteration :  45   Loss :  17042.8757553
Iteration :  46   Loss :  17261.8571446
Iteration :  47   Loss :  1497.08914706
Iteration :  48   Loss :  175.878067644
Iteration :  49   Loss :  20.6621594571
Iteration :  50   Loss :  2.42739097121
Iteration :  51   Loss :  0.285169947474
Iteration :  52   Loss :  109339.351767
Iteration :  53   Loss :  25655.0528026
Iteration :  54   Loss :  29224.5231273
Iteration :  55   Loss :  4952.84285472
Iteration :  56   Loss :  5451.50971189
Iteration :  57   Loss :  11862.0736057
Iteration :  58   Loss :  20948.0306604
Iteration :  59   Loss :  37676.9942934
Iteration :  60   Loss :  3125.07444395
Iteration :  61   Loss :  46809.0272987
Iteration :  62   Loss :  15475.9132638
Iteration :  63   Loss :  37924.2438426
Iteration :  64   Loss :  21126.6842448
Iteration :  65   Loss :  12929.6581072
Iteration :  66   Loss :  15539.1754623
Iteration :  67   Loss :  14416.8423245
Iteration :  68   Loss :  3585.73467781
Iteration :  69   Loss :  421.252192933
Iteration :  70   Loss :  49.4887173748
Iteration :  71   Loss :  5.81393566251
Iteration :  72   Loss :  0.683021296183
Iteration :  73   Loss :  0.0802413576827
Iteration :  74   Loss :  50051.3128733
Iteration :  75   Loss :  20252.7435722
Iteration :  76   Loss :  8448.34116941
Iteration :  77   Loss :  47998.1704265
Iteration :  78   Loss :  7244.97732198
Iteration :  79   Loss :  13748.3976056
Iteration :  80   Loss :  48644.3912865
Iteration :  81   Loss :  59439.0505141
Iteration :  82   Loss :  23628.5814753
Iteration :  83   Loss :  8662.88129456
Iteration :  84   Loss :  36812.9096525
Iteration :  85   Loss :  10091.1138014
Iteration :  86   Loss :  33921.593126
Iteration :  87   Loss :  54895.2446491
Iteration :  88   Loss :  10650.6303967
Iteration :  89   Loss :  35959.2169905
Iteration :  90   Loss :  16650.9125672
Iteration :  91   Loss :  4153.24737742
Iteration :  92   Loss :  15947.1068236
Iteration :  93   Loss :  24192.4859534
Iteration :  94   Loss :  5909.93541779
Iteration :  95   Loss :  22795.7682898
Iteration :  96   Loss :  28614.1091498
Iteration :  97   Loss :  11180.9573666
Iteration :  98   Loss :  9549.57636218
Iteration :  99   Loss :  24418.3164405
[-0.85093612 -1.38781625 -0.37961376 ...,  0.29815505 -0.31011937
  0.00423521]
CROSS VALIDATION 3
Iteration :  0   Loss :  18790.7246421
Iteration :  1   Loss :  17146.1648236
Iteration :  2   Loss :  16678.3412293
Iteration :  3   Loss :  10276.654125
Iteration :  4   Loss :  5574.48979109
Iteration :  5   Loss :  25954.1712048
Iteration :  6   Loss :  17616.3658217
Iteration :  7   Loss :  23100.9380073
Iteration :  8   Loss :  17817.8911894
Iteration :  9   Loss :  16754.8733659
Iteration :  10   Loss :  21083.5581797
Iteration :  11   Loss :  28784.9882161
Iteration :  12   Loss :  51233.4048384
Iteration :  13   Loss :  6046.27760118
Iteration :  14   Loss :  710.316832515
Iteration :  15   Loss :  83.4480379227
Iteration :  16   Loss :  9.80347742639
Iteration :  17   Loss :  1.15171275493
Iteration :  18   Loss :  0.83564907578
Iteration :  19   Loss :  33228.7544249
Iteration :  20   Loss :  5392.20169646
Iteration :  21   Loss :  44432.0345178
Iteration :  22   Loss :  19468.455413
Iteration :  23   Loss :  20074.5776871
Iteration :  24   Loss :  9579.29353293
Iteration :  25   Loss :  37042.0013356
Iteration :  26   Loss :  9122.79276636
Iteration :  27   Loss :  16003.3817436
Iteration :  28   Loss :  23152.2939977
Iteration :  29   Loss :  15115.9910097
Iteration :  30   Loss :  8008.46268094
Iteration :  31   Loss :  71506.130625
Iteration :  32   Loss :  18710.4481589
Iteration :  33   Loss :  3210.34029281
Iteration :  34   Loss :  377.150851896
Iteration :  35   Loss :  44.3076908091
Iteration :  36   Loss :  5.20526854166
Iteration :  37   Loss :  0.611515068739
Iteration :  38   Loss :  0.0718408044278
Iteration :  39   Loss :  0.577295773472
Iteration :  40   Loss :  45217.4062486
Iteration :  41   Loss :  68285.6143448
Iteration :  42   Loss :  5263.16785958
Iteration :  43   Loss :  8611.46592853
Iteration :  44   Loss :  23564.5794777
Iteration :  45   Loss :  19801.111563
Iteration :  46   Loss :  11035.3354664
Iteration :  47   Loss :  9190.41903609
Iteration :  48   Loss :  20853.1645035
Iteration :  49   Loss :  8008.6055215
Iteration :  50   Loss :  41906.4774554
Iteration :  51   Loss :  14775.539636
Iteration :  52   Loss :  1138.56294211
Iteration :  53   Loss :  133.758400789
Iteration :  54   Loss :  15.7139400202
Iteration :  55   Loss :  1.84607403724
Iteration :  56   Loss :  0.216876820619
Iteration :  57   Loss :  38502.8537556
Iteration :  58   Loss :  19332.1452204
Iteration :  59   Loss :  3467.11632597
Iteration :  60   Loss :  50112.8442525
Iteration :  61   Loss :  13157.3673293
Iteration :  62   Loss :  16240.7831496
Iteration :  63   Loss :  20409.7811427
Iteration :  64   Loss :  30783.0305728
Iteration :  65   Loss :  3948.59681394
Iteration :  66   Loss :  28704.3049262
Iteration :  67   Loss :  17544.3976829
Iteration :  68   Loss :  1192.20671987
Iteration :  69   Loss :  18599.0678686
Iteration :  70   Loss :  18133.7611433
Iteration :  71   Loss :  19527.934409
Iteration :  72   Loss :  33236.3237567
Iteration :  73   Loss :  3464.89513849
Iteration :  74   Loss :  1389.81239568
Iteration :  75   Loss :  48584.6113032
Iteration :  76   Loss :  19132.7534549
Iteration :  77   Loss :  43083.6502254
Iteration :  78   Loss :  5824.57174157
Iteration :  79   Loss :  31297.9661217
Iteration :  80   Loss :  6456.37563841
Iteration :  81   Loss :  25365.5307859
Iteration :  82   Loss :  48586.9003642
Iteration :  83   Loss :  7961.93756679
Iteration :  84   Loss :  8849.86839229
Iteration :  85   Loss :  9480.39439183
Iteration :  86   Loss :  5843.85602852
Iteration :  87   Loss :  6404.07810342
Iteration :  88   Loss :  20204.5442256
Iteration :  89   Loss :  8317.54829288
Iteration :  90   Loss :  37988.8547545
Iteration :  91   Loss :  55236.0437382
Iteration :  92   Loss :  12299.467929
Iteration :  93   Loss :  14122.6282141
Iteration :  94   Loss :  4333.5077196
Iteration :  95   Loss :  21006.739721
Iteration :  96   Loss :  47610.2580634
Iteration :  97   Loss :  6216.44698788
Iteration :  98   Loss :  6862.67002577
Iteration :  99   Loss :  25830.5752576
[-1.0508007  -0.34018476 -0.6972628  ...,  0.96430433 -0.27006619
  0.15679307]
CROSS VALIDATION 4
Iteration :  0   Loss :  18790.7246421
Iteration :  1   Loss :  17146.1648236
Iteration :  2   Loss :  35892.5663742
Iteration :  3   Loss :  12223.2499186
Iteration :  4   Loss :  16974.4241769
Iteration :  5   Loss :  25941.8957714
Iteration :  6   Loss :  21083.6332712
Iteration :  7   Loss :  19825.4604445
Iteration :  8   Loss :  12296.5369095
Iteration :  9   Loss :  65056.5772456
Iteration :  10   Loss :  5048.34744618
Iteration :  11   Loss :  593.079974811
Iteration :  12   Loss :  69.6750491664
Iteration :  13   Loss :  8.18542638855
Iteration :  14   Loss :  0.961624081562
Iteration :  15   Loss :  0.112971619357
Iteration :  16   Loss :  0.0132719084565
Iteration :  17   Loss :  0.00155918411262
Iteration :  18   Loss :  69354.8357202
Iteration :  19   Loss :  12812.2707379
Iteration :  20   Loss :  6802.94096102
Iteration :  21   Loss :  12898.1781439
Iteration :  22   Loss :  14086.6133383
Iteration :  23   Loss :  8108.59469768
Iteration :  24   Loss :  9580.60896906
Iteration :  25   Loss :  22824.9758691
Iteration :  26   Loss :  16371.8833562
Iteration :  27   Loss :  3278.82512349
Iteration :  28   Loss :  28218.4908965
Iteration :  29   Loss :  6667.65179206
Iteration :  30   Loss :  30009.0326127
Iteration :  31   Loss :  13807.9306833
Iteration :  32   Loss :  19803.7127733
Iteration :  33   Loss :  15652.4007563
Iteration :  34   Loss :  21632.4713071
Iteration :  35   Loss :  2976.25307504
Iteration :  36   Loss :  349.650280135
Iteration :  37   Loss :  41.0769230022
Iteration :  38   Loss :  4.82571786493
Iteration :  39   Loss :  0.566925446454
Iteration :  40   Loss :  0.0666024145697
Iteration :  41   Loss :  0.283943670333
Iteration :  42   Loss :  20330.9176537
Iteration :  43   Loss :  17313.8221482
Iteration :  44   Loss :  9587.15180965
Iteration :  45   Loss :  13470.7008411
Iteration :  46   Loss :  18884.1890142
Iteration :  47   Loss :  10725.7052701
Iteration :  48   Loss :  41110.5423275
Iteration :  49   Loss :  4377.54768277
Iteration :  50   Loss :  514.274403081
Iteration :  51   Loss :  60.4169687758
Iteration :  52   Loss :  7.09778689001
Iteration :  53   Loss :  0.833848168102
Iteration :  54   Loss :  0.685744175703
Iteration :  55   Loss :  31966.1236506
Iteration :  56   Loss :  22965.1526714
Iteration :  57   Loss :  22059.3076955
Iteration :  58   Loss :  14466.0351739
Iteration :  59   Loss :  16060.6204202
Iteration :  60   Loss :  4471.40727721
Iteration :  61   Loss :  16891.9348834
Iteration :  62   Loss :  15095.5745502
Iteration :  63   Loss :  16817.5583477
Iteration :  64   Loss :  38561.3845716
Iteration :  65   Loss :  17657.7336926
Iteration :  66   Loss :  51087.762174
Iteration :  67   Loss :  4960.49112566
Iteration :  68   Loss :  22480.2456886
Iteration :  69   Loss :  14116.8666159
Iteration :  70   Loss :  1516.98993013
Iteration :  71   Loss :  178.216012099
Iteration :  72   Loss :  20.9368212258
Iteration :  73   Loss :  2.45965824215
Iteration :  74   Loss :  0.288960707212
Iteration :  75   Loss :  0.0339471105706
Iteration :  76   Loss :  0.00398810733547
Iteration :  77   Loss :  45851.0662296
Iteration :  78   Loss :  16481.8526782
Iteration :  79   Loss :  23274.8308972
Iteration :  80   Loss :  18828.9217195
Iteration :  81   Loss :  23933.9984272
Iteration :  82   Loss :  24792.1571694
Iteration :  83   Loss :  11333.4110686
Iteration :  84   Loss :  16206.3894755
Iteration :  85   Loss :  20912.0921026
Iteration :  86   Loss :  20410.7085497
Iteration :  87   Loss :  21522.8671198
Iteration :  88   Loss :  21264.7168502
Iteration :  89   Loss :  38456.0447466
Iteration :  90   Loss :  15491.1770187
Iteration :  91   Loss :  36486.7952821
Iteration :  92   Loss :  33333.0307473
Iteration :  93   Loss :  18863.2435252
Iteration :  94   Loss :  8666.1155943
Iteration :  95   Loss :  15360.4676153
Iteration :  96   Loss :  24798.9827554
Iteration :  97   Loss :  14220.7676213
Iteration :  98   Loss :  46109.8962341
Iteration :  99   Loss :  24007.3577023
[-0.58067816 -0.84570429  0.34604901 ...,  0.52709397  0.0443354
  0.07414424]
CROSS VALIDATION 5
Iteration :  0   Loss :  14094.2907
Iteration :  1   Loss :  1527.80358214
Iteration :  2   Loss :  179.486400188
Iteration :  3   Loss :  21.0860664479
Iteration :  4   Loss :  2.47719157429
Iteration :  5   Loss :  0.291020523477
Iteration :  6   Loss :  103258.25867
Iteration :  7   Loss :  33518.415933
Iteration :  8   Loss :  2733.36467717
Iteration :  9   Loss :  321.115745532
Iteration :  10   Loss :  37.7246852167
Iteration :  11   Loss :  4.43189689232
Iteration :  12   Loss :  0.907567686804
Iteration :  13   Loss :  14051.1526507
Iteration :  14   Loss :  1530.20745068
Iteration :  15   Loss :  179.768806719
Iteration :  16   Loss :  21.1192435738
Iteration :  17   Loss :  2.48108922381
Iteration :  18   Loss :  0.291478419432
Iteration :  19   Loss :  103256.212704
Iteration :  20   Loss :  33517.8525959
Iteration :  21   Loss :  2733.36960317
Iteration :  22   Loss :  321.116324238
Iteration :  23   Loss :  37.7247532031
Iteration :  24   Loss :  4.43190487936
Iteration :  25   Loss :  0.907568072674
Iteration :  26   Loss :  14051.1526119
Iteration :  27   Loss :  1530.20745285
Iteration :  28   Loss :  179.768806974
Iteration :  29   Loss :  21.1192436037
Iteration :  30   Loss :  2.48108922733
Iteration :  31   Loss :  0.291478419846
Iteration :  32   Loss :  103256.212702
Iteration :  33   Loss :  33517.8525954
Iteration :  34   Loss :  2733.36960317
Iteration :  35   Loss :  321.116324239
Iteration :  36   Loss :  37.7247532031
Iteration :  37   Loss :  4.43190487937
Iteration :  38   Loss :  0.907568072674
Iteration :  39   Loss :  14051.1526119
Iteration :  40   Loss :  1530.20745285
Iteration :  41   Loss :  179.768806974
Iteration :  42   Loss :  21.1192436037
Iteration :  43   Loss :  2.48108922733
Iteration :  44   Loss :  0.291478419846
Iteration :  45   Loss :  103256.212702
Iteration :  46   Loss :  33517.8525954
Iteration :  47   Loss :  2733.36960317
Iteration :  48   Loss :  321.116324239
Iteration :  49   Loss :  37.7247532031
Iteration :  50   Loss :  4.43190487937
Iteration :  51   Loss :  0.907568072674
Iteration :  52   Loss :  14051.1526119
Iteration :  53   Loss :  1530.20745285
Iteration :  54   Loss :  179.768806974
Iteration :  55   Loss :  21.1192436037
Iteration :  56   Loss :  2.48108922733
Iteration :  57   Loss :  0.291478419846
Iteration :  58   Loss :  103256.212702
Iteration :  59   Loss :  33517.8525954
Iteration :  60   Loss :  2733.36960317
Iteration :  61   Loss :  321.116324239
Iteration :  62   Loss :  37.7247532031
Iteration :  63   Loss :  4.43190487937
Iteration :  64   Loss :  0.907568072674
Iteration :  65   Loss :  14051.1526119
Iteration :  66   Loss :  1530.20745285
Iteration :  67   Loss :  179.768806974
Iteration :  68   Loss :  21.1192436037
Iteration :  69   Loss :  2.48108922733
Iteration :  70   Loss :  0.291478419846
Iteration :  71   Loss :  103256.212702
Iteration :  72   Loss :  33517.8525954
Iteration :  73   Loss :  2733.36960317
Iteration :  74   Loss :  321.116324239
Iteration :  75   Loss :  37.7247532031
Iteration :  76   Loss :  4.43190487937
Iteration :  77   Loss :  0.907568072674
Iteration :  78   Loss :  14051.1526119
Iteration :  79   Loss :  1530.20745285
Iteration :  80   Loss :  179.768806974
Iteration :  81   Loss :  21.1192436037
Iteration :  82   Loss :  2.48108922733
Iteration :  83   Loss :  0.291478419846
Iteration :  84   Loss :  103256.212702
Iteration :  85   Loss :  33517.8525954
Iteration :  86   Loss :  2733.36960317
Iteration :  87   Loss :  321.116324239
Iteration :  88   Loss :  37.7247532031
Iteration :  89   Loss :  4.43190487937
Iteration :  90   Loss :  0.907568072674
Iteration :  91   Loss :  14051.1526119
Iteration :  92   Loss :  1530.20745285
Iteration :  93   Loss :  179.768806974
Iteration :  94   Loss :  21.1192436037
Iteration :  95   Loss :  2.48108922733
Iteration :  96   Loss :  0.291478419846
Iteration :  97   Loss :  103256.212702
Iteration :  98   Loss :  33517.8525954
Iteration :  99   Loss :  2733.36960317
[-0.32953874 -0.2292994  -0.29689339 ...,  0.3578335   0.05043813
  0.08727569]
CROSS VALIDATION 6
Iteration :  0   Loss :  32367.6514689
Iteration :  1   Loss :  14516.8531797
Iteration :  2   Loss :  4320.26195334
Iteration :  3   Loss :  37188.8763772
Iteration :  4   Loss :  15263.5401376
Iteration :  5   Loss :  30310.294357
Iteration :  6   Loss :  2501.47982737
Iteration :  7   Loss :  53673.4077012
Iteration :  8   Loss :  9264.61888952
Iteration :  9   Loss :  5052.88475348
Iteration :  10   Loss :  31411.8944427
Iteration :  11   Loss :  17700.9612713
Iteration :  12   Loss :  15432.4035111
Iteration :  13   Loss :  31521.0934851
Iteration :  14   Loss :  15620.1158394
Iteration :  15   Loss :  17254.9642462
Iteration :  16   Loss :  49418.3854791
Iteration :  17   Loss :  8832.4087141
Iteration :  18   Loss :  19669.2590953
Iteration :  19   Loss :  15534.9749458
Iteration :  20   Loss :  9777.5048064
Iteration :  21   Loss :  17366.6702786
Iteration :  22   Loss :  12764.2762868
Iteration :  23   Loss :  15515.0909644
Iteration :  24   Loss :  7977.044595
Iteration :  25   Loss :  17890.4961608
Iteration :  26   Loss :  17157.3276461
Iteration :  27   Loss :  3320.49380269
Iteration :  28   Loss :  17965.6303732
Iteration :  29   Loss :  71201.0779023
Iteration :  30   Loss :  8716.38677405
Iteration :  31   Loss :  15868.6276025
Iteration :  32   Loss :  27821.8155165
Iteration :  33   Loss :  1482.70440161
Iteration :  34   Loss :  174.188147416
Iteration :  35   Loss :  20.4636275896
Iteration :  36   Loss :  2.40406744282
Iteration :  37   Loss :  106805.600211
Iteration :  38   Loss :  30900.605801
Iteration :  39   Loss :  30454.5066401
Iteration :  40   Loss :  3344.77753341
Iteration :  41   Loss :  22510.4394177
Iteration :  42   Loss :  6910.78780065
Iteration :  43   Loss :  14311.9028346
Iteration :  44   Loss :  1696.21604409
Iteration :  45   Loss :  199.271500115
Iteration :  46   Loss :  23.4104204452
Iteration :  47   Loss :  2.75025673569
Iteration :  48   Loss :  0.323100224958
Iteration :  49   Loss :  0.0379578219056
Iteration :  50   Loss :  57908.7702005
Iteration :  51   Loss :  41658.0551701
Iteration :  52   Loss :  22038.4815683
Iteration :  53   Loss :  25800.7698255
Iteration :  54   Loss :  2917.02879819
Iteration :  55   Loss :  11588.7353595
Iteration :  56   Loss :  7270.8156231
Iteration :  57   Loss :  36916.2574515
Iteration :  58   Loss :  18554.2345203
Iteration :  59   Loss :  12818.6873742
Iteration :  60   Loss :  12432.1456978
Iteration :  61   Loss :  17423.3723354
Iteration :  62   Loss :  7308.51863482
Iteration :  63   Loss :  20556.6118221
Iteration :  64   Loss :  3727.49658635
Iteration :  65   Loss :  31777.0199511
Iteration :  66   Loss :  19738.3202348
Iteration :  67   Loss :  34657.4796984
Iteration :  68   Loss :  14845.886346
Iteration :  69   Loss :  18412.1099767
Iteration :  70   Loss :  48381.8004692
Iteration :  71   Loss :  37059.1157594
Iteration :  72   Loss :  24581.1674675
Iteration :  73   Loss :  5752.81070311
Iteration :  74   Loss :  12326.1690049
Iteration :  75   Loss :  22730.0219897
Iteration :  76   Loss :  19478.9939953
Iteration :  77   Loss :  2720.17898829
Iteration :  78   Loss :  28952.5598536
Iteration :  79   Loss :  17751.721892
Iteration :  80   Loss :  13705.9332568
Iteration :  81   Loss :  23026.4669317
Iteration :  82   Loss :  16845.2914466
Iteration :  83   Loss :  6133.56620735
Iteration :  84   Loss :  37263.6201754
Iteration :  85   Loss :  27761.9230468
Iteration :  86   Loss :  7858.31868859
Iteration :  87   Loss :  2285.56693913
Iteration :  88   Loss :  18610.4032732
Iteration :  89   Loss :  11363.1912809
Iteration :  90   Loss :  30320.8195511
Iteration :  91   Loss :  27518.2623951
Iteration :  92   Loss :  5247.87363936
Iteration :  93   Loss :  1251.29135178
Iteration :  94   Loss :  30698.5323232
Iteration :  95   Loss :  15364.0130126
Iteration :  96   Loss :  4145.70915002
Iteration :  97   Loss :  487.038006888
Iteration :  98   Loss :  57.2172363207
Iteration :  99   Loss :  6.72188224713
[-0.03364902 -0.01065782  0.00923997 ..., -0.00925499  0.01778413
  0.00314568]
CROSS VALIDATION 7
Iteration :  0   Loss :  32367.6514689
Iteration :  1   Loss :  14516.8531797
Iteration :  2   Loss :  4408.54378779
Iteration :  3   Loss :  20285.8710747
Iteration :  4   Loss :  10507.0560641
Iteration :  5   Loss :  18995.0431784
Iteration :  6   Loss :  13798.4101803
Iteration :  7   Loss :  23433.0572553
Iteration :  8   Loss :  14569.8318509
Iteration :  9   Loss :  11423.642911
Iteration :  10   Loss :  5597.73073363
Iteration :  11   Loss :  11222.0099118
Iteration :  12   Loss :  47656.2508414
Iteration :  13   Loss :  2886.15967165
Iteration :  14   Loss :  339.06610502
Iteration :  15   Loss :  39.8334938647
Iteration :  16   Loss :  4.67963978109
Iteration :  17   Loss :  128182.086003
Iteration :  18   Loss :  33442.4753064
Iteration :  19   Loss :  6918.56428933
Iteration :  20   Loss :  12905.0237074
Iteration :  21   Loss :  11794.1056167
Iteration :  22   Loss :  9007.66095892
Iteration :  23   Loss :  32197.995045
Iteration :  24   Loss :  2133.40964935
Iteration :  25   Loss :  24603.3356303
Iteration :  26   Loss :  18430.1003061
Iteration :  27   Loss :  4131.6486428
Iteration :  28   Loss :  4160.73082697
Iteration :  29   Loss :  47459.8139405
Iteration :  30   Loss :  19590.9398837
Iteration :  31   Loss :  7599.54920742
Iteration :  32   Loss :  37720.9950077
Iteration :  33   Loss :  6017.33103534
Iteration :  34   Loss :  28329.0708267
Iteration :  35   Loss :  7904.12990899
Iteration :  36   Loss :  16482.1738995
Iteration :  37   Loss :  16458.1142537
Iteration :  38   Loss :  22907.8083564
Iteration :  39   Loss :  47978.3883369
Iteration :  40   Loss :  2375.56743618
Iteration :  41   Loss :  22945.6732171
Iteration :  42   Loss :  68130.9268736
Iteration :  43   Loss :  13506.254477
Iteration :  44   Loss :  19902.0786325
Iteration :  45   Loss :  17877.65893
Iteration :  46   Loss :  20936.1701676
Iteration :  47   Loss :  6075.45025189
Iteration :  48   Loss :  40160.1490232
Iteration :  49   Loss :  6104.55864277
Iteration :  50   Loss :  14648.5892576
Iteration :  51   Loss :  16256.6512764
Iteration :  52   Loss :  31874.6737002
Iteration :  53   Loss :  22546.9609416
Iteration :  54   Loss :  31707.0107429
Iteration :  55   Loss :  25945.5436818
Iteration :  56   Loss :  1645.77152633
Iteration :  57   Loss :  193.345277001
Iteration :  58   Loss :  22.7142076166
Iteration :  59   Loss :  2.66846563647
Iteration :  60   Loss :  0.313491404729
Iteration :  61   Loss :  0.0368289774827
Iteration :  62   Loss :  0.004326669127
Iteration :  63   Loss :  104921.984253
Iteration :  64   Loss :  16638.7946064
Iteration :  65   Loss :  12495.8262289
Iteration :  66   Loss :  18399.1908902
Iteration :  67   Loss :  15157.7378435
Iteration :  68   Loss :  18103.0283812
Iteration :  69   Loss :  27896.0652034
Iteration :  70   Loss :  9902.71486339
Iteration :  71   Loss :  3419.17606318
Iteration :  72   Loss :  401.684883032
Iteration :  73   Loss :  47.1899493548
Iteration :  74   Loss :  5.54387634232
Iteration :  75   Loss :  0.651294720997
Iteration :  76   Loss :  0.0765141188956
Iteration :  77   Loss :  0.133078738702
Iteration :  78   Loss :  59458.8800867
Iteration :  79   Loss :  38136.5670219
Iteration :  80   Loss :  2269.17121514
Iteration :  81   Loss :  266.58228687
Iteration :  82   Loss :  31.318093231
Iteration :  83   Loss :  3.67925031757
Iteration :  84   Loss :  0.432238412457
Iteration :  85   Loss :  0.0507793786987
Iteration :  86   Loss :  0.220274730482
Iteration :  87   Loss :  98593.7643388
Iteration :  88   Loss :  9811.71015508
Iteration :  89   Loss :  3188.66371977
Iteration :  90   Loss :  374.604287593
Iteration :  91   Loss :  44.0085203759
Iteration :  92   Loss :  5.17012199225
Iteration :  93   Loss :  0.607386051301
Iteration :  94   Loss :  0.0713557273635
Iteration :  95   Loss :  0.00838287250205
Iteration :  96   Loss :  130942.59712
Iteration :  97   Loss :  42057.6176907
Iteration :  98   Loss :  9304.64215216
Iteration :  99   Loss :  14471.0610836
[-0.78531177 -0.04861578 -0.27564718 ...,  0.31377824  0.32101907
 -0.03229391]
CROSS VALIDATION 8
Iteration :  0   Loss :  32367.6514689
Iteration :  1   Loss :  14516.8531797
Iteration :  2   Loss :  4408.54378779
Iteration :  3   Loss :  20285.8710747
Iteration :  4   Loss :  10507.0560641
Iteration :  5   Loss :  18995.0431784
Iteration :  6   Loss :  13798.4101803
Iteration :  7   Loss :  23433.0572553
Iteration :  8   Loss :  14569.8318509
Iteration :  9   Loss :  11423.642911
Iteration :  10   Loss :  5597.73073363
Iteration :  11   Loss :  10855.3675748
Iteration :  12   Loss :  4284.31892635
Iteration :  13   Loss :  17111.2674403
Iteration :  14   Loss :  25885.3616608
Iteration :  15   Loss :  13210.858771
Iteration :  16   Loss :  29207.0893041
Iteration :  17   Loss :  14059.2811853
Iteration :  18   Loss :  13025.7415729
Iteration :  19   Loss :  22501.5524844
Iteration :  20   Loss :  12938.8373596
Iteration :  21   Loss :  6824.67641174
Iteration :  22   Loss :  23158.6511484
Iteration :  23   Loss :  1948.77478007
Iteration :  24   Loss :  34570.0297194
Iteration :  25   Loss :  54558.3625586
Iteration :  26   Loss :  5188.50599345
Iteration :  27   Loss :  23264.8548283
Iteration :  28   Loss :  14893.2778443
Iteration :  29   Loss :  11868.397474
Iteration :  30   Loss :  15338.1880588
Iteration :  31   Loss :  44554.2299593
Iteration :  32   Loss :  56874.2283372
Iteration :  33   Loss :  6863.50943033
Iteration :  34   Loss :  16088.4899788
Iteration :  35   Loss :  8613.13158315
Iteration :  36   Loss :  3711.51577965
Iteration :  37   Loss :  33408.0096116
Iteration :  38   Loss :  27179.6558909
Iteration :  39   Loss :  8875.47496162
Iteration :  40   Loss :  14940.3727056
Iteration :  41   Loss :  7455.18509657
Iteration :  42   Loss :  18291.571839
Iteration :  43   Loss :  35435.0328979
Iteration :  44   Loss :  82455.9210117
Iteration :  45   Loss :  17242.8062856
Iteration :  46   Loss :  3582.53839112
Iteration :  47   Loss :  42818.9788487
Iteration :  48   Loss :  19957.234574
Iteration :  49   Loss :  16178.6058463
Iteration :  50   Loss :  27585.0117549
Iteration :  51   Loss :  8287.38771866
Iteration :  52   Loss :  21683.0184653
Iteration :  53   Loss :  36526.9991903
Iteration :  54   Loss :  16299.4556591
Iteration :  55   Loss :  19062.85359
Iteration :  56   Loss :  3140.51359098
Iteration :  57   Loss :  25077.950245
Iteration :  58   Loss :  20074.1790161
Iteration :  59   Loss :  2802.1952466
Iteration :  60   Loss :  33578.006732
Iteration :  61   Loss :  14928.7796713
Iteration :  62   Loss :  19348.6842049
Iteration :  63   Loss :  68657.8809818
Iteration :  64   Loss :  8876.62000235
Iteration :  65   Loss :  29387.0756676
Iteration :  66   Loss :  33866.6944973
Iteration :  67   Loss :  16532.8665011
Iteration :  68   Loss :  6709.00043068
Iteration :  69   Loss :  1303.32890011
Iteration :  70   Loss :  153.115109347
Iteration :  71   Loss :  17.9879665897
Iteration :  72   Loss :  2.11322673125
Iteration :  73   Loss :  0.248261925293
Iteration :  74   Loss :  0.029165816729
Iteration :  75   Loss :  0.0034264008243
Iteration :  76   Loss :  36641.3085885
Iteration :  77   Loss :  10735.2827093
Iteration :  78   Loss :  1298.43042748
Iteration :  79   Loss :  152.539636669
Iteration :  80   Loss :  17.9203600461
Iteration :  81   Loss :  2.263428181
Iteration :  82   Loss :  53542.1523383
Iteration :  83   Loss :  4967.43112752
Iteration :  84   Loss :  18428.847834
Iteration :  85   Loss :  25791.5606056
Iteration :  86   Loss :  13206.3024305
Iteration :  87   Loss :  13122.2816975
Iteration :  88   Loss :  34303.1400397
Iteration :  89   Loss :  28747.7329763
Iteration :  90   Loss :  15193.2004498
Iteration :  91   Loss :  2541.49051125
Iteration :  92   Loss :  298.574364079
Iteration :  93   Loss :  35.0765232019
Iteration :  94   Loss :  4.12079075753
Iteration :  95   Loss :  0.484110593562
Iteration :  96   Loss :  0.0568733237353
Iteration :  97   Loss :  0.00668147938864
Iteration :  98   Loss :  60725.199048
Iteration :  99   Loss :  60404.2686352
[-0.3304257  -1.1393747  -0.10058335 ...,  0.51846241  0.43873947
  0.19623512]
CROSS VALIDATION 9
Iteration :  0   Loss :  30700.0336207
Iteration :  1   Loss :  14516.8531797
Iteration :  2   Loss :  4408.54378779
Iteration :  3   Loss :  19055.8185092
Iteration :  4   Loss :  10507.0560641
Iteration :  5   Loss :  18995.0431784
Iteration :  6   Loss :  12021.4945632
Iteration :  7   Loss :  20440.9236573
Iteration :  8   Loss :  9368.63685091
Iteration :  9   Loss :  44318.9574933
Iteration :  10   Loss :  15636.8177705
Iteration :  11   Loss :  1977.29829973
Iteration :  12   Loss :  18649.7740651
Iteration :  13   Loss :  4300.0987738
Iteration :  14   Loss :  505.175703463
Iteration :  15   Loss :  59.3480533341
Iteration :  16   Loss :  6.97221067917
Iteration :  17   Loss :  0.819095471947
Iteration :  18   Loss :  0.0962273550006
Iteration :  19   Loss :  0.0113047918925
Iteration :  20   Loss :  104086.84215
Iteration :  21   Loss :  13483.2807292
Iteration :  22   Loss :  5614.21413268
Iteration :  23   Loss :  44682.461397
Iteration :  24   Loss :  20879.8226005
Iteration :  25   Loss :  7645.81685414
Iteration :  26   Loss :  898.230740971
Iteration :  27   Loss :  105.524168237
Iteration :  28   Loss :  12.3969817266
Iteration :  29   Loss :  1.45639770015
Iteration :  30   Loss :  0.171097635518
Iteration :  31   Loss :  0.0201005541804
Iteration :  32   Loss :  68194.2336269
Iteration :  33   Loss :  12983.8634441
Iteration :  34   Loss :  6764.52623308
Iteration :  35   Loss :  12755.6916077
Iteration :  36   Loss :  14076.7092226
Iteration :  37   Loss :  5295.75946346
Iteration :  38   Loss :  13716.8038052
Iteration :  39   Loss :  25711.4219662
Iteration :  40   Loss :  16661.4741451
Iteration :  41   Loss :  3333.45417561
Iteration :  42   Loss :  28102.8751728
Iteration :  43   Loss :  6554.83515831
Iteration :  44   Loss :  29363.6868649
Iteration :  45   Loss :  33051.815832
Iteration :  46   Loss :  9151.16945385
Iteration :  47   Loss :  17525.8793542
Iteration :  48   Loss :  30150.6738237
Iteration :  49   Loss :  6803.08369143
Iteration :  50   Loss :  31863.2767733
Iteration :  51   Loss :  41342.9710607
Iteration :  52   Loss :  18236.7796894
Iteration :  53   Loss :  5430.68688716
Iteration :  54   Loss :  10909.0552194
Iteration :  55   Loss :  10867.721136
Iteration :  56   Loss :  17811.7413146
Iteration :  57   Loss :  37138.13018
Iteration :  58   Loss :  8626.65305083
Iteration :  59   Loss :  1023.81810172
Iteration :  60   Loss :  120.278174285
Iteration :  61   Loss :  14.1302826986
Iteration :  62   Loss :  1.66002593844
Iteration :  63   Loss :  0.195019885666
Iteration :  64   Loss :  103651.545656
Iteration :  65   Loss :  13450.018399
Iteration :  66   Loss :  5611.89789147
Iteration :  67   Loss :  44676.8793781
Iteration :  68   Loss :  20873.0525045
Iteration :  69   Loss :  7645.84341392
Iteration :  70   Loss :  898.233861214
Iteration :  71   Loss :  105.524534803
Iteration :  72   Loss :  12.3970247908
Iteration :  73   Loss :  1.45640275933
Iteration :  74   Loss :  0.171098229872
Iteration :  75   Loss :  0.020100624005
Iteration :  76   Loss :  68194.2317671
Iteration :  77   Loss :  12983.8633842
Iteration :  78   Loss :  6764.52612002
Iteration :  79   Loss :  12755.6915878
Iteration :  80   Loss :  14076.7092171
Iteration :  81   Loss :  5295.75946245
Iteration :  82   Loss :  13716.8038045
Iteration :  83   Loss :  25711.4219653
Iteration :  84   Loss :  16661.4741451
Iteration :  85   Loss :  3333.45417555
Iteration :  86   Loss :  28102.8751728
Iteration :  87   Loss :  6554.8351583
Iteration :  88   Loss :  29363.6868649
Iteration :  89   Loss :  33051.815832
Iteration :  90   Loss :  9151.16945385
Iteration :  91   Loss :  17525.8793542
Iteration :  92   Loss :  30150.6738237
Iteration :  93   Loss :  6803.08369143
Iteration :  94   Loss :  31863.2767733
Iteration :  95   Loss :  41342.9710607
Iteration :  96   Loss :  18236.7796894
Iteration :  97   Loss :  5430.68688716
Iteration :  98   Loss :  10909.0552194
Iteration :  99   Loss :  10867.721136
[-0.17195873 -0.36887995 -0.34328342 ...,  0.4050584  -0.01471235
  0.11259338]
CROSS VALIDATION 10
Iteration :  0   Loss :  32706.785689
Iteration :  1   Loss :  16661.1199136
Iteration :  2   Loss :  4466.47136092
Iteration :  3   Loss :  28199.6731134
Iteration :  4   Loss :  18128.0805661
Iteration :  5   Loss :  9963.26830043
Iteration :  6   Loss :  37931.233645
Iteration :  7   Loss :  23650.6284081
Iteration :  8   Loss :  3123.35838502
Iteration :  9   Loss :  366.932215356
Iteration :  10   Loss :  43.1072051519
Iteration :  11   Loss :  5.06423545887
Iteration :  12   Loss :  0.594946498909
Iteration :  13   Loss :  0.0698943284607
Iteration :  14   Loss :  69005.1057783
Iteration :  15   Loss :  9187.21820508
Iteration :  16   Loss :  5983.22078099
Iteration :  17   Loss :  16380.3781131
Iteration :  18   Loss :  27824.6136293
Iteration :  19   Loss :  31524.538346
Iteration :  20   Loss :  53414.6389639
Iteration :  21   Loss :  3830.85138159
Iteration :  22   Loss :  6433.54721458
Iteration :  23   Loss :  36786.4972964
Iteration :  24   Loss :  23240.6573993
Iteration :  25   Loss :  9177.67538518
Iteration :  26   Loss :  35961.044726
Iteration :  27   Loss :  24238.3388945
Iteration :  28   Loss :  6197.65966454
Iteration :  29   Loss :  15641.4907156
Iteration :  30   Loss :  3639.75962496
Iteration :  31   Loss :  14367.0419599
Iteration :  32   Loss :  22973.7506198
Iteration :  33   Loss :  9088.78596307
Iteration :  34   Loss :  18558.0626719
Iteration :  35   Loss :  7257.59321031
Iteration :  36   Loss :  33715.9227458
Iteration :  37   Loss :  17518.2380711
Iteration :  38   Loss :  2135.86798241
Iteration :  39   Loss :  250.921820003
Iteration :  40   Loss :  29.4783012211
Iteration :  41   Loss :  3.46311150966
Iteration :  42   Loss :  0.406846420302
Iteration :  43   Loss :  0.0477963268727
Iteration :  44   Loss :  0.00561511358715
Iteration :  45   Loss :  53571.1319825
Iteration :  46   Loss :  9941.83452421
Iteration :  47   Loss :  843.944534499
Iteration :  48   Loss :  99.1466234443
Iteration :  49   Loss :  11.6477476168
Iteration :  50   Loss :  1.36837765958
Iteration :  51   Loss :  0.160757039116
Iteration :  52   Loss :  0.0188857406757
Iteration :  53   Loss :  98501.0765198
Iteration :  54   Loss :  6431.33687661
Iteration :  55   Loss :  67027.7436979
Iteration :  56   Loss :  10024.8760823
Iteration :  57   Loss :  4692.34935021
Iteration :  58   Loss :  22183.0237985
Iteration :  59   Loss :  14396.1765808
Iteration :  60   Loss :  24139.3761321
Iteration :  61   Loss :  16772.7202729
Iteration :  62   Loss :  45293.8115863
Iteration :  63   Loss :  6797.97127862
Iteration :  64   Loss :  4907.53333578
Iteration :  65   Loss :  724.216333885
Iteration :  66   Loss :  85.0809516654
Iteration :  67   Loss :  9.99531217067
Iteration :  68   Loss :  1.17424950513
Iteration :  69   Loss :  0.422732783227
Iteration :  70   Loss :  66955.197172
Iteration :  71   Loss :  17561.8435038
Iteration :  72   Loss :  6592.58100947
Iteration :  73   Loss :  73822.237966
Iteration :  74   Loss :  16505.0668537
Iteration :  75   Loss :  19189.1277388
Iteration :  76   Loss :  23686.7510465
Iteration :  77   Loss :  34798.2437786
Iteration :  78   Loss :  10501.4681648
Iteration :  79   Loss :  2577.03945371
Iteration :  80   Loss :  16253.6012439
Iteration :  81   Loss :  37915.5931876
Iteration :  82   Loss :  5853.774656
Iteration :  83   Loss :  13681.0083943
Iteration :  84   Loss :  2492.30404655
Iteration :  85   Loss :  36115.6869304
Iteration :  86   Loss :  29129.988325
Iteration :  87   Loss :  4914.25975608
Iteration :  88   Loss :  6354.97393116
Iteration :  89   Loss :  17139.4140947
Iteration :  90   Loss :  2440.33286153
Iteration :  91   Loss :  286.69036105
Iteration :  92   Loss :  33.680390251
Iteration :  93   Loss :  3.95677302615
Iteration :  94   Loss :  0.464841786683
Iteration :  95   Loss :  0.0546096238573
Iteration :  96   Loss :  0.393996392116
Iteration :  97   Loss :  37855.459101
Iteration :  98   Loss :  8111.49840712
Iteration :  99   Loss :  30586.4429481
[-0.33659919  0.17055561 -0.13066745 ...,  0.37684733  0.07567874
  0.20847399]
CROSS VALIDATION 11
Iteration :  0   Loss :  30973.9011063
Iteration :  1   Loss :  12990.6608328
Iteration :  2   Loss :  10779.1320604
Iteration :  3   Loss :  11907.3334445
Iteration :  4   Loss :  22047.6417219
Iteration :  5   Loss :  9810.24027241
Iteration :  6   Loss :  11973.4471384
Iteration :  7   Loss :  11667.0499559
Iteration :  8   Loss :  6412.56922919
Iteration :  9   Loss :  32500.3456616
Iteration :  10   Loss :  29242.5240628
Iteration :  11   Loss :  8759.1323184
Iteration :  12   Loss :  15644.2229685
Iteration :  13   Loss :  2837.92795431
Iteration :  14   Loss :  333.39984175
Iteration :  15   Loss :  39.5933699384
Iteration :  16   Loss :  53320.4319899
Iteration :  17   Loss :  7589.68272788
Iteration :  18   Loss :  2083.60496663
Iteration :  19   Loss :  244.7819597
Iteration :  20   Loss :  28.7569902905
Iteration :  21   Loss :  3.37837188485
Iteration :  22   Loss :  0.396891207218
Iteration :  23   Loss :  0.0466267882092
Iteration :  24   Loss :  0.0314467244919
Iteration :  25   Loss :  37080.9415046
Iteration :  26   Loss :  11407.0610267
Iteration :  27   Loss :  7595.50830218
Iteration :  28   Loss :  35486.4369378
Iteration :  29   Loss :  24541.8926135
Iteration :  30   Loss :  5244.83136655
Iteration :  31   Loss :  41071.6167558
Iteration :  32   Loss :  21095.7140644
Iteration :  33   Loss :  18144.4364004
Iteration :  34   Loss :  39949.6299004
Iteration :  35   Loss :  22798.6631337
Iteration :  36   Loss :  1311.65014748
Iteration :  37   Loss :  27651.7942316
Iteration :  38   Loss :  6170.50172058
Iteration :  39   Loss :  724.910684937
Iteration :  40   Loss :  85.162524043
Iteration :  41   Loss :  10.0048952955
Iteration :  42   Loss :  1.17537533086
Iteration :  43   Loss :  0.138083121071
Iteration :  44   Loss :  0.016222008259
Iteration :  45   Loss :  0.883625353595
Iteration :  46   Loss :  45138.3070786
Iteration :  47   Loss :  60435.9451581
Iteration :  48   Loss :  5294.72420511
Iteration :  49   Loss :  8537.61183004
Iteration :  50   Loss :  23561.2613876
Iteration :  51   Loss :  19253.3923257
Iteration :  52   Loss :  5953.17126169
Iteration :  53   Loss :  7429.05939086
Iteration :  54   Loss :  3112.34019384
Iteration :  55   Loss :  365.637798001
Iteration :  56   Loss :  42.9551369711
Iteration :  57   Loss :  5.04637048547
Iteration :  58   Loss :  0.592847721421
Iteration :  59   Loss :  0.0696477640327
Iteration :  60   Loss :  0.00818222093042
Iteration :  61   Loss :  107300.409841
Iteration :  62   Loss :  26998.8740976
Iteration :  63   Loss :  5903.33722457
Iteration :  64   Loss :  6963.40498906
Iteration :  65   Loss :  36625.0986036
Iteration :  66   Loss :  19370.257384
Iteration :  67   Loss :  1296.04513596
Iteration :  68   Loss :  152.259412567
Iteration :  69   Loss :  17.8874393121
Iteration :  70   Loss :  2.10141678435
Iteration :  71   Loss :  0.246874492459
Iteration :  72   Loss :  75603.1029951
Iteration :  73   Loss :  11839.428782
Iteration :  74   Loss :  5423.14241794
Iteration :  75   Loss :  23589.2230465
Iteration :  76   Loss :  10757.3864213
Iteration :  77   Loss :  27844.2739997
Iteration :  78   Loss :  23108.3954026
Iteration :  79   Loss :  4942.01087334
Iteration :  80   Loss :  41704.7211836
Iteration :  81   Loss :  22507.2281948
Iteration :  82   Loss :  3735.43468245
Iteration :  83   Loss :  36438.9509067
Iteration :  84   Loss :  29771.1075878
Iteration :  85   Loss :  7388.92877755
Iteration :  86   Loss :  9938.88125427
Iteration :  87   Loss :  20574.2062066
Iteration :  88   Loss :  5763.58915207
Iteration :  89   Loss :  3124.20318742
Iteration :  90   Loss :  18510.7711782
Iteration :  91   Loss :  14437.9327068
Iteration :  92   Loss :  4759.65915978
Iteration :  93   Loss :  25611.0898825
Iteration :  94   Loss :  31745.2813385
Iteration :  95   Loss :  32167.1178973
Iteration :  96   Loss :  3481.79485784
Iteration :  97   Loss :  46606.6225449
Iteration :  98   Loss :  18467.4723738
Iteration :  99   Loss :  9388.0951643
[-0.15463852 -0.09598082  0.00191139 ...,  0.2186267  -0.29133588
  0.02581324]
CROSS VALIDATION 12
Iteration :  0   Loss :  32261.8544409
Iteration :  1   Loss :  8409.31928834
Iteration :  2   Loss :  4009.16475259
Iteration :  3   Loss :  24903.1949137
Iteration :  4   Loss :  3024.00613003
Iteration :  5   Loss :  355.26031014
Iteration :  6   Loss :  41.7359894571
Iteration :  7   Loss :  4.90314500731
Iteration :  8   Loss :  0.576021588932
Iteration :  9   Loss :  0.0685248505018
Iteration :  10   Loss :  56524.6309137
Iteration :  11   Loss :  11650.0632041
Iteration :  12   Loss :  6356.5414674
Iteration :  13   Loss :  485.448498257
Iteration :  14   Loss :  57.03050081
Iteration :  15   Loss :  6.69994455501
Iteration :  16   Loss :  0.787109641379
Iteration :  17   Loss :  0.0924696588852
Iteration :  18   Loss :  51667.8171488
Iteration :  19   Loss :  61560.3564981
Iteration :  20   Loss :  16809.1798843
Iteration :  21   Loss :  20964.4809534
Iteration :  22   Loss :  17633.2101918
Iteration :  23   Loss :  22817.4269591
Iteration :  24   Loss :  49931.1761858
Iteration :  25   Loss :  8140.91149277
Iteration :  26   Loss :  3301.79977967
Iteration :  27   Loss :  24774.4988726
Iteration :  28   Loss :  8599.71334663
Iteration :  29   Loss :  4170.01474581
Iteration :  30   Loss :  14636.8294458
Iteration :  31   Loss :  13355.8662595
Iteration :  32   Loss :  32946.5816202
Iteration :  33   Loss :  14402.6953855
Iteration :  34   Loss :  14248.6903374
Iteration :  35   Loss :  34699.3509337
Iteration :  36   Loss :  25192.9129278
Iteration :  37   Loss :  10221.1588756
Iteration :  38   Loss :  14463.3946476
Iteration :  39   Loss :  13487.1078113
Iteration :  40   Loss :  8385.07332527
Iteration :  41   Loss :  35039.814416
Iteration :  42   Loss :  14442.4688429
Iteration :  43   Loss :  24745.2377552
Iteration :  44   Loss :  16883.8264788
Iteration :  45   Loss :  21561.6634449
Iteration :  46   Loss :  37776.3376564
Iteration :  47   Loss :  21818.6818336
Iteration :  48   Loss :  4475.44872336
Iteration :  49   Loss :  525.775819595
Iteration :  50   Loss :  61.768155454
Iteration :  51   Loss :  7.25652433224
Iteration :  52   Loss :  0.852496646488
Iteration :  53   Loss :  0.100151325759
Iteration :  54   Loss :  0.0117657800679
Iteration :  55   Loss :  0.0953865638844
Iteration :  56   Loss :  31277.8823415
Iteration :  57   Loss :  9927.77164554
Iteration :  58   Loss :  3246.25148642
Iteration :  59   Loss :  29814.6721127
Iteration :  60   Loss :  16572.0360248
Iteration :  61   Loss :  14084.1108243
Iteration :  62   Loss :  17088.704546
Iteration :  63   Loss :  19498.2467701
Iteration :  64   Loss :  12201.2084996
Iteration :  65   Loss :  36747.4940324
Iteration :  66   Loss :  14643.7263822
Iteration :  67   Loss :  6743.32258674
Iteration :  68   Loss :  13975.5311326
Iteration :  69   Loss :  20497.3622849
Iteration :  70   Loss :  10853.9064884
Iteration :  71   Loss :  47531.7335442
Iteration :  72   Loss :  19499.9289829
Iteration :  73   Loss :  20294.3035377
Iteration :  74   Loss :  25402.2936613
Iteration :  75   Loss :  9404.77829921
Iteration :  76   Loss :  14998.948424
Iteration :  77   Loss :  4143.49900506
Iteration :  78   Loss :  52223.6981364
Iteration :  79   Loss :  32508.6380099
Iteration :  80   Loss :  26317.0566489
Iteration :  81   Loss :  24069.2228401
Iteration :  82   Loss :  11611.9649428
Iteration :  83   Loss :  1924.598692
Iteration :  84   Loss :  54933.8662764
Iteration :  85   Loss :  19786.5379696
Iteration :  86   Loss :  3197.19446784
Iteration :  87   Loss :  22754.2939569
Iteration :  88   Loss :  26345.339269
Iteration :  89   Loss :  14687.7258443
Iteration :  90   Loss :  15065.8114191
Iteration :  91   Loss :  15494.875408
Iteration :  92   Loss :  7379.97334304
Iteration :  93   Loss :  15993.8285432
Iteration :  94   Loss :  7340.65548421
Iteration :  95   Loss :  36982.0060191
Iteration :  96   Loss :  12647.8899546
Iteration :  97   Loss :  16200.1254313
Iteration :  98   Loss :  23563.0025108
Iteration :  99   Loss :  18387.4403884
[-0.80621707 -0.76449949  0.42648441 ..., -0.38894272  0.17819409
  0.09969604]
CROSS VALIDATION 13
Iteration :  0   Loss :  23736.4361125
Iteration :  1   Loss :  24856.2222281
Iteration :  2   Loss :  20370.631581
Iteration :  3   Loss :  22867.5474128
Iteration :  4   Loss :  21830.1985361
Iteration :  5   Loss :  2002.66859546
Iteration :  6   Loss :  35223.3086255
Iteration :  7   Loss :  32770.7208931
Iteration :  8   Loss :  29981.5485219
Iteration :  9   Loss :  1086.92153221
Iteration :  10   Loss :  127.691566758
Iteration :  11   Loss :  15.0012082179
Iteration :  12   Loss :  1.76234228861
Iteration :  13   Loss :  0.207040012852
Iteration :  14   Loss :  0.345258381739
Iteration :  15   Loss :  69250.2009909
Iteration :  16   Loss :  13104.0788973
Iteration :  17   Loss :  6760.41352267
Iteration :  18   Loss :  12728.8450077
Iteration :  19   Loss :  14117.2843885
Iteration :  20   Loss :  8103.10832469
Iteration :  21   Loss :  30576.3601668
Iteration :  22   Loss :  11464.3042756
Iteration :  23   Loss :  13425.1906505
Iteration :  24   Loss :  26500.2340819
Iteration :  25   Loss :  46968.3115761
Iteration :  26   Loss :  27100.4962707
Iteration :  27   Loss :  12528.4534888
Iteration :  28   Loss :  55857.3233421
Iteration :  29   Loss :  1845.63738623
Iteration :  30   Loss :  216.825522848
Iteration :  31   Loss :  25.4726674421
Iteration :  32   Loss :  2.99252956061
Iteration :  33   Loss :  0.351562442037
Iteration :  34   Loss :  46681.4269266
Iteration :  35   Loss :  13661.800842
Iteration :  36   Loss :  7423.7601885
Iteration :  37   Loss :  28633.9887453
Iteration :  38   Loss :  27709.6580277
Iteration :  39   Loss :  8426.25250137
Iteration :  40   Loss :  26285.7441642
Iteration :  41   Loss :  19656.403494
Iteration :  42   Loss :  22036.07873
Iteration :  43   Loss :  1206.22615896
Iteration :  44   Loss :  141.70747707
Iteration :  45   Loss :  16.6477976857
Iteration :  46   Loss :  1.9557836574
Iteration :  47   Loss :  0.229765509333
Iteration :  48   Loss :  0.0269928573539
Iteration :  49   Loss :  0.565483797398
Iteration :  50   Loss :  15935.579865
Iteration :  51   Loss :  19137.8701604
Iteration :  52   Loss :  10678.46584
Iteration :  53   Loss :  39567.1242159
Iteration :  54   Loss :  38308.2089573
Iteration :  55   Loss :  30094.9315451
Iteration :  56   Loss :  5763.77444659
Iteration :  57   Loss :  45165.637868
Iteration :  58   Loss :  12665.8715756
Iteration :  59   Loss :  13151.0859994
Iteration :  60   Loss :  33104.5253246
Iteration :  61   Loss :  15437.2391421
Iteration :  62   Loss :  18359.8165562
Iteration :  63   Loss :  4908.99676927
Iteration :  64   Loss :  14158.4018686
Iteration :  65   Loss :  19547.7784771
Iteration :  66   Loss :  20528.9083805
Iteration :  67   Loss :  63184.1575199
Iteration :  68   Loss :  23646.1953538
Iteration :  69   Loss :  1811.04995851
Iteration :  70   Loss :  35762.9621691
Iteration :  71   Loss :  30161.5002513
Iteration :  72   Loss :  25714.9237111
Iteration :  73   Loss :  45116.0022847
Iteration :  74   Loss :  10907.8004266
Iteration :  75   Loss :  909.801620813
Iteration :  76   Loss :  106.883515469
Iteration :  77   Loss :  12.5566778708
Iteration :  78   Loss :  102298.070167
Iteration :  79   Loss :  68082.1811775
Iteration :  80   Loss :  12149.8336439
Iteration :  81   Loss :  1562.75583671
Iteration :  82   Loss :  183.592591864
Iteration :  83   Loss :  21.568461941
Iteration :  84   Loss :  2.53386340799
Iteration :  85   Loss :  0.297678331812
Iteration :  86   Loss :  0.0349712573104
Iteration :  87   Loss :  0.00410842411816
Iteration :  88   Loss :  0.12656637894
Iteration :  89   Loss :  63555.196629
Iteration :  90   Loss :  10102.4958267
Iteration :  91   Loss :  2316.73949554
Iteration :  92   Loss :  10951.9470347
Iteration :  93   Loss :  39334.9746334
Iteration :  94   Loss :  7278.35616947
Iteration :  95   Loss :  44225.2250117
Iteration :  96   Loss :  3820.29486523
Iteration :  97   Loss :  448.80832918
Iteration :  98   Loss :  52.7260128988
Iteration :  99   Loss :  6.19425321558
[-0.03416809 -0.0176723  -0.0156601  ...,  0.01165847  0.01036112
  0.00293398]
CROSS VALIDATION 14
Iteration :  0   Loss :  32735.0697873
Iteration :  1   Loss :  11092.7954562
Iteration :  2   Loss :  5188.05791716
Iteration :  3   Loss :  5117.05796335
Iteration :  4   Loss :  25854.9180442
Iteration :  5   Loss :  17141.0153849
Iteration :  6   Loss :  22427.2236616
Iteration :  7   Loss :  15859.8720902
Iteration :  8   Loss :  5568.9854852
Iteration :  9   Loss :  11456.9714892
Iteration :  10   Loss :  25107.846219
Iteration :  11   Loss :  9360.89660512
Iteration :  12   Loss :  17422.7880203
Iteration :  13   Loss :  42592.6254726
Iteration :  14   Loss :  7599.64445499
Iteration :  15   Loss :  21532.6473053
Iteration :  16   Loss :  27145.2511546
Iteration :  17   Loss :  5887.24476936
Iteration :  18   Loss :  28497.6267827
Iteration :  19   Loss :  4670.56784146
Iteration :  20   Loss :  19830.4422368
Iteration :  21   Loss :  9340.32566167
Iteration :  22   Loss :  25345.4434324
Iteration :  23   Loss :  3805.72068066
Iteration :  24   Loss :  27320.2478766
Iteration :  25   Loss :  17546.3576905
Iteration :  26   Loss :  29775.5143569
Iteration :  27   Loss :  6108.50534142
Iteration :  28   Loss :  44743.9454158
Iteration :  29   Loss :  14967.7865132
Iteration :  30   Loss :  9469.94297214
Iteration :  31   Loss :  1574.01345421
Iteration :  32   Loss :  184.915137028
Iteration :  33   Loss :  21.7238345775
Iteration :  34   Loss :  2.55211658892
Iteration :  35   Loss :  0.604595952139
Iteration :  36   Loss :  45727.027718
Iteration :  37   Loss :  11242.4480378
Iteration :  38   Loss :  9654.2351051
Iteration :  39   Loss :  31118.0019758
Iteration :  40   Loss :  28376.4268395
Iteration :  41   Loss :  33328.8113724
Iteration :  42   Loss :  4722.34058923
Iteration :  43   Loss :  5406.3644508
Iteration :  44   Loss :  26197.8693589
Iteration :  45   Loss :  24726.1156119
Iteration :  46   Loss :  2154.71341326
Iteration :  47   Loss :  253.135781657
Iteration :  48   Loss :  29.7383974873
Iteration :  49   Loss :  3.49366762505
Iteration :  50   Loss :  0.410436153444
Iteration :  51   Loss :  0.0482180488053
Iteration :  52   Loss :  0.00566465748955
Iteration :  53   Loss :  0.990771624763
Iteration :  54   Loss :  32184.114307
Iteration :  55   Loss :  8402.40986739
Iteration :  56   Loss :  8322.60449626
Iteration :  57   Loss :  25602.0875867
Iteration :  58   Loss :  25709.1184764
Iteration :  59   Loss :  10519.8305442
Iteration :  60   Loss :  44359.8234499
Iteration :  61   Loss :  14515.5029701
Iteration :  62   Loss :  3603.85884299
Iteration :  63   Loss :  35427.1091876
Iteration :  64   Loss :  7371.44367934
Iteration :  65   Loss :  1108.90082608
Iteration :  66   Loss :  130.273694711
Iteration :  67   Loss :  15.3045566696
Iteration :  68   Loss :  1.79797967173
Iteration :  69   Loss :  0.211226693446
Iteration :  70   Loss :  0.0248149168344
Iteration :  71   Loss :  58497.7086875
Iteration :  72   Loss :  10087.8431889
Iteration :  73   Loss :  2316.61153996
Iteration :  74   Loss :  808.110641847
Iteration :  75   Loss :  94.9368569062
Iteration :  76   Loss :  11.1531841465
Iteration :  77   Loss :  1.31027633164
Iteration :  78   Loss :  0.153931293765
Iteration :  79   Loss :  0.0180838519539
Iteration :  80   Loss :  72213.9292275
Iteration :  81   Loss :  28574.6853465
Iteration :  82   Loss :  43671.1413495
Iteration :  83   Loss :  3874.0915051
Iteration :  84   Loss :  455.128359677
Iteration :  85   Loss :  53.4684902278
Iteration :  86   Loss :  6.28147946938
Iteration :  87   Loss :  0.737948353435
Iteration :  88   Loss :  0.0866941896398
Iteration :  89   Loss :  0.0101848354052
Iteration :  90   Loss :  110022.196047
Iteration :  91   Loss :  25635.7239307
Iteration :  92   Loss :  30541.56816
Iteration :  93   Loss :  43175.8087048
Iteration :  94   Loss :  15283.0384843
Iteration :  95   Loss :  18448.3547652
Iteration :  96   Loss :  20017.8686452
Iteration :  97   Loss :  6241.1035732
Iteration :  98   Loss :  733.204992217
Iteration :  99   Loss :  86.1369394541
[-0.03385525  0.07679917  0.0347123  ...,  0.03612774  0.03497231
  0.03904547]
CROSS VALIDATION 15
Iteration :  0   Loss :  32735.0697873
Iteration :  1   Loss :  14900.2319797
Iteration :  2   Loss :  10800.1331495
Iteration :  3   Loss :  24657.300742
Iteration :  4   Loss :  22060.8966967
Iteration :  5   Loss :  17656.9106758
Iteration :  6   Loss :  12064.227896
Iteration :  7   Loss :  42051.2711266
Iteration :  8   Loss :  11291.7241216
Iteration :  9   Loss :  15335.994091
Iteration :  10   Loss :  16913.5302582
Iteration :  11   Loss :  4553.09456815
Iteration :  12   Loss :  31348.5699613
Iteration :  13   Loss :  28830.5810283
Iteration :  14   Loss :  44555.1965924
Iteration :  15   Loss :  8883.59781044
Iteration :  16   Loss :  24222.7702414
Iteration :  17   Loss :  21654.9812227
Iteration :  18   Loss :  17426.2741324
Iteration :  19   Loss :  10265.6285227
Iteration :  20   Loss :  3778.98919163
Iteration :  21   Loss :  17814.9783289
Iteration :  22   Loss :  26479.3045494
Iteration :  23   Loss :  14860.417628
Iteration :  24   Loss :  34408.6719247
Iteration :  25   Loss :  24070.5054163
Iteration :  26   Loss :  21262.2889727
Iteration :  27   Loss :  24564.7266767
Iteration :  28   Loss :  33632.0051221
Iteration :  29   Loss :  2727.84730766
Iteration :  30   Loss :  13467.6630622
Iteration :  31   Loss :  38120.1187506
Iteration :  32   Loss :  4245.11484724
Iteration :  33   Loss :  29452.4807084
Iteration :  34   Loss :  25984.9641326
Iteration :  35   Loss :  21080.2131575
Iteration :  36   Loss :  18473.2123894
Iteration :  37   Loss :  37740.9815455
Iteration :  38   Loss :  36489.8671446
Iteration :  39   Loss :  28786.5414461
Iteration :  40   Loss :  9084.74940246
Iteration :  41   Loss :  35795.7515099
Iteration :  42   Loss :  36825.5312275
Iteration :  43   Loss :  2652.5993222
Iteration :  44   Loss :  14240.3284507
Iteration :  45   Loss :  58008.2798935
Iteration :  46   Loss :  7703.79509325
Iteration :  47   Loss :  8565.38394002
Iteration :  48   Loss :  2223.7351664
Iteration :  49   Loss :  25414.4028275
Iteration :  50   Loss :  27351.1811628
Iteration :  51   Loss :  8415.34592532
Iteration :  52   Loss :  9113.06282064
Iteration :  53   Loss :  13472.9334661
Iteration :  54   Loss :  27860.5985541
Iteration :  55   Loss :  14305.889379
Iteration :  56   Loss :  8346.20306176
Iteration :  57   Loss :  12797.9695538
Iteration :  58   Loss :  22135.0691832
Iteration :  59   Loss :  9564.71784228
Iteration :  60   Loss :  25152.5785282
Iteration :  61   Loss :  28636.6179545
Iteration :  62   Loss :  23613.9284303
Iteration :  63   Loss :  42843.3676774
Iteration :  64   Loss :  23485.9248438
Iteration :  65   Loss :  19629.5166986
Iteration :  66   Loss :  52089.7772311
Iteration :  67   Loss :  6873.64114911
Iteration :  68   Loss :  5334.23349137
Iteration :  69   Loss :  32418.2785118
Iteration :  70   Loss :  7056.9805007
Iteration :  71   Loss :  33865.5793287
Iteration :  72   Loss :  29599.9132472
Iteration :  73   Loss :  3341.46547738
Iteration :  74   Loss :  392.555441614
Iteration :  75   Loss :  46.1174223657
Iteration :  76   Loss :  5.41787584681
Iteration :  77   Loss :  0.636492179869
Iteration :  78   Loss :  58826.8993117
Iteration :  79   Loss :  17699.8067113
Iteration :  80   Loss :  18744.5197718
Iteration :  81   Loss :  15290.6483228
Iteration :  82   Loss :  24137.1957074
Iteration :  83   Loss :  15658.6519176
Iteration :  84   Loss :  5773.6391921
Iteration :  85   Loss :  35177.9952869
Iteration :  86   Loss :  29464.9207549
Iteration :  87   Loss :  11712.6092281
Iteration :  88   Loss :  43330.3582102
Iteration :  89   Loss :  28944.0693564
Iteration :  90   Loss :  19812.7614702
Iteration :  91   Loss :  13620.8834563
Iteration :  92   Loss :  34124.7324306
Iteration :  93   Loss :  3633.61287743
Iteration :  94   Loss :  18830.3297054
Iteration :  95   Loss :  41393.8845246
Iteration :  96   Loss :  23258.5493982
Iteration :  97   Loss :  39977.2439582
Iteration :  98   Loss :  6428.92743574
Iteration :  99   Loss :  3725.57142622
[-0.58069875 -0.00232891 -0.38640173 ...,  0.20806543  0.08175363
  0.11553493]
CROSS VALIDATION 16
Iteration :  0   Loss :  32735.0697873
Iteration :  1   Loss :  7332.02111418
Iteration :  2   Loss :  22282.9545754
Iteration :  3   Loss :  13026.663417
Iteration :  4   Loss :  22959.0582185
Iteration :  5   Loss :  15624.6912114
Iteration :  6   Loss :  14257.0141382
Iteration :  7   Loss :  24321.8098444
Iteration :  8   Loss :  14366.432415
Iteration :  9   Loss :  5119.48824255
Iteration :  10   Loss :  34813.1526983
Iteration :  11   Loss :  7106.15990035
Iteration :  12   Loss :  21478.3478798
Iteration :  13   Loss :  25561.6568954
Iteration :  14   Loss :  2809.55892481
Iteration :  15   Loss :  18473.3651544
Iteration :  16   Loss :  19030.1563383
Iteration :  17   Loss :  2590.25117643
Iteration :  18   Loss :  11936.7091299
Iteration :  19   Loss :  38174.8224301
Iteration :  20   Loss :  15415.9385525
Iteration :  21   Loss :  15611.1689818
Iteration :  22   Loss :  46059.2354077
Iteration :  23   Loss :  10682.295262
Iteration :  24   Loss :  16414.0063472
Iteration :  25   Loss :  25701.7600455
Iteration :  26   Loss :  3460.23124883
Iteration :  27   Loss :  11448.7183397
Iteration :  28   Loss :  8775.46478884
Iteration :  29   Loss :  35554.3231104
Iteration :  30   Loss :  14806.5380084
Iteration :  31   Loss :  7629.09512971
Iteration :  32   Loss :  27951.5000986
Iteration :  33   Loss :  22182.6811442
Iteration :  34   Loss :  10499.2069365
Iteration :  35   Loss :  62356.1656806
Iteration :  36   Loss :  21016.8035825
Iteration :  37   Loss :  37180.5197835
Iteration :  38   Loss :  7493.12348006
Iteration :  39   Loss :  694.456071768
Iteration :  40   Loss :  81.584715383
Iteration :  41   Loss :  9.58457425131
Iteration :  42   Loss :  1.12599600486
Iteration :  43   Loss :  0.13228203671
Iteration :  44   Loss :  0.0386955785908
Iteration :  45   Loss :  29317.7762608
Iteration :  46   Loss :  6685.86716944
Iteration :  47   Loss :  5502.70002823
Iteration :  48   Loss :  22929.9400741
Iteration :  49   Loss :  14786.0793283
Iteration :  50   Loss :  4557.32957136
Iteration :  51   Loss :  11667.8846186
Iteration :  52   Loss :  6256.01748219
Iteration :  53   Loss :  19942.945044
Iteration :  54   Loss :  26845.8666247
Iteration :  55   Loss :  23414.7659556
Iteration :  56   Loss :  28348.4244322
Iteration :  57   Loss :  9959.15051511
Iteration :  58   Loss :  31548.3921101
Iteration :  59   Loss :  11668.6577569
Iteration :  60   Loss :  16469.8460734
Iteration :  61   Loss :  22143.8666374
Iteration :  62   Loss :  15455.3984726
Iteration :  63   Loss :  2156.7648178
Iteration :  64   Loss :  253.376780709
Iteration :  65   Loss :  29.7667100615
Iteration :  66   Loss :  95834.7752634
Iteration :  67   Loss :  33679.1199871
Iteration :  68   Loss :  3975.79532784
Iteration :  69   Loss :  9169.98067715
Iteration :  70   Loss :  58485.7995523
Iteration :  71   Loss :  32873.7590974
Iteration :  72   Loss :  4526.91681246
Iteration :  73   Loss :  43804.0354123
Iteration :  74   Loss :  18661.8654015
Iteration :  75   Loss :  20289.4839313
Iteration :  76   Loss :  10712.383094
Iteration :  77   Loss :  40823.2848772
Iteration :  78   Loss :  17041.5292034
Iteration :  79   Loss :  11063.6311896
Iteration :  80   Loss :  31899.5957914
Iteration :  81   Loss :  11584.9098659
Iteration :  82   Loss :  16872.7747573
Iteration :  83   Loss :  25207.5887133
Iteration :  84   Loss :  10033.3344717
Iteration :  85   Loss :  6506.95338441
Iteration :  86   Loss :  10667.6313009
Iteration :  87   Loss :  43050.8654181
Iteration :  88   Loss :  5159.85535537
Iteration :  89   Loss :  21215.5564925
Iteration :  90   Loss :  5270.57111674
Iteration :  91   Loss :  80656.3059762
Iteration :  92   Loss :  20719.0950598
Iteration :  93   Loss :  9222.99070948
Iteration :  94   Loss :  37068.8609841
Iteration :  95   Loss :  2010.51775014
Iteration :  96   Loss :  236.195671815
Iteration :  97   Loss :  27.7482729911
Iteration :  98   Loss :  3.25986775317
Iteration :  99   Loss :  60589.6734742
[-1.66421556 -0.57852281  0.04077772 ...,  0.91530974 -0.32273402
  0.1151133 ]
CROSS VALIDATION 17
Iteration :  0   Loss :  32735.0697873
Iteration :  1   Loss :  15057.3375755
Iteration :  2   Loss :  7578.19664598
Iteration :  3   Loss :  24698.3459089
Iteration :  4   Loss :  22066.0278874
Iteration :  5   Loss :  17661.3062861
Iteration :  6   Loss :  9090.68353042
Iteration :  7   Loss :  16167.4303416
Iteration :  8   Loss :  29502.1751397
Iteration :  9   Loss :  5030.03229018
Iteration :  10   Loss :  590.92830986
Iteration :  11   Loss :  69.4222715181
Iteration :  12   Loss :  8.15573006458
Iteration :  13   Loss :  0.958135356735
Iteration :  14   Loss :  0.112561763883
Iteration :  15   Loss :  0.0775622885254
Iteration :  16   Loss :  44952.9402473
Iteration :  17   Loss :  22834.9420481
Iteration :  18   Loss :  11315.262668
Iteration :  19   Loss :  2855.29793513
Iteration :  20   Loss :  335.440467499
Iteration :  21   Loss :  39.4075538849
Iteration :  22   Loss :  4.62960034241
Iteration :  23   Loss :  0.543885555368
Iteration :  24   Loss :  0.0638956876318
Iteration :  25   Loss :  0.00750646686172
Iteration :  26   Loss :  59492.4628476
Iteration :  27   Loss :  26335.2635273
Iteration :  28   Loss :  26291.8489411
Iteration :  29   Loss :  9527.32885635
Iteration :  30   Loss :  21218.2574985
Iteration :  31   Loss :  7317.33145039
Iteration :  32   Loss :  19798.8795052
Iteration :  33   Loss :  8659.88064882
Iteration :  34   Loss :  6765.16225925
Iteration :  35   Loss :  10203.5726054
Iteration :  36   Loss :  26235.5752888
Iteration :  37   Loss :  10400.925577
Iteration :  38   Loss :  23214.2917763
Iteration :  39   Loss :  11416.9887517
Iteration :  40   Loss :  7627.47027004
Iteration :  41   Loss :  74651.3360016
Iteration :  42   Loss :  11026.8402542
Iteration :  43   Loss :  7911.40594385
Iteration :  44   Loss :  36002.6442163
Iteration :  45   Loss :  26412.1735356
Iteration :  46   Loss :  18063.6068155
Iteration :  47   Loss :  19396.6581759
Iteration :  48   Loss :  5189.21474346
Iteration :  49   Loss :  38789.7308206
Iteration :  50   Loss :  31024.4293418
Iteration :  51   Loss :  13837.4923944
Iteration :  52   Loss :  5370.52629453
Iteration :  53   Loss :  1448.29932443
Iteration :  54   Loss :  47669.9343074
Iteration :  55   Loss :  20822.2127631
Iteration :  56   Loss :  4997.30956246
Iteration :  57   Loss :  19315.3119056
Iteration :  58   Loss :  3588.81467694
Iteration :  59   Loss :  5182.40811863
Iteration :  60   Loss :  21398.3695195
Iteration :  61   Loss :  15416.833492
Iteration :  62   Loss :  60056.8043818
Iteration :  63   Loss :  6068.58499319
Iteration :  64   Loss :  12924.0031396
Iteration :  65   Loss :  37252.9135933
Iteration :  66   Loss :  3081.35498052
Iteration :  67   Loss :  20530.1495687
Iteration :  68   Loss :  22579.020933
Iteration :  69   Loss :  10125.3979475
Iteration :  70   Loss :  36838.5687064
Iteration :  71   Loss :  22999.2880715
Iteration :  72   Loss :  55434.4349642
Iteration :  73   Loss :  3938.98689124
Iteration :  74   Loss :  12666.2868797
Iteration :  75   Loss :  12835.8636165
Iteration :  76   Loss :  38106.4726912
Iteration :  77   Loss :  22491.7630336
Iteration :  78   Loss :  4538.42018045
Iteration :  79   Loss :  533.173707832
Iteration :  80   Loss :  62.6372595353
Iteration :  81   Loss :  7.35862669981
Iteration :  82   Loss :  0.864491634994
Iteration :  83   Loss :  0.101560497286
Iteration :  84   Loss :  0.0119313295716
Iteration :  85   Loss :  0.00140169287419
Iteration :  86   Loss :  47375.449383
Iteration :  87   Loss :  17904.2002664
Iteration :  88   Loss :  6511.91805621
Iteration :  89   Loss :  40850.738519
Iteration :  90   Loss :  18466.5326503
Iteration :  91   Loss :  7692.83852496
Iteration :  92   Loss :  27498.0567581
Iteration :  93   Loss :  30427.816154
Iteration :  94   Loss :  5078.32889455
Iteration :  95   Loss :  596.602196059
Iteration :  96   Loss :  70.0888398001
Iteration :  97   Loss :  8.23403852178
Iteration :  98   Loss :  0.967335036099
Iteration :  99   Loss :  0.113642542428
[-0.00562703  0.00025368 -0.0031997  ...,  0.00485656  0.00191977
  0.0002596 ]
CROSS VALIDATION 18
Iteration :  0   Loss :  28019.4150306
Iteration :  1   Loss :  13617.3755024
Iteration :  2   Loss :  884.242333064
Iteration :  3   Loss :  103.880809752
Iteration :  4   Loss :  12.2039199338
Iteration :  5   Loss :  1.43371679626
Iteration :  6   Loss :  0.168433082405
Iteration :  7   Loss :  0.019787522419
Iteration :  8   Loss :  0.224759470898
Iteration :  9   Loss :  18949.2985705
Iteration :  10   Loss :  25726.6610503
Iteration :  11   Loss :  5815.26015877
Iteration :  12   Loss :  3143.85299493
Iteration :  13   Loss :  40899.7031318
Iteration :  14   Loss :  16729.4485947
Iteration :  15   Loss :  15740.7362353
Iteration :  16   Loss :  12378.4690908
Iteration :  17   Loss :  13455.0634972
Iteration :  18   Loss :  41476.4139366
Iteration :  19   Loss :  8180.39845166
Iteration :  20   Loss :  5456.0296272
Iteration :  21   Loss :  5159.71103798
Iteration :  22   Loss :  29126.4054398
Iteration :  23   Loss :  34428.7007849
Iteration :  24   Loss :  14859.1663315
Iteration :  25   Loss :  7105.02606065
Iteration :  26   Loss :  25025.871907
Iteration :  27   Loss :  45264.6765391
Iteration :  28   Loss :  14158.4980374
Iteration :  29   Loss :  12451.7425338
Iteration :  30   Loss :  5755.05108405
Iteration :  31   Loss :  45893.3413972
Iteration :  32   Loss :  19726.5553526
Iteration :  33   Loss :  2090.62065147
Iteration :  34   Loss :  28413.3898205
Iteration :  35   Loss :  30468.569645
Iteration :  36   Loss :  23993.9534785
Iteration :  37   Loss :  2675.03089174
Iteration :  38   Loss :  314.26269107
Iteration :  39   Loss :  36.9195882199
Iteration :  40   Loss :  4.3373140785
Iteration :  41   Loss :  0.50954775832
Iteration :  42   Loss :  0.0598616824399
Iteration :  43   Loss :  0.132169098572
Iteration :  44   Loss :  78511.6470187
Iteration :  45   Loss :  11223.6749737
Iteration :  46   Loss :  6084.09196004
Iteration :  47   Loss :  27480.6878989
Iteration :  48   Loss :  30647.3063667
Iteration :  49   Loss :  2746.67417121
Iteration :  50   Loss :  322.6793452
Iteration :  51   Loss :  37.9083769419
Iteration :  52   Loss :  4.45347700044
Iteration :  53   Loss :  0.523194581079
Iteration :  54   Loss :  0.0614649114934
Iteration :  55   Loss :  0.00722089922472
Iteration :  56   Loss :  48026.8133503
Iteration :  57   Loss :  17449.9945452
Iteration :  58   Loss :  2181.68187508
Iteration :  59   Loss :  20996.3285042
Iteration :  60   Loss :  15085.2993426
Iteration :  61   Loss :  13996.2407296
Iteration :  62   Loss :  8139.13229684
Iteration :  63   Loss :  43299.8878476
Iteration :  64   Loss :  4345.50863032
Iteration :  65   Loss :  19138.3052035
Iteration :  66   Loss :  40280.2049041
Iteration :  67   Loss :  10528.3372255
Iteration :  68   Loss :  42467.9101253
Iteration :  69   Loss :  26140.9774427
Iteration :  70   Loss :  33601.6796889
Iteration :  71   Loss :  15450.0807982
Iteration :  72   Loss :  18902.6407304
Iteration :  73   Loss :  11514.0747102
Iteration :  74   Loss :  15127.3301626
Iteration :  75   Loss :  4472.79778824
Iteration :  76   Loss :  943.357339567
Iteration :  77   Loss :  50404.4897731
Iteration :  78   Loss :  59640.6099551
Iteration :  79   Loss :  43850.5268421
Iteration :  80   Loss :  2612.90981536
Iteration :  81   Loss :  15380.4018901
Iteration :  82   Loss :  38320.2102804
Iteration :  83   Loss :  19241.0889828
Iteration :  84   Loss :  16425.714288
Iteration :  85   Loss :  16202.3879618
Iteration :  86   Loss :  35099.1930574
Iteration :  87   Loss :  3507.7625685
Iteration :  88   Loss :  412.092027728
Iteration :  89   Loss :  48.4125809545
Iteration :  90   Loss :  5.68751113094
Iteration :  91   Loss :  0.668168939289
Iteration :  92   Loss :  0.078496502451
Iteration :  93   Loss :  0.00922177092459
Iteration :  94   Loss :  94084.341928
Iteration :  95   Loss :  35011.3901329
Iteration :  96   Loss :  11724.7650311
Iteration :  97   Loss :  6964.49897021
Iteration :  98   Loss :  20295.567568
Iteration :  99   Loss :  7188.38674686
[-0.3887692  -0.2443487  -0.28528225 ...,  0.78607815  0.01091601
 -0.01946124]
CROSS VALIDATION 19
Iteration :  0   Loss :  32051.5217848
Iteration :  1   Loss :  7677.0211266
Iteration :  2   Loss :  10832.5586964
Iteration :  3   Loss :  58095.2443168
Iteration :  4   Loss :  14918.0879262
Iteration :  5   Loss :  26171.9816655
Iteration :  6   Loss :  2737.67134169
Iteration :  7   Loss :  321.62169258
Iteration :  8   Loss :  37.7841238876
Iteration :  9   Loss :  4.43887974875
Iteration :  10   Loss :  0.521479695611
Iteration :  11   Loss :  0.061263446709
Iteration :  12   Loss :  0.00719723113719
Iteration :  13   Loss :  50328.1656999
Iteration :  14   Loss :  33326.4973099
Iteration :  15   Loss :  9433.38683309
Iteration :  16   Loss :  28023.5423474
Iteration :  17   Loss :  18783.0612641
Iteration :  18   Loss :  20803.5479352
Iteration :  19   Loss :  16330.87216
Iteration :  20   Loss :  18709.8478811
Iteration :  21   Loss :  15401.6523269
Iteration :  22   Loss :  29752.2648681
Iteration :  23   Loss :  13925.7845295
Iteration :  24   Loss :  3911.7793073
Iteration :  25   Loss :  19105.3730462
Iteration :  26   Loss :  23047.0162972
Iteration :  27   Loss :  5539.42707297
Iteration :  28   Loss :  46798.5415046
Iteration :  29   Loss :  13727.9661015
Iteration :  30   Loss :  338.581534934
Iteration :  31   Loss :  39.7765665598
Iteration :  32   Loss :  4.67295195999
Iteration :  33   Loss :  0.548978504406
Iteration :  34   Loss :  0.0644940074026
Iteration :  35   Loss :  0.00757675748223
Iteration :  36   Loss :  67383.060238
Iteration :  37   Loss :  13538.6307769
Iteration :  38   Loss :  13465.2067551
Iteration :  39   Loss :  23607.5505144
Iteration :  40   Loss :  17637.8169649
Iteration :  41   Loss :  27660.3412398
Iteration :  42   Loss :  18210.2150669
Iteration :  43   Loss :  24899.3372505
Iteration :  44   Loss :  6471.24607272
Iteration :  45   Loss :  29775.5922075
Iteration :  46   Loss :  54654.6995013
Iteration :  47   Loss :  13776.4175487
Iteration :  48   Loss :  26044.7839505
Iteration :  49   Loss :  3468.14013315
Iteration :  50   Loss :  407.437183105
Iteration :  51   Loss :  47.8657297005
Iteration :  52   Loss :  5.62326703296
Iteration :  53   Loss :  0.660621541172
Iteration :  54   Loss :  0.340927363428
Iteration :  55   Loss :  44782.8339726
Iteration :  56   Loss :  51301.3778304
Iteration :  57   Loss :  5130.37130544
Iteration :  58   Loss :  34489.4319874
Iteration :  59   Loss :  22103.2241908
Iteration :  60   Loss :  8307.61221702
Iteration :  61   Loss :  5903.58088649
Iteration :  62   Loss :  10760.5509438
Iteration :  63   Loss :  25970.2899207
Iteration :  64   Loss :  6096.0652714
Iteration :  65   Loss :  40556.0209918
Iteration :  66   Loss :  25491.9097106
Iteration :  67   Loss :  8987.61231102
Iteration :  68   Loss :  39347.6041848
Iteration :  69   Loss :  18909.9881989
Iteration :  70   Loss :  26093.9079763
Iteration :  71   Loss :  3222.55610823
Iteration :  72   Loss :  54211.1972207
Iteration :  73   Loss :  4225.53802304
Iteration :  74   Loss :  18763.9041501
Iteration :  75   Loss :  19077.8430857
Iteration :  76   Loss :  8309.24148323
Iteration :  77   Loss :  4184.2794785
Iteration :  78   Loss :  48405.839711
Iteration :  79   Loss :  19360.4846076
Iteration :  80   Loss :  15979.3356085
Iteration :  81   Loss :  8997.27572611
Iteration :  82   Loss :  42322.9782321
Iteration :  83   Loss :  4544.22329656
Iteration :  84   Loss :  25256.9517236
Iteration :  85   Loss :  23023.7705421
Iteration :  86   Loss :  9028.64903398
Iteration :  87   Loss :  16493.9156632
Iteration :  88   Loss :  12500.8135182
Iteration :  89   Loss :  7028.81330547
Iteration :  90   Loss :  29984.1596897
Iteration :  91   Loss :  22231.4451343
Iteration :  92   Loss :  4886.43094657
Iteration :  93   Loss :  34145.7757097
Iteration :  94   Loss :  2951.03864872
Iteration :  95   Loss :  16386.4482942
Iteration :  96   Loss :  34309.7263643
Iteration :  97   Loss :  619.943949168
Iteration :  98   Loss :  72.8310294956
Iteration :  99   Loss :  8.55619103066
[-0.0343012  -0.0216817   0.00803494 ..., -0.0194802   0.00070884
 -0.0021146 ]
Accuracy (Hinge Loss):	0.9
lmda : 0.5  eta : 0.1
Logistic Loss
CROSS VALIDATION 0
Iteration :  0   Loss :  55783.4517655
Iteration :  1   Loss :  51194.4514819
Iteration :  2   Loss :  46292.1939886
Iteration :  3   Loss :  55687.2950664
Iteration :  4   Loss :  51207.4463546
Iteration :  5   Loss :  52682.5220359
Iteration :  6   Loss :  49435.155869
Iteration :  7   Loss :  50967.5827472
Iteration :  8   Loss :  49962.8896143
Iteration :  9   Loss :  50967.0530822
Iteration :  10   Loss :  49962.8970671
Iteration :  11   Loss :  50967.0530748
Iteration :  12   Loss :  49962.8970672
Iteration :  13   Loss :  50967.0530748
Iteration :  14   Loss :  49962.8970672
Iteration :  15   Loss :  50967.0530748
Iteration :  16   Loss :  49962.8970672
Iteration :  17   Loss :  50967.0530748
Iteration :  18   Loss :  49962.8970672
Iteration :  19   Loss :  50967.0530748
Iteration :  20   Loss :  49962.8970672
Iteration :  21   Loss :  50967.0530748
Iteration :  22   Loss :  49962.8970672
Iteration :  23   Loss :  50967.0530748
Iteration :  24   Loss :  49962.8970672
Iteration :  25   Loss :  50967.0530748
Iteration :  26   Loss :  49962.8970672
Iteration :  27   Loss :  50967.0530748
Iteration :  28   Loss :  49962.8970672
Iteration :  29   Loss :  50967.0530748
Iteration :  30   Loss :  49962.8970672
Iteration :  31   Loss :  50967.0530748
Iteration :  32   Loss :  49962.8970672
Iteration :  33   Loss :  50967.0530748
Iteration :  34   Loss :  49962.8970672
Iteration :  35   Loss :  50967.0530748
Iteration :  36   Loss :  49962.8970672
Iteration :  37   Loss :  50967.0530748
Iteration :  38   Loss :  49962.8970672
Iteration :  39   Loss :  50967.0530748
Iteration :  40   Loss :  49962.8970672
Iteration :  41   Loss :  50967.0530748
Iteration :  42   Loss :  49962.8970672
Iteration :  43   Loss :  50967.0530748
Iteration :  44   Loss :  49962.8970672
Iteration :  45   Loss :  50967.0530748
Iteration :  46   Loss :  49962.8970672
Iteration :  47   Loss :  50967.0530748
Iteration :  48   Loss :  49962.8970672
Iteration :  49   Loss :  50967.0530748
Iteration :  50   Loss :  49962.8970672
Iteration :  51   Loss :  50967.0530748
Iteration :  52   Loss :  49962.8970672
Iteration :  53   Loss :  50967.0530748
Iteration :  54   Loss :  49962.8970672
Iteration :  55   Loss :  50967.0530748
Iteration :  56   Loss :  49962.8970672
Iteration :  57   Loss :  50967.0530748
Iteration :  58   Loss :  49962.8970672
Iteration :  59   Loss :  50967.0530748
Iteration :  60   Loss :  49962.8970672
Iteration :  61   Loss :  50967.0530748
Iteration :  62   Loss :  49962.8970672
Iteration :  63   Loss :  50967.0530748
Iteration :  64   Loss :  49962.8970672
Iteration :  65   Loss :  50967.0530748
Iteration :  66   Loss :  49962.8970672
Iteration :  67   Loss :  50967.0530748
Iteration :  68   Loss :  49962.8970672
Iteration :  69   Loss :  50967.0530748
Iteration :  70   Loss :  49962.8970672
Iteration :  71   Loss :  50967.0530748
Iteration :  72   Loss :  49962.8970672
Iteration :  73   Loss :  50967.0530748
Iteration :  74   Loss :  49962.8970672
Iteration :  75   Loss :  50967.0530748
Iteration :  76   Loss :  49962.8970672
Iteration :  77   Loss :  50967.0530748
Iteration :  78   Loss :  49962.8970672
Iteration :  79   Loss :  50967.0530748
Iteration :  80   Loss :  49962.8970672
Iteration :  81   Loss :  50967.0530748
Iteration :  82   Loss :  49962.8970672
Iteration :  83   Loss :  50967.0530748
Iteration :  84   Loss :  49962.8970672
Iteration :  85   Loss :  50967.0530748
Iteration :  86   Loss :  49962.8970672
Iteration :  87   Loss :  50967.0530748
Iteration :  88   Loss :  49962.8970672
Iteration :  89   Loss :  50967.0530748
Iteration :  90   Loss :  49962.8970672
Iteration :  91   Loss :  50967.0530748
Iteration :  92   Loss :  49962.8970672
Iteration :  93   Loss :  50967.0530748
Iteration :  94   Loss :  49962.8970672
Iteration :  95   Loss :  50967.0530748
Iteration :  96   Loss :  49962.8970672
Iteration :  97   Loss :  50967.0530748
Iteration :  98   Loss :  49962.8970672
Iteration :  99   Loss :  50967.0530748
[-0.30336147 -0.59355926 -0.32891075 ...,  0.42249456 -0.32302489
 -0.07516166]
CROSS VALIDATION 1
Iteration :  0   Loss :  52562.3541852
Iteration :  1   Loss :  56987.7014037
Iteration :  2   Loss :  47480.8536652
Iteration :  3   Loss :  43120.164468
Iteration :  4   Loss :  52633.3612502
Iteration :  5   Loss :  64303.4523965
Iteration :  6   Loss :  43114.5001786
Iteration :  7   Loss :  52633.4685437
Iteration :  8   Loss :  64302.0069437
Iteration :  9   Loss :  43114.4993333
Iteration :  10   Loss :  52633.4685263
Iteration :  11   Loss :  64302.0079362
Iteration :  12   Loss :  43114.4993339
Iteration :  13   Loss :  52633.4685263
Iteration :  14   Loss :  64302.0079355
Iteration :  15   Loss :  43114.4993339
Iteration :  16   Loss :  52633.4685263
Iteration :  17   Loss :  64302.0079355
Iteration :  18   Loss :  43114.4993339
Iteration :  19   Loss :  52633.4685263
Iteration :  20   Loss :  64302.0079355
Iteration :  21   Loss :  43114.4993339
Iteration :  22   Loss :  52633.4685263
Iteration :  23   Loss :  64302.0079355
Iteration :  24   Loss :  43114.4993339
Iteration :  25   Loss :  52633.4685263
Iteration :  26   Loss :  64302.0079355
Iteration :  27   Loss :  43114.4993339
Iteration :  28   Loss :  52633.4685263
Iteration :  29   Loss :  64302.0079355
Iteration :  30   Loss :  43114.4993339
Iteration :  31   Loss :  52633.4685263
Iteration :  32   Loss :  64302.0079355
Iteration :  33   Loss :  43114.4993339
Iteration :  34   Loss :  52633.4685263
Iteration :  35   Loss :  64302.0079355
Iteration :  36   Loss :  43114.4993339
Iteration :  37   Loss :  52633.4685263
Iteration :  38   Loss :  64302.0079355
Iteration :  39   Loss :  43114.4993339
Iteration :  40   Loss :  52633.4685263
Iteration :  41   Loss :  64302.0079355
Iteration :  42   Loss :  43114.4993339
Iteration :  43   Loss :  52633.4685263
Iteration :  44   Loss :  64302.0079355
Iteration :  45   Loss :  43114.4993339
Iteration :  46   Loss :  52633.4685263
Iteration :  47   Loss :  64302.0079355
Iteration :  48   Loss :  43114.4993339
Iteration :  49   Loss :  52633.4685263
Iteration :  50   Loss :  64302.0079355
Iteration :  51   Loss :  43114.4993339
Iteration :  52   Loss :  52633.4685263
Iteration :  53   Loss :  64302.0079355
Iteration :  54   Loss :  43114.4993339
Iteration :  55   Loss :  52633.4685263
Iteration :  56   Loss :  64302.0079355
Iteration :  57   Loss :  43114.4993339
Iteration :  58   Loss :  52633.4685263
Iteration :  59   Loss :  64302.0079355
Iteration :  60   Loss :  43114.4993339
Iteration :  61   Loss :  52633.4685263
Iteration :  62   Loss :  64302.0079355
Iteration :  63   Loss :  43114.4993339
Iteration :  64   Loss :  52633.4685263
Iteration :  65   Loss :  64302.0079355
Iteration :  66   Loss :  43114.4993339
Iteration :  67   Loss :  52633.4685263
Iteration :  68   Loss :  64302.0079355
Iteration :  69   Loss :  43114.4993339
Iteration :  70   Loss :  52633.4685263
Iteration :  71   Loss :  64302.0079355
Iteration :  72   Loss :  43114.4993339
Iteration :  73   Loss :  52633.4685263
Iteration :  74   Loss :  64302.0079355
Iteration :  75   Loss :  43114.4993339
Iteration :  76   Loss :  52633.4685263
Iteration :  77   Loss :  64302.0079355
Iteration :  78   Loss :  43114.4993339
Iteration :  79   Loss :  52633.4685263
Iteration :  80   Loss :  64302.0079355
Iteration :  81   Loss :  43114.4993339
Iteration :  82   Loss :  52633.4685263
Iteration :  83   Loss :  64302.0079355
Iteration :  84   Loss :  43114.4993339
Iteration :  85   Loss :  52633.4685263
Iteration :  86   Loss :  64302.0079355
Iteration :  87   Loss :  43114.4993339
Iteration :  88   Loss :  52633.4685263
Iteration :  89   Loss :  64302.0079355
Iteration :  90   Loss :  43114.4993339
Iteration :  91   Loss :  52633.4685263
Iteration :  92   Loss :  64302.0079355
Iteration :  93   Loss :  43114.4993339
Iteration :  94   Loss :  52633.4685263
Iteration :  95   Loss :  64302.0079355
Iteration :  96   Loss :  43114.4993339
Iteration :  97   Loss :  52633.4685263
Iteration :  98   Loss :  64302.0079355
Iteration :  99   Loss :  43114.4993339
[-0.10053969 -0.67751942  0.06327097 ...,  0.28773538 -0.30524427
 -0.06539631]
CROSS VALIDATION 2
Iteration :  0   Loss :  48053.5350062
Iteration :  1   Loss :  43097.7919187
Iteration :  2   Loss :  52633.4241397
Iteration :  3   Loss :  64303.2406612
Iteration :  4   Loss :  43114.5000528
Iteration :  5   Loss :  52633.4685411
Iteration :  6   Loss :  64302.00709
Iteration :  7   Loss :  43114.4993334
Iteration :  8   Loss :  52633.4685263
Iteration :  9   Loss :  64302.0079361
Iteration :  10   Loss :  43114.4993339
Iteration :  11   Loss :  52633.4685263
Iteration :  12   Loss :  64302.0079355
Iteration :  13   Loss :  43114.4993339
Iteration :  14   Loss :  52633.4685263
Iteration :  15   Loss :  64302.0079355
Iteration :  16   Loss :  43114.4993339
Iteration :  17   Loss :  52633.4685263
Iteration :  18   Loss :  64302.0079355
Iteration :  19   Loss :  43114.4993339
Iteration :  20   Loss :  52633.4685263
Iteration :  21   Loss :  64302.0079355
Iteration :  22   Loss :  43114.4993339
Iteration :  23   Loss :  52633.4685263
Iteration :  24   Loss :  64302.0079355
Iteration :  25   Loss :  43114.4993339
Iteration :  26   Loss :  52633.4685263
Iteration :  27   Loss :  64302.0079355
Iteration :  28   Loss :  43114.4993339
Iteration :  29   Loss :  52633.4685263
Iteration :  30   Loss :  64302.0079355
Iteration :  31   Loss :  43114.4993339
Iteration :  32   Loss :  52633.4685263
Iteration :  33   Loss :  64302.0079355
Iteration :  34   Loss :  43114.4993339
Iteration :  35   Loss :  52633.4685263
Iteration :  36   Loss :  64302.0079355
Iteration :  37   Loss :  43114.4993339
Iteration :  38   Loss :  52633.4685263
Iteration :  39   Loss :  64302.0079355
Iteration :  40   Loss :  43114.4993339
Iteration :  41   Loss :  52633.4685263
Iteration :  42   Loss :  64302.0079355
Iteration :  43   Loss :  43114.4993339
Iteration :  44   Loss :  52633.4685263
Iteration :  45   Loss :  64302.0079355
Iteration :  46   Loss :  43114.4993339
Iteration :  47   Loss :  52633.4685263
Iteration :  48   Loss :  64302.0079355
Iteration :  49   Loss :  43114.4993339
Iteration :  50   Loss :  52633.4685263
Iteration :  51   Loss :  64302.0079355
Iteration :  52   Loss :  43114.4993339
Iteration :  53   Loss :  52633.4685263
Iteration :  54   Loss :  64302.0079355
Iteration :  55   Loss :  43114.4993339
Iteration :  56   Loss :  52633.4685263
Iteration :  57   Loss :  64302.0079355
Iteration :  58   Loss :  43114.4993339
Iteration :  59   Loss :  52633.4685263
Iteration :  60   Loss :  64302.0079355
Iteration :  61   Loss :  43114.4993339
Iteration :  62   Loss :  52633.4685263
Iteration :  63   Loss :  64302.0079355
Iteration :  64   Loss :  43114.4993339
Iteration :  65   Loss :  52633.4685263
Iteration :  66   Loss :  64302.0079355
Iteration :  67   Loss :  43114.4993339
Iteration :  68   Loss :  52633.4685263
Iteration :  69   Loss :  64302.0079355
Iteration :  70   Loss :  43114.4993339
Iteration :  71   Loss :  52633.4685263
Iteration :  72   Loss :  64302.0079355
Iteration :  73   Loss :  43114.4993339
Iteration :  74   Loss :  52633.4685263
Iteration :  75   Loss :  64302.0079355
Iteration :  76   Loss :  43114.4993339
Iteration :  77   Loss :  52633.4685263
Iteration :  78   Loss :  64302.0079355
Iteration :  79   Loss :  43114.4993339
Iteration :  80   Loss :  52633.4685263
Iteration :  81   Loss :  64302.0079355
Iteration :  82   Loss :  43114.4993339
Iteration :  83   Loss :  52633.4685263
Iteration :  84   Loss :  64302.0079355
Iteration :  85   Loss :  43114.4993339
Iteration :  86   Loss :  52633.4685263
Iteration :  87   Loss :  64302.0079355
Iteration :  88   Loss :  43114.4993339
Iteration :  89   Loss :  52633.4685263
Iteration :  90   Loss :  64302.0079355
Iteration :  91   Loss :  43114.4993339
Iteration :  92   Loss :  52633.4685263
Iteration :  93   Loss :  64302.0079355
Iteration :  94   Loss :  43114.4993339
Iteration :  95   Loss :  52633.4685263
Iteration :  96   Loss :  64302.0079355
Iteration :  97   Loss :  43114.4993339
Iteration :  98   Loss :  52633.4685263
Iteration :  99   Loss :  64302.0079355
[-0.13484622 -0.70881721 -0.00832721 ...,  0.31762772 -0.36567506
 -0.08223868]
CROSS VALIDATION 3
Iteration :  0   Loss :  52601.1884012
Iteration :  1   Loss :  56987.6328559
Iteration :  2   Loss :  47480.8539955
Iteration :  3   Loss :  43120.1644692
Iteration :  4   Loss :  52633.3612502
Iteration :  5   Loss :  64303.4523965
Iteration :  6   Loss :  43114.5001786
Iteration :  7   Loss :  52633.4685437
Iteration :  8   Loss :  64302.0069437
Iteration :  9   Loss :  43114.4993333
Iteration :  10   Loss :  52633.4685263
Iteration :  11   Loss :  64302.0079362
Iteration :  12   Loss :  43114.4993339
Iteration :  13   Loss :  52633.4685263
Iteration :  14   Loss :  64302.0079355
Iteration :  15   Loss :  43114.4993339
Iteration :  16   Loss :  52633.4685263
Iteration :  17   Loss :  64302.0079355
Iteration :  18   Loss :  43114.4993339
Iteration :  19   Loss :  52633.4685263
Iteration :  20   Loss :  64302.0079355
Iteration :  21   Loss :  43114.4993339
Iteration :  22   Loss :  52633.4685263
Iteration :  23   Loss :  64302.0079355
Iteration :  24   Loss :  43114.4993339
Iteration :  25   Loss :  52633.4685263
Iteration :  26   Loss :  64302.0079355
Iteration :  27   Loss :  43114.4993339
Iteration :  28   Loss :  52633.4685263
Iteration :  29   Loss :  64302.0079355
Iteration :  30   Loss :  43114.4993339
Iteration :  31   Loss :  52633.4685263
Iteration :  32   Loss :  64302.0079355
Iteration :  33   Loss :  43114.4993339
Iteration :  34   Loss :  52633.4685263
Iteration :  35   Loss :  64302.0079355
Iteration :  36   Loss :  43114.4993339
Iteration :  37   Loss :  52633.4685263
Iteration :  38   Loss :  64302.0079355
Iteration :  39   Loss :  43114.4993339
Iteration :  40   Loss :  52633.4685263
Iteration :  41   Loss :  64302.0079355
Iteration :  42   Loss :  43114.4993339
Iteration :  43   Loss :  52633.4685263
Iteration :  44   Loss :  64302.0079355
Iteration :  45   Loss :  43114.4993339
Iteration :  46   Loss :  52633.4685263
Iteration :  47   Loss :  64302.0079355
Iteration :  48   Loss :  43114.4993339
Iteration :  49   Loss :  52633.4685263
Iteration :  50   Loss :  64302.0079355
Iteration :  51   Loss :  43114.4993339
Iteration :  52   Loss :  52633.4685263
Iteration :  53   Loss :  64302.0079355
Iteration :  54   Loss :  43114.4993339
Iteration :  55   Loss :  52633.4685263
Iteration :  56   Loss :  64302.0079355
Iteration :  57   Loss :  43114.4993339
Iteration :  58   Loss :  52633.4685263
Iteration :  59   Loss :  64302.0079355
Iteration :  60   Loss :  43114.4993339
Iteration :  61   Loss :  52633.4685263
Iteration :  62   Loss :  64302.0079355
Iteration :  63   Loss :  43114.4993339
Iteration :  64   Loss :  52633.4685263
Iteration :  65   Loss :  64302.0079355
Iteration :  66   Loss :  43114.4993339
Iteration :  67   Loss :  52633.4685263
Iteration :  68   Loss :  64302.0079355
Iteration :  69   Loss :  43114.4993339
Iteration :  70   Loss :  52633.4685263
Iteration :  71   Loss :  64302.0079355
Iteration :  72   Loss :  43114.4993339
Iteration :  73   Loss :  52633.4685263
Iteration :  74   Loss :  64302.0079355
Iteration :  75   Loss :  43114.4993339
Iteration :  76   Loss :  52633.4685263
Iteration :  77   Loss :  64302.0079355
Iteration :  78   Loss :  43114.4993339
Iteration :  79   Loss :  52633.4685263
Iteration :  80   Loss :  64302.0079355
Iteration :  81   Loss :  43114.4993339
Iteration :  82   Loss :  52633.4685263
Iteration :  83   Loss :  64302.0079355
Iteration :  84   Loss :  43114.4993339
Iteration :  85   Loss :  52633.4685263
Iteration :  86   Loss :  64302.0079355
Iteration :  87   Loss :  43114.4993339
Iteration :  88   Loss :  52633.4685263
Iteration :  89   Loss :  64302.0079355
Iteration :  90   Loss :  43114.4993339
Iteration :  91   Loss :  52633.4685263
Iteration :  92   Loss :  64302.0079355
Iteration :  93   Loss :  43114.4993339
Iteration :  94   Loss :  52633.4685263
Iteration :  95   Loss :  64302.0079355
Iteration :  96   Loss :  43114.4993339
Iteration :  97   Loss :  52633.4685263
Iteration :  98   Loss :  64302.0079355
Iteration :  99   Loss :  43114.4993339
[-0.10053969 -0.67751942  0.06327097 ...,  0.28773538 -0.30524427
 -0.06539631]
CROSS VALIDATION 4
Iteration :  0   Loss :  52601.1884012
Iteration :  1   Loss :  56987.6328559
Iteration :  2   Loss :  47480.8539955
Iteration :  3   Loss :  43120.1644692
Iteration :  4   Loss :  52633.3612502
Iteration :  5   Loss :  64303.4523965
Iteration :  6   Loss :  43114.5001786
Iteration :  7   Loss :  52633.4685437
Iteration :  8   Loss :  64302.0069437
Iteration :  9   Loss :  43114.4993333
Iteration :  10   Loss :  52633.4685263
Iteration :  11   Loss :  64302.0079362
Iteration :  12   Loss :  43114.4993339
Iteration :  13   Loss :  52633.4685263
Iteration :  14   Loss :  64302.0079355
Iteration :  15   Loss :  43114.4993339
Iteration :  16   Loss :  52633.4685263
Iteration :  17   Loss :  64302.0079355
Iteration :  18   Loss :  43114.4993339
Iteration :  19   Loss :  52633.4685263
Iteration :  20   Loss :  64302.0079355
Iteration :  21   Loss :  43114.4993339
Iteration :  22   Loss :  52633.4685263
Iteration :  23   Loss :  64302.0079355
Iteration :  24   Loss :  43114.4993339
Iteration :  25   Loss :  52633.4685263
Iteration :  26   Loss :  64302.0079355
Iteration :  27   Loss :  43114.4993339
Iteration :  28   Loss :  52633.4685263
Iteration :  29   Loss :  64302.0079355
Iteration :  30   Loss :  43114.4993339
Iteration :  31   Loss :  52633.4685263
Iteration :  32   Loss :  64302.0079355
Iteration :  33   Loss :  43114.4993339
Iteration :  34   Loss :  52633.4685263
Iteration :  35   Loss :  64302.0079355
Iteration :  36   Loss :  43114.4993339
Iteration :  37   Loss :  52633.4685263
Iteration :  38   Loss :  64302.0079355
Iteration :  39   Loss :  43114.4993339
Iteration :  40   Loss :  52633.4685263
Iteration :  41   Loss :  64302.0079355
Iteration :  42   Loss :  43114.4993339
Iteration :  43   Loss :  52633.4685263
Iteration :  44   Loss :  64302.0079355
Iteration :  45   Loss :  43114.4993339
Iteration :  46   Loss :  52633.4685263
Iteration :  47   Loss :  64302.0079355
Iteration :  48   Loss :  43114.4993339
Iteration :  49   Loss :  52633.4685263
Iteration :  50   Loss :  64302.0079355
Iteration :  51   Loss :  43114.4993339
Iteration :  52   Loss :  52633.4685263
Iteration :  53   Loss :  64302.0079355
Iteration :  54   Loss :  43114.4993339
Iteration :  55   Loss :  52633.4685263
Iteration :  56   Loss :  64302.0079355
Iteration :  57   Loss :  43114.4993339
Iteration :  58   Loss :  52633.4685263
Iteration :  59   Loss :  64302.0079355
Iteration :  60   Loss :  43114.4993339
Iteration :  61   Loss :  52633.4685263
Iteration :  62   Loss :  64302.0079355
Iteration :  63   Loss :  43114.4993339
Iteration :  64   Loss :  52633.4685263
Iteration :  65   Loss :  64302.0079355
Iteration :  66   Loss :  43114.4993339
Iteration :  67   Loss :  52633.4685263
Iteration :  68   Loss :  64302.0079355
Iteration :  69   Loss :  43114.4993339
Iteration :  70   Loss :  52633.4685263
Iteration :  71   Loss :  64302.0079355
Iteration :  72   Loss :  43114.4993339
Iteration :  73   Loss :  52633.4685263
Iteration :  74   Loss :  64302.0079355
Iteration :  75   Loss :  43114.4993339
Iteration :  76   Loss :  52633.4685263
Iteration :  77   Loss :  64302.0079355
Iteration :  78   Loss :  43114.4993339
Iteration :  79   Loss :  52633.4685263
Iteration :  80   Loss :  64302.0079355
Iteration :  81   Loss :  43114.4993339
Iteration :  82   Loss :  52633.4685263
Iteration :  83   Loss :  64302.0079355
Iteration :  84   Loss :  43114.4993339
Iteration :  85   Loss :  52633.4685263
Iteration :  86   Loss :  64302.0079355
Iteration :  87   Loss :  43114.4993339
Iteration :  88   Loss :  52633.4685263
Iteration :  89   Loss :  64302.0079355
Iteration :  90   Loss :  43114.4993339
Iteration :  91   Loss :  52633.4685263
Iteration :  92   Loss :  64302.0079355
Iteration :  93   Loss :  43114.4993339
Iteration :  94   Loss :  52633.4685263
Iteration :  95   Loss :  64302.0079355
Iteration :  96   Loss :  43114.4993339
Iteration :  97   Loss :  52633.4685263
Iteration :  98   Loss :  64302.0079355
Iteration :  99   Loss :  43114.4993339
[-0.10053969 -0.67751942  0.06327097 ...,  0.28773538 -0.30524427
 -0.06539631]
CROSS VALIDATION 5
Iteration :  0   Loss :  51948.1858923
Iteration :  1   Loss :  42877.9016533
Iteration :  2   Loss :  48127.6497243
Iteration :  3   Loss :  51341.7490102
Iteration :  4   Loss :  42876.3597048
Iteration :  5   Loss :  48127.624638
Iteration :  6   Loss :  51341.7489781
Iteration :  7   Loss :  42876.3597047
Iteration :  8   Loss :  48127.624638
Iteration :  9   Loss :  51341.7489781
Iteration :  10   Loss :  42876.3597047
Iteration :  11   Loss :  48127.624638
Iteration :  12   Loss :  51341.7489781
Iteration :  13   Loss :  42876.3597047
Iteration :  14   Loss :  48127.624638
Iteration :  15   Loss :  51341.7489781
Iteration :  16   Loss :  42876.3597047
Iteration :  17   Loss :  48127.624638
Iteration :  18   Loss :  51341.7489781
Iteration :  19   Loss :  42876.3597047
Iteration :  20   Loss :  48127.624638
Iteration :  21   Loss :  51341.7489781
Iteration :  22   Loss :  42876.3597047
Iteration :  23   Loss :  48127.624638
Iteration :  24   Loss :  51341.7489781
Iteration :  25   Loss :  42876.3597047
Iteration :  26   Loss :  48127.624638
Iteration :  27   Loss :  51341.7489781
Iteration :  28   Loss :  42876.3597047
Iteration :  29   Loss :  48127.624638
Iteration :  30   Loss :  51341.7489781
Iteration :  31   Loss :  42876.3597047
Iteration :  32   Loss :  48127.624638
Iteration :  33   Loss :  51341.7489781
Iteration :  34   Loss :  42876.3597047
Iteration :  35   Loss :  48127.624638
Iteration :  36   Loss :  51341.7489781
Iteration :  37   Loss :  42876.3597047
Iteration :  38   Loss :  48127.624638
Iteration :  39   Loss :  51341.7489781
Iteration :  40   Loss :  42876.3597047
Iteration :  41   Loss :  48127.624638
Iteration :  42   Loss :  51341.7489781
Iteration :  43   Loss :  42876.3597047
Iteration :  44   Loss :  48127.624638
Iteration :  45   Loss :  51341.7489781
Iteration :  46   Loss :  42876.3597047
Iteration :  47   Loss :  48127.624638
Iteration :  48   Loss :  51341.7489781
Iteration :  49   Loss :  42876.3597047
Iteration :  50   Loss :  48127.624638
Iteration :  51   Loss :  51341.7489781
Iteration :  52   Loss :  42876.3597047
Iteration :  53   Loss :  48127.624638
Iteration :  54   Loss :  51341.7489781
Iteration :  55   Loss :  42876.3597047
Iteration :  56   Loss :  48127.624638
Iteration :  57   Loss :  51341.7489781
Iteration :  58   Loss :  42876.3597047
Iteration :  59   Loss :  48127.624638
Iteration :  60   Loss :  51341.7489781
Iteration :  61   Loss :  42876.3597047
Iteration :  62   Loss :  48127.624638
Iteration :  63   Loss :  51341.7489781
Iteration :  64   Loss :  42876.3597047
Iteration :  65   Loss :  48127.624638
Iteration :  66   Loss :  51341.7489781
Iteration :  67   Loss :  42876.3597047
Iteration :  68   Loss :  48127.624638
Iteration :  69   Loss :  51341.7489781
Iteration :  70   Loss :  42876.3597047
Iteration :  71   Loss :  48127.624638
Iteration :  72   Loss :  51341.7489781
Iteration :  73   Loss :  42876.3597047
Iteration :  74   Loss :  48127.624638
Iteration :  75   Loss :  51341.7489781
Iteration :  76   Loss :  42876.3597047
Iteration :  77   Loss :  48127.624638
Iteration :  78   Loss :  51341.7489781
Iteration :  79   Loss :  42876.3597047
Iteration :  80   Loss :  48127.624638
Iteration :  81   Loss :  51341.7489781
Iteration :  82   Loss :  42876.3597047
Iteration :  83   Loss :  48127.624638
Iteration :  84   Loss :  51341.7489781
Iteration :  85   Loss :  42876.3597047
Iteration :  86   Loss :  48127.624638
Iteration :  87   Loss :  51341.7489781
Iteration :  88   Loss :  42876.3597047
Iteration :  89   Loss :  48127.624638
Iteration :  90   Loss :  51341.7489781
Iteration :  91   Loss :  42876.3597047
Iteration :  92   Loss :  48127.624638
Iteration :  93   Loss :  51341.7489781
Iteration :  94   Loss :  42876.3597047
Iteration :  95   Loss :  48127.624638
Iteration :  96   Loss :  51341.7489781
Iteration :  97   Loss :  42876.3597047
Iteration :  98   Loss :  48127.624638
Iteration :  99   Loss :  51341.7489781
[-0.2973228  -0.59251438 -0.32961844 ...,  0.41907904 -0.32830652
 -0.07344772]
CROSS VALIDATION 6
Iteration :  0   Loss :  52601.1884012
Iteration :  1   Loss :  54963.7240391
Iteration :  2   Loss :  50889.7869142
Iteration :  3   Loss :  49962.0141987
Iteration :  4   Loss :  50971.5458923
Iteration :  5   Loss :  49962.463869
Iteration :  6   Loss :  50971.5470455
Iteration :  7   Loss :  49962.4638753
Iteration :  8   Loss :  50971.5470455
Iteration :  9   Loss :  49962.4638753
Iteration :  10   Loss :  50971.5470455
Iteration :  11   Loss :  49962.4638753
Iteration :  12   Loss :  50971.5470455
Iteration :  13   Loss :  49962.4638753
Iteration :  14   Loss :  50971.5470455
Iteration :  15   Loss :  49962.4638753
Iteration :  16   Loss :  50971.5470455
Iteration :  17   Loss :  49962.4638753
Iteration :  18   Loss :  50971.5470455
Iteration :  19   Loss :  49962.4638753
Iteration :  20   Loss :  50971.5470455
Iteration :  21   Loss :  49962.4638753
Iteration :  22   Loss :  50971.5470455
Iteration :  23   Loss :  49962.4638753
Iteration :  24   Loss :  50971.5470455
Iteration :  25   Loss :  49962.4638753
Iteration :  26   Loss :  50971.5470455
Iteration :  27   Loss :  49962.4638753
Iteration :  28   Loss :  50971.5470455
Iteration :  29   Loss :  49962.4638753
Iteration :  30   Loss :  50971.5470455
Iteration :  31   Loss :  49962.4638753
Iteration :  32   Loss :  50971.5470455
Iteration :  33   Loss :  49962.4638753
Iteration :  34   Loss :  50971.5470455
Iteration :  35   Loss :  49962.4638753
Iteration :  36   Loss :  50971.5470455
Iteration :  37   Loss :  49962.4638753
Iteration :  38   Loss :  50971.5470455
Iteration :  39   Loss :  49962.4638753
Iteration :  40   Loss :  50971.5470455
Iteration :  41   Loss :  49962.4638753
Iteration :  42   Loss :  50971.5470455
Iteration :  43   Loss :  49962.4638753
Iteration :  44   Loss :  50971.5470455
Iteration :  45   Loss :  49962.4638753
Iteration :  46   Loss :  50971.5470455
Iteration :  47   Loss :  49962.4638753
Iteration :  48   Loss :  50971.5470455
Iteration :  49   Loss :  49962.4638753
Iteration :  50   Loss :  50971.5470455
Iteration :  51   Loss :  49962.4638753
Iteration :  52   Loss :  50971.5470455
Iteration :  53   Loss :  49962.4638753
Iteration :  54   Loss :  50971.5470455
Iteration :  55   Loss :  49962.4638753
Iteration :  56   Loss :  50971.5470455
Iteration :  57   Loss :  49962.4638753
Iteration :  58   Loss :  50971.5470455
Iteration :  59   Loss :  49962.4638753
Iteration :  60   Loss :  50971.5470455
Iteration :  61   Loss :  49962.4638753
Iteration :  62   Loss :  50971.5470455
Iteration :  63   Loss :  49962.4638753
Iteration :  64   Loss :  50971.5470455
Iteration :  65   Loss :  49962.4638753
Iteration :  66   Loss :  50971.5470455
Iteration :  67   Loss :  49962.4638753
Iteration :  68   Loss :  50971.5470455
Iteration :  69   Loss :  49962.4638753
Iteration :  70   Loss :  50971.5470455
Iteration :  71   Loss :  49962.4638753
Iteration :  72   Loss :  50971.5470455
Iteration :  73   Loss :  49962.4638753
Iteration :  74   Loss :  50971.5470455
Iteration :  75   Loss :  49962.4638753
Iteration :  76   Loss :  50971.5470455
Iteration :  77   Loss :  49962.4638753
Iteration :  78   Loss :  50971.5470455
Iteration :  79   Loss :  49962.4638753
Iteration :  80   Loss :  50971.5470455
Iteration :  81   Loss :  49962.4638753
Iteration :  82   Loss :  50971.5470455
Iteration :  83   Loss :  49962.4638753
Iteration :  84   Loss :  50971.5470455
Iteration :  85   Loss :  49962.4638753
Iteration :  86   Loss :  50971.5470455
Iteration :  87   Loss :  49962.4638753
Iteration :  88   Loss :  50971.5470455
Iteration :  89   Loss :  49962.4638753
Iteration :  90   Loss :  50971.5470455
Iteration :  91   Loss :  49962.4638753
Iteration :  92   Loss :  50971.5470455
Iteration :  93   Loss :  49962.4638753
Iteration :  94   Loss :  50971.5470455
Iteration :  95   Loss :  49962.4638753
Iteration :  96   Loss :  50971.5470455
Iteration :  97   Loss :  49962.4638753
Iteration :  98   Loss :  50971.5470455
Iteration :  99   Loss :  49962.4638753
[-0.3111528  -0.63016737 -0.15172103 ...,  0.28681017 -0.36556882
 -0.08298277]
CROSS VALIDATION 7
Iteration :  0   Loss :  52601.1884012
Iteration :  1   Loss :  55588.1649551
Iteration :  2   Loss :  50889.7869142
Iteration :  3   Loss :  49962.0141987
Iteration :  4   Loss :  50971.5458923
Iteration :  5   Loss :  49962.463869
Iteration :  6   Loss :  50971.5470455
Iteration :  7   Loss :  49962.4638753
Iteration :  8   Loss :  50971.5470455
Iteration :  9   Loss :  49962.4638753
Iteration :  10   Loss :  50971.5470455
Iteration :  11   Loss :  49962.4638753
Iteration :  12   Loss :  50971.5470455
Iteration :  13   Loss :  49962.4638753
Iteration :  14   Loss :  50971.5470455
Iteration :  15   Loss :  49962.4638753
Iteration :  16   Loss :  50971.5470455
Iteration :  17   Loss :  49962.4638753
Iteration :  18   Loss :  50971.5470455
Iteration :  19   Loss :  49962.4638753
Iteration :  20   Loss :  50971.5470455
Iteration :  21   Loss :  49962.4638753
Iteration :  22   Loss :  50971.5470455
Iteration :  23   Loss :  49962.4638753
Iteration :  24   Loss :  50971.5470455
Iteration :  25   Loss :  49962.4638753
Iteration :  26   Loss :  50971.5470455
Iteration :  27   Loss :  49962.4638753
Iteration :  28   Loss :  50971.5470455
Iteration :  29   Loss :  49962.4638753
Iteration :  30   Loss :  50971.5470455
Iteration :  31   Loss :  49962.4638753
Iteration :  32   Loss :  50971.5470455
Iteration :  33   Loss :  49962.4638753
Iteration :  34   Loss :  50971.5470455
Iteration :  35   Loss :  49962.4638753
Iteration :  36   Loss :  50971.5470455
Iteration :  37   Loss :  49962.4638753
Iteration :  38   Loss :  50971.5470455
Iteration :  39   Loss :  49962.4638753
Iteration :  40   Loss :  50971.5470455
Iteration :  41   Loss :  49962.4638753
Iteration :  42   Loss :  50971.5470455
Iteration :  43   Loss :  49962.4638753
Iteration :  44   Loss :  50971.5470455
Iteration :  45   Loss :  49962.4638753
Iteration :  46   Loss :  50971.5470455
Iteration :  47   Loss :  49962.4638753
Iteration :  48   Loss :  50971.5470455
Iteration :  49   Loss :  49962.4638753
Iteration :  50   Loss :  50971.5470455
Iteration :  51   Loss :  49962.4638753
Iteration :  52   Loss :  50971.5470455
Iteration :  53   Loss :  49962.4638753
Iteration :  54   Loss :  50971.5470455
Iteration :  55   Loss :  49962.4638753
Iteration :  56   Loss :  50971.5470455
Iteration :  57   Loss :  49962.4638753
Iteration :  58   Loss :  50971.5470455
Iteration :  59   Loss :  49962.4638753
Iteration :  60   Loss :  50971.5470455
Iteration :  61   Loss :  49962.4638753
Iteration :  62   Loss :  50971.5470455
Iteration :  63   Loss :  49962.4638753
Iteration :  64   Loss :  50971.5470455
Iteration :  65   Loss :  49962.4638753
Iteration :  66   Loss :  50971.5470455
Iteration :  67   Loss :  49962.4638753
Iteration :  68   Loss :  50971.5470455
Iteration :  69   Loss :  49962.4638753
Iteration :  70   Loss :  50971.5470455
Iteration :  71   Loss :  49962.4638753
Iteration :  72   Loss :  50971.5470455
Iteration :  73   Loss :  49962.4638753
Iteration :  74   Loss :  50971.5470455
Iteration :  75   Loss :  49962.4638753
Iteration :  76   Loss :  50971.5470455
Iteration :  77   Loss :  49962.4638753
Iteration :  78   Loss :  50971.5470455
Iteration :  79   Loss :  49962.4638753
Iteration :  80   Loss :  50971.5470455
Iteration :  81   Loss :  49962.4638753
Iteration :  82   Loss :  50971.5470455
Iteration :  83   Loss :  49962.4638753
Iteration :  84   Loss :  50971.5470455
Iteration :  85   Loss :  49962.4638753
Iteration :  86   Loss :  50971.5470455
Iteration :  87   Loss :  49962.4638753
Iteration :  88   Loss :  50971.5470455
Iteration :  89   Loss :  49962.4638753
Iteration :  90   Loss :  50971.5470455
Iteration :  91   Loss :  49962.4638753
Iteration :  92   Loss :  50971.5470455
Iteration :  93   Loss :  49962.4638753
Iteration :  94   Loss :  50971.5470455
Iteration :  95   Loss :  49962.4638753
Iteration :  96   Loss :  50971.5470455
Iteration :  97   Loss :  49962.4638753
Iteration :  98   Loss :  50971.5470455
Iteration :  99   Loss :  49962.4638753
[-0.3111528  -0.63016737 -0.15172103 ...,  0.28681017 -0.36556882
 -0.08298277]
CROSS VALIDATION 8
Iteration :  0   Loss :  52601.1884012
Iteration :  1   Loss :  54726.3751046
Iteration :  2   Loss :  42994.7779596
Iteration :  3   Loss :  52799.1368313
Iteration :  4   Loss :  50159.6000302
Iteration :  5   Loss :  54748.443102
Iteration :  6   Loss :  42994.7596656
Iteration :  7   Loss :  52799.1368602
Iteration :  8   Loss :  50159.6000304
Iteration :  9   Loss :  54748.443102
Iteration :  10   Loss :  42994.7596656
Iteration :  11   Loss :  52799.1368602
Iteration :  12   Loss :  50159.6000304
Iteration :  13   Loss :  54748.443102
Iteration :  14   Loss :  42994.7596656
Iteration :  15   Loss :  52799.1368602
Iteration :  16   Loss :  50159.6000304
Iteration :  17   Loss :  54748.443102
Iteration :  18   Loss :  42994.7596656
Iteration :  19   Loss :  52799.1368602
Iteration :  20   Loss :  50159.6000304
Iteration :  21   Loss :  54748.443102
Iteration :  22   Loss :  42994.7596656
Iteration :  23   Loss :  52799.1368602
Iteration :  24   Loss :  50159.6000304
Iteration :  25   Loss :  54748.443102
Iteration :  26   Loss :  42994.7596656
Iteration :  27   Loss :  52799.1368602
Iteration :  28   Loss :  50159.6000304
Iteration :  29   Loss :  54748.443102
Iteration :  30   Loss :  42994.7596656
Iteration :  31   Loss :  52799.1368602
Iteration :  32   Loss :  50159.6000304
Iteration :  33   Loss :  54748.443102
Iteration :  34   Loss :  42994.7596656
Iteration :  35   Loss :  52799.1368602
Iteration :  36   Loss :  50159.6000304
Iteration :  37   Loss :  54748.443102
Iteration :  38   Loss :  42994.7596656
Iteration :  39   Loss :  52799.1368602
Iteration :  40   Loss :  50159.6000304
Iteration :  41   Loss :  54748.443102
Iteration :  42   Loss :  42994.7596656
Iteration :  43   Loss :  52799.1368602
Iteration :  44   Loss :  50159.6000304
Iteration :  45   Loss :  54748.443102
Iteration :  46   Loss :  42994.7596656
Iteration :  47   Loss :  52799.1368602
Iteration :  48   Loss :  50159.6000304
Iteration :  49   Loss :  54748.443102
Iteration :  50   Loss :  42994.7596656
Iteration :  51   Loss :  52799.1368602
Iteration :  52   Loss :  50159.6000304
Iteration :  53   Loss :  54748.443102
Iteration :  54   Loss :  42994.7596656
Iteration :  55   Loss :  52799.1368602
Iteration :  56   Loss :  50159.6000304
Iteration :  57   Loss :  54748.443102
Iteration :  58   Loss :  42994.7596656
Iteration :  59   Loss :  52799.1368602
Iteration :  60   Loss :  50159.6000304
Iteration :  61   Loss :  54748.443102
Iteration :  62   Loss :  42994.7596656
Iteration :  63   Loss :  52799.1368602
Iteration :  64   Loss :  50159.6000304
Iteration :  65   Loss :  54748.443102
Iteration :  66   Loss :  42994.7596656
Iteration :  67   Loss :  52799.1368602
Iteration :  68   Loss :  50159.6000304
Iteration :  69   Loss :  54748.443102
Iteration :  70   Loss :  42994.7596656
Iteration :  71   Loss :  52799.1368602
Iteration :  72   Loss :  50159.6000304
Iteration :  73   Loss :  54748.443102
Iteration :  74   Loss :  42994.7596656
Iteration :  75   Loss :  52799.1368602
Iteration :  76   Loss :  50159.6000304
Iteration :  77   Loss :  54748.443102
Iteration :  78   Loss :  42994.7596656
Iteration :  79   Loss :  52799.1368602
Iteration :  80   Loss :  50159.6000304
Iteration :  81   Loss :  54748.443102
Iteration :  82   Loss :  42994.7596656
Iteration :  83   Loss :  52799.1368602
Iteration :  84   Loss :  50159.6000304
Iteration :  85   Loss :  54748.443102
Iteration :  86   Loss :  42994.7596656
Iteration :  87   Loss :  52799.1368602
Iteration :  88   Loss :  50159.6000304
Iteration :  89   Loss :  54748.443102
Iteration :  90   Loss :  42994.7596656
Iteration :  91   Loss :  52799.1368602
Iteration :  92   Loss :  50159.6000304
Iteration :  93   Loss :  54748.443102
Iteration :  94   Loss :  42994.7596656
Iteration :  95   Loss :  52799.1368602
Iteration :  96   Loss :  50159.6000304
Iteration :  97   Loss :  54748.443102
Iteration :  98   Loss :  42994.7596656
Iteration :  99   Loss :  52799.1368602
[-0.25383757 -0.63033123 -0.35119374 ...,  0.30381463 -0.36627351
 -0.09119179]
CROSS VALIDATION 9
Iteration :  0   Loss :  40792.8063263
Iteration :  1   Loss :  40751.928885
Iteration :  2   Loss :  53068.2872412
Iteration :  3   Loss :  50986.4637429
Iteration :  4   Loss :  40774.4872092
Iteration :  5   Loss :  53068.6514876
Iteration :  6   Loss :  50986.4653438
Iteration :  7   Loss :  40774.48721
Iteration :  8   Loss :  53068.6514877
Iteration :  9   Loss :  50986.4653438
Iteration :  10   Loss :  40774.48721
Iteration :  11   Loss :  53068.6514877
Iteration :  12   Loss :  50986.4653438
Iteration :  13   Loss :  40774.48721
Iteration :  14   Loss :  53068.6514877
Iteration :  15   Loss :  50986.4653438
Iteration :  16   Loss :  40774.48721
Iteration :  17   Loss :  53068.6514877
Iteration :  18   Loss :  50986.4653438
Iteration :  19   Loss :  40774.48721
Iteration :  20   Loss :  53068.6514877
Iteration :  21   Loss :  50986.4653438
Iteration :  22   Loss :  40774.48721
Iteration :  23   Loss :  53068.6514877
Iteration :  24   Loss :  50986.4653438
Iteration :  25   Loss :  40774.48721
Iteration :  26   Loss :  53068.6514877
Iteration :  27   Loss :  50986.4653438
Iteration :  28   Loss :  40774.48721
Iteration :  29   Loss :  53068.6514877
Iteration :  30   Loss :  50986.4653438
Iteration :  31   Loss :  40774.48721
Iteration :  32   Loss :  53068.6514877
Iteration :  33   Loss :  50986.4653438
Iteration :  34   Loss :  40774.48721
Iteration :  35   Loss :  53068.6514877
Iteration :  36   Loss :  50986.4653438
Iteration :  37   Loss :  40774.48721
Iteration :  38   Loss :  53068.6514877
Iteration :  39   Loss :  50986.4653438
Iteration :  40   Loss :  40774.48721
Iteration :  41   Loss :  53068.6514877
Iteration :  42   Loss :  50986.4653438
Iteration :  43   Loss :  40774.48721
Iteration :  44   Loss :  53068.6514877
Iteration :  45   Loss :  50986.4653438
Iteration :  46   Loss :  40774.48721
Iteration :  47   Loss :  53068.6514877
Iteration :  48   Loss :  50986.4653438
Iteration :  49   Loss :  40774.48721
Iteration :  50   Loss :  53068.6514877
Iteration :  51   Loss :  50986.4653438
Iteration :  52   Loss :  40774.48721
Iteration :  53   Loss :  53068.6514877
Iteration :  54   Loss :  50986.4653438
Iteration :  55   Loss :  40774.48721
Iteration :  56   Loss :  53068.6514877
Iteration :  57   Loss :  50986.4653438
Iteration :  58   Loss :  40774.48721
Iteration :  59   Loss :  53068.6514877
Iteration :  60   Loss :  50986.4653438
Iteration :  61   Loss :  40774.48721
Iteration :  62   Loss :  53068.6514877
Iteration :  63   Loss :  50986.4653438
Iteration :  64   Loss :  40774.48721
Iteration :  65   Loss :  53068.6514877
Iteration :  66   Loss :  50986.4653438
Iteration :  67   Loss :  40774.48721
Iteration :  68   Loss :  53068.6514877
Iteration :  69   Loss :  50986.4653438
Iteration :  70   Loss :  40774.48721
Iteration :  71   Loss :  53068.6514877
Iteration :  72   Loss :  50986.4653438
Iteration :  73   Loss :  40774.48721
Iteration :  74   Loss :  53068.6514877
Iteration :  75   Loss :  50986.4653438
Iteration :  76   Loss :  40774.48721
Iteration :  77   Loss :  53068.6514877
Iteration :  78   Loss :  50986.4653438
Iteration :  79   Loss :  40774.48721
Iteration :  80   Loss :  53068.6514877
Iteration :  81   Loss :  50986.4653438
Iteration :  82   Loss :  40774.48721
Iteration :  83   Loss :  53068.6514877
Iteration :  84   Loss :  50986.4653438
Iteration :  85   Loss :  40774.48721
Iteration :  86   Loss :  53068.6514877
Iteration :  87   Loss :  50986.4653438
Iteration :  88   Loss :  40774.48721
Iteration :  89   Loss :  53068.6514877
Iteration :  90   Loss :  50986.4653438
Iteration :  91   Loss :  40774.48721
Iteration :  92   Loss :  53068.6514877
Iteration :  93   Loss :  50986.4653438
Iteration :  94   Loss :  40774.48721
Iteration :  95   Loss :  53068.6514877
Iteration :  96   Loss :  50986.4653438
Iteration :  97   Loss :  40774.48721
Iteration :  98   Loss :  53068.6514877
Iteration :  99   Loss :  50986.4653438
[-0.05268969 -0.70322301  0.02729177 ...,  0.32307671 -0.39467479
 -0.08907954]
CROSS VALIDATION 10
Iteration :  0   Loss :  52355.6720245
Iteration :  1   Loss :  50827.5026134
Iteration :  2   Loss :  51898.2749816
Iteration :  3   Loss :  51391.100973
Iteration :  4   Loss :  48875.4580234
Iteration :  5   Loss :  52043.954395
Iteration :  6   Loss :  50798.4133458
Iteration :  7   Loss :  63798.7232849
Iteration :  8   Loss :  50876.8747074
Iteration :  9   Loss :  63799.2825062
Iteration :  10   Loss :  50876.8758152
Iteration :  11   Loss :  63799.2825141
Iteration :  12   Loss :  50876.8758152
Iteration :  13   Loss :  63799.2825141
Iteration :  14   Loss :  50876.8758152
Iteration :  15   Loss :  63799.2825141
Iteration :  16   Loss :  50876.8758152
Iteration :  17   Loss :  63799.2825141
Iteration :  18   Loss :  50876.8758152
Iteration :  19   Loss :  63799.2825141
Iteration :  20   Loss :  50876.8758152
Iteration :  21   Loss :  63799.2825141
Iteration :  22   Loss :  50876.8758152
Iteration :  23   Loss :  63799.2825141
Iteration :  24   Loss :  50876.8758152
Iteration :  25   Loss :  63799.2825141
Iteration :  26   Loss :  50876.8758152
Iteration :  27   Loss :  63799.2825141
Iteration :  28   Loss :  50876.8758152
Iteration :  29   Loss :  63799.2825141
Iteration :  30   Loss :  50876.8758152
Iteration :  31   Loss :  63799.2825141
Iteration :  32   Loss :  50876.8758152
Iteration :  33   Loss :  63799.2825141
Iteration :  34   Loss :  50876.8758152
Iteration :  35   Loss :  63799.2825141
Iteration :  36   Loss :  50876.8758152
Iteration :  37   Loss :  63799.2825141
Iteration :  38   Loss :  50876.8758152
Iteration :  39   Loss :  63799.2825141
Iteration :  40   Loss :  50876.8758152
Iteration :  41   Loss :  63799.2825141
Iteration :  42   Loss :  50876.8758152
Iteration :  43   Loss :  63799.2825141
Iteration :  44   Loss :  50876.8758152
Iteration :  45   Loss :  63799.2825141
Iteration :  46   Loss :  50876.8758152
Iteration :  47   Loss :  63799.2825141
Iteration :  48   Loss :  50876.8758152
Iteration :  49   Loss :  63799.2825141
Iteration :  50   Loss :  50876.8758152
Iteration :  51   Loss :  63799.2825141
Iteration :  52   Loss :  50876.8758152
Iteration :  53   Loss :  63799.2825141
Iteration :  54   Loss :  50876.8758152
Iteration :  55   Loss :  63799.2825141
Iteration :  56   Loss :  50876.8758152
Iteration :  57   Loss :  63799.2825141
Iteration :  58   Loss :  50876.8758152
Iteration :  59   Loss :  63799.2825141
Iteration :  60   Loss :  50876.8758152
Iteration :  61   Loss :  63799.2825141
Iteration :  62   Loss :  50876.8758152
Iteration :  63   Loss :  63799.2825141
Iteration :  64   Loss :  50876.8758152
Iteration :  65   Loss :  63799.2825141
Iteration :  66   Loss :  50876.8758152
Iteration :  67   Loss :  63799.2825141
Iteration :  68   Loss :  50876.8758152
Iteration :  69   Loss :  63799.2825141
Iteration :  70   Loss :  50876.8758152
Iteration :  71   Loss :  63799.2825141
Iteration :  72   Loss :  50876.8758152
Iteration :  73   Loss :  63799.2825141
Iteration :  74   Loss :  50876.8758152
Iteration :  75   Loss :  63799.2825141
Iteration :  76   Loss :  50876.8758152
Iteration :  77   Loss :  63799.2825141
Iteration :  78   Loss :  50876.8758152
Iteration :  79   Loss :  63799.2825141
Iteration :  80   Loss :  50876.8758152
Iteration :  81   Loss :  63799.2825141
Iteration :  82   Loss :  50876.8758152
Iteration :  83   Loss :  63799.2825141
Iteration :  84   Loss :  50876.8758152
Iteration :  85   Loss :  63799.2825141
Iteration :  86   Loss :  50876.8758152
Iteration :  87   Loss :  63799.2825141
Iteration :  88   Loss :  50876.8758152
Iteration :  89   Loss :  63799.2825141
Iteration :  90   Loss :  50876.8758152
Iteration :  91   Loss :  63799.2825141
Iteration :  92   Loss :  50876.8758152
Iteration :  93   Loss :  63799.2825141
Iteration :  94   Loss :  50876.8758152
Iteration :  95   Loss :  63799.2825141
Iteration :  96   Loss :  50876.8758152
Iteration :  97   Loss :  63799.2825141
Iteration :  98   Loss :  50876.8758152
Iteration :  99   Loss :  63799.2825141
[-0.13525267 -0.70941725 -0.0092994  ...,  0.31721158 -0.36530824
 -0.08215396]
CROSS VALIDATION 11
Iteration :  0   Loss :  52613.7824218
Iteration :  1   Loss :  55605.6248394
Iteration :  2   Loss :  50854.1768214
Iteration :  3   Loss :  49960.3811377
Iteration :  4   Loss :  50934.5838316
Iteration :  5   Loss :  49960.8029012
Iteration :  6   Loss :  50934.5849658
Iteration :  7   Loss :  49960.8029071
Iteration :  8   Loss :  50934.5849658
Iteration :  9   Loss :  49960.8029071
Iteration :  10   Loss :  50934.5849658
Iteration :  11   Loss :  49960.8029071
Iteration :  12   Loss :  50934.5849658
Iteration :  13   Loss :  49960.8029071
Iteration :  14   Loss :  50934.5849658
Iteration :  15   Loss :  49960.8029071
Iteration :  16   Loss :  50934.5849658
Iteration :  17   Loss :  49960.8029071
Iteration :  18   Loss :  50934.5849658
Iteration :  19   Loss :  49960.8029071
Iteration :  20   Loss :  50934.5849658
Iteration :  21   Loss :  49960.8029071
Iteration :  22   Loss :  50934.5849658
Iteration :  23   Loss :  49960.8029071
Iteration :  24   Loss :  50934.5849658
Iteration :  25   Loss :  49960.8029071
Iteration :  26   Loss :  50934.5849658
Iteration :  27   Loss :  49960.8029071
Iteration :  28   Loss :  50934.5849658
Iteration :  29   Loss :  49960.8029071
Iteration :  30   Loss :  50934.5849658
Iteration :  31   Loss :  49960.8029071
Iteration :  32   Loss :  50934.5849658
Iteration :  33   Loss :  49960.8029071
Iteration :  34   Loss :  50934.5849658
Iteration :  35   Loss :  49960.8029071
Iteration :  36   Loss :  50934.5849658
Iteration :  37   Loss :  49960.8029071
Iteration :  38   Loss :  50934.5849658
Iteration :  39   Loss :  49960.8029071
Iteration :  40   Loss :  50934.5849658
Iteration :  41   Loss :  49960.8029071
Iteration :  42   Loss :  50934.5849658
Iteration :  43   Loss :  49960.8029071
Iteration :  44   Loss :  50934.5849658
Iteration :  45   Loss :  49960.8029071
Iteration :  46   Loss :  50934.5849658
Iteration :  47   Loss :  49960.8029071
Iteration :  48   Loss :  50934.5849658
Iteration :  49   Loss :  49960.8029071
Iteration :  50   Loss :  50934.5849658
Iteration :  51   Loss :  49960.8029071
Iteration :  52   Loss :  50934.5849658
Iteration :  53   Loss :  49960.8029071
Iteration :  54   Loss :  50934.5849658
Iteration :  55   Loss :  49960.8029071
Iteration :  56   Loss :  50934.5849658
Iteration :  57   Loss :  49960.8029071
Iteration :  58   Loss :  50934.5849658
Iteration :  59   Loss :  49960.8029071
Iteration :  60   Loss :  50934.5849658
Iteration :  61   Loss :  49960.8029071
Iteration :  62   Loss :  50934.5849658
Iteration :  63   Loss :  49960.8029071
Iteration :  64   Loss :  50934.5849658
Iteration :  65   Loss :  49960.8029071
Iteration :  66   Loss :  50934.5849658
Iteration :  67   Loss :  49960.8029071
Iteration :  68   Loss :  50934.5849658
Iteration :  69   Loss :  49960.8029071
Iteration :  70   Loss :  50934.5849658
Iteration :  71   Loss :  49960.8029071
Iteration :  72   Loss :  50934.5849658
Iteration :  73   Loss :  49960.8029071
Iteration :  74   Loss :  50934.5849658
Iteration :  75   Loss :  49960.8029071
Iteration :  76   Loss :  50934.5849658
Iteration :  77   Loss :  49960.8029071
Iteration :  78   Loss :  50934.5849658
Iteration :  79   Loss :  49960.8029071
Iteration :  80   Loss :  50934.5849658
Iteration :  81   Loss :  49960.8029071
Iteration :  82   Loss :  50934.5849658
Iteration :  83   Loss :  49960.8029071
Iteration :  84   Loss :  50934.5849658
Iteration :  85   Loss :  49960.8029071
Iteration :  86   Loss :  50934.5849658
Iteration :  87   Loss :  49960.8029071
Iteration :  88   Loss :  50934.5849658
Iteration :  89   Loss :  49960.8029071
Iteration :  90   Loss :  50934.5849658
Iteration :  91   Loss :  49960.8029071
Iteration :  92   Loss :  50934.5849658
Iteration :  93   Loss :  49960.8029071
Iteration :  94   Loss :  50934.5849658
Iteration :  95   Loss :  49960.8029071
Iteration :  96   Loss :  50934.5849658
Iteration :  97   Loss :  49960.8029071
Iteration :  98   Loss :  50934.5849658
Iteration :  99   Loss :  49960.8029071
[-0.31115533 -0.63016912 -0.15172334 ...,  0.28681051 -0.36556679
 -0.08298286]
CROSS VALIDATION 12
Iteration :  0   Loss :  49779.7860132
Iteration :  1   Loss :  49949.0758531
Iteration :  2   Loss :  46042.4984962
Iteration :  3   Loss :  52240.7769245
Iteration :  4   Loss :  53965.3948199
Iteration :  5   Loss :  39732.1599636
Iteration :  6   Loss :  49846.1331052
Iteration :  7   Loss :  49471.1105903
Iteration :  8   Loss :  49941.5907703
Iteration :  9   Loss :  46042.4845294
Iteration :  10   Loss :  52240.7779232
Iteration :  11   Loss :  53965.3948238
Iteration :  12   Loss :  39732.1599636
Iteration :  13   Loss :  49846.1331052
Iteration :  14   Loss :  49471.1105903
Iteration :  15   Loss :  49941.5907703
Iteration :  16   Loss :  46042.4845294
Iteration :  17   Loss :  52240.7779232
Iteration :  18   Loss :  53965.3948238
Iteration :  19   Loss :  39732.1599636
Iteration :  20   Loss :  49846.1331052
Iteration :  21   Loss :  49471.1105903
Iteration :  22   Loss :  49941.5907703
Iteration :  23   Loss :  46042.4845294
Iteration :  24   Loss :  52240.7779232
Iteration :  25   Loss :  53965.3948238
Iteration :  26   Loss :  39732.1599636
Iteration :  27   Loss :  49846.1331052
Iteration :  28   Loss :  49471.1105903
Iteration :  29   Loss :  49941.5907703
Iteration :  30   Loss :  46042.4845294
Iteration :  31   Loss :  52240.7779232
Iteration :  32   Loss :  53965.3948238
Iteration :  33   Loss :  39732.1599636
Iteration :  34   Loss :  49846.1331052
Iteration :  35   Loss :  49471.1105903
Iteration :  36   Loss :  49941.5907703
Iteration :  37   Loss :  46042.4845294
Iteration :  38   Loss :  52240.7779232
Iteration :  39   Loss :  53965.3948238
Iteration :  40   Loss :  39732.1599636
Iteration :  41   Loss :  49846.1331052
Iteration :  42   Loss :  49471.1105903
Iteration :  43   Loss :  49941.5907703
Iteration :  44   Loss :  46042.4845294
Iteration :  45   Loss :  52240.7779232
Iteration :  46   Loss :  53965.3948238
Iteration :  47   Loss :  39732.1599636
Iteration :  48   Loss :  49846.1331052
Iteration :  49   Loss :  49471.1105903
Iteration :  50   Loss :  49941.5907703
Iteration :  51   Loss :  46042.4845294
Iteration :  52   Loss :  52240.7779232
Iteration :  53   Loss :  53965.3948238
Iteration :  54   Loss :  39732.1599636
Iteration :  55   Loss :  49846.1331052
Iteration :  56   Loss :  49471.1105903
Iteration :  57   Loss :  49941.5907703
Iteration :  58   Loss :  46042.4845294
Iteration :  59   Loss :  52240.7779232
Iteration :  60   Loss :  53965.3948238
Iteration :  61   Loss :  39732.1599636
Iteration :  62   Loss :  49846.1331052
Iteration :  63   Loss :  49471.1105903
Iteration :  64   Loss :  49941.5907703
Iteration :  65   Loss :  46042.4845294
Iteration :  66   Loss :  52240.7779232
Iteration :  67   Loss :  53965.3948238
Iteration :  68   Loss :  39732.1599636
Iteration :  69   Loss :  49846.1331052
Iteration :  70   Loss :  49471.1105903
Iteration :  71   Loss :  49941.5907703
Iteration :  72   Loss :  46042.4845294
Iteration :  73   Loss :  52240.7779232
Iteration :  74   Loss :  53965.3948238
Iteration :  75   Loss :  39732.1599636
Iteration :  76   Loss :  49846.1331052
Iteration :  77   Loss :  49471.1105903
Iteration :  78   Loss :  49941.5907703
Iteration :  79   Loss :  46042.4845294
Iteration :  80   Loss :  52240.7779232
Iteration :  81   Loss :  53965.3948238
Iteration :  82   Loss :  39732.1599636
Iteration :  83   Loss :  49846.1331052
Iteration :  84   Loss :  49471.1105903
Iteration :  85   Loss :  49941.5907703
Iteration :  86   Loss :  46042.4845294
Iteration :  87   Loss :  52240.7779232
Iteration :  88   Loss :  53965.3948238
Iteration :  89   Loss :  39732.1599636
Iteration :  90   Loss :  49846.1331052
Iteration :  91   Loss :  49471.1105903
Iteration :  92   Loss :  49941.5907703
Iteration :  93   Loss :  46042.4845294
Iteration :  94   Loss :  52240.7779232
Iteration :  95   Loss :  53965.3948238
Iteration :  96   Loss :  39732.1599636
Iteration :  97   Loss :  49846.1331052
Iteration :  98   Loss :  49471.1105903
Iteration :  99   Loss :  49941.5907703
[-0.36476034 -0.67267728 -0.17013499 ...,  0.29555451 -0.36792392
 -0.0953367 ]
CROSS VALIDATION 13
Iteration :  0   Loss :  49367.735247
Iteration :  1   Loss :  32416.4131551
Iteration :  2   Loss :  51947.5751732
Iteration :  3   Loss :  57182.1526816
Iteration :  4   Loss :  52339.405788
Iteration :  5   Loss :  55181.5136897
Iteration :  6   Loss :  52567.4017582
Iteration :  7   Loss :  55173.0580747
Iteration :  8   Loss :  52567.4050129
Iteration :  9   Loss :  55173.0579554
Iteration :  10   Loss :  52567.405013
Iteration :  11   Loss :  55173.0579554
Iteration :  12   Loss :  52567.405013
Iteration :  13   Loss :  55173.0579554
Iteration :  14   Loss :  52567.405013
Iteration :  15   Loss :  55173.0579554
Iteration :  16   Loss :  52567.405013
Iteration :  17   Loss :  55173.0579554
Iteration :  18   Loss :  52567.405013
Iteration :  19   Loss :  55173.0579554
Iteration :  20   Loss :  52567.405013
Iteration :  21   Loss :  55173.0579554
Iteration :  22   Loss :  52567.405013
Iteration :  23   Loss :  55173.0579554
Iteration :  24   Loss :  52567.405013
Iteration :  25   Loss :  55173.0579554
Iteration :  26   Loss :  52567.405013
Iteration :  27   Loss :  55173.0579554
Iteration :  28   Loss :  52567.405013
Iteration :  29   Loss :  55173.0579554
Iteration :  30   Loss :  52567.405013
Iteration :  31   Loss :  55173.0579554
Iteration :  32   Loss :  52567.405013
Iteration :  33   Loss :  55173.0579554
Iteration :  34   Loss :  52567.405013
Iteration :  35   Loss :  55173.0579554
Iteration :  36   Loss :  52567.405013
Iteration :  37   Loss :  55173.0579554
Iteration :  38   Loss :  52567.405013
Iteration :  39   Loss :  55173.0579554
Iteration :  40   Loss :  52567.405013
Iteration :  41   Loss :  55173.0579554
Iteration :  42   Loss :  52567.405013
Iteration :  43   Loss :  55173.0579554
Iteration :  44   Loss :  52567.405013
Iteration :  45   Loss :  55173.0579554
Iteration :  46   Loss :  52567.405013
Iteration :  47   Loss :  55173.0579554
Iteration :  48   Loss :  52567.405013
Iteration :  49   Loss :  55173.0579554
Iteration :  50   Loss :  52567.405013
Iteration :  51   Loss :  55173.0579554
Iteration :  52   Loss :  52567.405013
Iteration :  53   Loss :  55173.0579554
Iteration :  54   Loss :  52567.405013
Iteration :  55   Loss :  55173.0579554
Iteration :  56   Loss :  52567.405013
Iteration :  57   Loss :  55173.0579554
Iteration :  58   Loss :  52567.405013
Iteration :  59   Loss :  55173.0579554
Iteration :  60   Loss :  52567.405013
Iteration :  61   Loss :  55173.0579554
Iteration :  62   Loss :  52567.405013
Iteration :  63   Loss :  55173.0579554
Iteration :  64   Loss :  52567.405013
Iteration :  65   Loss :  55173.0579554
Iteration :  66   Loss :  52567.405013
Iteration :  67   Loss :  55173.0579554
Iteration :  68   Loss :  52567.405013
Iteration :  69   Loss :  55173.0579554
Iteration :  70   Loss :  52567.405013
Iteration :  71   Loss :  55173.0579554
Iteration :  72   Loss :  52567.405013
Iteration :  73   Loss :  55173.0579554
Iteration :  74   Loss :  52567.405013
Iteration :  75   Loss :  55173.0579554
Iteration :  76   Loss :  52567.405013
Iteration :  77   Loss :  55173.0579554
Iteration :  78   Loss :  52567.405013
Iteration :  79   Loss :  55173.0579554
Iteration :  80   Loss :  52567.405013
Iteration :  81   Loss :  55173.0579554
Iteration :  82   Loss :  52567.405013
Iteration :  83   Loss :  55173.0579554
Iteration :  84   Loss :  52567.405013
Iteration :  85   Loss :  55173.0579554
Iteration :  86   Loss :  52567.405013
Iteration :  87   Loss :  55173.0579554
Iteration :  88   Loss :  52567.405013
Iteration :  89   Loss :  55173.0579554
Iteration :  90   Loss :  52567.405013
Iteration :  91   Loss :  55173.0579554
Iteration :  92   Loss :  52567.405013
Iteration :  93   Loss :  55173.0579554
Iteration :  94   Loss :  52567.405013
Iteration :  95   Loss :  55173.0579554
Iteration :  96   Loss :  52567.405013
Iteration :  97   Loss :  55173.0579554
Iteration :  98   Loss :  52567.405013
Iteration :  99   Loss :  55173.0579554
[-0.34522328 -0.61567849 -0.35319612 ...,  0.28294845 -0.37118678
 -0.07940224]
CROSS VALIDATION 14
Iteration :  0   Loss :  52349.2490421
Iteration :  1   Loss :  42267.6118951
Iteration :  2   Loss :  52406.6091457
Iteration :  3   Loss :  62596.1892261
Iteration :  4   Loss :  42295.217489
Iteration :  5   Loss :  52406.1255284
Iteration :  6   Loss :  62595.0999583
Iteration :  7   Loss :  42295.2169247
Iteration :  8   Loss :  52406.1255151
Iteration :  9   Loss :  62595.1012318
Iteration :  10   Loss :  42295.2169254
Iteration :  11   Loss :  52406.1255151
Iteration :  12   Loss :  62595.1012304
Iteration :  13   Loss :  42295.2169254
Iteration :  14   Loss :  52406.1255151
Iteration :  15   Loss :  62595.1012304
Iteration :  16   Loss :  42295.2169254
Iteration :  17   Loss :  52406.1255151
Iteration :  18   Loss :  62595.1012304
Iteration :  19   Loss :  42295.2169254
Iteration :  20   Loss :  52406.1255151
Iteration :  21   Loss :  62595.1012304
Iteration :  22   Loss :  42295.2169254
Iteration :  23   Loss :  52406.1255151
Iteration :  24   Loss :  62595.1012304
Iteration :  25   Loss :  42295.2169254
Iteration :  26   Loss :  52406.1255151
Iteration :  27   Loss :  62595.1012304
Iteration :  28   Loss :  42295.2169254
Iteration :  29   Loss :  52406.1255151
Iteration :  30   Loss :  62595.1012304
Iteration :  31   Loss :  42295.2169254
Iteration :  32   Loss :  52406.1255151
Iteration :  33   Loss :  62595.1012304
Iteration :  34   Loss :  42295.2169254
Iteration :  35   Loss :  52406.1255151
Iteration :  36   Loss :  62595.1012304
Iteration :  37   Loss :  42295.2169254
Iteration :  38   Loss :  52406.1255151
Iteration :  39   Loss :  62595.1012304
Iteration :  40   Loss :  42295.2169254
Iteration :  41   Loss :  52406.1255151
Iteration :  42   Loss :  62595.1012304
Iteration :  43   Loss :  42295.2169254
Iteration :  44   Loss :  52406.1255151
Iteration :  45   Loss :  62595.1012304
Iteration :  46   Loss :  42295.2169254
Iteration :  47   Loss :  52406.1255151
Iteration :  48   Loss :  62595.1012304
Iteration :  49   Loss :  42295.2169254
Iteration :  50   Loss :  52406.1255151
Iteration :  51   Loss :  62595.1012304
Iteration :  52   Loss :  42295.2169254
Iteration :  53   Loss :  52406.1255151
Iteration :  54   Loss :  62595.1012304
Iteration :  55   Loss :  42295.2169254
Iteration :  56   Loss :  52406.1255151
Iteration :  57   Loss :  62595.1012304
Iteration :  58   Loss :  42295.2169254
Iteration :  59   Loss :  52406.1255151
Iteration :  60   Loss :  62595.1012304
Iteration :  61   Loss :  42295.2169254
Iteration :  62   Loss :  52406.1255151
Iteration :  63   Loss :  62595.1012304
Iteration :  64   Loss :  42295.2169254
Iteration :  65   Loss :  52406.1255151
Iteration :  66   Loss :  62595.1012304
Iteration :  67   Loss :  42295.2169254
Iteration :  68   Loss :  52406.1255151
Iteration :  69   Loss :  62595.1012304
Iteration :  70   Loss :  42295.2169254
Iteration :  71   Loss :  52406.1255151
Iteration :  72   Loss :  62595.1012304
Iteration :  73   Loss :  42295.2169254
Iteration :  74   Loss :  52406.1255151
Iteration :  75   Loss :  62595.1012304
Iteration :  76   Loss :  42295.2169254
Iteration :  77   Loss :  52406.1255151
Iteration :  78   Loss :  62595.1012304
Iteration :  79   Loss :  42295.2169254
Iteration :  80   Loss :  52406.1255151
Iteration :  81   Loss :  62595.1012304
Iteration :  82   Loss :  42295.2169254
Iteration :  83   Loss :  52406.1255151
Iteration :  84   Loss :  62595.1012304
Iteration :  85   Loss :  42295.2169254
Iteration :  86   Loss :  52406.1255151
Iteration :  87   Loss :  62595.1012304
Iteration :  88   Loss :  42295.2169254
Iteration :  89   Loss :  52406.1255151
Iteration :  90   Loss :  62595.1012304
Iteration :  91   Loss :  42295.2169254
Iteration :  92   Loss :  52406.1255151
Iteration :  93   Loss :  62595.1012304
Iteration :  94   Loss :  42295.2169254
Iteration :  95   Loss :  52406.1255151
Iteration :  96   Loss :  62595.1012304
Iteration :  97   Loss :  42295.2169254
Iteration :  98   Loss :  52406.1255151
Iteration :  99   Loss :  62595.1012304
[-0.13348624 -0.70829009 -0.00865208 ...,  0.31786281 -0.36541986
 -0.08189663]
CROSS VALIDATION 15
Iteration :  0   Loss :  52349.2490421
Iteration :  1   Loss :  42714.1749205
Iteration :  2   Loss :  52406.6091457
Iteration :  3   Loss :  63916.7038687
Iteration :  4   Loss :  42750.9641516
Iteration :  5   Loss :  52406.1255284
Iteration :  6   Loss :  63915.5924494
Iteration :  7   Loss :  42750.9635041
Iteration :  8   Loss :  52406.1255151
Iteration :  9   Loss :  63915.5937489
Iteration :  10   Loss :  42750.9635048
Iteration :  11   Loss :  52406.1255151
Iteration :  12   Loss :  63915.5937474
Iteration :  13   Loss :  42750.9635048
Iteration :  14   Loss :  52406.1255151
Iteration :  15   Loss :  63915.5937474
Iteration :  16   Loss :  42750.9635048
Iteration :  17   Loss :  52406.1255151
Iteration :  18   Loss :  63915.5937474
Iteration :  19   Loss :  42750.9635048
Iteration :  20   Loss :  52406.1255151
Iteration :  21   Loss :  63915.5937474
Iteration :  22   Loss :  42750.9635048
Iteration :  23   Loss :  52406.1255151
Iteration :  24   Loss :  63915.5937474
Iteration :  25   Loss :  42750.9635048
Iteration :  26   Loss :  52406.1255151
Iteration :  27   Loss :  63915.5937474
Iteration :  28   Loss :  42750.9635048
Iteration :  29   Loss :  52406.1255151
Iteration :  30   Loss :  63915.5937474
Iteration :  31   Loss :  42750.9635048
Iteration :  32   Loss :  52406.1255151
Iteration :  33   Loss :  63915.5937474
Iteration :  34   Loss :  42750.9635048
Iteration :  35   Loss :  52406.1255151
Iteration :  36   Loss :  63915.5937474
Iteration :  37   Loss :  42750.9635048
Iteration :  38   Loss :  52406.1255151
Iteration :  39   Loss :  63915.5937474
Iteration :  40   Loss :  42750.9635048
Iteration :  41   Loss :  52406.1255151
Iteration :  42   Loss :  63915.5937474
Iteration :  43   Loss :  42750.9635048
Iteration :  44   Loss :  52406.1255151
Iteration :  45   Loss :  63915.5937474
Iteration :  46   Loss :  42750.9635048
Iteration :  47   Loss :  52406.1255151
Iteration :  48   Loss :  63915.5937474
Iteration :  49   Loss :  42750.9635048
Iteration :  50   Loss :  52406.1255151
Iteration :  51   Loss :  63915.5937474
Iteration :  52   Loss :  42750.9635048
Iteration :  53   Loss :  52406.1255151
Iteration :  54   Loss :  63915.5937474
Iteration :  55   Loss :  42750.9635048
Iteration :  56   Loss :  52406.1255151
Iteration :  57   Loss :  63915.5937474
Iteration :  58   Loss :  42750.9635048
Iteration :  59   Loss :  52406.1255151
Iteration :  60   Loss :  63915.5937474
Iteration :  61   Loss :  42750.9635048
Iteration :  62   Loss :  52406.1255151
Iteration :  63   Loss :  63915.5937474
Iteration :  64   Loss :  42750.9635048
Iteration :  65   Loss :  52406.1255151
Iteration :  66   Loss :  63915.5937474
Iteration :  67   Loss :  42750.9635048
Iteration :  68   Loss :  52406.1255151
Iteration :  69   Loss :  63915.5937474
Iteration :  70   Loss :  42750.9635048
Iteration :  71   Loss :  52406.1255151
Iteration :  72   Loss :  63915.5937474
Iteration :  73   Loss :  42750.9635048
Iteration :  74   Loss :  52406.1255151
Iteration :  75   Loss :  63915.5937474
Iteration :  76   Loss :  42750.9635048
Iteration :  77   Loss :  52406.1255151
Iteration :  78   Loss :  63915.5937474
Iteration :  79   Loss :  42750.9635048
Iteration :  80   Loss :  52406.1255151
Iteration :  81   Loss :  63915.5937474
Iteration :  82   Loss :  42750.9635048
Iteration :  83   Loss :  52406.1255151
Iteration :  84   Loss :  63915.5937474
Iteration :  85   Loss :  42750.9635048
Iteration :  86   Loss :  52406.1255151
Iteration :  87   Loss :  63915.5937474
Iteration :  88   Loss :  42750.9635048
Iteration :  89   Loss :  52406.1255151
Iteration :  90   Loss :  63915.5937474
Iteration :  91   Loss :  42750.9635048
Iteration :  92   Loss :  52406.1255151
Iteration :  93   Loss :  63915.5937474
Iteration :  94   Loss :  42750.9635048
Iteration :  95   Loss :  52406.1255151
Iteration :  96   Loss :  63915.5937474
Iteration :  97   Loss :  42750.9635048
Iteration :  98   Loss :  52406.1255151
Iteration :  99   Loss :  63915.5937474
[-0.13348624 -0.70829009 -0.00865208 ...,  0.31786281 -0.36541986
 -0.08189663]
CROSS VALIDATION 16
Iteration :  0   Loss :  51969.3988526
Iteration :  1   Loss :  42402.0858596
Iteration :  2   Loss :  52406.6091457
Iteration :  3   Loss :  63916.7038687
Iteration :  4   Loss :  42445.4504759
Iteration :  5   Loss :  52406.1255284
Iteration :  6   Loss :  63915.5924494
Iteration :  7   Loss :  42445.4498042
Iteration :  8   Loss :  52406.1255151
Iteration :  9   Loss :  63915.5937489
Iteration :  10   Loss :  42445.449805
Iteration :  11   Loss :  52406.1255151
Iteration :  12   Loss :  63915.5937474
Iteration :  13   Loss :  42445.449805
Iteration :  14   Loss :  52406.1255151
Iteration :  15   Loss :  63915.5937474
Iteration :  16   Loss :  42445.449805
Iteration :  17   Loss :  52406.1255151
Iteration :  18   Loss :  63915.5937474
Iteration :  19   Loss :  42445.449805
Iteration :  20   Loss :  52406.1255151
Iteration :  21   Loss :  63915.5937474
Iteration :  22   Loss :  42445.449805
Iteration :  23   Loss :  52406.1255151
Iteration :  24   Loss :  63915.5937474
Iteration :  25   Loss :  42445.449805
Iteration :  26   Loss :  52406.1255151
Iteration :  27   Loss :  63915.5937474
Iteration :  28   Loss :  42445.449805
Iteration :  29   Loss :  52406.1255151
Iteration :  30   Loss :  63915.5937474
Iteration :  31   Loss :  42445.449805
Iteration :  32   Loss :  52406.1255151
Iteration :  33   Loss :  63915.5937474
Iteration :  34   Loss :  42445.449805
Iteration :  35   Loss :  52406.1255151
Iteration :  36   Loss :  63915.5937474
Iteration :  37   Loss :  42445.449805
Iteration :  38   Loss :  52406.1255151
Iteration :  39   Loss :  63915.5937474
Iteration :  40   Loss :  42445.449805
Iteration :  41   Loss :  52406.1255151
Iteration :  42   Loss :  63915.5937474
Iteration :  43   Loss :  42445.449805
Iteration :  44   Loss :  52406.1255151
Iteration :  45   Loss :  63915.5937474
Iteration :  46   Loss :  42445.449805
Iteration :  47   Loss :  52406.1255151
Iteration :  48   Loss :  63915.5937474
Iteration :  49   Loss :  42445.449805
Iteration :  50   Loss :  52406.1255151
Iteration :  51   Loss :  63915.5937474
Iteration :  52   Loss :  42445.449805
Iteration :  53   Loss :  52406.1255151
Iteration :  54   Loss :  63915.5937474
Iteration :  55   Loss :  42445.449805
Iteration :  56   Loss :  52406.1255151
Iteration :  57   Loss :  63915.5937474
Iteration :  58   Loss :  42445.449805
Iteration :  59   Loss :  52406.1255151
Iteration :  60   Loss :  63915.5937474
Iteration :  61   Loss :  42445.449805
Iteration :  62   Loss :  52406.1255151
Iteration :  63   Loss :  63915.5937474
Iteration :  64   Loss :  42445.449805
Iteration :  65   Loss :  52406.1255151
Iteration :  66   Loss :  63915.5937474
Iteration :  67   Loss :  42445.449805
Iteration :  68   Loss :  52406.1255151
Iteration :  69   Loss :  63915.5937474
Iteration :  70   Loss :  42445.449805
Iteration :  71   Loss :  52406.1255151
Iteration :  72   Loss :  63915.5937474
Iteration :  73   Loss :  42445.449805
Iteration :  74   Loss :  52406.1255151
Iteration :  75   Loss :  63915.5937474
Iteration :  76   Loss :  42445.449805
Iteration :  77   Loss :  52406.1255151
Iteration :  78   Loss :  63915.5937474
Iteration :  79   Loss :  42445.449805
Iteration :  80   Loss :  52406.1255151
Iteration :  81   Loss :  63915.5937474
Iteration :  82   Loss :  42445.449805
Iteration :  83   Loss :  52406.1255151
Iteration :  84   Loss :  63915.5937474
Iteration :  85   Loss :  42445.449805
Iteration :  86   Loss :  52406.1255151
Iteration :  87   Loss :  63915.5937474
Iteration :  88   Loss :  42445.449805
Iteration :  89   Loss :  52406.1255151
Iteration :  90   Loss :  63915.5937474
Iteration :  91   Loss :  42445.449805
Iteration :  92   Loss :  52406.1255151
Iteration :  93   Loss :  63915.5937474
Iteration :  94   Loss :  42445.449805
Iteration :  95   Loss :  52406.1255151
Iteration :  96   Loss :  63915.5937474
Iteration :  97   Loss :  42445.449805
Iteration :  98   Loss :  52406.1255151
Iteration :  99   Loss :  63915.5937474
[-0.13348624 -0.70829009 -0.00865208 ...,  0.31786281 -0.36541986
 -0.08189663]
CROSS VALIDATION 17
Iteration :  0   Loss :  52349.2490421
Iteration :  1   Loss :  52823.2067582
Iteration :  2   Loss :  47018.3664108
Iteration :  3   Loss :  52767.5831114
Iteration :  4   Loss :  47018.5092421
Iteration :  5   Loss :  52767.5823265
Iteration :  6   Loss :  47018.5092442
Iteration :  7   Loss :  52767.5823265
Iteration :  8   Loss :  47018.5092442
Iteration :  9   Loss :  52767.5823265
Iteration :  10   Loss :  47018.5092442
Iteration :  11   Loss :  52767.5823265
Iteration :  12   Loss :  47018.5092442
Iteration :  13   Loss :  52767.5823265
Iteration :  14   Loss :  47018.5092442
Iteration :  15   Loss :  52767.5823265
Iteration :  16   Loss :  47018.5092442
Iteration :  17   Loss :  52767.5823265
Iteration :  18   Loss :  47018.5092442
Iteration :  19   Loss :  52767.5823265
Iteration :  20   Loss :  47018.5092442
Iteration :  21   Loss :  52767.5823265
Iteration :  22   Loss :  47018.5092442
Iteration :  23   Loss :  52767.5823265
Iteration :  24   Loss :  47018.5092442
Iteration :  25   Loss :  52767.5823265
Iteration :  26   Loss :  47018.5092442
Iteration :  27   Loss :  52767.5823265
Iteration :  28   Loss :  47018.5092442
Iteration :  29   Loss :  52767.5823265
Iteration :  30   Loss :  47018.5092442
Iteration :  31   Loss :  52767.5823265
Iteration :  32   Loss :  47018.5092442
Iteration :  33   Loss :  52767.5823265
Iteration :  34   Loss :  47018.5092442
Iteration :  35   Loss :  52767.5823265
Iteration :  36   Loss :  47018.5092442
Iteration :  37   Loss :  52767.5823265
Iteration :  38   Loss :  47018.5092442
Iteration :  39   Loss :  52767.5823265
Iteration :  40   Loss :  47018.5092442
Iteration :  41   Loss :  52767.5823265
Iteration :  42   Loss :  47018.5092442
Iteration :  43   Loss :  52767.5823265
Iteration :  44   Loss :  47018.5092442
Iteration :  45   Loss :  52767.5823265
Iteration :  46   Loss :  47018.5092442
Iteration :  47   Loss :  52767.5823265
Iteration :  48   Loss :  47018.5092442
Iteration :  49   Loss :  52767.5823265
Iteration :  50   Loss :  47018.5092442
Iteration :  51   Loss :  52767.5823265
Iteration :  52   Loss :  47018.5092442
Iteration :  53   Loss :  52767.5823265
Iteration :  54   Loss :  47018.5092442
Iteration :  55   Loss :  52767.5823265
Iteration :  56   Loss :  47018.5092442
Iteration :  57   Loss :  52767.5823265
Iteration :  58   Loss :  47018.5092442
Iteration :  59   Loss :  52767.5823265
Iteration :  60   Loss :  47018.5092442
Iteration :  61   Loss :  52767.5823265
Iteration :  62   Loss :  47018.5092442
Iteration :  63   Loss :  52767.5823265
Iteration :  64   Loss :  47018.5092442
Iteration :  65   Loss :  52767.5823265
Iteration :  66   Loss :  47018.5092442
Iteration :  67   Loss :  52767.5823265
Iteration :  68   Loss :  47018.5092442
Iteration :  69   Loss :  52767.5823265
Iteration :  70   Loss :  47018.5092442
Iteration :  71   Loss :  52767.5823265
Iteration :  72   Loss :  47018.5092442
Iteration :  73   Loss :  52767.5823265
Iteration :  74   Loss :  47018.5092442
Iteration :  75   Loss :  52767.5823265
Iteration :  76   Loss :  47018.5092442
Iteration :  77   Loss :  52767.5823265
Iteration :  78   Loss :  47018.5092442
Iteration :  79   Loss :  52767.5823265
Iteration :  80   Loss :  47018.5092442
Iteration :  81   Loss :  52767.5823265
Iteration :  82   Loss :  47018.5092442
Iteration :  83   Loss :  52767.5823265
Iteration :  84   Loss :  47018.5092442
Iteration :  85   Loss :  52767.5823265
Iteration :  86   Loss :  47018.5092442
Iteration :  87   Loss :  52767.5823265
Iteration :  88   Loss :  47018.5092442
Iteration :  89   Loss :  52767.5823265
Iteration :  90   Loss :  47018.5092442
Iteration :  91   Loss :  52767.5823265
Iteration :  92   Loss :  47018.5092442
Iteration :  93   Loss :  52767.5823265
Iteration :  94   Loss :  47018.5092442
Iteration :  95   Loss :  52767.5823265
Iteration :  96   Loss :  47018.5092442
Iteration :  97   Loss :  52767.5823265
Iteration :  98   Loss :  47018.5092442
Iteration :  99   Loss :  52767.5823265
[-0.0979874  -0.70400513  0.01787992 ...,  0.31081267 -0.38225349
 -0.08601786]
CROSS VALIDATION 18
Iteration :  0   Loss :  51506.67607
Iteration :  1   Loss :  42129.6836043
Iteration :  2   Loss :  39877.2269738
Iteration :  3   Loss :  45973.4284013
Iteration :  4   Loss :  47422.0146596
Iteration :  5   Loss :  45593.2753677
Iteration :  6   Loss :  38897.3723146
Iteration :  7   Loss :  38348.9383032
Iteration :  8   Loss :  45960.9872862
Iteration :  9   Loss :  47424.4379927
Iteration :  10   Loss :  45593.2847741
Iteration :  11   Loss :  38897.372612
Iteration :  12   Loss :  38348.9328717
Iteration :  13   Loss :  45960.987284
Iteration :  14   Loss :  47424.4380024
Iteration :  15   Loss :  45593.2847741
Iteration :  16   Loss :  38897.372612
Iteration :  17   Loss :  38348.9328717
Iteration :  18   Loss :  45960.987284
Iteration :  19   Loss :  47424.4380024
Iteration :  20   Loss :  45593.2847741
Iteration :  21   Loss :  38897.372612
Iteration :  22   Loss :  38348.9328717
Iteration :  23   Loss :  45960.987284
Iteration :  24   Loss :  47424.4380024
Iteration :  25   Loss :  45593.2847741
Iteration :  26   Loss :  38897.372612
Iteration :  27   Loss :  38348.9328717
Iteration :  28   Loss :  45960.987284
Iteration :  29   Loss :  47424.4380024
Iteration :  30   Loss :  45593.2847741
Iteration :  31   Loss :  38897.372612
Iteration :  32   Loss :  38348.9328717
Iteration :  33   Loss :  45960.987284
Iteration :  34   Loss :  47424.4380024
Iteration :  35   Loss :  45593.2847741
Iteration :  36   Loss :  38897.372612
Iteration :  37   Loss :  38348.9328717
Iteration :  38   Loss :  45960.987284
Iteration :  39   Loss :  47424.4380024
Iteration :  40   Loss :  45593.2847741
Iteration :  41   Loss :  38897.372612
Iteration :  42   Loss :  38348.9328717
Iteration :  43   Loss :  45960.987284
Iteration :  44   Loss :  47424.4380024
Iteration :  45   Loss :  45593.2847741
Iteration :  46   Loss :  38897.372612
Iteration :  47   Loss :  38348.9328717
Iteration :  48   Loss :  45960.987284
Iteration :  49   Loss :  47424.4380024
Iteration :  50   Loss :  45593.2847741
Iteration :  51   Loss :  38897.372612
Iteration :  52   Loss :  38348.9328717
Iteration :  53   Loss :  45960.987284
Iteration :  54   Loss :  47424.4380024
Iteration :  55   Loss :  45593.2847741
Iteration :  56   Loss :  38897.372612
Iteration :  57   Loss :  38348.9328717
Iteration :  58   Loss :  45960.987284
Iteration :  59   Loss :  47424.4380024
Iteration :  60   Loss :  45593.2847741
Iteration :  61   Loss :  38897.372612
Iteration :  62   Loss :  38348.9328717
Iteration :  63   Loss :  45960.987284
Iteration :  64   Loss :  47424.4380024
Iteration :  65   Loss :  45593.2847741
Iteration :  66   Loss :  38897.372612
Iteration :  67   Loss :  38348.9328717
Iteration :  68   Loss :  45960.987284
Iteration :  69   Loss :  47424.4380024
Iteration :  70   Loss :  45593.2847741
Iteration :  71   Loss :  38897.372612
Iteration :  72   Loss :  38348.9328717
Iteration :  73   Loss :  45960.987284
Iteration :  74   Loss :  47424.4380024
Iteration :  75   Loss :  45593.2847741
Iteration :  76   Loss :  38897.372612
Iteration :  77   Loss :  38348.9328717
Iteration :  78   Loss :  45960.987284
Iteration :  79   Loss :  47424.4380024
Iteration :  80   Loss :  45593.2847741
Iteration :  81   Loss :  38897.372612
Iteration :  82   Loss :  38348.9328717
Iteration :  83   Loss :  45960.987284
Iteration :  84   Loss :  47424.4380024
Iteration :  85   Loss :  45593.2847741
Iteration :  86   Loss :  38897.372612
Iteration :  87   Loss :  38348.9328717
Iteration :  88   Loss :  45960.987284
Iteration :  89   Loss :  47424.4380024
Iteration :  90   Loss :  45593.2847741
Iteration :  91   Loss :  38897.372612
Iteration :  92   Loss :  38348.9328717
Iteration :  93   Loss :  45960.987284
Iteration :  94   Loss :  47424.4380024
Iteration :  95   Loss :  45593.2847741
Iteration :  96   Loss :  38897.372612
Iteration :  97   Loss :  38348.9328717
Iteration :  98   Loss :  45960.987284
Iteration :  99   Loss :  47424.4380024
[-0.27769655 -0.81149463  0.0056864  ...,  0.20196119 -0.14293847
 -0.05088268]
CROSS VALIDATION 19
Iteration :  0   Loss :  56266.5992963
Iteration :  1   Loss :  50708.7706639
Iteration :  2   Loss :  51000.1101763
Iteration :  3   Loss :  42526.0485074
Iteration :  4   Loss :  51816.949753
Iteration :  5   Loss :  63839.3053164
Iteration :  6   Loss :  42180.0261435
Iteration :  7   Loss :  51822.943775
Iteration :  8   Loss :  63839.6170239
Iteration :  9   Loss :  42180.0263122
Iteration :  10   Loss :  51822.9437792
Iteration :  11   Loss :  63839.6167268
Iteration :  12   Loss :  42180.026312
Iteration :  13   Loss :  51822.9437792
Iteration :  14   Loss :  63839.6167271
Iteration :  15   Loss :  42180.026312
Iteration :  16   Loss :  51822.9437792
Iteration :  17   Loss :  63839.6167271
Iteration :  18   Loss :  42180.026312
Iteration :  19   Loss :  51822.9437792
Iteration :  20   Loss :  63839.6167271
Iteration :  21   Loss :  42180.026312
Iteration :  22   Loss :  51822.9437792
Iteration :  23   Loss :  63839.6167271
Iteration :  24   Loss :  42180.026312
Iteration :  25   Loss :  51822.9437792
Iteration :  26   Loss :  63839.6167271
Iteration :  27   Loss :  42180.026312
Iteration :  28   Loss :  51822.9437792
Iteration :  29   Loss :  63839.6167271
Iteration :  30   Loss :  42180.026312
Iteration :  31   Loss :  51822.9437792
Iteration :  32   Loss :  63839.6167271
Iteration :  33   Loss :  42180.026312
Iteration :  34   Loss :  51822.9437792
Iteration :  35   Loss :  63839.6167271
Iteration :  36   Loss :  42180.026312
Iteration :  37   Loss :  51822.9437792
Iteration :  38   Loss :  63839.6167271
Iteration :  39   Loss :  42180.026312
Iteration :  40   Loss :  51822.9437792
Iteration :  41   Loss :  63839.6167271
Iteration :  42   Loss :  42180.026312
Iteration :  43   Loss :  51822.9437792
Iteration :  44   Loss :  63839.6167271
Iteration :  45   Loss :  42180.026312
Iteration :  46   Loss :  51822.9437792
Iteration :  47   Loss :  63839.6167271
Iteration :  48   Loss :  42180.026312
Iteration :  49   Loss :  51822.9437792
Iteration :  50   Loss :  63839.6167271
Iteration :  51   Loss :  42180.026312
Iteration :  52   Loss :  51822.9437792
Iteration :  53   Loss :  63839.6167271
Iteration :  54   Loss :  42180.026312
Iteration :  55   Loss :  51822.9437792
Iteration :  56   Loss :  63839.6167271
Iteration :  57   Loss :  42180.026312
Iteration :  58   Loss :  51822.9437792
Iteration :  59   Loss :  63839.6167271
Iteration :  60   Loss :  42180.026312
Iteration :  61   Loss :  51822.9437792
Iteration :  62   Loss :  63839.6167271
Iteration :  63   Loss :  42180.026312
Iteration :  64   Loss :  51822.9437792
Iteration :  65   Loss :  63839.6167271
Iteration :  66   Loss :  42180.026312
Iteration :  67   Loss :  51822.9437792
Iteration :  68   Loss :  63839.6167271
Iteration :  69   Loss :  42180.026312
Iteration :  70   Loss :  51822.9437792
Iteration :  71   Loss :  63839.6167271
Iteration :  72   Loss :  42180.026312
Iteration :  73   Loss :  51822.9437792
Iteration :  74   Loss :  63839.6167271
Iteration :  75   Loss :  42180.026312
Iteration :  76   Loss :  51822.9437792
Iteration :  77   Loss :  63839.6167271
Iteration :  78   Loss :  42180.026312
Iteration :  79   Loss :  51822.9437792
Iteration :  80   Loss :  63839.6167271
Iteration :  81   Loss :  42180.026312
Iteration :  82   Loss :  51822.9437792
Iteration :  83   Loss :  63839.6167271
Iteration :  84   Loss :  42180.026312
Iteration :  85   Loss :  51822.9437792
Iteration :  86   Loss :  63839.6167271
Iteration :  87   Loss :  42180.026312
Iteration :  88   Loss :  51822.9437792
Iteration :  89   Loss :  63839.6167271
Iteration :  90   Loss :  42180.026312
Iteration :  91   Loss :  51822.9437792
Iteration :  92   Loss :  63839.6167271
Iteration :  93   Loss :  42180.026312
Iteration :  94   Loss :  51822.9437792
Iteration :  95   Loss :  63839.6167271
Iteration :  96   Loss :  42180.026312
Iteration :  97   Loss :  51822.9437792
Iteration :  98   Loss :  63839.6167271
Iteration :  99   Loss :  42180.026312
[-0.10040981 -0.67703418  0.06284152 ...,  0.28718433 -0.30481243
 -0.06474484]
Accuracy (Logistic Loss):	0.6
Hinge Loss
CROSS VALIDATION 0
Iteration :  0   Loss :  55718.3078068
Iteration :  1   Loss :  51206.2923515
Iteration :  2   Loss :  43130.6974029
Iteration :  3   Loss :  52644.0852401
Iteration :  4   Loss :  64164.4441519
Iteration :  5   Loss :  43125.408595
Iteration :  6   Loss :  52644.461413
Iteration :  7   Loss :  64164.4465192
Iteration :  8   Loss :  43125.4085947
Iteration :  9   Loss :  52644.461413
Iteration :  10   Loss :  64164.4465192
Iteration :  11   Loss :  43125.4085947
Iteration :  12   Loss :  52644.461413
Iteration :  13   Loss :  64164.4465192
Iteration :  14   Loss :  43125.4085947
Iteration :  15   Loss :  52644.461413
Iteration :  16   Loss :  64164.4465192
Iteration :  17   Loss :  43125.4085947
Iteration :  18   Loss :  52644.461413
Iteration :  19   Loss :  64164.4465192
Iteration :  20   Loss :  43125.4085947
Iteration :  21   Loss :  52644.461413
Iteration :  22   Loss :  64164.4465192
Iteration :  23   Loss :  43125.4085947
Iteration :  24   Loss :  52644.461413
Iteration :  25   Loss :  64164.4465192
Iteration :  26   Loss :  43125.4085947
Iteration :  27   Loss :  52644.461413
Iteration :  28   Loss :  64164.4465192
Iteration :  29   Loss :  43125.4085947
Iteration :  30   Loss :  52644.461413
Iteration :  31   Loss :  64164.4465192
Iteration :  32   Loss :  43125.4085947
Iteration :  33   Loss :  52644.461413
Iteration :  34   Loss :  64164.4465192
Iteration :  35   Loss :  43125.4085947
Iteration :  36   Loss :  52644.461413
Iteration :  37   Loss :  64164.4465192
Iteration :  38   Loss :  43125.4085947
Iteration :  39   Loss :  52644.461413
Iteration :  40   Loss :  64164.4465192
Iteration :  41   Loss :  43125.4085947
Iteration :  42   Loss :  52644.461413
Iteration :  43   Loss :  64164.4465192
Iteration :  44   Loss :  43125.4085947
Iteration :  45   Loss :  52644.461413
Iteration :  46   Loss :  64164.4465192
Iteration :  47   Loss :  43125.4085947
Iteration :  48   Loss :  52644.461413
Iteration :  49   Loss :  64164.4465192
Iteration :  50   Loss :  43125.4085947
Iteration :  51   Loss :  52644.461413
Iteration :  52   Loss :  64164.4465192
Iteration :  53   Loss :  43125.4085947
Iteration :  54   Loss :  52644.461413
Iteration :  55   Loss :  64164.4465192
Iteration :  56   Loss :  43125.4085947
Iteration :  57   Loss :  52644.461413
Iteration :  58   Loss :  64164.4465192
Iteration :  59   Loss :  43125.4085947
Iteration :  60   Loss :  52644.461413
Iteration :  61   Loss :  64164.4465192
Iteration :  62   Loss :  43125.4085947
Iteration :  63   Loss :  52644.461413
Iteration :  64   Loss :  64164.4465192
Iteration :  65   Loss :  43125.4085947
Iteration :  66   Loss :  52644.461413
Iteration :  67   Loss :  64164.4465192
Iteration :  68   Loss :  43125.4085947
Iteration :  69   Loss :  52644.461413
Iteration :  70   Loss :  64164.4465192
Iteration :  71   Loss :  43125.4085947
Iteration :  72   Loss :  52644.461413
Iteration :  73   Loss :  64164.4465192
Iteration :  74   Loss :  43125.4085947
Iteration :  75   Loss :  52644.461413
Iteration :  76   Loss :  64164.4465192
Iteration :  77   Loss :  43125.4085947
Iteration :  78   Loss :  52644.461413
Iteration :  79   Loss :  64164.4465192
Iteration :  80   Loss :  43125.4085947
Iteration :  81   Loss :  52644.461413
Iteration :  82   Loss :  64164.4465192
Iteration :  83   Loss :  43125.4085947
Iteration :  84   Loss :  52644.461413
Iteration :  85   Loss :  64164.4465192
Iteration :  86   Loss :  43125.4085947
Iteration :  87   Loss :  52644.461413
Iteration :  88   Loss :  64164.4465192
Iteration :  89   Loss :  43125.4085947
Iteration :  90   Loss :  52644.461413
Iteration :  91   Loss :  64164.4465192
Iteration :  92   Loss :  43125.4085947
Iteration :  93   Loss :  52644.461413
Iteration :  94   Loss :  64164.4465192
Iteration :  95   Loss :  43125.4085947
Iteration :  96   Loss :  52644.461413
Iteration :  97   Loss :  64164.4465192
Iteration :  98   Loss :  43125.4085947
Iteration :  99   Loss :  52644.461413
[-0.31399653 -0.55945638 -0.33603634 ...,  0.40765587 -0.32116485
 -0.06328346]
CROSS VALIDATION 1
Iteration :  0   Loss :  51530.0013291
Iteration :  1   Loss :  49454.7878277
Iteration :  2   Loss :  50977.5818862
Iteration :  3   Loss :  49974.8910497
Iteration :  4   Loss :  50977.0530876
Iteration :  5   Loss :  49974.898395
Iteration :  6   Loss :  50977.0530801
Iteration :  7   Loss :  49974.8983951
Iteration :  8   Loss :  50977.0530801
Iteration :  9   Loss :  49974.8983951
Iteration :  10   Loss :  50977.0530801
Iteration :  11   Loss :  49974.8983951
Iteration :  12   Loss :  50977.0530801
Iteration :  13   Loss :  49974.8983951
Iteration :  14   Loss :  50977.0530801
Iteration :  15   Loss :  49974.8983951
Iteration :  16   Loss :  50977.0530801
Iteration :  17   Loss :  49974.8983951
Iteration :  18   Loss :  50977.0530801
Iteration :  19   Loss :  49974.8983951
Iteration :  20   Loss :  50977.0530801
Iteration :  21   Loss :  49974.8983951
Iteration :  22   Loss :  50977.0530801
Iteration :  23   Loss :  49974.8983951
Iteration :  24   Loss :  50977.0530801
Iteration :  25   Loss :  49974.8983951
Iteration :  26   Loss :  50977.0530801
Iteration :  27   Loss :  49974.8983951
Iteration :  28   Loss :  50977.0530801
Iteration :  29   Loss :  49974.8983951
Iteration :  30   Loss :  50977.0530801
Iteration :  31   Loss :  49974.8983951
Iteration :  32   Loss :  50977.0530801
Iteration :  33   Loss :  49974.8983951
Iteration :  34   Loss :  50977.0530801
Iteration :  35   Loss :  49974.8983951
Iteration :  36   Loss :  50977.0530801
Iteration :  37   Loss :  49974.8983951
Iteration :  38   Loss :  50977.0530801
Iteration :  39   Loss :  49974.8983951
Iteration :  40   Loss :  50977.0530801
Iteration :  41   Loss :  49974.8983951
Iteration :  42   Loss :  50977.0530801
Iteration :  43   Loss :  49974.8983951
Iteration :  44   Loss :  50977.0530801
Iteration :  45   Loss :  49974.8983951
Iteration :  46   Loss :  50977.0530801
Iteration :  47   Loss :  49974.8983951
Iteration :  48   Loss :  50977.0530801
Iteration :  49   Loss :  49974.8983951
Iteration :  50   Loss :  50977.0530801
Iteration :  51   Loss :  49974.8983951
Iteration :  52   Loss :  50977.0530801
Iteration :  53   Loss :  49974.8983951
Iteration :  54   Loss :  50977.0530801
Iteration :  55   Loss :  49974.8983951
Iteration :  56   Loss :  50977.0530801
Iteration :  57   Loss :  49974.8983951
Iteration :  58   Loss :  50977.0530801
Iteration :  59   Loss :  49974.8983951
Iteration :  60   Loss :  50977.0530801
Iteration :  61   Loss :  49974.8983951
Iteration :  62   Loss :  50977.0530801
Iteration :  63   Loss :  49974.8983951
Iteration :  64   Loss :  50977.0530801
Iteration :  65   Loss :  49974.8983951
Iteration :  66   Loss :  50977.0530801
Iteration :  67   Loss :  49974.8983951
Iteration :  68   Loss :  50977.0530801
Iteration :  69   Loss :  49974.8983951
Iteration :  70   Loss :  50977.0530801
Iteration :  71   Loss :  49974.8983951
Iteration :  72   Loss :  50977.0530801
Iteration :  73   Loss :  49974.8983951
Iteration :  74   Loss :  50977.0530801
Iteration :  75   Loss :  49974.8983951
Iteration :  76   Loss :  50977.0530801
Iteration :  77   Loss :  49974.8983951
Iteration :  78   Loss :  50977.0530801
Iteration :  79   Loss :  49974.8983951
Iteration :  80   Loss :  50977.0530801
Iteration :  81   Loss :  49974.8983951
Iteration :  82   Loss :  50977.0530801
Iteration :  83   Loss :  49974.8983951
Iteration :  84   Loss :  50977.0530801
Iteration :  85   Loss :  49974.8983951
Iteration :  86   Loss :  50977.0530801
Iteration :  87   Loss :  49974.8983951
Iteration :  88   Loss :  50977.0530801
Iteration :  89   Loss :  49974.8983951
Iteration :  90   Loss :  50977.0530801
Iteration :  91   Loss :  49974.8983951
Iteration :  92   Loss :  50977.0530801
Iteration :  93   Loss :  49974.8983951
Iteration :  94   Loss :  50977.0530801
Iteration :  95   Loss :  49974.8983951
Iteration :  96   Loss :  50977.0530801
Iteration :  97   Loss :  49974.8983951
Iteration :  98   Loss :  50977.0530801
Iteration :  99   Loss :  49974.8983951
[-0.31115166 -0.63016734 -0.15172092 ...,  0.28680905 -0.36557022
 -0.08298251]
CROSS VALIDATION 2
Iteration :  0   Loss :  49014.9845219
Iteration :  1   Loss :  43110.5583293
Iteration :  2   Loss :  52644.3724493
Iteration :  3   Loss :  64164.445251
Iteration :  4   Loss :  43125.4085939
Iteration :  5   Loss :  52644.461413
Iteration :  6   Loss :  64164.4465192
Iteration :  7   Loss :  43125.4085947
Iteration :  8   Loss :  52644.461413
Iteration :  9   Loss :  64164.4465192
Iteration :  10   Loss :  43125.4085947
Iteration :  11   Loss :  52644.461413
Iteration :  12   Loss :  64164.4465192
Iteration :  13   Loss :  43125.4085947
Iteration :  14   Loss :  52644.461413
Iteration :  15   Loss :  64164.4465192
Iteration :  16   Loss :  43125.4085947
Iteration :  17   Loss :  52644.461413
Iteration :  18   Loss :  64164.4465192
Iteration :  19   Loss :  43125.4085947
Iteration :  20   Loss :  52644.461413
Iteration :  21   Loss :  64164.4465192
Iteration :  22   Loss :  43125.4085947
Iteration :  23   Loss :  52644.461413
Iteration :  24   Loss :  64164.4465192
Iteration :  25   Loss :  43125.4085947
Iteration :  26   Loss :  52644.461413
Iteration :  27   Loss :  64164.4465192
Iteration :  28   Loss :  43125.4085947
Iteration :  29   Loss :  52644.461413
Iteration :  30   Loss :  64164.4465192
Iteration :  31   Loss :  43125.4085947
Iteration :  32   Loss :  52644.461413
Iteration :  33   Loss :  64164.4465192
Iteration :  34   Loss :  43125.4085947
Iteration :  35   Loss :  52644.461413
Iteration :  36   Loss :  64164.4465192
Iteration :  37   Loss :  43125.4085947
Iteration :  38   Loss :  52644.461413
Iteration :  39   Loss :  64164.4465192
Iteration :  40   Loss :  43125.4085947
Iteration :  41   Loss :  52644.461413
Iteration :  42   Loss :  64164.4465192
Iteration :  43   Loss :  43125.4085947
Iteration :  44   Loss :  52644.461413
Iteration :  45   Loss :  64164.4465192
Iteration :  46   Loss :  43125.4085947
Iteration :  47   Loss :  52644.461413
Iteration :  48   Loss :  64164.4465192
Iteration :  49   Loss :  43125.4085947
Iteration :  50   Loss :  52644.461413
Iteration :  51   Loss :  64164.4465192
Iteration :  52   Loss :  43125.4085947
Iteration :  53   Loss :  52644.461413
Iteration :  54   Loss :  64164.4465192
Iteration :  55   Loss :  43125.4085947
Iteration :  56   Loss :  52644.461413
Iteration :  57   Loss :  64164.4465192
Iteration :  58   Loss :  43125.4085947
Iteration :  59   Loss :  52644.461413
Iteration :  60   Loss :  64164.4465192
Iteration :  61   Loss :  43125.4085947
Iteration :  62   Loss :  52644.461413
Iteration :  63   Loss :  64164.4465192
Iteration :  64   Loss :  43125.4085947
Iteration :  65   Loss :  52644.461413
Iteration :  66   Loss :  64164.4465192
Iteration :  67   Loss :  43125.4085947
Iteration :  68   Loss :  52644.461413
Iteration :  69   Loss :  64164.4465192
Iteration :  70   Loss :  43125.4085947
Iteration :  71   Loss :  52644.461413
Iteration :  72   Loss :  64164.4465192
Iteration :  73   Loss :  43125.4085947
Iteration :  74   Loss :  52644.461413
Iteration :  75   Loss :  64164.4465192
Iteration :  76   Loss :  43125.4085947
Iteration :  77   Loss :  52644.461413
Iteration :  78   Loss :  64164.4465192
Iteration :  79   Loss :  43125.4085947
Iteration :  80   Loss :  52644.461413
Iteration :  81   Loss :  64164.4465192
Iteration :  82   Loss :  43125.4085947
Iteration :  83   Loss :  52644.461413
Iteration :  84   Loss :  64164.4465192
Iteration :  85   Loss :  43125.4085947
Iteration :  86   Loss :  52644.461413
Iteration :  87   Loss :  64164.4465192
Iteration :  88   Loss :  43125.4085947
Iteration :  89   Loss :  52644.461413
Iteration :  90   Loss :  64164.4465192
Iteration :  91   Loss :  43125.4085947
Iteration :  92   Loss :  52644.461413
Iteration :  93   Loss :  64164.4465192
Iteration :  94   Loss :  43125.4085947
Iteration :  95   Loss :  52644.461413
Iteration :  96   Loss :  64164.4465192
Iteration :  97   Loss :  43125.4085947
Iteration :  98   Loss :  52644.461413
Iteration :  99   Loss :  64164.4465192
[-0.13501362 -0.7088659  -0.00861614 ...,  0.31763534 -0.36545407
 -0.08217313]
CROSS VALIDATION 3
Iteration :  0   Loss :  51570.6713775
Iteration :  1   Loss :  49454.8569706
Iteration :  2   Loss :  50977.5824522
Iteration :  3   Loss :  49974.8910506
Iteration :  4   Loss :  50977.0530876
Iteration :  5   Loss :  49974.898395
Iteration :  6   Loss :  50977.0530801
Iteration :  7   Loss :  49974.8983951
Iteration :  8   Loss :  50977.0530801
Iteration :  9   Loss :  49974.8983951
Iteration :  10   Loss :  50977.0530801
Iteration :  11   Loss :  49974.8983951
Iteration :  12   Loss :  50977.0530801
Iteration :  13   Loss :  49974.8983951
Iteration :  14   Loss :  50977.0530801
Iteration :  15   Loss :  49974.8983951
Iteration :  16   Loss :  50977.0530801
Iteration :  17   Loss :  49974.8983951
Iteration :  18   Loss :  50977.0530801
Iteration :  19   Loss :  49974.8983951
Iteration :  20   Loss :  50977.0530801
Iteration :  21   Loss :  49974.8983951
Iteration :  22   Loss :  50977.0530801
Iteration :  23   Loss :  49974.8983951
Iteration :  24   Loss :  50977.0530801
Iteration :  25   Loss :  49974.8983951
Iteration :  26   Loss :  50977.0530801
Iteration :  27   Loss :  49974.8983951
Iteration :  28   Loss :  50977.0530801
Iteration :  29   Loss :  49974.8983951
Iteration :  30   Loss :  50977.0530801
Iteration :  31   Loss :  49974.8983951
Iteration :  32   Loss :  50977.0530801
Iteration :  33   Loss :  49974.8983951
Iteration :  34   Loss :  50977.0530801
Iteration :  35   Loss :  49974.8983951
Iteration :  36   Loss :  50977.0530801
Iteration :  37   Loss :  49974.8983951
Iteration :  38   Loss :  50977.0530801
Iteration :  39   Loss :  49974.8983951
Iteration :  40   Loss :  50977.0530801
Iteration :  41   Loss :  49974.8983951
Iteration :  42   Loss :  50977.0530801
Iteration :  43   Loss :  49974.8983951
Iteration :  44   Loss :  50977.0530801
Iteration :  45   Loss :  49974.8983951
Iteration :  46   Loss :  50977.0530801
Iteration :  47   Loss :  49974.8983951
Iteration :  48   Loss :  50977.0530801
Iteration :  49   Loss :  49974.8983951
Iteration :  50   Loss :  50977.0530801
Iteration :  51   Loss :  49974.8983951
Iteration :  52   Loss :  50977.0530801
Iteration :  53   Loss :  49974.8983951
Iteration :  54   Loss :  50977.0530801
Iteration :  55   Loss :  49974.8983951
Iteration :  56   Loss :  50977.0530801
Iteration :  57   Loss :  49974.8983951
Iteration :  58   Loss :  50977.0530801
Iteration :  59   Loss :  49974.8983951
Iteration :  60   Loss :  50977.0530801
Iteration :  61   Loss :  49974.8983951
Iteration :  62   Loss :  50977.0530801
Iteration :  63   Loss :  49974.8983951
Iteration :  64   Loss :  50977.0530801
Iteration :  65   Loss :  49974.8983951
Iteration :  66   Loss :  50977.0530801
Iteration :  67   Loss :  49974.8983951
Iteration :  68   Loss :  50977.0530801
Iteration :  69   Loss :  49974.8983951
Iteration :  70   Loss :  50977.0530801
Iteration :  71   Loss :  49974.8983951
Iteration :  72   Loss :  50977.0530801
Iteration :  73   Loss :  49974.8983951
Iteration :  74   Loss :  50977.0530801
Iteration :  75   Loss :  49974.8983951
Iteration :  76   Loss :  50977.0530801
Iteration :  77   Loss :  49974.8983951
Iteration :  78   Loss :  50977.0530801
Iteration :  79   Loss :  49974.8983951
Iteration :  80   Loss :  50977.0530801
Iteration :  81   Loss :  49974.8983951
Iteration :  82   Loss :  50977.0530801
Iteration :  83   Loss :  49974.8983951
Iteration :  84   Loss :  50977.0530801
Iteration :  85   Loss :  49974.8983951
Iteration :  86   Loss :  50977.0530801
Iteration :  87   Loss :  49974.8983951
Iteration :  88   Loss :  50977.0530801
Iteration :  89   Loss :  49974.8983951
Iteration :  90   Loss :  50977.0530801
Iteration :  91   Loss :  49974.8983951
Iteration :  92   Loss :  50977.0530801
Iteration :  93   Loss :  49974.8983951
Iteration :  94   Loss :  50977.0530801
Iteration :  95   Loss :  49974.8983951
Iteration :  96   Loss :  50977.0530801
Iteration :  97   Loss :  49974.8983951
Iteration :  98   Loss :  50977.0530801
Iteration :  99   Loss :  49974.8983951
[-0.31115166 -0.63016734 -0.15172092 ...,  0.28680905 -0.36557022
 -0.08298251]
CROSS VALIDATION 4
Iteration :  0   Loss :  51570.6713775
Iteration :  1   Loss :  49454.8569706
Iteration :  2   Loss :  50977.5824522
Iteration :  3   Loss :  49974.8910506
Iteration :  4   Loss :  50977.0530876
Iteration :  5   Loss :  49974.898395
Iteration :  6   Loss :  50977.0530801
Iteration :  7   Loss :  49974.8983951
Iteration :  8   Loss :  50977.0530801
Iteration :  9   Loss :  49974.8983951
Iteration :  10   Loss :  50977.0530801
Iteration :  11   Loss :  49974.8983951
Iteration :  12   Loss :  50977.0530801
Iteration :  13   Loss :  49974.8983951
Iteration :  14   Loss :  50977.0530801
Iteration :  15   Loss :  49974.8983951
Iteration :  16   Loss :  50977.0530801
Iteration :  17   Loss :  49974.8983951
Iteration :  18   Loss :  50977.0530801
Iteration :  19   Loss :  49974.8983951
Iteration :  20   Loss :  50977.0530801
Iteration :  21   Loss :  49974.8983951
Iteration :  22   Loss :  50977.0530801
Iteration :  23   Loss :  49974.8983951
Iteration :  24   Loss :  50977.0530801
Iteration :  25   Loss :  49974.8983951
Iteration :  26   Loss :  50977.0530801
Iteration :  27   Loss :  49974.8983951
Iteration :  28   Loss :  50977.0530801
Iteration :  29   Loss :  49974.8983951
Iteration :  30   Loss :  50977.0530801
Iteration :  31   Loss :  49974.8983951
Iteration :  32   Loss :  50977.0530801
Iteration :  33   Loss :  49974.8983951
Iteration :  34   Loss :  50977.0530801
Iteration :  35   Loss :  49974.8983951
Iteration :  36   Loss :  50977.0530801
Iteration :  37   Loss :  49974.8983951
Iteration :  38   Loss :  50977.0530801
Iteration :  39   Loss :  49974.8983951
Iteration :  40   Loss :  50977.0530801
Iteration :  41   Loss :  49974.8983951
Iteration :  42   Loss :  50977.0530801
Iteration :  43   Loss :  49974.8983951
Iteration :  44   Loss :  50977.0530801
Iteration :  45   Loss :  49974.8983951
Iteration :  46   Loss :  50977.0530801
Iteration :  47   Loss :  49974.8983951
Iteration :  48   Loss :  50977.0530801
Iteration :  49   Loss :  49974.8983951
Iteration :  50   Loss :  50977.0530801
Iteration :  51   Loss :  49974.8983951
Iteration :  52   Loss :  50977.0530801
Iteration :  53   Loss :  49974.8983951
Iteration :  54   Loss :  50977.0530801
Iteration :  55   Loss :  49974.8983951
Iteration :  56   Loss :  50977.0530801
Iteration :  57   Loss :  49974.8983951
Iteration :  58   Loss :  50977.0530801
Iteration :  59   Loss :  49974.8983951
Iteration :  60   Loss :  50977.0530801
Iteration :  61   Loss :  49974.8983951
Iteration :  62   Loss :  50977.0530801
Iteration :  63   Loss :  49974.8983951
Iteration :  64   Loss :  50977.0530801
Iteration :  65   Loss :  49974.8983951
Iteration :  66   Loss :  50977.0530801
Iteration :  67   Loss :  49974.8983951
Iteration :  68   Loss :  50977.0530801
Iteration :  69   Loss :  49974.8983951
Iteration :  70   Loss :  50977.0530801
Iteration :  71   Loss :  49974.8983951
Iteration :  72   Loss :  50977.0530801
Iteration :  73   Loss :  49974.8983951
Iteration :  74   Loss :  50977.0530801
Iteration :  75   Loss :  49974.8983951
Iteration :  76   Loss :  50977.0530801
Iteration :  77   Loss :  49974.8983951
Iteration :  78   Loss :  50977.0530801
Iteration :  79   Loss :  49974.8983951
Iteration :  80   Loss :  50977.0530801
Iteration :  81   Loss :  49974.8983951
Iteration :  82   Loss :  50977.0530801
Iteration :  83   Loss :  49974.8983951
Iteration :  84   Loss :  50977.0530801
Iteration :  85   Loss :  49974.8983951
Iteration :  86   Loss :  50977.0530801
Iteration :  87   Loss :  49974.8983951
Iteration :  88   Loss :  50977.0530801
Iteration :  89   Loss :  49974.8983951
Iteration :  90   Loss :  50977.0530801
Iteration :  91   Loss :  49974.8983951
Iteration :  92   Loss :  50977.0530801
Iteration :  93   Loss :  49974.8983951
Iteration :  94   Loss :  50977.0530801
Iteration :  95   Loss :  49974.8983951
Iteration :  96   Loss :  50977.0530801
Iteration :  97   Loss :  49974.8983951
Iteration :  98   Loss :  50977.0530801
Iteration :  99   Loss :  49974.8983951
[-0.31115166 -0.63016734 -0.15172092 ...,  0.28680905 -0.36557022
 -0.08298251]
CROSS VALIDATION 5
Iteration :  0   Loss :  51992.125708
Iteration :  1   Loss :  42888.0795886
Iteration :  2   Loss :  48138.6503763
Iteration :  3   Loss :  51352.7490119
Iteration :  4   Loss :  42886.3597048
Iteration :  5   Loss :  48138.624638
Iteration :  6   Loss :  51352.7489781
Iteration :  7   Loss :  42886.3597047
Iteration :  8   Loss :  48138.624638
Iteration :  9   Loss :  51352.7489781
Iteration :  10   Loss :  42886.3597047
Iteration :  11   Loss :  48138.624638
Iteration :  12   Loss :  51352.7489781
Iteration :  13   Loss :  42886.3597047
Iteration :  14   Loss :  48138.624638
Iteration :  15   Loss :  51352.7489781
Iteration :  16   Loss :  42886.3597047
Iteration :  17   Loss :  48138.624638
Iteration :  18   Loss :  51352.7489781
Iteration :  19   Loss :  42886.3597047
Iteration :  20   Loss :  48138.624638
Iteration :  21   Loss :  51352.7489781
Iteration :  22   Loss :  42886.3597047
Iteration :  23   Loss :  48138.624638
Iteration :  24   Loss :  51352.7489781
Iteration :  25   Loss :  42886.3597047
Iteration :  26   Loss :  48138.624638
Iteration :  27   Loss :  51352.7489781
Iteration :  28   Loss :  42886.3597047
Iteration :  29   Loss :  48138.624638
Iteration :  30   Loss :  51352.7489781
Iteration :  31   Loss :  42886.3597047
Iteration :  32   Loss :  48138.624638
Iteration :  33   Loss :  51352.7489781
Iteration :  34   Loss :  42886.3597047
Iteration :  35   Loss :  48138.624638
Iteration :  36   Loss :  51352.7489781
Iteration :  37   Loss :  42886.3597047
Iteration :  38   Loss :  48138.624638
Iteration :  39   Loss :  51352.7489781
Iteration :  40   Loss :  42886.3597047
Iteration :  41   Loss :  48138.624638
Iteration :  42   Loss :  51352.7489781
Iteration :  43   Loss :  42886.3597047
Iteration :  44   Loss :  48138.624638
Iteration :  45   Loss :  51352.7489781
Iteration :  46   Loss :  42886.3597047
Iteration :  47   Loss :  48138.624638
Iteration :  48   Loss :  51352.7489781
Iteration :  49   Loss :  42886.3597047
Iteration :  50   Loss :  48138.624638
Iteration :  51   Loss :  51352.7489781
Iteration :  52   Loss :  42886.3597047
Iteration :  53   Loss :  48138.624638
Iteration :  54   Loss :  51352.7489781
Iteration :  55   Loss :  42886.3597047
Iteration :  56   Loss :  48138.624638
Iteration :  57   Loss :  51352.7489781
Iteration :  58   Loss :  42886.3597047
Iteration :  59   Loss :  48138.624638
Iteration :  60   Loss :  51352.7489781
Iteration :  61   Loss :  42886.3597047
Iteration :  62   Loss :  48138.624638
Iteration :  63   Loss :  51352.7489781
Iteration :  64   Loss :  42886.3597047
Iteration :  65   Loss :  48138.624638
Iteration :  66   Loss :  51352.7489781
Iteration :  67   Loss :  42886.3597047
Iteration :  68   Loss :  48138.624638
Iteration :  69   Loss :  51352.7489781
Iteration :  70   Loss :  42886.3597047
Iteration :  71   Loss :  48138.624638
Iteration :  72   Loss :  51352.7489781
Iteration :  73   Loss :  42886.3597047
Iteration :  74   Loss :  48138.624638
Iteration :  75   Loss :  51352.7489781
Iteration :  76   Loss :  42886.3597047
Iteration :  77   Loss :  48138.624638
Iteration :  78   Loss :  51352.7489781
Iteration :  79   Loss :  42886.3597047
Iteration :  80   Loss :  48138.624638
Iteration :  81   Loss :  51352.7489781
Iteration :  82   Loss :  42886.3597047
Iteration :  83   Loss :  48138.624638
Iteration :  84   Loss :  51352.7489781
Iteration :  85   Loss :  42886.3597047
Iteration :  86   Loss :  48138.624638
Iteration :  87   Loss :  51352.7489781
Iteration :  88   Loss :  42886.3597047
Iteration :  89   Loss :  48138.624638
Iteration :  90   Loss :  51352.7489781
Iteration :  91   Loss :  42886.3597047
Iteration :  92   Loss :  48138.624638
Iteration :  93   Loss :  51352.7489781
Iteration :  94   Loss :  42886.3597047
Iteration :  95   Loss :  48138.624638
Iteration :  96   Loss :  51352.7489781
Iteration :  97   Loss :  42886.3597047
Iteration :  98   Loss :  48138.624638
Iteration :  99   Loss :  51352.7489781
[-0.2973228  -0.59251438 -0.32961844 ...,  0.41907904 -0.32830652
 -0.07344772]
CROSS VALIDATION 6
Iteration :  0   Loss :  51573.700238
Iteration :  1   Loss :  49454.4238076
Iteration :  2   Loss :  54389.6349264
Iteration :  3   Loss :  48028.2965102
Iteration :  4   Loss :  52658.5417264
Iteration :  5   Loss :  63874.8952222
Iteration :  6   Loss :  43011.8818698
Iteration :  7   Loss :  53363.2142099
Iteration :  8   Loss :  52091.8847087
Iteration :  9   Loss :  54971.8582877
Iteration :  10   Loss :  50899.3887112
Iteration :  11   Loss :  49974.0067522
Iteration :  12   Loss :  50981.5458909
Iteration :  13   Loss :  49974.4652143
Iteration :  14   Loss :  50981.547051
Iteration :  15   Loss :  49974.4652207
Iteration :  16   Loss :  50981.547051
Iteration :  17   Loss :  49974.4652207
Iteration :  18   Loss :  50981.547051
Iteration :  19   Loss :  49974.4652207
Iteration :  20   Loss :  50981.547051
Iteration :  21   Loss :  49974.4652207
Iteration :  22   Loss :  50981.547051
Iteration :  23   Loss :  49974.4652207
Iteration :  24   Loss :  50981.547051
Iteration :  25   Loss :  49974.4652207
Iteration :  26   Loss :  50981.547051
Iteration :  27   Loss :  49974.4652207
Iteration :  28   Loss :  50981.547051
Iteration :  29   Loss :  49974.4652207
Iteration :  30   Loss :  50981.547051
Iteration :  31   Loss :  49974.4652207
Iteration :  32   Loss :  50981.547051
Iteration :  33   Loss :  49974.4652207
Iteration :  34   Loss :  50981.547051
Iteration :  35   Loss :  49974.4652207
Iteration :  36   Loss :  50981.547051
Iteration :  37   Loss :  49974.4652207
Iteration :  38   Loss :  50981.547051
Iteration :  39   Loss :  49974.4652207
Iteration :  40   Loss :  50981.547051
Iteration :  41   Loss :  49974.4652207
Iteration :  42   Loss :  50981.547051
Iteration :  43   Loss :  49974.4652207
Iteration :  44   Loss :  50981.547051
Iteration :  45   Loss :  49974.4652207
Iteration :  46   Loss :  50981.547051
Iteration :  47   Loss :  49974.4652207
Iteration :  48   Loss :  50981.547051
Iteration :  49   Loss :  49974.4652207
Iteration :  50   Loss :  50981.547051
Iteration :  51   Loss :  49974.4652207
Iteration :  52   Loss :  50981.547051
Iteration :  53   Loss :  49974.4652207
Iteration :  54   Loss :  50981.547051
Iteration :  55   Loss :  49974.4652207
Iteration :  56   Loss :  50981.547051
Iteration :  57   Loss :  49974.4652207
Iteration :  58   Loss :  50981.547051
Iteration :  59   Loss :  49974.4652207
Iteration :  60   Loss :  50981.547051
Iteration :  61   Loss :  49974.4652207
Iteration :  62   Loss :  50981.547051
Iteration :  63   Loss :  49974.4652207
Iteration :  64   Loss :  50981.547051
Iteration :  65   Loss :  49974.4652207
Iteration :  66   Loss :  50981.547051
Iteration :  67   Loss :  49974.4652207
Iteration :  68   Loss :  50981.547051
Iteration :  69   Loss :  49974.4652207
Iteration :  70   Loss :  50981.547051
Iteration :  71   Loss :  49974.4652207
Iteration :  72   Loss :  50981.547051
Iteration :  73   Loss :  49974.4652207
Iteration :  74   Loss :  50981.547051
Iteration :  75   Loss :  49974.4652207
Iteration :  76   Loss :  50981.547051
Iteration :  77   Loss :  49974.4652207
Iteration :  78   Loss :  50981.547051
Iteration :  79   Loss :  49974.4652207
Iteration :  80   Loss :  50981.547051
Iteration :  81   Loss :  49974.4652207
Iteration :  82   Loss :  50981.547051
Iteration :  83   Loss :  49974.4652207
Iteration :  84   Loss :  50981.547051
Iteration :  85   Loss :  49974.4652207
Iteration :  86   Loss :  50981.547051
Iteration :  87   Loss :  49974.4652207
Iteration :  88   Loss :  50981.547051
Iteration :  89   Loss :  49974.4652207
Iteration :  90   Loss :  50981.547051
Iteration :  91   Loss :  49974.4652207
Iteration :  92   Loss :  50981.547051
Iteration :  93   Loss :  49974.4652207
Iteration :  94   Loss :  50981.547051
Iteration :  95   Loss :  49974.4652207
Iteration :  96   Loss :  50981.547051
Iteration :  97   Loss :  49974.4652207
Iteration :  98   Loss :  50981.547051
Iteration :  99   Loss :  49974.4652207
[-0.3111528  -0.63016737 -0.15172103 ...,  0.28681017 -0.36556882
 -0.08298277]
CROSS VALIDATION 7
Iteration :  0   Loss :  51573.700238
Iteration :  1   Loss :  49454.4238076
Iteration :  2   Loss :  54918.9247957
Iteration :  3   Loss :  48028.2965102
Iteration :  4   Loss :  52658.5417264
Iteration :  5   Loss :  64164.0640815
Iteration :  6   Loss :  43011.8818698
Iteration :  7   Loss :  53363.2142099
Iteration :  8   Loss :  52091.8847087
Iteration :  9   Loss :  55597.4995107
Iteration :  10   Loss :  50899.3887112
Iteration :  11   Loss :  49974.0067522
Iteration :  12   Loss :  50981.5458909
Iteration :  13   Loss :  49974.4652143
Iteration :  14   Loss :  50981.547051
Iteration :  15   Loss :  49974.4652207
Iteration :  16   Loss :  50981.547051
Iteration :  17   Loss :  49974.4652207
Iteration :  18   Loss :  50981.547051
Iteration :  19   Loss :  49974.4652207
Iteration :  20   Loss :  50981.547051
Iteration :  21   Loss :  49974.4652207
Iteration :  22   Loss :  50981.547051
Iteration :  23   Loss :  49974.4652207
Iteration :  24   Loss :  50981.547051
Iteration :  25   Loss :  49974.4652207
Iteration :  26   Loss :  50981.547051
Iteration :  27   Loss :  49974.4652207
Iteration :  28   Loss :  50981.547051
Iteration :  29   Loss :  49974.4652207
Iteration :  30   Loss :  50981.547051
Iteration :  31   Loss :  49974.4652207
Iteration :  32   Loss :  50981.547051
Iteration :  33   Loss :  49974.4652207
Iteration :  34   Loss :  50981.547051
Iteration :  35   Loss :  49974.4652207
Iteration :  36   Loss :  50981.547051
Iteration :  37   Loss :  49974.4652207
Iteration :  38   Loss :  50981.547051
Iteration :  39   Loss :  49974.4652207
Iteration :  40   Loss :  50981.547051
Iteration :  41   Loss :  49974.4652207
Iteration :  42   Loss :  50981.547051
Iteration :  43   Loss :  49974.4652207
Iteration :  44   Loss :  50981.547051
Iteration :  45   Loss :  49974.4652207
Iteration :  46   Loss :  50981.547051
Iteration :  47   Loss :  49974.4652207
Iteration :  48   Loss :  50981.547051
Iteration :  49   Loss :  49974.4652207
Iteration :  50   Loss :  50981.547051
Iteration :  51   Loss :  49974.4652207
Iteration :  52   Loss :  50981.547051
Iteration :  53   Loss :  49974.4652207
Iteration :  54   Loss :  50981.547051
Iteration :  55   Loss :  49974.4652207
Iteration :  56   Loss :  50981.547051
Iteration :  57   Loss :  49974.4652207
Iteration :  58   Loss :  50981.547051
Iteration :  59   Loss :  49974.4652207
Iteration :  60   Loss :  50981.547051
Iteration :  61   Loss :  49974.4652207
Iteration :  62   Loss :  50981.547051
Iteration :  63   Loss :  49974.4652207
Iteration :  64   Loss :  50981.547051
Iteration :  65   Loss :  49974.4652207
Iteration :  66   Loss :  50981.547051
Iteration :  67   Loss :  49974.4652207
Iteration :  68   Loss :  50981.547051
Iteration :  69   Loss :  49974.4652207
Iteration :  70   Loss :  50981.547051
Iteration :  71   Loss :  49974.4652207
Iteration :  72   Loss :  50981.547051
Iteration :  73   Loss :  49974.4652207
Iteration :  74   Loss :  50981.547051
Iteration :  75   Loss :  49974.4652207
Iteration :  76   Loss :  50981.547051
Iteration :  77   Loss :  49974.4652207
Iteration :  78   Loss :  50981.547051
Iteration :  79   Loss :  49974.4652207
Iteration :  80   Loss :  50981.547051
Iteration :  81   Loss :  49974.4652207
Iteration :  82   Loss :  50981.547051
Iteration :  83   Loss :  49974.4652207
Iteration :  84   Loss :  50981.547051
Iteration :  85   Loss :  49974.4652207
Iteration :  86   Loss :  50981.547051
Iteration :  87   Loss :  49974.4652207
Iteration :  88   Loss :  50981.547051
Iteration :  89   Loss :  49974.4652207
Iteration :  90   Loss :  50981.547051
Iteration :  91   Loss :  49974.4652207
Iteration :  92   Loss :  50981.547051
Iteration :  93   Loss :  49974.4652207
Iteration :  94   Loss :  50981.547051
Iteration :  95   Loss :  49974.4652207
Iteration :  96   Loss :  50981.547051
Iteration :  97   Loss :  49974.4652207
Iteration :  98   Loss :  50981.547051
Iteration :  99   Loss :  49974.4652207
[-0.3111528  -0.63016737 -0.15172103 ...,  0.28681017 -0.36556882
 -0.08298277]
CROSS VALIDATION 8
Iteration :  0   Loss :  51573.700238
Iteration :  1   Loss :  48396.1831581
Iteration :  2   Loss :  29766.4902387
Iteration :  3   Loss :  55840.1110644
Iteration :  4   Loss :  48033.9881885
Iteration :  5   Loss :  52652.7917175
Iteration :  6   Loss :  48398.5215302
Iteration :  7   Loss :  29766.4929855
Iteration :  8   Loss :  55840.1111528
Iteration :  9   Loss :  48033.9881888
Iteration :  10   Loss :  52652.7917175
Iteration :  11   Loss :  48398.5215302
Iteration :  12   Loss :  29766.4929855
Iteration :  13   Loss :  55840.1111528
Iteration :  14   Loss :  48033.9881888
Iteration :  15   Loss :  52652.7917175
Iteration :  16   Loss :  48398.5215302
Iteration :  17   Loss :  29766.4929855
Iteration :  18   Loss :  55840.1111528
Iteration :  19   Loss :  48033.9881888
Iteration :  20   Loss :  52652.7917175
Iteration :  21   Loss :  48398.5215302
Iteration :  22   Loss :  29766.4929855
Iteration :  23   Loss :  55840.1111528
Iteration :  24   Loss :  48033.9881888
Iteration :  25   Loss :  52652.7917175
Iteration :  26   Loss :  48398.5215302
Iteration :  27   Loss :  29766.4929855
Iteration :  28   Loss :  55840.1111528
Iteration :  29   Loss :  48033.9881888
Iteration :  30   Loss :  52652.7917175
Iteration :  31   Loss :  48398.5215302
Iteration :  32   Loss :  29766.4929855
Iteration :  33   Loss :  55840.1111528
Iteration :  34   Loss :  48033.9881888
Iteration :  35   Loss :  52652.7917175
Iteration :  36   Loss :  48398.5215302
Iteration :  37   Loss :  29766.4929855
Iteration :  38   Loss :  55840.1111528
Iteration :  39   Loss :  48033.9881888
Iteration :  40   Loss :  52652.7917175
Iteration :  41   Loss :  48398.5215302
Iteration :  42   Loss :  29766.4929855
Iteration :  43   Loss :  55840.1111528
Iteration :  44   Loss :  48033.9881888
Iteration :  45   Loss :  52652.7917175
Iteration :  46   Loss :  48398.5215302
Iteration :  47   Loss :  29766.4929855
Iteration :  48   Loss :  55840.1111528
Iteration :  49   Loss :  48033.9881888
Iteration :  50   Loss :  52652.7917175
Iteration :  51   Loss :  48398.5215302
Iteration :  52   Loss :  29766.4929855
Iteration :  53   Loss :  55840.1111528
Iteration :  54   Loss :  48033.9881888
Iteration :  55   Loss :  52652.7917175
Iteration :  56   Loss :  48398.5215302
Iteration :  57   Loss :  29766.4929855
Iteration :  58   Loss :  55840.1111528
Iteration :  59   Loss :  48033.9881888
Iteration :  60   Loss :  52652.7917175
Iteration :  61   Loss :  48398.5215302
Iteration :  62   Loss :  29766.4929855
Iteration :  63   Loss :  55840.1111528
Iteration :  64   Loss :  48033.9881888
Iteration :  65   Loss :  52652.7917175
Iteration :  66   Loss :  48398.5215302
Iteration :  67   Loss :  29766.4929855
Iteration :  68   Loss :  55840.1111528
Iteration :  69   Loss :  48033.9881888
Iteration :  70   Loss :  52652.7917175
Iteration :  71   Loss :  48398.5215302
Iteration :  72   Loss :  29766.4929855
Iteration :  73   Loss :  55840.1111528
Iteration :  74   Loss :  48033.9881888
Iteration :  75   Loss :  52652.7917175
Iteration :  76   Loss :  48398.5215302
Iteration :  77   Loss :  29766.4929855
Iteration :  78   Loss :  55840.1111528
Iteration :  79   Loss :  48033.9881888
Iteration :  80   Loss :  52652.7917175
Iteration :  81   Loss :  48398.5215302
Iteration :  82   Loss :  29766.4929855
Iteration :  83   Loss :  55840.1111528
Iteration :  84   Loss :  48033.9881888
Iteration :  85   Loss :  52652.7917175
Iteration :  86   Loss :  48398.5215302
Iteration :  87   Loss :  29766.4929855
Iteration :  88   Loss :  55840.1111528
Iteration :  89   Loss :  48033.9881888
Iteration :  90   Loss :  52652.7917175
Iteration :  91   Loss :  48398.5215302
Iteration :  92   Loss :  29766.4929855
Iteration :  93   Loss :  55840.1111528
Iteration :  94   Loss :  48033.9881888
Iteration :  95   Loss :  52652.7917175
Iteration :  96   Loss :  48398.5215302
Iteration :  97   Loss :  29766.4929855
Iteration :  98   Loss :  55840.1111528
Iteration :  99   Loss :  48033.9881888
[-0.09556723 -0.67910091  0.06299094 ...,  0.29349306 -0.29374166
 -0.06798057]
CROSS VALIDATION 9
Iteration :  0   Loss :  51478.2590396
Iteration :  1   Loss :  49257.3663963
Iteration :  2   Loss :  52150.519556
Iteration :  3   Loss :  45777.9683309
Iteration :  4   Loss :  52500.1328444
Iteration :  5   Loss :  39390.8393171
Iteration :  6   Loss :  40757.0765401
Iteration :  7   Loss :  53078.3450116
Iteration :  8   Loss :  51001.462574
Iteration :  9   Loss :  40784.5402622
Iteration :  10   Loss :  53078.6520548
Iteration :  11   Loss :  51001.4653449
Iteration :  12   Loss :  40784.5402637
Iteration :  13   Loss :  53078.6520548
Iteration :  14   Loss :  51001.4653449
Iteration :  15   Loss :  40784.5402637
Iteration :  16   Loss :  53078.6520548
Iteration :  17   Loss :  51001.4653449
Iteration :  18   Loss :  40784.5402637
Iteration :  19   Loss :  53078.6520548
Iteration :  20   Loss :  51001.4653449
Iteration :  21   Loss :  40784.5402637
Iteration :  22   Loss :  53078.6520548
Iteration :  23   Loss :  51001.4653449
Iteration :  24   Loss :  40784.5402637
Iteration :  25   Loss :  53078.6520548
Iteration :  26   Loss :  51001.4653449
Iteration :  27   Loss :  40784.5402637
Iteration :  28   Loss :  53078.6520548
Iteration :  29   Loss :  51001.4653449
Iteration :  30   Loss :  40784.5402637
Iteration :  31   Loss :  53078.6520548
Iteration :  32   Loss :  51001.4653449
Iteration :  33   Loss :  40784.5402637
Iteration :  34   Loss :  53078.6520548
Iteration :  35   Loss :  51001.4653449
Iteration :  36   Loss :  40784.5402637
Iteration :  37   Loss :  53078.6520548
Iteration :  38   Loss :  51001.4653449
Iteration :  39   Loss :  40784.5402637
Iteration :  40   Loss :  53078.6520548
Iteration :  41   Loss :  51001.4653449
Iteration :  42   Loss :  40784.5402637
Iteration :  43   Loss :  53078.6520548
Iteration :  44   Loss :  51001.4653449
Iteration :  45   Loss :  40784.5402637
Iteration :  46   Loss :  53078.6520548
Iteration :  47   Loss :  51001.4653449
Iteration :  48   Loss :  40784.5402637
Iteration :  49   Loss :  53078.6520548
Iteration :  50   Loss :  51001.4653449
Iteration :  51   Loss :  40784.5402637
Iteration :  52   Loss :  53078.6520548
Iteration :  53   Loss :  51001.4653449
Iteration :  54   Loss :  40784.5402637
Iteration :  55   Loss :  53078.6520548
Iteration :  56   Loss :  51001.4653449
Iteration :  57   Loss :  40784.5402637
Iteration :  58   Loss :  53078.6520548
Iteration :  59   Loss :  51001.4653449
Iteration :  60   Loss :  40784.5402637
Iteration :  61   Loss :  53078.6520548
Iteration :  62   Loss :  51001.4653449
Iteration :  63   Loss :  40784.5402637
Iteration :  64   Loss :  53078.6520548
Iteration :  65   Loss :  51001.4653449
Iteration :  66   Loss :  40784.5402637
Iteration :  67   Loss :  53078.6520548
Iteration :  68   Loss :  51001.4653449
Iteration :  69   Loss :  40784.5402637
Iteration :  70   Loss :  53078.6520548
Iteration :  71   Loss :  51001.4653449
Iteration :  72   Loss :  40784.5402637
Iteration :  73   Loss :  53078.6520548
Iteration :  74   Loss :  51001.4653449
Iteration :  75   Loss :  40784.5402637
Iteration :  76   Loss :  53078.6520548
Iteration :  77   Loss :  51001.4653449
Iteration :  78   Loss :  40784.5402637
Iteration :  79   Loss :  53078.6520548
Iteration :  80   Loss :  51001.4653449
Iteration :  81   Loss :  40784.5402637
Iteration :  82   Loss :  53078.6520548
Iteration :  83   Loss :  51001.4653449
Iteration :  84   Loss :  40784.5402637
Iteration :  85   Loss :  53078.6520548
Iteration :  86   Loss :  51001.4653449
Iteration :  87   Loss :  40784.5402637
Iteration :  88   Loss :  53078.6520548
Iteration :  89   Loss :  51001.4653449
Iteration :  90   Loss :  40784.5402637
Iteration :  91   Loss :  53078.6520548
Iteration :  92   Loss :  51001.4653449
Iteration :  93   Loss :  40784.5402637
Iteration :  94   Loss :  53078.6520548
Iteration :  95   Loss :  51001.4653449
Iteration :  96   Loss :  40784.5402637
Iteration :  97   Loss :  53078.6520548
Iteration :  98   Loss :  51001.4653449
Iteration :  99   Loss :  40784.5402637
[-0.10053556 -0.6775059   0.0633752  ...,  0.28805486 -0.30498031
 -0.06549271]
CROSS VALIDATION 10
Iteration :  0   Loss :  51743.5585703
Iteration :  1   Loss :  48886.8053877
Iteration :  2   Loss :  52056.9583094
Iteration :  3   Loss :  50809.4133636
Iteration :  4   Loss :  63814.723285
Iteration :  5   Loss :  50887.8747074
Iteration :  6   Loss :  63815.2825062
Iteration :  7   Loss :  50887.8758152
Iteration :  8   Loss :  63815.2825141
Iteration :  9   Loss :  50887.8758152
Iteration :  10   Loss :  63815.2825141
Iteration :  11   Loss :  50887.8758152
Iteration :  12   Loss :  63815.2825141
Iteration :  13   Loss :  50887.8758152
Iteration :  14   Loss :  63815.2825141
Iteration :  15   Loss :  50887.8758152
Iteration :  16   Loss :  63815.2825141
Iteration :  17   Loss :  50887.8758152
Iteration :  18   Loss :  63815.2825141
Iteration :  19   Loss :  50887.8758152
Iteration :  20   Loss :  63815.2825141
Iteration :  21   Loss :  50887.8758152
Iteration :  22   Loss :  63815.2825141
Iteration :  23   Loss :  50887.8758152
Iteration :  24   Loss :  63815.2825141
Iteration :  25   Loss :  50887.8758152
Iteration :  26   Loss :  63815.2825141
Iteration :  27   Loss :  50887.8758152
Iteration :  28   Loss :  63815.2825141
Iteration :  29   Loss :  50887.8758152
Iteration :  30   Loss :  63815.2825141
Iteration :  31   Loss :  50887.8758152
Iteration :  32   Loss :  63815.2825141
Iteration :  33   Loss :  50887.8758152
Iteration :  34   Loss :  63815.2825141
Iteration :  35   Loss :  50887.8758152
Iteration :  36   Loss :  63815.2825141
Iteration :  37   Loss :  50887.8758152
Iteration :  38   Loss :  63815.2825141
Iteration :  39   Loss :  50887.8758152
Iteration :  40   Loss :  63815.2825141
Iteration :  41   Loss :  50887.8758152
Iteration :  42   Loss :  63815.2825141
Iteration :  43   Loss :  50887.8758152
Iteration :  44   Loss :  63815.2825141
Iteration :  45   Loss :  50887.8758152
Iteration :  46   Loss :  63815.2825141
Iteration :  47   Loss :  50887.8758152
Iteration :  48   Loss :  63815.2825141
Iteration :  49   Loss :  50887.8758152
Iteration :  50   Loss :  63815.2825141
Iteration :  51   Loss :  50887.8758152
Iteration :  52   Loss :  63815.2825141
Iteration :  53   Loss :  50887.8758152
Iteration :  54   Loss :  63815.2825141
Iteration :  55   Loss :  50887.8758152
Iteration :  56   Loss :  63815.2825141
Iteration :  57   Loss :  50887.8758152
Iteration :  58   Loss :  63815.2825141
Iteration :  59   Loss :  50887.8758152
Iteration :  60   Loss :  63815.2825141
Iteration :  61   Loss :  50887.8758152
Iteration :  62   Loss :  63815.2825141
Iteration :  63   Loss :  50887.8758152
Iteration :  64   Loss :  63815.2825141
Iteration :  65   Loss :  50887.8758152
Iteration :  66   Loss :  63815.2825141
Iteration :  67   Loss :  50887.8758152
Iteration :  68   Loss :  63815.2825141
Iteration :  69   Loss :  50887.8758152
Iteration :  70   Loss :  63815.2825141
Iteration :  71   Loss :  50887.8758152
Iteration :  72   Loss :  63815.2825141
Iteration :  73   Loss :  50887.8758152
Iteration :  74   Loss :  63815.2825141
Iteration :  75   Loss :  50887.8758152
Iteration :  76   Loss :  63815.2825141
Iteration :  77   Loss :  50887.8758152
Iteration :  78   Loss :  63815.2825141
Iteration :  79   Loss :  50887.8758152
Iteration :  80   Loss :  63815.2825141
Iteration :  81   Loss :  50887.8758152
Iteration :  82   Loss :  63815.2825141
Iteration :  83   Loss :  50887.8758152
Iteration :  84   Loss :  63815.2825141
Iteration :  85   Loss :  50887.8758152
Iteration :  86   Loss :  63815.2825141
Iteration :  87   Loss :  50887.8758152
Iteration :  88   Loss :  63815.2825141
Iteration :  89   Loss :  50887.8758152
Iteration :  90   Loss :  63815.2825141
Iteration :  91   Loss :  50887.8758152
Iteration :  92   Loss :  63815.2825141
Iteration :  93   Loss :  50887.8758152
Iteration :  94   Loss :  63815.2825141
Iteration :  95   Loss :  50887.8758152
Iteration :  96   Loss :  63815.2825141
Iteration :  97   Loss :  50887.8758152
Iteration :  98   Loss :  63815.2825141
Iteration :  99   Loss :  50887.8758152
[-0.29590781 -0.61099638 -0.3259207  ...,  0.29186245 -0.36937185
 -0.08399387]
CROSS VALIDATION 11
Iteration :  0   Loss :  51554.942279
Iteration :  1   Loss :  49394.9908622
Iteration :  2   Loss :  50945.1728958
Iteration :  3   Loss :  49972.7960515
Iteration :  4   Loss :  50944.5849794
Iteration :  5   Loss :  49972.8042123
Iteration :  6   Loss :  50944.5849711
Iteration :  7   Loss :  49972.8042124
Iteration :  8   Loss :  50944.5849711
Iteration :  9   Loss :  49972.8042124
Iteration :  10   Loss :  50944.5849711
Iteration :  11   Loss :  49972.8042124
Iteration :  12   Loss :  50944.5849711
Iteration :  13   Loss :  49972.8042124
Iteration :  14   Loss :  50944.5849711
Iteration :  15   Loss :  49972.8042124
Iteration :  16   Loss :  50944.5849711
Iteration :  17   Loss :  49972.8042124
Iteration :  18   Loss :  50944.5849711
Iteration :  19   Loss :  49972.8042124
Iteration :  20   Loss :  50944.5849711
Iteration :  21   Loss :  49972.8042124
Iteration :  22   Loss :  50944.5849711
Iteration :  23   Loss :  49972.8042124
Iteration :  24   Loss :  50944.5849711
Iteration :  25   Loss :  49972.8042124
Iteration :  26   Loss :  50944.5849711
Iteration :  27   Loss :  49972.8042124
Iteration :  28   Loss :  50944.5849711
Iteration :  29   Loss :  49972.8042124
Iteration :  30   Loss :  50944.5849711
Iteration :  31   Loss :  49972.8042124
Iteration :  32   Loss :  50944.5849711
Iteration :  33   Loss :  49972.8042124
Iteration :  34   Loss :  50944.5849711
Iteration :  35   Loss :  49972.8042124
Iteration :  36   Loss :  50944.5849711
Iteration :  37   Loss :  49972.8042124
Iteration :  38   Loss :  50944.5849711
Iteration :  39   Loss :  49972.8042124
Iteration :  40   Loss :  50944.5849711
Iteration :  41   Loss :  49972.8042124
Iteration :  42   Loss :  50944.5849711
Iteration :  43   Loss :  49972.8042124
Iteration :  44   Loss :  50944.5849711
Iteration :  45   Loss :  49972.8042124
Iteration :  46   Loss :  50944.5849711
Iteration :  47   Loss :  49972.8042124
Iteration :  48   Loss :  50944.5849711
Iteration :  49   Loss :  49972.8042124
Iteration :  50   Loss :  50944.5849711
Iteration :  51   Loss :  49972.8042124
Iteration :  52   Loss :  50944.5849711
Iteration :  53   Loss :  49972.8042124
Iteration :  54   Loss :  50944.5849711
Iteration :  55   Loss :  49972.8042124
Iteration :  56   Loss :  50944.5849711
Iteration :  57   Loss :  49972.8042124
Iteration :  58   Loss :  50944.5849711
Iteration :  59   Loss :  49972.8042124
Iteration :  60   Loss :  50944.5849711
Iteration :  61   Loss :  49972.8042124
Iteration :  62   Loss :  50944.5849711
Iteration :  63   Loss :  49972.8042124
Iteration :  64   Loss :  50944.5849711
Iteration :  65   Loss :  49972.8042124
Iteration :  66   Loss :  50944.5849711
Iteration :  67   Loss :  49972.8042124
Iteration :  68   Loss :  50944.5849711
Iteration :  69   Loss :  49972.8042124
Iteration :  70   Loss :  50944.5849711
Iteration :  71   Loss :  49972.8042124
Iteration :  72   Loss :  50944.5849711
Iteration :  73   Loss :  49972.8042124
Iteration :  74   Loss :  50944.5849711
Iteration :  75   Loss :  49972.8042124
Iteration :  76   Loss :  50944.5849711
Iteration :  77   Loss :  49972.8042124
Iteration :  78   Loss :  50944.5849711
Iteration :  79   Loss :  49972.8042124
Iteration :  80   Loss :  50944.5849711
Iteration :  81   Loss :  49972.8042124
Iteration :  82   Loss :  50944.5849711
Iteration :  83   Loss :  49972.8042124
Iteration :  84   Loss :  50944.5849711
Iteration :  85   Loss :  49972.8042124
Iteration :  86   Loss :  50944.5849711
Iteration :  87   Loss :  49972.8042124
Iteration :  88   Loss :  50944.5849711
Iteration :  89   Loss :  49972.8042124
Iteration :  90   Loss :  50944.5849711
Iteration :  91   Loss :  49972.8042124
Iteration :  92   Loss :  50944.5849711
Iteration :  93   Loss :  49972.8042124
Iteration :  94   Loss :  50944.5849711
Iteration :  95   Loss :  49972.8042124
Iteration :  96   Loss :  50944.5849711
Iteration :  97   Loss :  49972.8042124
Iteration :  98   Loss :  50944.5849711
Iteration :  99   Loss :  49972.8042124
[-0.31115533 -0.63016913 -0.15172334 ...,  0.28681052 -0.36556679
 -0.08298286]
CROSS VALIDATION 12
Iteration :  0   Loss :  54597.0862364
Iteration :  1   Loss :  48216.3813044
Iteration :  2   Loss :  52219.9718719
Iteration :  3   Loss :  44282.1500514
Iteration :  4   Loss :  53870.0921814
Iteration :  5   Loss :  39741.1709738
Iteration :  6   Loss :  49857.3721076
Iteration :  7   Loss :  49548.4397798
Iteration :  8   Loss :  49952.8470008
Iteration :  9   Loss :  46052.5303653
Iteration :  10   Loss :  52250.7695373
Iteration :  11   Loss :  53974.3947934
Iteration :  12   Loss :  39741.1599635
Iteration :  13   Loss :  49857.3736019
Iteration :  14   Loss :  49548.4397852
Iteration :  15   Loss :  49952.8470008
Iteration :  16   Loss :  46052.5303653
Iteration :  17   Loss :  52250.7695373
Iteration :  18   Loss :  53974.3947934
Iteration :  19   Loss :  39741.1599635
Iteration :  20   Loss :  49857.3736019
Iteration :  21   Loss :  49548.4397852
Iteration :  22   Loss :  49952.8470008
Iteration :  23   Loss :  46052.5303653
Iteration :  24   Loss :  52250.7695373
Iteration :  25   Loss :  53974.3947934
Iteration :  26   Loss :  39741.1599635
Iteration :  27   Loss :  49857.3736019
Iteration :  28   Loss :  49548.4397852
Iteration :  29   Loss :  49952.8470008
Iteration :  30   Loss :  46052.5303653
Iteration :  31   Loss :  52250.7695373
Iteration :  32   Loss :  53974.3947934
Iteration :  33   Loss :  39741.1599635
Iteration :  34   Loss :  49857.3736019
Iteration :  35   Loss :  49548.4397852
Iteration :  36   Loss :  49952.8470008
Iteration :  37   Loss :  46052.5303653
Iteration :  38   Loss :  52250.7695373
Iteration :  39   Loss :  53974.3947934
Iteration :  40   Loss :  39741.1599635
Iteration :  41   Loss :  49857.3736019
Iteration :  42   Loss :  49548.4397852
Iteration :  43   Loss :  49952.8470008
Iteration :  44   Loss :  46052.5303653
Iteration :  45   Loss :  52250.7695373
Iteration :  46   Loss :  53974.3947934
Iteration :  47   Loss :  39741.1599635
Iteration :  48   Loss :  49857.3736019
Iteration :  49   Loss :  49548.4397852
Iteration :  50   Loss :  49952.8470008
Iteration :  51   Loss :  46052.5303653
Iteration :  52   Loss :  52250.7695373
Iteration :  53   Loss :  53974.3947934
Iteration :  54   Loss :  39741.1599635
Iteration :  55   Loss :  49857.3736019
Iteration :  56   Loss :  49548.4397852
Iteration :  57   Loss :  49952.8470008
Iteration :  58   Loss :  46052.5303653
Iteration :  59   Loss :  52250.7695373
Iteration :  60   Loss :  53974.3947934
Iteration :  61   Loss :  39741.1599635
Iteration :  62   Loss :  49857.3736019
Iteration :  63   Loss :  49548.4397852
Iteration :  64   Loss :  49952.8470008
Iteration :  65   Loss :  46052.5303653
Iteration :  66   Loss :  52250.7695373
Iteration :  67   Loss :  53974.3947934
Iteration :  68   Loss :  39741.1599635
Iteration :  69   Loss :  49857.3736019
Iteration :  70   Loss :  49548.4397852
Iteration :  71   Loss :  49952.8470008
Iteration :  72   Loss :  46052.5303653
Iteration :  73   Loss :  52250.7695373
Iteration :  74   Loss :  53974.3947934
Iteration :  75   Loss :  39741.1599635
Iteration :  76   Loss :  49857.3736019
Iteration :  77   Loss :  49548.4397852
Iteration :  78   Loss :  49952.8470008
Iteration :  79   Loss :  46052.5303653
Iteration :  80   Loss :  52250.7695373
Iteration :  81   Loss :  53974.3947934
Iteration :  82   Loss :  39741.1599635
Iteration :  83   Loss :  49857.3736019
Iteration :  84   Loss :  49548.4397852
Iteration :  85   Loss :  49952.8470008
Iteration :  86   Loss :  46052.5303653
Iteration :  87   Loss :  52250.7695373
Iteration :  88   Loss :  53974.3947934
Iteration :  89   Loss :  39741.1599635
Iteration :  90   Loss :  49857.3736019
Iteration :  91   Loss :  49548.4397852
Iteration :  92   Loss :  49952.8470008
Iteration :  93   Loss :  46052.5303653
Iteration :  94   Loss :  52250.7695373
Iteration :  95   Loss :  53974.3947934
Iteration :  96   Loss :  39741.1599635
Iteration :  97   Loss :  49857.3736019
Iteration :  98   Loss :  49548.4397852
Iteration :  99   Loss :  49952.8470008
[-0.36475968 -0.67267726 -0.17013492 ...,  0.29555386 -0.36792473
 -0.09533654]
CROSS VALIDATION 13
Iteration :  0   Loss :  39448.8670954
Iteration :  1   Loss :  52469.681944
Iteration :  2   Loss :  55183.5289333
Iteration :  3   Loss :  52576.4035065
Iteration :  4   Loss :  55184.0579479
Iteration :  5   Loss :  52576.405013
Iteration :  6   Loss :  55184.0579554
Iteration :  7   Loss :  52576.405013
Iteration :  8   Loss :  55184.0579554
Iteration :  9   Loss :  52576.405013
Iteration :  10   Loss :  55184.0579554
Iteration :  11   Loss :  52576.405013
Iteration :  12   Loss :  55184.0579554
Iteration :  13   Loss :  52576.405013
Iteration :  14   Loss :  55184.0579554
Iteration :  15   Loss :  52576.405013
Iteration :  16   Loss :  55184.0579554
Iteration :  17   Loss :  52576.405013
Iteration :  18   Loss :  55184.0579554
Iteration :  19   Loss :  52576.405013
Iteration :  20   Loss :  55184.0579554
Iteration :  21   Loss :  52576.405013
Iteration :  22   Loss :  55184.0579554
Iteration :  23   Loss :  52576.405013
Iteration :  24   Loss :  55184.0579554
Iteration :  25   Loss :  52576.405013
Iteration :  26   Loss :  55184.0579554
Iteration :  27   Loss :  52576.405013
Iteration :  28   Loss :  55184.0579554
Iteration :  29   Loss :  52576.405013
Iteration :  30   Loss :  55184.0579554
Iteration :  31   Loss :  52576.405013
Iteration :  32   Loss :  55184.0579554
Iteration :  33   Loss :  52576.405013
Iteration :  34   Loss :  55184.0579554
Iteration :  35   Loss :  52576.405013
Iteration :  36   Loss :  55184.0579554
Iteration :  37   Loss :  52576.405013
Iteration :  38   Loss :  55184.0579554
Iteration :  39   Loss :  52576.405013
Iteration :  40   Loss :  55184.0579554
Iteration :  41   Loss :  52576.405013
Iteration :  42   Loss :  55184.0579554
Iteration :  43   Loss :  52576.405013
Iteration :  44   Loss :  55184.0579554
Iteration :  45   Loss :  52576.405013
Iteration :  46   Loss :  55184.0579554
Iteration :  47   Loss :  52576.405013
Iteration :  48   Loss :  55184.0579554
Iteration :  49   Loss :  52576.405013
Iteration :  50   Loss :  55184.0579554
Iteration :  51   Loss :  52576.405013
Iteration :  52   Loss :  55184.0579554
Iteration :  53   Loss :  52576.405013
Iteration :  54   Loss :  55184.0579554
Iteration :  55   Loss :  52576.405013
Iteration :  56   Loss :  55184.0579554
Iteration :  57   Loss :  52576.405013
Iteration :  58   Loss :  55184.0579554
Iteration :  59   Loss :  52576.405013
Iteration :  60   Loss :  55184.0579554
Iteration :  61   Loss :  52576.405013
Iteration :  62   Loss :  55184.0579554
Iteration :  63   Loss :  52576.405013
Iteration :  64   Loss :  55184.0579554
Iteration :  65   Loss :  52576.405013
Iteration :  66   Loss :  55184.0579554
Iteration :  67   Loss :  52576.405013
Iteration :  68   Loss :  55184.0579554
Iteration :  69   Loss :  52576.405013
Iteration :  70   Loss :  55184.0579554
Iteration :  71   Loss :  52576.405013
Iteration :  72   Loss :  55184.0579554
Iteration :  73   Loss :  52576.405013
Iteration :  74   Loss :  55184.0579554
Iteration :  75   Loss :  52576.405013
Iteration :  76   Loss :  55184.0579554
Iteration :  77   Loss :  52576.405013
Iteration :  78   Loss :  55184.0579554
Iteration :  79   Loss :  52576.405013
Iteration :  80   Loss :  55184.0579554
Iteration :  81   Loss :  52576.405013
Iteration :  82   Loss :  55184.0579554
Iteration :  83   Loss :  52576.405013
Iteration :  84   Loss :  55184.0579554
Iteration :  85   Loss :  52576.405013
Iteration :  86   Loss :  55184.0579554
Iteration :  87   Loss :  52576.405013
Iteration :  88   Loss :  55184.0579554
Iteration :  89   Loss :  52576.405013
Iteration :  90   Loss :  55184.0579554
Iteration :  91   Loss :  52576.405013
Iteration :  92   Loss :  55184.0579554
Iteration :  93   Loss :  52576.405013
Iteration :  94   Loss :  55184.0579554
Iteration :  95   Loss :  52576.405013
Iteration :  96   Loss :  55184.0579554
Iteration :  97   Loss :  52576.405013
Iteration :  98   Loss :  55184.0579554
Iteration :  99   Loss :  52576.405013
[-0.24824445 -0.82223049  0.01798484 ...,  0.20000122 -0.13369987
 -0.05586802]
CROSS VALIDATION 14
Iteration :  0   Loss :  51293.7383679
Iteration :  1   Loss :  49148.2212897
Iteration :  2   Loss :  54892.079456
Iteration :  3   Loss :  42327.7725142
Iteration :  4   Loss :  52416.8856904
Iteration :  5   Loss :  62459.6651272
Iteration :  6   Loss :  42305.0565622
Iteration :  7   Loss :  52417.107862
Iteration :  8   Loss :  62459.6670174
Iteration :  9   Loss :  42305.056561
Iteration :  10   Loss :  52417.107862
Iteration :  11   Loss :  62459.6670174
Iteration :  12   Loss :  42305.056561
Iteration :  13   Loss :  52417.107862
Iteration :  14   Loss :  62459.6670174
Iteration :  15   Loss :  42305.056561
Iteration :  16   Loss :  52417.107862
Iteration :  17   Loss :  62459.6670174
Iteration :  18   Loss :  42305.056561
Iteration :  19   Loss :  52417.107862
Iteration :  20   Loss :  62459.6670174
Iteration :  21   Loss :  42305.056561
Iteration :  22   Loss :  52417.107862
Iteration :  23   Loss :  62459.6670174
Iteration :  24   Loss :  42305.056561
Iteration :  25   Loss :  52417.107862
Iteration :  26   Loss :  62459.6670174
Iteration :  27   Loss :  42305.056561
Iteration :  28   Loss :  52417.107862
Iteration :  29   Loss :  62459.6670174
Iteration :  30   Loss :  42305.056561
Iteration :  31   Loss :  52417.107862
Iteration :  32   Loss :  62459.6670174
Iteration :  33   Loss :  42305.056561
Iteration :  34   Loss :  52417.107862
Iteration :  35   Loss :  62459.6670174
Iteration :  36   Loss :  42305.056561
Iteration :  37   Loss :  52417.107862
Iteration :  38   Loss :  62459.6670174
Iteration :  39   Loss :  42305.056561
Iteration :  40   Loss :  52417.107862
Iteration :  41   Loss :  62459.6670174
Iteration :  42   Loss :  42305.056561
Iteration :  43   Loss :  52417.107862
Iteration :  44   Loss :  62459.6670174
Iteration :  45   Loss :  42305.056561
Iteration :  46   Loss :  52417.107862
Iteration :  47   Loss :  62459.6670174
Iteration :  48   Loss :  42305.056561
Iteration :  49   Loss :  52417.107862
Iteration :  50   Loss :  62459.6670174
Iteration :  51   Loss :  42305.056561
Iteration :  52   Loss :  52417.107862
Iteration :  53   Loss :  62459.6670174
Iteration :  54   Loss :  42305.056561
Iteration :  55   Loss :  52417.107862
Iteration :  56   Loss :  62459.6670174
Iteration :  57   Loss :  42305.056561
Iteration :  58   Loss :  52417.107862
Iteration :  59   Loss :  62459.6670174
Iteration :  60   Loss :  42305.056561
Iteration :  61   Loss :  52417.107862
Iteration :  62   Loss :  62459.6670174
Iteration :  63   Loss :  42305.056561
Iteration :  64   Loss :  52417.107862
Iteration :  65   Loss :  62459.6670174
Iteration :  66   Loss :  42305.056561
Iteration :  67   Loss :  52417.107862
Iteration :  68   Loss :  62459.6670174
Iteration :  69   Loss :  42305.056561
Iteration :  70   Loss :  52417.107862
Iteration :  71   Loss :  62459.6670174
Iteration :  72   Loss :  42305.056561
Iteration :  73   Loss :  52417.107862
Iteration :  74   Loss :  62459.6670174
Iteration :  75   Loss :  42305.056561
Iteration :  76   Loss :  52417.107862
Iteration :  77   Loss :  62459.6670174
Iteration :  78   Loss :  42305.056561
Iteration :  79   Loss :  52417.107862
Iteration :  80   Loss :  62459.6670174
Iteration :  81   Loss :  42305.056561
Iteration :  82   Loss :  52417.107862
Iteration :  83   Loss :  62459.6670174
Iteration :  84   Loss :  42305.056561
Iteration :  85   Loss :  52417.107862
Iteration :  86   Loss :  62459.6670174
Iteration :  87   Loss :  42305.056561
Iteration :  88   Loss :  52417.107862
Iteration :  89   Loss :  62459.6670174
Iteration :  90   Loss :  42305.056561
Iteration :  91   Loss :  52417.107862
Iteration :  92   Loss :  62459.6670174
Iteration :  93   Loss :  42305.056561
Iteration :  94   Loss :  52417.107862
Iteration :  95   Loss :  62459.6670174
Iteration :  96   Loss :  42305.056561
Iteration :  97   Loss :  52417.107862
Iteration :  98   Loss :  62459.6670174
Iteration :  99   Loss :  42305.056561
[-0.09964065 -0.67701973  0.06303158 ...,  0.28834496 -0.30450195
 -0.06527291]
CROSS VALIDATION 15
Iteration :  0   Loss :  51293.7383679
Iteration :  1   Loss :  49148.2212897
Iteration :  2   Loss :  56868.9404269
Iteration :  3   Loss :  42786.9813798
Iteration :  4   Loss :  52416.8856904
Iteration :  5   Loss :  63774.842621
Iteration :  6   Loss :  42761.7794155
Iteration :  7   Loss :  52417.107862
Iteration :  8   Loss :  63774.844476
Iteration :  9   Loss :  42761.7794141
Iteration :  10   Loss :  52417.107862
Iteration :  11   Loss :  63774.844476
Iteration :  12   Loss :  42761.7794141
Iteration :  13   Loss :  52417.107862
Iteration :  14   Loss :  63774.844476
Iteration :  15   Loss :  42761.7794141
Iteration :  16   Loss :  52417.107862
Iteration :  17   Loss :  63774.844476
Iteration :  18   Loss :  42761.7794141
Iteration :  19   Loss :  52417.107862
Iteration :  20   Loss :  63774.844476
Iteration :  21   Loss :  42761.7794141
Iteration :  22   Loss :  52417.107862
Iteration :  23   Loss :  63774.844476
Iteration :  24   Loss :  42761.7794141
Iteration :  25   Loss :  52417.107862
Iteration :  26   Loss :  63774.844476
Iteration :  27   Loss :  42761.7794141
Iteration :  28   Loss :  52417.107862
Iteration :  29   Loss :  63774.844476
Iteration :  30   Loss :  42761.7794141
Iteration :  31   Loss :  52417.107862
Iteration :  32   Loss :  63774.844476
Iteration :  33   Loss :  42761.7794141
Iteration :  34   Loss :  52417.107862
Iteration :  35   Loss :  63774.844476
Iteration :  36   Loss :  42761.7794141
Iteration :  37   Loss :  52417.107862
Iteration :  38   Loss :  63774.844476
Iteration :  39   Loss :  42761.7794141
Iteration :  40   Loss :  52417.107862
Iteration :  41   Loss :  63774.844476
Iteration :  42   Loss :  42761.7794141
Iteration :  43   Loss :  52417.107862
Iteration :  44   Loss :  63774.844476
Iteration :  45   Loss :  42761.7794141
Iteration :  46   Loss :  52417.107862
Iteration :  47   Loss :  63774.844476
Iteration :  48   Loss :  42761.7794141
Iteration :  49   Loss :  52417.107862
Iteration :  50   Loss :  63774.844476
Iteration :  51   Loss :  42761.7794141
Iteration :  52   Loss :  52417.107862
Iteration :  53   Loss :  63774.844476
Iteration :  54   Loss :  42761.7794141
Iteration :  55   Loss :  52417.107862
Iteration :  56   Loss :  63774.844476
Iteration :  57   Loss :  42761.7794141
Iteration :  58   Loss :  52417.107862
Iteration :  59   Loss :  63774.844476
Iteration :  60   Loss :  42761.7794141
Iteration :  61   Loss :  52417.107862
Iteration :  62   Loss :  63774.844476
Iteration :  63   Loss :  42761.7794141
Iteration :  64   Loss :  52417.107862
Iteration :  65   Loss :  63774.844476
Iteration :  66   Loss :  42761.7794141
Iteration :  67   Loss :  52417.107862
Iteration :  68   Loss :  63774.844476
Iteration :  69   Loss :  42761.7794141
Iteration :  70   Loss :  52417.107862
Iteration :  71   Loss :  63774.844476
Iteration :  72   Loss :  42761.7794141
Iteration :  73   Loss :  52417.107862
Iteration :  74   Loss :  63774.844476
Iteration :  75   Loss :  42761.7794141
Iteration :  76   Loss :  52417.107862
Iteration :  77   Loss :  63774.844476
Iteration :  78   Loss :  42761.7794141
Iteration :  79   Loss :  52417.107862
Iteration :  80   Loss :  63774.844476
Iteration :  81   Loss :  42761.7794141
Iteration :  82   Loss :  52417.107862
Iteration :  83   Loss :  63774.844476
Iteration :  84   Loss :  42761.7794141
Iteration :  85   Loss :  52417.107862
Iteration :  86   Loss :  63774.844476
Iteration :  87   Loss :  42761.7794141
Iteration :  88   Loss :  52417.107862
Iteration :  89   Loss :  63774.844476
Iteration :  90   Loss :  42761.7794141
Iteration :  91   Loss :  52417.107862
Iteration :  92   Loss :  63774.844476
Iteration :  93   Loss :  42761.7794141
Iteration :  94   Loss :  52417.107862
Iteration :  95   Loss :  63774.844476
Iteration :  96   Loss :  42761.7794141
Iteration :  97   Loss :  52417.107862
Iteration :  98   Loss :  63774.844476
Iteration :  99   Loss :  42761.7794141
[-0.09964065 -0.67701973  0.06303158 ...,  0.28834496 -0.30450195
 -0.06527291]
CROSS VALIDATION 16
Iteration :  0   Loss :  51041.2561892
Iteration :  1   Loss :  49148.2212897
Iteration :  2   Loss :  53916.5305203
Iteration :  3   Loss :  42434.3341877
Iteration :  4   Loss :  52416.9461027
Iteration :  5   Loss :  63774.8437851
Iteration :  6   Loss :  42455.2587652
Iteration :  7   Loss :  52417.107862
Iteration :  8   Loss :  63774.844476
Iteration :  9   Loss :  42455.2587663
Iteration :  10   Loss :  52417.107862
Iteration :  11   Loss :  63774.844476
Iteration :  12   Loss :  42455.2587663
Iteration :  13   Loss :  52417.107862
Iteration :  14   Loss :  63774.844476
Iteration :  15   Loss :  42455.2587663
Iteration :  16   Loss :  52417.107862
Iteration :  17   Loss :  63774.844476
Iteration :  18   Loss :  42455.2587663
Iteration :  19   Loss :  52417.107862
Iteration :  20   Loss :  63774.844476
Iteration :  21   Loss :  42455.2587663
Iteration :  22   Loss :  52417.107862
Iteration :  23   Loss :  63774.844476
Iteration :  24   Loss :  42455.2587663
Iteration :  25   Loss :  52417.107862
Iteration :  26   Loss :  63774.844476
Iteration :  27   Loss :  42455.2587663
Iteration :  28   Loss :  52417.107862
Iteration :  29   Loss :  63774.844476
Iteration :  30   Loss :  42455.2587663
Iteration :  31   Loss :  52417.107862
Iteration :  32   Loss :  63774.844476
Iteration :  33   Loss :  42455.2587663
Iteration :  34   Loss :  52417.107862
Iteration :  35   Loss :  63774.844476
Iteration :  36   Loss :  42455.2587663
Iteration :  37   Loss :  52417.107862
Iteration :  38   Loss :  63774.844476
Iteration :  39   Loss :  42455.2587663
Iteration :  40   Loss :  52417.107862
Iteration :  41   Loss :  63774.844476
Iteration :  42   Loss :  42455.2587663
Iteration :  43   Loss :  52417.107862
Iteration :  44   Loss :  63774.844476
Iteration :  45   Loss :  42455.2587663
Iteration :  46   Loss :  52417.107862
Iteration :  47   Loss :  63774.844476
Iteration :  48   Loss :  42455.2587663
Iteration :  49   Loss :  52417.107862
Iteration :  50   Loss :  63774.844476
Iteration :  51   Loss :  42455.2587663
Iteration :  52   Loss :  52417.107862
Iteration :  53   Loss :  63774.844476
Iteration :  54   Loss :  42455.2587663
Iteration :  55   Loss :  52417.107862
Iteration :  56   Loss :  63774.844476
Iteration :  57   Loss :  42455.2587663
Iteration :  58   Loss :  52417.107862
Iteration :  59   Loss :  63774.844476
Iteration :  60   Loss :  42455.2587663
Iteration :  61   Loss :  52417.107862
Iteration :  62   Loss :  63774.844476
Iteration :  63   Loss :  42455.2587663
Iteration :  64   Loss :  52417.107862
Iteration :  65   Loss :  63774.844476
Iteration :  66   Loss :  42455.2587663
Iteration :  67   Loss :  52417.107862
Iteration :  68   Loss :  63774.844476
Iteration :  69   Loss :  42455.2587663
Iteration :  70   Loss :  52417.107862
Iteration :  71   Loss :  63774.844476
Iteration :  72   Loss :  42455.2587663
Iteration :  73   Loss :  52417.107862
Iteration :  74   Loss :  63774.844476
Iteration :  75   Loss :  42455.2587663
Iteration :  76   Loss :  52417.107862
Iteration :  77   Loss :  63774.844476
Iteration :  78   Loss :  42455.2587663
Iteration :  79   Loss :  52417.107862
Iteration :  80   Loss :  63774.844476
Iteration :  81   Loss :  42455.2587663
Iteration :  82   Loss :  52417.107862
Iteration :  83   Loss :  63774.844476
Iteration :  84   Loss :  42455.2587663
Iteration :  85   Loss :  52417.107862
Iteration :  86   Loss :  63774.844476
Iteration :  87   Loss :  42455.2587663
Iteration :  88   Loss :  52417.107862
Iteration :  89   Loss :  63774.844476
Iteration :  90   Loss :  42455.2587663
Iteration :  91   Loss :  52417.107862
Iteration :  92   Loss :  63774.844476
Iteration :  93   Loss :  42455.2587663
Iteration :  94   Loss :  52417.107862
Iteration :  95   Loss :  63774.844476
Iteration :  96   Loss :  42455.2587663
Iteration :  97   Loss :  52417.107862
Iteration :  98   Loss :  63774.844476
Iteration :  99   Loss :  42455.2587663
[-0.09964065 -0.67701973  0.06303158 ...,  0.28834496 -0.30450195
 -0.06527291]
CROSS VALIDATION 17
Iteration :  0   Loss :  51293.7383679
Iteration :  1   Loss :  62734.4356543
Iteration :  2   Loss :  48901.1509558
Iteration :  3   Loss :  50252.0144486
Iteration :  4   Loss :  60037.9909067
Iteration :  5   Loss :  52509.9059194
Iteration :  6   Loss :  60010.8475183
Iteration :  7   Loss :  52509.973877
Iteration :  8   Loss :  60010.8471352
Iteration :  9   Loss :  52509.973878
Iteration :  10   Loss :  60010.8471352
Iteration :  11   Loss :  52509.973878
Iteration :  12   Loss :  60010.8471352
Iteration :  13   Loss :  52509.973878
Iteration :  14   Loss :  60010.8471352
Iteration :  15   Loss :  52509.973878
Iteration :  16   Loss :  60010.8471352
Iteration :  17   Loss :  52509.973878
Iteration :  18   Loss :  60010.8471352
Iteration :  19   Loss :  52509.973878
Iteration :  20   Loss :  60010.8471352
Iteration :  21   Loss :  52509.973878
Iteration :  22   Loss :  60010.8471352
Iteration :  23   Loss :  52509.973878
Iteration :  24   Loss :  60010.8471352
Iteration :  25   Loss :  52509.973878
Iteration :  26   Loss :  60010.8471352
Iteration :  27   Loss :  52509.973878
Iteration :  28   Loss :  60010.8471352
Iteration :  29   Loss :  52509.973878
Iteration :  30   Loss :  60010.8471352
Iteration :  31   Loss :  52509.973878
Iteration :  32   Loss :  60010.8471352
Iteration :  33   Loss :  52509.973878
Iteration :  34   Loss :  60010.8471352
Iteration :  35   Loss :  52509.973878
Iteration :  36   Loss :  60010.8471352
Iteration :  37   Loss :  52509.973878
Iteration :  38   Loss :  60010.8471352
Iteration :  39   Loss :  52509.973878
Iteration :  40   Loss :  60010.8471352
Iteration :  41   Loss :  52509.973878
Iteration :  42   Loss :  60010.8471352
Iteration :  43   Loss :  52509.973878
Iteration :  44   Loss :  60010.8471352
Iteration :  45   Loss :  52509.973878
Iteration :  46   Loss :  60010.8471352
Iteration :  47   Loss :  52509.973878
Iteration :  48   Loss :  60010.8471352
Iteration :  49   Loss :  52509.973878
Iteration :  50   Loss :  60010.8471352
Iteration :  51   Loss :  52509.973878
Iteration :  52   Loss :  60010.8471352
Iteration :  53   Loss :  52509.973878
Iteration :  54   Loss :  60010.8471352
Iteration :  55   Loss :  52509.973878
Iteration :  56   Loss :  60010.8471352
Iteration :  57   Loss :  52509.973878
Iteration :  58   Loss :  60010.8471352
Iteration :  59   Loss :  52509.973878
Iteration :  60   Loss :  60010.8471352
Iteration :  61   Loss :  52509.973878
Iteration :  62   Loss :  60010.8471352
Iteration :  63   Loss :  52509.973878
Iteration :  64   Loss :  60010.8471352
Iteration :  65   Loss :  52509.973878
Iteration :  66   Loss :  60010.8471352
Iteration :  67   Loss :  52509.973878
Iteration :  68   Loss :  60010.8471352
Iteration :  69   Loss :  52509.973878
Iteration :  70   Loss :  60010.8471352
Iteration :  71   Loss :  52509.973878
Iteration :  72   Loss :  60010.8471352
Iteration :  73   Loss :  52509.973878
Iteration :  74   Loss :  60010.8471352
Iteration :  75   Loss :  52509.973878
Iteration :  76   Loss :  60010.8471352
Iteration :  77   Loss :  52509.973878
Iteration :  78   Loss :  60010.8471352
Iteration :  79   Loss :  52509.973878
Iteration :  80   Loss :  60010.8471352
Iteration :  81   Loss :  52509.973878
Iteration :  82   Loss :  60010.8471352
Iteration :  83   Loss :  52509.973878
Iteration :  84   Loss :  60010.8471352
Iteration :  85   Loss :  52509.973878
Iteration :  86   Loss :  60010.8471352
Iteration :  87   Loss :  52509.973878
Iteration :  88   Loss :  60010.8471352
Iteration :  89   Loss :  52509.973878
Iteration :  90   Loss :  60010.8471352
Iteration :  91   Loss :  52509.973878
Iteration :  92   Loss :  60010.8471352
Iteration :  93   Loss :  52509.973878
Iteration :  94   Loss :  60010.8471352
Iteration :  95   Loss :  52509.973878
Iteration :  96   Loss :  60010.8471352
Iteration :  97   Loss :  52509.973878
Iteration :  98   Loss :  60010.8471352
Iteration :  99   Loss :  52509.973878
[-0.31322638 -0.55907428 -0.33654473 ...,  0.40837776 -0.32064959
 -0.06322333]
CROSS VALIDATION 18
Iteration :  0   Loss :  38228.6525041
Iteration :  1   Loss :  52299.3304631
Iteration :  2   Loss :  42008.4434058
Iteration :  3   Loss :  38900.786257
Iteration :  4   Loss :  45992.4600849
Iteration :  5   Loss :  47434.2699512
Iteration :  6   Loss :  45607.2882983
Iteration :  7   Loss :  38947.7378231
Iteration :  8   Loss :  38708.657345
Iteration :  9   Loss :  38839.3577408
Iteration :  10   Loss :  38708.7674968
Iteration :  11   Loss :  38839.356211
Iteration :  12   Loss :  38708.7674983
Iteration :  13   Loss :  38839.3562109
Iteration :  14   Loss :  38708.7674983
Iteration :  15   Loss :  38839.3562109
Iteration :  16   Loss :  38708.7674983
Iteration :  17   Loss :  38839.3562109
Iteration :  18   Loss :  38708.7674983
Iteration :  19   Loss :  38839.3562109
Iteration :  20   Loss :  38708.7674983
Iteration :  21   Loss :  38839.3562109
Iteration :  22   Loss :  38708.7674983
Iteration :  23   Loss :  38839.3562109
Iteration :  24   Loss :  38708.7674983
Iteration :  25   Loss :  38839.3562109
Iteration :  26   Loss :  38708.7674983
Iteration :  27   Loss :  38839.3562109
Iteration :  28   Loss :  38708.7674983
Iteration :  29   Loss :  38839.3562109
Iteration :  30   Loss :  38708.7674983
Iteration :  31   Loss :  38839.3562109
Iteration :  32   Loss :  38708.7674983
Iteration :  33   Loss :  38839.3562109
Iteration :  34   Loss :  38708.7674983
Iteration :  35   Loss :  38839.3562109
Iteration :  36   Loss :  38708.7674983
Iteration :  37   Loss :  38839.3562109
Iteration :  38   Loss :  38708.7674983
Iteration :  39   Loss :  38839.3562109
Iteration :  40   Loss :  38708.7674983
Iteration :  41   Loss :  38839.3562109
Iteration :  42   Loss :  38708.7674983
Iteration :  43   Loss :  38839.3562109
Iteration :  44   Loss :  38708.7674983
Iteration :  45   Loss :  38839.3562109
Iteration :  46   Loss :  38708.7674983
Iteration :  47   Loss :  38839.3562109
Iteration :  48   Loss :  38708.7674983
Iteration :  49   Loss :  38839.3562109
Iteration :  50   Loss :  38708.7674983
Iteration :  51   Loss :  38839.3562109
Iteration :  52   Loss :  38708.7674983
Iteration :  53   Loss :  38839.3562109
Iteration :  54   Loss :  38708.7674983
Iteration :  55   Loss :  38839.3562109
Iteration :  56   Loss :  38708.7674983
Iteration :  57   Loss :  38839.3562109
Iteration :  58   Loss :  38708.7674983
Iteration :  59   Loss :  38839.3562109
Iteration :  60   Loss :  38708.7674983
Iteration :  61   Loss :  38839.3562109
Iteration :  62   Loss :  38708.7674983
Iteration :  63   Loss :  38839.3562109
Iteration :  64   Loss :  38708.7674983
Iteration :  65   Loss :  38839.3562109
Iteration :  66   Loss :  38708.7674983
Iteration :  67   Loss :  38839.3562109
Iteration :  68   Loss :  38708.7674983
Iteration :  69   Loss :  38839.3562109
Iteration :  70   Loss :  38708.7674983
Iteration :  71   Loss :  38839.3562109
Iteration :  72   Loss :  38708.7674983
Iteration :  73   Loss :  38839.3562109
Iteration :  74   Loss :  38708.7674983
Iteration :  75   Loss :  38839.3562109
Iteration :  76   Loss :  38708.7674983
Iteration :  77   Loss :  38839.3562109
Iteration :  78   Loss :  38708.7674983
Iteration :  79   Loss :  38839.3562109
Iteration :  80   Loss :  38708.7674983
Iteration :  81   Loss :  38839.3562109
Iteration :  82   Loss :  38708.7674983
Iteration :  83   Loss :  38839.3562109
Iteration :  84   Loss :  38708.7674983
Iteration :  85   Loss :  38839.3562109
Iteration :  86   Loss :  38708.7674983
Iteration :  87   Loss :  38839.3562109
Iteration :  88   Loss :  38708.7674983
Iteration :  89   Loss :  38839.3562109
Iteration :  90   Loss :  38708.7674983
Iteration :  91   Loss :  38839.3562109
Iteration :  92   Loss :  38708.7674983
Iteration :  93   Loss :  38839.3562109
Iteration :  94   Loss :  38708.7674983
Iteration :  95   Loss :  38839.3562109
Iteration :  96   Loss :  38708.7674983
Iteration :  97   Loss :  38839.3562109
Iteration :  98   Loss :  38708.7674983
Iteration :  99   Loss :  38839.3562109
[-0.31497133 -0.63340934 -0.15415896 ...,  0.28981594 -0.35852099
 -0.08430148]
CROSS VALIDATION 19
Iteration :  0   Loss :  55869.6359724
Iteration :  1   Loss :  50720.3975485
Iteration :  2   Loss :  51014.0804467
Iteration :  3   Loss :  42151.8161247
Iteration :  4   Loss :  51834.3473236
Iteration :  5   Loss :  63772.884795
Iteration :  6   Loss :  42190.8854015
Iteration :  7   Loss :  51833.9340055
Iteration :  8   Loss :  63772.88628
Iteration :  9   Loss :  42190.8854036
Iteration :  10   Loss :  51833.9340055
Iteration :  11   Loss :  63772.88628
Iteration :  12   Loss :  42190.8854036
Iteration :  13   Loss :  51833.9340055
Iteration :  14   Loss :  63772.88628
Iteration :  15   Loss :  42190.8854036
Iteration :  16   Loss :  51833.9340055
Iteration :  17   Loss :  63772.88628
Iteration :  18   Loss :  42190.8854036
Iteration :  19   Loss :  51833.9340055
Iteration :  20   Loss :  63772.88628
Iteration :  21   Loss :  42190.8854036
Iteration :  22   Loss :  51833.9340055
Iteration :  23   Loss :  63772.88628
Iteration :  24   Loss :  42190.8854036
Iteration :  25   Loss :  51833.9340055
Iteration :  26   Loss :  63772.88628
Iteration :  27   Loss :  42190.8854036
Iteration :  28   Loss :  51833.9340055
Iteration :  29   Loss :  63772.88628
Iteration :  30   Loss :  42190.8854036
Iteration :  31   Loss :  51833.9340055
Iteration :  32   Loss :  63772.88628
Iteration :  33   Loss :  42190.8854036
Iteration :  34   Loss :  51833.9340055
Iteration :  35   Loss :  63772.88628
Iteration :  36   Loss :  42190.8854036
Iteration :  37   Loss :  51833.9340055
Iteration :  38   Loss :  63772.88628
Iteration :  39   Loss :  42190.8854036
Iteration :  40   Loss :  51833.9340055
Iteration :  41   Loss :  63772.88628
Iteration :  42   Loss :  42190.8854036
Iteration :  43   Loss :  51833.9340055
Iteration :  44   Loss :  63772.88628
Iteration :  45   Loss :  42190.8854036
Iteration :  46   Loss :  51833.9340055
Iteration :  47   Loss :  63772.88628
Iteration :  48   Loss :  42190.8854036
Iteration :  49   Loss :  51833.9340055
Iteration :  50   Loss :  63772.88628
Iteration :  51   Loss :  42190.8854036
Iteration :  52   Loss :  51833.9340055
Iteration :  53   Loss :  63772.88628
Iteration :  54   Loss :  42190.8854036
Iteration :  55   Loss :  51833.9340055
Iteration :  56   Loss :  63772.88628
Iteration :  57   Loss :  42190.8854036
Iteration :  58   Loss :  51833.9340055
Iteration :  59   Loss :  63772.88628
Iteration :  60   Loss :  42190.8854036
Iteration :  61   Loss :  51833.9340055
Iteration :  62   Loss :  63772.88628
Iteration :  63   Loss :  42190.8854036
Iteration :  64   Loss :  51833.9340055
Iteration :  65   Loss :  63772.88628
Iteration :  66   Loss :  42190.8854036
Iteration :  67   Loss :  51833.9340055
Iteration :  68   Loss :  63772.88628
Iteration :  69   Loss :  42190.8854036
Iteration :  70   Loss :  51833.9340055
Iteration :  71   Loss :  63772.88628
Iteration :  72   Loss :  42190.8854036
Iteration :  73   Loss :  51833.9340055
Iteration :  74   Loss :  63772.88628
Iteration :  75   Loss :  42190.8854036
Iteration :  76   Loss :  51833.9340055
Iteration :  77   Loss :  63772.88628
Iteration :  78   Loss :  42190.8854036
Iteration :  79   Loss :  51833.9340055
Iteration :  80   Loss :  63772.88628
Iteration :  81   Loss :  42190.8854036
Iteration :  82   Loss :  51833.9340055
Iteration :  83   Loss :  63772.88628
Iteration :  84   Loss :  42190.8854036
Iteration :  85   Loss :  51833.9340055
Iteration :  86   Loss :  63772.88628
Iteration :  87   Loss :  42190.8854036
Iteration :  88   Loss :  51833.9340055
Iteration :  89   Loss :  63772.88628
Iteration :  90   Loss :  42190.8854036
Iteration :  91   Loss :  51833.9340055
Iteration :  92   Loss :  63772.88628
Iteration :  93   Loss :  42190.8854036
Iteration :  94   Loss :  51833.9340055
Iteration :  95   Loss :  63772.88628
Iteration :  96   Loss :  42190.8854036
Iteration :  97   Loss :  51833.9340055
Iteration :  98   Loss :  63772.88628
Iteration :  99   Loss :  42190.8854036
[-0.10041079 -0.67703446  0.06283983 ...,  0.28718438 -0.30481114
 -0.06474446]
Accuracy (Hinge Loss):	0.65
lmda : 1  eta : 0.01
Logistic Loss
CROSS VALIDATION 0
Iteration :  0   Loss :  2190.3204803
Iteration :  1   Loss :  394.832584314
Iteration :  2   Loss :  565.749878369
Iteration :  3   Loss :  35.2333208925
Iteration :  4   Loss :  4.1392113643
Iteration :  5   Loss :  0.487972345341
Iteration :  6   Loss :  5282.46263401
Iteration :  7   Loss :  555.676200573
Iteration :  8   Loss :  856.46388153
Iteration :  9   Loss :  4331.95035891
Iteration :  10   Loss :  1901.89492354
Iteration :  11   Loss :  2709.70157228
Iteration :  12   Loss :  1210.21211452
Iteration :  13   Loss :  1634.29487361
Iteration :  14   Loss :  2535.39793496
Iteration :  15   Loss :  525.630832558
Iteration :  16   Loss :  1323.620445
Iteration :  17   Loss :  4346.53331171
Iteration :  18   Loss :  2141.64463074
Iteration :  19   Loss :  460.387601829
Iteration :  20   Loss :  4971.09690108
Iteration :  21   Loss :  1330.23813296
Iteration :  22   Loss :  2683.00593773
Iteration :  23   Loss :  3682.89622354
Iteration :  24   Loss :  2425.44630954
Iteration :  25   Loss :  2264.88334539
Iteration :  26   Loss :  240.242483286
Iteration :  27   Loss :  28.2236930774
Iteration :  28   Loss :  3.32268142794
Iteration :  29   Loss :  0.427940124187
Iteration :  30   Loss :  3302.88929954
Iteration :  31   Loss :  988.730768104
Iteration :  32   Loss :  130.168527578
Iteration :  33   Loss :  15.2985344528
Iteration :  34   Loss :  1.81652711694
Iteration :  35   Loss :  0.278000475559
Iteration :  36   Loss :  6182.33350591
Iteration :  37   Loss :  985.414339742
Iteration :  38   Loss :  1335.97680139
Iteration :  39   Loss :  5547.10859448
Iteration :  40   Loss :  216.172702148
Iteration :  41   Loss :  26.2292743542
Iteration :  42   Loss :  4032.74600459
Iteration :  43   Loss :  1171.949623
Iteration :  44   Loss :  834.303655012
Iteration :  45   Loss :  1303.08224625
Iteration :  46   Loss :  5495.71842045
Iteration :  47   Loss :  598.95968169
Iteration :  48   Loss :  702.057608003
Iteration :  49   Loss :  3898.47629453
Iteration :  50   Loss :  1493.34455855
Iteration :  51   Loss :  2346.74661788
Iteration :  52   Loss :  2378.73576264
Iteration :  53   Loss :  1666.35245954
Iteration :  54   Loss :  569.029235033
Iteration :  55   Loss :  2384.08815107
Iteration :  56   Loss :  112.37595226
Iteration :  57   Loss :  13.2019294723
Iteration :  58   Loss :  1.55101643143
Iteration :  59   Loss :  0.197189254339
Iteration :  60   Loss :  5078.78929038
Iteration :  61   Loss :  3274.03224386
Iteration :  62   Loss :  205.080745529
Iteration :  63   Loss :  3494.36589631
Iteration :  64   Loss :  2174.94043367
Iteration :  65   Loss :  3480.41731995
Iteration :  66   Loss :  1369.43691609
Iteration :  67   Loss :  2335.0085916
Iteration :  68   Loss :  9090.12164101
Iteration :  69   Loss :  1555.10933229
Iteration :  70   Loss :  733.736334245
Iteration :  71   Loss :  3511.41041732
Iteration :  72   Loss :  1065.13347419
Iteration :  73   Loss :  1705.4817811
Iteration :  74   Loss :  3428.55032877
Iteration :  75   Loss :  952.69288833
Iteration :  76   Loss :  110.468820788
Iteration :  77   Loss :  12.9779050464
Iteration :  78   Loss :  1.58450898039
Iteration :  79   Loss :  4848.40694143
Iteration :  80   Loss :  524.596741562
Iteration :  81   Loss :  312.849544257
Iteration :  82   Loss :  36.7535717
Iteration :  83   Loss :  4.31819479803
Iteration :  84   Loss :  6561.27366863
Iteration :  85   Loss :  1655.8694908
Iteration :  86   Loss :  1261.47093427
Iteration :  87   Loss :  1191.2433829
Iteration :  88   Loss :  1287.17377555
Iteration :  89   Loss :  2108.97369693
Iteration :  90   Loss :  2832.99355245
Iteration :  91   Loss :  4056.83116381
Iteration :  92   Loss :  335.460898807
Iteration :  93   Loss :  3032.15688909
Iteration :  94   Loss :  3847.47325373
Iteration :  95   Loss :  4024.05177863
Iteration :  96   Loss :  276.238340968
Iteration :  97   Loss :  2109.06817886
Iteration :  98   Loss :  1925.66268348
Iteration :  99   Loss :  1801.45991884
[-0.09682417 -0.0391092  -0.08195338 ...,  0.0699524  -0.0756923
  0.01270601]
CROSS VALIDATION 1
Iteration :  0   Loss :  2736.11104953
Iteration :  1   Loss :  1199.86409404
Iteration :  2   Loss :  474.423782983
Iteration :  3   Loss :  4265.3193603
Iteration :  4   Loss :  1438.88160879
Iteration :  5   Loss :  740.121030505
Iteration :  6   Loss :  3130.26556232
Iteration :  7   Loss :  2390.67151922
Iteration :  8   Loss :  364.177817855
Iteration :  9   Loss :  2010.87664977
Iteration :  10   Loss :  374.450444892
Iteration :  11   Loss :  43.9904862357
Iteration :  12   Loss :  5.18865796137
Iteration :  13   Loss :  0.624881525835
Iteration :  14   Loss :  4564.28787704
Iteration :  15   Loss :  3245.54821061
Iteration :  16   Loss :  1347.4123201
Iteration :  17   Loss :  137.774192587
Iteration :  18   Loss :  1741.37574117
Iteration :  19   Loss :  637.178491834
Iteration :  20   Loss :  2585.97357379
Iteration :  21   Loss :  4039.46326538
Iteration :  22   Loss :  1877.0181035
Iteration :  23   Loss :  2176.14659882
Iteration :  24   Loss :  1416.81813536
Iteration :  25   Loss :  3080.45176501
Iteration :  26   Loss :  2138.2337999
Iteration :  27   Loss :  624.190519776
Iteration :  28   Loss :  1680.19361257
Iteration :  29   Loss :  2219.59051254
Iteration :  30   Loss :  1480.94376657
Iteration :  31   Loss :  1546.18685994
Iteration :  32   Loss :  130.142091318
Iteration :  33   Loss :  15.2896398064
Iteration :  34   Loss :  1.79636695653
Iteration :  35   Loss :  0.917834510407
Iteration :  36   Loss :  5219.20354397
Iteration :  37   Loss :  1804.99419294
Iteration :  38   Loss :  1036.70468723
Iteration :  39   Loss :  2469.31138921
Iteration :  40   Loss :  677.066583319
Iteration :  41   Loss :  1798.67077827
Iteration :  42   Loss :  199.513613222
Iteration :  43   Loss :  1997.55222064
Iteration :  44   Loss :  4470.16067993
Iteration :  45   Loss :  1682.67609598
Iteration :  46   Loss :  1722.17477661
Iteration :  47   Loss :  2718.31000883
Iteration :  48   Loss :  1275.19295047
Iteration :  49   Loss :  1924.52306163
Iteration :  50   Loss :  1386.12533766
Iteration :  51   Loss :  445.284387477
Iteration :  52   Loss :  1263.86288219
Iteration :  53   Loss :  1533.06600495
Iteration :  54   Loss :  1591.12052528
Iteration :  55   Loss :  2801.9914681
Iteration :  56   Loss :  6359.66275202
Iteration :  57   Loss :  741.502768212
Iteration :  58   Loss :  1337.7658711
Iteration :  59   Loss :  1687.83985625
Iteration :  60   Loss :  1381.674442
Iteration :  61   Loss :  808.073118058
Iteration :  62   Loss :  94.9324486119
Iteration :  63   Loss :  11.1536723378
Iteration :  64   Loss :  1.32033609622
Iteration :  65   Loss :  1.49393310985
Iteration :  66   Loss :  2222.54833145
Iteration :  67   Loss :  1652.45558366
Iteration :  68   Loss :  2555.02285374
Iteration :  69   Loss :  1609.81658701
Iteration :  70   Loss :  1490.9313095
Iteration :  71   Loss :  2291.07547334
Iteration :  72   Loss :  2611.84206551
Iteration :  73   Loss :  2356.41807886
Iteration :  74   Loss :  1824.76936923
Iteration :  75   Loss :  901.230940622
Iteration :  76   Loss :  2696.24070686
Iteration :  77   Loss :  1330.8535164
Iteration :  78   Loss :  2291.08750147
Iteration :  79   Loss :  5209.09946944
Iteration :  80   Loss :  430.427903829
Iteration :  81   Loss :  790.590692882
Iteration :  82   Loss :  2085.43366773
Iteration :  83   Loss :  3058.03856059
Iteration :  84   Loss :  110.967581216
Iteration :  85   Loss :  13.0364740133
Iteration :  86   Loss :  1.53269698481
Iteration :  87   Loss :  12.008824904
Iteration :  88   Loss :  2788.36459546
Iteration :  89   Loss :  813.15565874
Iteration :  90   Loss :  548.555820531
Iteration :  91   Loss :  1040.44743965
Iteration :  92   Loss :  1563.60507388
Iteration :  93   Loss :  2555.92311008
Iteration :  94   Loss :  1540.86548758
Iteration :  95   Loss :  4664.46998571
Iteration :  96   Loss :  563.389531505
Iteration :  97   Loss :  1418.52058448
Iteration :  98   Loss :  4093.60395428
Iteration :  99   Loss :  238.963688326
[-0.00969171 -0.00069055  0.00929812 ...,  0.01703518  0.00571333
  0.00900798]
CROSS VALIDATION 2
Iteration :  0   Loss :  3722.28408834
Iteration :  1   Loss :  981.664140498
Iteration :  2   Loss :  913.451446205
Iteration :  3   Loss :  4686.75264725
Iteration :  4   Loss :  1683.1959583
Iteration :  5   Loss :  155.174038948
Iteration :  6   Loss :  1975.36617834
Iteration :  7   Loss :  283.915138164
Iteration :  8   Loss :  2353.11806213
Iteration :  9   Loss :  2099.09440624
Iteration :  10   Loss :  228.732938949
Iteration :  11   Loss :  3222.34069533
Iteration :  12   Loss :  1659.25223018
Iteration :  13   Loss :  1410.08027629
Iteration :  14   Loss :  1079.36606509
Iteration :  15   Loss :  1790.90232966
Iteration :  16   Loss :  2438.12732057
Iteration :  17   Loss :  2074.6155502
Iteration :  18   Loss :  337.366144719
Iteration :  19   Loss :  752.645004273
Iteration :  20   Loss :  3228.17646275
Iteration :  21   Loss :  2091.95618671
Iteration :  22   Loss :  1824.09128296
Iteration :  23   Loss :  1882.50448473
Iteration :  24   Loss :  1255.02831593
Iteration :  25   Loss :  2871.89367677
Iteration :  26   Loss :  3782.92924316
Iteration :  27   Loss :  288.399414668
Iteration :  28   Loss :  1956.46048084
Iteration :  29   Loss :  2341.92345229
Iteration :  30   Loss :  2375.72684325
Iteration :  31   Loss :  2388.41425844
Iteration :  32   Loss :  6248.38465134
Iteration :  33   Loss :  433.147509455
Iteration :  34   Loss :  3933.18837176
Iteration :  35   Loss :  697.856243372
Iteration :  36   Loss :  1979.27318699
Iteration :  37   Loss :  2326.23107952
Iteration :  38   Loss :  1221.59124979
Iteration :  39   Loss :  74.8925831027
Iteration :  40   Loss :  8.79838974759
Iteration :  41   Loss :  1.05809427539
Iteration :  42   Loss :  5808.75455284
Iteration :  43   Loss :  2592.44365885
Iteration :  44   Loss :  1288.69850081
Iteration :  45   Loss :  372.593712789
Iteration :  46   Loss :  1706.04982522
Iteration :  47   Loss :  2866.19776707
Iteration :  48   Loss :  3615.16665038
Iteration :  49   Loss :  745.237617837
Iteration :  50   Loss :  1147.83785499
Iteration :  51   Loss :  2893.16791819
Iteration :  52   Loss :  895.7795362
Iteration :  53   Loss :  310.042457781
Iteration :  54   Loss :  1772.7820813
Iteration :  55   Loss :  623.569265079
Iteration :  56   Loss :  3065.32289707
Iteration :  57   Loss :  2535.60700831
Iteration :  58   Loss :  405.14665067
Iteration :  59   Loss :  2493.81539852
Iteration :  60   Loss :  2247.764221
Iteration :  61   Loss :  1067.07579691
Iteration :  62   Loss :  418.963375905
Iteration :  63   Loss :  49.2198270974
Iteration :  64   Loss :  5.78234645113
Iteration :  65   Loss :  0.682809294522
Iteration :  66   Loss :  0.115332225961
Iteration :  67   Loss :  1894.52454588
Iteration :  68   Loss :  3310.33785154
Iteration :  69   Loss :  1433.00720821
Iteration :  70   Loss :  2031.89784679
Iteration :  71   Loss :  178.434886442
Iteration :  72   Loss :  1485.2640968
Iteration :  73   Loss :  4067.1747473
Iteration :  74   Loss :  425.960640765
Iteration :  75   Loss :  50.0421386642
Iteration :  76   Loss :  5.94113163839
Iteration :  77   Loss :  0.718541883754
Iteration :  78   Loss :  0.105476566403
Iteration :  79   Loss :  5005.79193198
Iteration :  80   Loss :  2300.96197737
Iteration :  81   Loss :  408.590576321
Iteration :  82   Loss :  2741.37906294
Iteration :  83   Loss :  3736.88185875
Iteration :  84   Loss :  1979.23275951
Iteration :  85   Loss :  2504.26046607
Iteration :  86   Loss :  1657.67064439
Iteration :  87   Loss :  465.551862084
Iteration :  88   Loss :  54.6930493326
Iteration :  89   Loss :  6.6302058547
Iteration :  90   Loss :  16.8273729803
Iteration :  91   Loss :  6603.48487206
Iteration :  92   Loss :  1255.05758847
Iteration :  93   Loss :  672.41783876
Iteration :  94   Loss :  1274.03768509
Iteration :  95   Loss :  1738.09257072
Iteration :  96   Loss :  2441.85007903
Iteration :  97   Loss :  1458.63328324
Iteration :  98   Loss :  4185.97971128
Iteration :  99   Loss :  639.498162626
[-0.01848257 -0.04465844 -0.05026868 ...,  0.04905098 -0.03372004
  0.00209951]
CROSS VALIDATION 3
Iteration :  0   Loss :  2740.74260099
Iteration :  1   Loss :  1701.86386912
Iteration :  2   Loss :  661.883740935
Iteration :  3   Loss :  1629.1507957
Iteration :  4   Loss :  1702.14609663
Iteration :  5   Loss :  505.219712567
Iteration :  6   Loss :  4058.04366549
Iteration :  7   Loss :  1196.97078984
Iteration :  8   Loss :  2512.56678094
Iteration :  9   Loss :  1353.21779988
Iteration :  10   Loss :  2381.79289609
Iteration :  11   Loss :  2147.78458217
Iteration :  12   Loss :  2990.92187392
Iteration :  13   Loss :  777.950577741
Iteration :  14   Loss :  2538.40480586
Iteration :  15   Loss :  2837.69615708
Iteration :  16   Loss :  2193.8464996
Iteration :  17   Loss :  123.881470835
Iteration :  18   Loss :  14.5535988577
Iteration :  19   Loss :  1.72195538541
Iteration :  20   Loss :  0.324024348917
Iteration :  21   Loss :  1992.52844641
Iteration :  22   Loss :  1754.89094603
Iteration :  23   Loss :  2438.35591725
Iteration :  24   Loss :  3708.92268976
Iteration :  25   Loss :  2394.61577754
Iteration :  26   Loss :  3022.21761225
Iteration :  27   Loss :  1905.09688204
Iteration :  28   Loss :  1789.8225955
Iteration :  29   Loss :  2136.67633206
Iteration :  30   Loss :  1346.13901997
Iteration :  31   Loss :  1089.59459068
Iteration :  32   Loss :  883.651170126
Iteration :  33   Loss :  233.2287872
Iteration :  34   Loss :  3081.45119455
Iteration :  35   Loss :  3314.46903108
Iteration :  36   Loss :  929.422470427
Iteration :  37   Loss :  1033.14403942
Iteration :  38   Loss :  1548.94293274
Iteration :  39   Loss :  916.646105444
Iteration :  40   Loss :  1520.56584581
Iteration :  41   Loss :  4956.43387322
Iteration :  42   Loss :  2364.40318694
Iteration :  43   Loss :  726.479008227
Iteration :  44   Loss :  2820.76294886
Iteration :  45   Loss :  1643.72101691
Iteration :  46   Loss :  1164.84806978
Iteration :  47   Loss :  2026.95683961
Iteration :  48   Loss :  1132.57099578
Iteration :  49   Loss :  2360.7155372
Iteration :  50   Loss :  1108.2432921
Iteration :  51   Loss :  1092.64215137
Iteration :  52   Loss :  3611.02924551
Iteration :  53   Loss :  784.257496388
Iteration :  54   Loss :  3150.08799147
Iteration :  55   Loss :  3242.80342892
Iteration :  56   Loss :  697.876711096
Iteration :  57   Loss :  1150.48512286
Iteration :  58   Loss :  2496.26717792
Iteration :  59   Loss :  520.972710013
Iteration :  60   Loss :  1121.07530915
Iteration :  61   Loss :  5893.92003421
Iteration :  62   Loss :  1556.89174972
Iteration :  63   Loss :  1570.1095107
Iteration :  64   Loss :  1008.65693058
Iteration :  65   Loss :  2965.08630335
Iteration :  66   Loss :  1414.70884717
Iteration :  67   Loss :  5114.75255087
Iteration :  68   Loss :  933.554519216
Iteration :  69   Loss :  1656.08260793
Iteration :  70   Loss :  3360.46607579
Iteration :  71   Loss :  1943.70062348
Iteration :  72   Loss :  863.303264939
Iteration :  73   Loss :  2300.50565416
Iteration :  74   Loss :  496.370472144
Iteration :  75   Loss :  2488.78193299
Iteration :  76   Loss :  2765.78866048
Iteration :  77   Loss :  485.683855668
Iteration :  78   Loss :  2327.29816908
Iteration :  79   Loss :  2860.14356606
Iteration :  80   Loss :  1443.37160871
Iteration :  81   Loss :  2702.37500079
Iteration :  82   Loss :  2490.79039967
Iteration :  83   Loss :  1467.52469221
Iteration :  84   Loss :  6397.79401345
Iteration :  85   Loss :  311.136038163
Iteration :  86   Loss :  36.552289536
Iteration :  87   Loss :  4.3150955695
Iteration :  88   Loss :  0.521981846967
Iteration :  89   Loss :  0.0938296570956
Iteration :  90   Loss :  1859.19776285
Iteration :  91   Loss :  3289.58654693
Iteration :  92   Loss :  1429.34473801
Iteration :  93   Loss :  2029.85237694
Iteration :  94   Loss :  177.944986117
Iteration :  95   Loss :  1485.21563441
Iteration :  96   Loss :  4067.17536229
Iteration :  97   Loss :  425.960291756
Iteration :  98   Loss :  50.0420977843
Iteration :  99   Loss :  5.94116756621
[  4.56174904e-03   1.56974407e-04  -5.65375007e-05 ...,  -3.25754555e-05
  -2.55563951e-03   1.96468632e-03]
CROSS VALIDATION 4
Iteration :  0   Loss :  2740.74260099
Iteration :  1   Loss :  1701.86386912
Iteration :  2   Loss :  624.50202155
Iteration :  3   Loss :  2366.92150407
Iteration :  4   Loss :  2972.04312507
Iteration :  5   Loss :  1475.95675917
Iteration :  6   Loss :  2121.77147771
Iteration :  7   Loss :  1526.86084009
Iteration :  8   Loss :  159.211327595
Iteration :  9   Loss :  1834.09319554
Iteration :  10   Loss :  574.527641502
Iteration :  11   Loss :  1803.32626355
Iteration :  12   Loss :  3310.36400346
Iteration :  13   Loss :  808.749702361
Iteration :  14   Loss :  1818.2172474
Iteration :  15   Loss :  3251.53022037
Iteration :  16   Loss :  990.570295124
Iteration :  17   Loss :  84.4208594197
Iteration :  18   Loss :  9.91777508649
Iteration :  19   Loss :  1.18773650019
Iteration :  20   Loss :  10647.8243988
Iteration :  21   Loss :  7354.42512795
Iteration :  22   Loss :  1642.53558286
Iteration :  23   Loss :  1831.99790188
Iteration :  24   Loss :  151.777184714
Iteration :  25   Loss :  17.8307871728
Iteration :  26   Loss :  2.0952573212
Iteration :  27   Loss :  0.275366685913
Iteration :  28   Loss :  4758.20847204
Iteration :  29   Loss :  2472.20911664
Iteration :  30   Loss :  599.312983022
Iteration :  31   Loss :  2587.24817435
Iteration :  32   Loss :  2299.46208571
Iteration :  33   Loss :  579.237494451
Iteration :  34   Loss :  299.977936023
Iteration :  35   Loss :  2718.15137962
Iteration :  36   Loss :  4017.72197086
Iteration :  37   Loss :  387.29364216
Iteration :  38   Loss :  1163.91624082
Iteration :  39   Loss :  3633.94345504
Iteration :  40   Loss :  1604.80197903
Iteration :  41   Loss :  506.886024663
Iteration :  42   Loss :  2205.90394394
Iteration :  43   Loss :  1780.57257049
Iteration :  44   Loss :  3585.73449367
Iteration :  45   Loss :  815.930237197
Iteration :  46   Loss :  2473.20145805
Iteration :  47   Loss :  3309.40189493
Iteration :  48   Loss :  1564.32048477
Iteration :  49   Loss :  1686.22601049
Iteration :  50   Loss :  264.328057732
Iteration :  51   Loss :  3079.38413002
Iteration :  52   Loss :  3212.17041407
Iteration :  53   Loss :  1178.02408647
Iteration :  54   Loss :  2768.2228412
Iteration :  55   Loss :  391.858242039
Iteration :  56   Loss :  46.0407322303
Iteration :  57   Loss :  5.40975441466
Iteration :  58   Loss :  27.5459449222
Iteration :  59   Loss :  4170.45140871
Iteration :  60   Loss :  3634.70022592
Iteration :  61   Loss :  689.96180598
Iteration :  62   Loss :  4759.37515122
Iteration :  63   Loss :  428.284137576
Iteration :  64   Loss :  1565.38898852
Iteration :  65   Loss :  4757.80883641
Iteration :  66   Loss :  376.232479978
Iteration :  67   Loss :  1395.47939355
Iteration :  68   Loss :  1904.58382407
Iteration :  69   Loss :  2849.53860078
Iteration :  70   Loss :  1008.33515063
Iteration :  71   Loss :  1378.12938689
Iteration :  72   Loss :  2429.59938936
Iteration :  73   Loss :  280.595721062
Iteration :  74   Loss :  32.9827502222
Iteration :  75   Loss :  3.87604076193
Iteration :  76   Loss :  4963.20186662
Iteration :  77   Loss :  2576.53812131
Iteration :  78   Loss :  1108.44707251
Iteration :  79   Loss :  2299.97585717
Iteration :  80   Loss :  303.123301155
Iteration :  81   Loss :  2686.7548047
Iteration :  82   Loss :  2089.45447505
Iteration :  83   Loss :  1708.63288351
Iteration :  84   Loss :  2030.71934221
Iteration :  85   Loss :  2561.51380606
Iteration :  86   Loss :  2654.78347013
Iteration :  87   Loss :  1872.48269787
Iteration :  88   Loss :  1431.71059162
Iteration :  89   Loss :  334.075636942
Iteration :  90   Loss :  1622.89582636
Iteration :  91   Loss :  2814.31522917
Iteration :  92   Loss :  1791.20288347
Iteration :  93   Loss :  3698.62981202
Iteration :  94   Loss :  566.461164776
Iteration :  95   Loss :  2874.7414218
Iteration :  96   Loss :  3399.69610449
Iteration :  97   Loss :  903.824693814
Iteration :  98   Loss :  79.4510169344
Iteration :  99   Loss :  9.33394656176
[-0.01072228 -0.00467269 -0.01570644 ...,  0.0082776   0.00777533
  0.00270368]
CROSS VALIDATION 5
Iteration :  0   Loss :  2382.64791247
Iteration :  1   Loss :  1652.83845412
Iteration :  2   Loss :  235.495444097
Iteration :  3   Loss :  27.6660102231
Iteration :  4   Loss :  3.25100627183
Iteration :  5   Loss :  0.394419335377
Iteration :  6   Loss :  4.01021431347
Iteration :  7   Loss :  2226.72468291
Iteration :  8   Loss :  3378.8082585
Iteration :  9   Loss :  599.501319369
Iteration :  10   Loss :  2667.23157746
Iteration :  11   Loss :  1854.70161082
Iteration :  12   Loss :  1328.65696604
Iteration :  13   Loss :  1749.4930643
Iteration :  14   Loss :  1560.66457585
Iteration :  15   Loss :  139.506425171
Iteration :  16   Loss :  2473.82280989
Iteration :  17   Loss :  599.950606515
Iteration :  18   Loss :  3860.06260179
Iteration :  19   Loss :  3658.60938238
Iteration :  20   Loss :  1970.91528555
Iteration :  21   Loss :  1117.74396867
Iteration :  22   Loss :  1109.69052271
Iteration :  23   Loss :  1425.19232393
Iteration :  24   Loss :  1759.32976432
Iteration :  25   Loss :  164.69766009
Iteration :  26   Loss :  5688.58142176
Iteration :  27   Loss :  1341.34974388
Iteration :  28   Loss :  781.642715133
Iteration :  29   Loss :  1696.62676634
Iteration :  30   Loss :  1867.48823822
Iteration :  31   Loss :  688.83882986
Iteration :  32   Loss :  3338.18697965
Iteration :  33   Loss :  306.90784808
Iteration :  34   Loss :  1142.37896562
Iteration :  35   Loss :  1606.85141278
Iteration :  36   Loss :  1232.76279215
Iteration :  37   Loss :  1882.46203264
Iteration :  38   Loss :  5252.85225947
Iteration :  39   Loss :  1622.90730193
Iteration :  40   Loss :  691.676127478
Iteration :  41   Loss :  325.862803141
Iteration :  42   Loss :  3933.34275901
Iteration :  43   Loss :  2490.67895565
Iteration :  44   Loss :  437.151083162
Iteration :  45   Loss :  1177.02652532
Iteration :  46   Loss :  1389.03324508
Iteration :  47   Loss :  1042.1325969
Iteration :  48   Loss :  3306.90794356
Iteration :  49   Loss :  431.186494345
Iteration :  50   Loss :  3535.16504288
Iteration :  51   Loss :  2991.34363046
Iteration :  52   Loss :  309.943311045
Iteration :  53   Loss :  6198.30034603
Iteration :  54   Loss :  1117.50679313
Iteration :  55   Loss :  2397.6178439
Iteration :  56   Loss :  2145.22716238
Iteration :  57   Loss :  241.698346043
Iteration :  58   Loss :  4576.47363228
Iteration :  59   Loss :  333.429438463
Iteration :  60   Loss :  39.1716816473
Iteration :  61   Loss :  8.68195100975
Iteration :  62   Loss :  2276.49913667
Iteration :  63   Loss :  1852.48886578
Iteration :  64   Loss :  162.885329436
Iteration :  65   Loss :  19.1370755018
Iteration :  66   Loss :  2.2743872254
Iteration :  67   Loss :  0.290767089845
Iteration :  68   Loss :  4594.91934172
Iteration :  69   Loss :  3400.90609358
Iteration :  70   Loss :  579.435167254
Iteration :  71   Loss :  1948.65778162
Iteration :  72   Loss :  1098.29767239
Iteration :  73   Loss :  1740.79187629
Iteration :  74   Loss :  1207.10730353
Iteration :  75   Loss :  1569.51213266
Iteration :  76   Loss :  2155.67263229
Iteration :  77   Loss :  2901.39969046
Iteration :  78   Loss :  480.661147952
Iteration :  79   Loss :  2760.20615694
Iteration :  80   Loss :  1702.55071056
Iteration :  81   Loss :  480.719838269
Iteration :  82   Loss :  2105.09977136
Iteration :  83   Loss :  680.494034017
Iteration :  84   Loss :  2481.99838686
Iteration :  85   Loss :  2741.04241759
Iteration :  86   Loss :  1725.42715063
Iteration :  87   Loss :  2197.53821103
Iteration :  88   Loss :  4426.31949087
Iteration :  89   Loss :  957.384627938
Iteration :  90   Loss :  1799.81889886
Iteration :  91   Loss :  2336.72811622
Iteration :  92   Loss :  1297.34796712
Iteration :  93   Loss :  800.701171772
Iteration :  94   Loss :  3630.78333684
Iteration :  95   Loss :  4542.53310098
Iteration :  96   Loss :  262.999210084
Iteration :  97   Loss :  1748.70243775
Iteration :  98   Loss :  36.9588735925
Iteration :  99   Loss :  4.34290934091
[-0.00958173 -0.01036301 -0.00650194 ...,  0.00698262  0.00268861
  0.00128052]
CROSS VALIDATION 6
Iteration :  0   Loss :  2740.74260099
Iteration :  1   Loss :  1691.57400902
Iteration :  2   Loss :  658.002583267
Iteration :  3   Loss :  2389.89849916
Iteration :  4   Loss :  3167.88923095
Iteration :  5   Loss :  540.773657526
Iteration :  6   Loss :  63.5301019928
Iteration :  7   Loss :  7.46846709429
Iteration :  8   Loss :  0.88428749972
Iteration :  9   Loss :  10535.9991729
Iteration :  10   Loss :  6392.47951836
Iteration :  11   Loss :  2276.53991716
Iteration :  12   Loss :  490.788056793
Iteration :  13   Loss :  1905.60477998
Iteration :  14   Loss :  1826.60420885
Iteration :  15   Loss :  2224.33570648
Iteration :  16   Loss :  331.500018729
Iteration :  17   Loss :  3669.95204888
Iteration :  18   Loss :  3610.14318
Iteration :  19   Loss :  1321.76311446
Iteration :  20   Loss :  1908.08779602
Iteration :  21   Loss :  1280.58968086
Iteration :  22   Loss :  904.471248303
Iteration :  23   Loss :  3640.13170414
Iteration :  24   Loss :  2099.06477938
Iteration :  25   Loss :  323.389899235
Iteration :  26   Loss :  3862.24102734
Iteration :  27   Loss :  1487.63903697
Iteration :  28   Loss :  312.009635748
Iteration :  29   Loss :  1298.22835095
Iteration :  30   Loss :  3446.51210116
Iteration :  31   Loss :  1908.47893004
Iteration :  32   Loss :  2874.92634453
Iteration :  33   Loss :  2061.96291269
Iteration :  34   Loss :  1510.01579544
Iteration :  35   Loss :  5056.85683948
Iteration :  36   Loss :  1458.45666118
Iteration :  37   Loss :  1125.71424552
Iteration :  38   Loss :  2295.94456152
Iteration :  39   Loss :  1807.18673811
Iteration :  40   Loss :  357.734734001
Iteration :  41   Loss :  3157.03036736
Iteration :  42   Loss :  2255.4026024
Iteration :  43   Loss :  413.097256588
Iteration :  44   Loss :  1762.23782402
Iteration :  45   Loss :  3124.16699441
Iteration :  46   Loss :  1625.33663428
Iteration :  47   Loss :  247.235190795
Iteration :  48   Loss :  2638.75049184
Iteration :  49   Loss :  2698.75606882
Iteration :  50   Loss :  4344.28468063
Iteration :  51   Loss :  3483.1262793
Iteration :  52   Loss :  724.498335529
Iteration :  53   Loss :  1259.01756191
Iteration :  54   Loss :  2199.23805792
Iteration :  55   Loss :  2480.89398935
Iteration :  56   Loss :  932.148823895
Iteration :  57   Loss :  1123.90898912
Iteration :  58   Loss :  1554.92132848
Iteration :  59   Loss :  883.80380258
Iteration :  60   Loss :  2019.73849576
Iteration :  61   Loss :  1946.53794455
Iteration :  62   Loss :  1031.98649952
Iteration :  63   Loss :  3301.37948066
Iteration :  64   Loss :  886.674643389
Iteration :  65   Loss :  3178.2846971
Iteration :  66   Loss :  2816.01436733
Iteration :  67   Loss :  456.615027985
Iteration :  68   Loss :  1589.41740736
Iteration :  69   Loss :  5588.38545557
Iteration :  70   Loss :  6917.56893762
Iteration :  71   Loss :  2131.33860517
Iteration :  72   Loss :  2638.20962256
Iteration :  73   Loss :  380.734564937
Iteration :  74   Loss :  1474.55260599
Iteration :  75   Loss :  2479.87884807
Iteration :  76   Loss :  4312.35435186
Iteration :  77   Loss :  1165.29067062
Iteration :  78   Loss :  2232.82513903
Iteration :  79   Loss :  2982.72831632
Iteration :  80   Loss :  3089.19897974
Iteration :  81   Loss :  290.376726207
Iteration :  82   Loss :  34.113464515
Iteration :  83   Loss :  4.00783428454
Iteration :  84   Loss :  0.478362534433
Iteration :  85   Loss :  9464.49785627
Iteration :  86   Loss :  2094.73396924
Iteration :  87   Loss :  1366.83690318
Iteration :  88   Loss :  79.2649206968
Iteration :  89   Loss :  9.31204513645
Iteration :  90   Loss :  1.09516330646
Iteration :  91   Loss :  0.141372805218
Iteration :  92   Loss :  2509.72365333
Iteration :  93   Loss :  4426.18066036
Iteration :  94   Loss :  1009.26765399
Iteration :  95   Loss :  1415.02331855
Iteration :  96   Loss :  3508.56490587
Iteration :  97   Loss :  422.152948464
Iteration :  98   Loss :  49.5945381765
Iteration :  99   Loss :  5.82771555856
[ -6.41488671e-04   8.96905357e-04   2.38445085e-03 ...,   2.88469839e-03
  -3.63996812e-06   2.97963016e-03]
CROSS VALIDATION 7
Iteration :  0   Loss :  2740.74260099
Iteration :  1   Loss :  1691.57400902
Iteration :  2   Loss :  658.002583267
Iteration :  3   Loss :  1637.01189742
Iteration :  4   Loss :  1693.19143655
Iteration :  5   Loss :  504.051367495
Iteration :  6   Loss :  4052.34149074
Iteration :  7   Loss :  1196.17415521
Iteration :  8   Loss :  2514.82522033
Iteration :  9   Loss :  1338.88452079
Iteration :  10   Loss :  2240.23699451
Iteration :  11   Loss :  3319.43268279
Iteration :  12   Loss :  1406.84645087
Iteration :  13   Loss :  2371.51639704
Iteration :  14   Loss :  2725.98018999
Iteration :  15   Loss :  664.027205706
Iteration :  16   Loss :  3065.37313951
Iteration :  17   Loss :  3284.09592877
Iteration :  18   Loss :  1515.18727012
Iteration :  19   Loss :  267.730188842
Iteration :  20   Loss :  31.4529487996
Iteration :  21   Loss :  3.69731962756
Iteration :  22   Loss :  0.435634818376
Iteration :  23   Loss :  0.0611911090989
Iteration :  24   Loss :  5454.85933622
Iteration :  25   Loss :  370.780120562
Iteration :  26   Loss :  1174.28258421
Iteration :  27   Loss :  2044.83524618
Iteration :  28   Loss :  1762.98448966
Iteration :  29   Loss :  528.380183789
Iteration :  30   Loss :  2260.13953834
Iteration :  31   Loss :  1313.01279704
Iteration :  32   Loss :  1963.52942249
Iteration :  33   Loss :  3971.12134098
Iteration :  34   Loss :  1441.69919295
Iteration :  35   Loss :  557.105269774
Iteration :  36   Loss :  2726.23317919
Iteration :  37   Loss :  1713.29488112
Iteration :  38   Loss :  3311.03607913
Iteration :  39   Loss :  1129.30609283
Iteration :  40   Loss :  606.442004729
Iteration :  41   Loss :  1799.93828492
Iteration :  42   Loss :  1104.54135566
Iteration :  43   Loss :  767.796962662
Iteration :  44   Loss :  6761.53861973
Iteration :  45   Loss :  204.78436835
Iteration :  46   Loss :  24.0581534735
Iteration :  47   Loss :  2.84644053393
Iteration :  48   Loss :  9675.43289995
Iteration :  49   Loss :  3321.19055163
Iteration :  50   Loss :  1105.91402425
Iteration :  51   Loss :  5182.70137967
Iteration :  52   Loss :  2672.52823377
Iteration :  53   Loss :  891.069399059
Iteration :  54   Loss :  1960.84805978
Iteration :  55   Loss :  2123.84879774
Iteration :  56   Loss :  1101.37777785
Iteration :  57   Loss :  1977.86135694
Iteration :  58   Loss :  2981.96137264
Iteration :  59   Loss :  468.508222601
Iteration :  60   Loss :  1983.32476881
Iteration :  61   Loss :  4548.18395874
Iteration :  62   Loss :  443.47510026
Iteration :  63   Loss :  1429.15229582
Iteration :  64   Loss :  2369.15767316
Iteration :  65   Loss :  2847.18568594
Iteration :  66   Loss :  4137.65257399
Iteration :  67   Loss :  1479.82780104
Iteration :  68   Loss :  497.136814186
Iteration :  69   Loss :  1760.50240054
Iteration :  70   Loss :  1398.18204117
Iteration :  71   Loss :  1934.21442183
Iteration :  72   Loss :  488.512765818
Iteration :  73   Loss :  2897.20152851
Iteration :  74   Loss :  435.411965218
Iteration :  75   Loss :  1943.33808515
Iteration :  76   Loss :  320.857503096
Iteration :  77   Loss :  37.6957467246
Iteration :  78   Loss :  7348.11423407
Iteration :  79   Loss :  3356.56525368
Iteration :  80   Loss :  203.796076853
Iteration :  81   Loss :  23.9419747028
Iteration :  82   Loss :  2.82042801891
Iteration :  83   Loss :  93.7616813455
Iteration :  84   Loss :  4187.81984385
Iteration :  85   Loss :  885.753439035
Iteration :  86   Loss :  343.243397326
Iteration :  87   Loss :  40.3242422615
Iteration :  88   Loss :  4.73732986125
Iteration :  89   Loss :  0.557494727534
Iteration :  90   Loss :  9518.13955848
Iteration :  91   Loss :  2133.29037424
Iteration :  92   Loss :  1365.96426939
Iteration :  93   Loss :  79.2695204598
Iteration :  94   Loss :  9.31258550659
Iteration :  95   Loss :  1.09529996154
Iteration :  96   Loss :  0.175331559922
Iteration :  97   Loss :  4658.36189127
Iteration :  98   Loss :  2605.76483071
Iteration :  99   Loss :  1272.51257329
[-0.119276   -0.02793353 -0.04336553 ...,  0.03609916  0.04834259
  0.01027994]
CROSS VALIDATION 8
Iteration :  0   Loss :  2740.74260099
Iteration :  1   Loss :  1691.57400902
Iteration :  2   Loss :  658.002583267
Iteration :  3   Loss :  1637.01189742
Iteration :  4   Loss :  1693.19143655
Iteration :  5   Loss :  504.051367495
Iteration :  6   Loss :  4052.34149074
Iteration :  7   Loss :  1196.17415521
Iteration :  8   Loss :  2514.82522033
Iteration :  9   Loss :  1338.88452079
Iteration :  10   Loss :  2369.92357752
Iteration :  11   Loss :  1606.19699038
Iteration :  12   Loss :  2627.14940201
Iteration :  13   Loss :  962.882664849
Iteration :  14   Loss :  6026.69330753
Iteration :  15   Loss :  1629.63499885
Iteration :  16   Loss :  1037.9463968
Iteration :  17   Loss :  1128.95670925
Iteration :  18   Loss :  1294.60870826
Iteration :  19   Loss :  393.283965129
Iteration :  20   Loss :  405.021224647
Iteration :  21   Loss :  2996.80424318
Iteration :  22   Loss :  2825.62597566
Iteration :  23   Loss :  236.499702463
Iteration :  24   Loss :  2537.19398499
Iteration :  25   Loss :  320.713970217
Iteration :  26   Loss :  1540.97658747
Iteration :  27   Loss :  4515.72103941
Iteration :  28   Loss :  1087.63752315
Iteration :  29   Loss :  797.519106027
Iteration :  30   Loss :  1527.37061803
Iteration :  31   Loss :  489.982642194
Iteration :  32   Loss :  1154.57098515
Iteration :  33   Loss :  339.988832629
Iteration :  34   Loss :  39.9418959256
Iteration :  35   Loss :  4.69237664523
Iteration :  36   Loss :  0.570812087958
Iteration :  37   Loss :  2331.15852397
Iteration :  38   Loss :  1776.26111385
Iteration :  39   Loss :  878.178262181
Iteration :  40   Loss :  2126.91905207
Iteration :  41   Loss :  2351.84068659
Iteration :  42   Loss :  3008.87197351
Iteration :  43   Loss :  1628.2741893
Iteration :  44   Loss :  864.046212607
Iteration :  45   Loss :  2724.79573441
Iteration :  46   Loss :  3989.83083671
Iteration :  47   Loss :  1079.38459993
Iteration :  48   Loss :  1728.77083192
Iteration :  49   Loss :  3359.10973593
Iteration :  50   Loss :  788.023228832
Iteration :  51   Loss :  6374.29406814
Iteration :  52   Loss :  5832.44868732
Iteration :  53   Loss :  453.493068875
Iteration :  54   Loss :  2169.32079084
Iteration :  55   Loss :  958.937798013
Iteration :  56   Loss :  2242.9131421
Iteration :  57   Loss :  2802.92165371
Iteration :  58   Loss :  633.604717911
Iteration :  59   Loss :  4369.89455674
Iteration :  60   Loss :  4798.49795061
Iteration :  61   Loss :  1902.37227072
Iteration :  62   Loss :  1199.79898229
Iteration :  63   Loss :  4106.52449663
Iteration :  64   Loss :  1562.53770591
Iteration :  65   Loss :  1019.21036486
Iteration :  66   Loss :  1439.88351026
Iteration :  67   Loss :  1889.11257456
Iteration :  68   Loss :  470.19647728
Iteration :  69   Loss :  1741.276339
Iteration :  70   Loss :  1329.0814376
Iteration :  71   Loss :  2136.46857547
Iteration :  72   Loss :  1375.33413988
Iteration :  73   Loss :  464.496749474
Iteration :  74   Loss :  1898.81854354
Iteration :  75   Loss :  1238.4404542
Iteration :  76   Loss :  713.942283654
Iteration :  77   Loss :  679.464931362
Iteration :  78   Loss :  4172.07136852
Iteration :  79   Loss :  1239.37640762
Iteration :  80   Loss :  154.473643676
Iteration :  81   Loss :  18.1475672786
Iteration :  82   Loss :  2.13240387242
Iteration :  83   Loss :  3899.38084183
Iteration :  84   Loss :  4234.90608004
Iteration :  85   Loss :  1067.10093291
Iteration :  86   Loss :  3015.48917178
Iteration :  87   Loss :  1402.98964079
Iteration :  88   Loss :  2078.45816468
Iteration :  89   Loss :  2266.88881078
Iteration :  90   Loss :  2396.25127863
Iteration :  91   Loss :  473.241207497
Iteration :  92   Loss :  1848.47437345
Iteration :  93   Loss :  925.107184164
Iteration :  94   Loss :  3181.04898826
Iteration :  95   Loss :  1960.09508475
Iteration :  96   Loss :  998.909191796
Iteration :  97   Loss :  409.188004698
Iteration :  98   Loss :  9965.24293042
Iteration :  99   Loss :  1249.92918292
[-0.05184285 -0.01251955  0.00578612 ...,  0.07473783 -0.02333107
  0.01198777]
CROSS VALIDATION 9
Iteration :  0   Loss :  3496.90305443
Iteration :  1   Loss :  535.285106136
Iteration :  2   Loss :  62.8853746817
Iteration :  3   Loss :  7.41031128032
Iteration :  4   Loss :  0.886967461745
Iteration :  5   Loss :  0.126981053409
Iteration :  6   Loss :  7790.60412961
Iteration :  7   Loss :  1040.27165042
Iteration :  8   Loss :  4292.46881505
Iteration :  9   Loss :  503.35516385
Iteration :  10   Loss :  1549.71832666
Iteration :  11   Loss :  4282.31220983
Iteration :  12   Loss :  926.608567856
Iteration :  13   Loss :  1637.37590989
Iteration :  14   Loss :  3322.70202661
Iteration :  15   Loss :  1077.43304966
Iteration :  16   Loss :  1774.98111127
Iteration :  17   Loss :  2519.44371207
Iteration :  18   Loss :  524.541400106
Iteration :  19   Loss :  61.6231673036
Iteration :  20   Loss :  7.23957994104
Iteration :  21   Loss :  0.852276575453
Iteration :  22   Loss :  8283.37710862
Iteration :  23   Loss :  5401.73428977
Iteration :  24   Loss :  1970.34789947
Iteration :  25   Loss :  199.033557377
Iteration :  26   Loss :  2927.14619163
Iteration :  27   Loss :  2062.43098112
Iteration :  28   Loss :  1737.7262495
Iteration :  29   Loss :  6977.98076611
Iteration :  30   Loss :  1316.39819793
Iteration :  31   Loss :  1753.23535175
Iteration :  32   Loss :  1722.07019787
Iteration :  33   Loss :  1388.37844403
Iteration :  34   Loss :  922.262889465
Iteration :  35   Loss :  705.409106007
Iteration :  36   Loss :  2154.32172354
Iteration :  37   Loss :  396.358251052
Iteration :  38   Loss :  2229.44630272
Iteration :  39   Loss :  4601.05150801
Iteration :  40   Loss :  464.446512196
Iteration :  41   Loss :  1995.78002706
Iteration :  42   Loss :  3330.52655177
Iteration :  43   Loss :  338.100060877
Iteration :  44   Loss :  507.812930598
Iteration :  45   Loss :  414.983564807
Iteration :  46   Loss :  1910.52441468
Iteration :  47   Loss :  3500.66871503
Iteration :  48   Loss :  4074.21726486
Iteration :  49   Loss :  867.755302771
Iteration :  50   Loss :  1536.16165725
Iteration :  51   Loss :  2040.47430791
Iteration :  52   Loss :  619.556657627
Iteration :  53   Loss :  112.138036193
Iteration :  54   Loss :  13.1740022687
Iteration :  55   Loss :  1.55328336701
Iteration :  56   Loss :  0.209338407321
Iteration :  57   Loss :  5467.44683595
Iteration :  58   Loss :  1153.26381705
Iteration :  59   Loss :  2586.07260256
Iteration :  60   Loss :  69.3291828919
Iteration :  61   Loss :  8.14490416287
Iteration :  62   Loss :  0.968326446169
Iteration :  63   Loss :  5155.23431248
Iteration :  64   Loss :  1520.76485603
Iteration :  65   Loss :  861.156428559
Iteration :  66   Loss :  1195.37530142
Iteration :  67   Loss :  5182.78520318
Iteration :  68   Loss :  2636.69811583
Iteration :  69   Loss :  737.876978496
Iteration :  70   Loss :  1469.02171847
Iteration :  71   Loss :  752.555774495
Iteration :  72   Loss :  1548.88902607
Iteration :  73   Loss :  2184.02413607
Iteration :  74   Loss :  1921.07251258
Iteration :  75   Loss :  4866.18135956
Iteration :  76   Loss :  1603.59002376
Iteration :  77   Loss :  2077.74551106
Iteration :  78   Loss :  1590.53669548
Iteration :  79   Loss :  365.486376662
Iteration :  80   Loss :  2380.56516753
Iteration :  81   Loss :  1559.65251838
Iteration :  82   Loss :  1683.85396978
Iteration :  83   Loss :  192.323770119
Iteration :  84   Loss :  22.5942291908
Iteration :  85   Loss :  2.65703519514
Iteration :  86   Loss :  5820.67346972
Iteration :  87   Loss :  1932.00517809
Iteration :  88   Loss :  393.344728481
Iteration :  89   Loss :  46.2101478052
Iteration :  90   Loss :  5.42876923364
Iteration :  91   Loss :  0.63822841881
Iteration :  92   Loss :  0.131168523838
Iteration :  93   Loss :  1882.68476444
Iteration :  94   Loss :  2875.93065289
Iteration :  95   Loss :  626.44425532
Iteration :  96   Loss :  2957.83971716
Iteration :  97   Loss :  2227.61557264
Iteration :  98   Loss :  1463.98358295
Iteration :  99   Loss :  2984.81772944
[-0.10654387 -0.03793244 -0.07798182 ...,  0.1163042   0.0067664
  0.01686361]
CROSS VALIDATION 10
Iteration :  0   Loss :  2739.63021008
Iteration :  1   Loss :  1099.62766002
Iteration :  2   Loss :  87.6649351597
Iteration :  3   Loss :  10.2988786114
Iteration :  4   Loss :  1.21018322983
Iteration :  5   Loss :  0.48889648999
Iteration :  6   Loss :  4635.52361866
Iteration :  7   Loss :  2589.57537977
Iteration :  8   Loss :  1307.49950897
Iteration :  9   Loss :  3545.28608966
Iteration :  10   Loss :  1711.53350724
Iteration :  11   Loss :  1179.8386836
Iteration :  12   Loss :  3482.02119784
Iteration :  13   Loss :  2097.57820076
Iteration :  14   Loss :  5260.15052773
Iteration :  15   Loss :  187.971864363
Iteration :  16   Loss :  22.0829389758
Iteration :  17   Loss :  2.59478365352
Iteration :  18   Loss :  0.306680746728
Iteration :  19   Loss :  7782.95463878
Iteration :  20   Loss :  832.540891536
Iteration :  21   Loss :  802.786956529
Iteration :  22   Loss :  1783.67999888
Iteration :  23   Loss :  307.437796847
Iteration :  24   Loss :  36.1177994886
Iteration :  25   Loss :  4.24320500639
Iteration :  26   Loss :  7261.44390486
Iteration :  27   Loss :  4505.78067588
Iteration :  28   Loss :  250.959342332
Iteration :  29   Loss :  29.4827093471
Iteration :  30   Loss :  3.46463609166
Iteration :  31   Loss :  0.427494048386
Iteration :  32   Loss :  9677.19196355
Iteration :  33   Loss :  2382.81449512
Iteration :  34   Loss :  2068.64829586
Iteration :  35   Loss :  3675.5333545
Iteration :  36   Loss :  3364.49998393
Iteration :  37   Loss :  1539.42516025
Iteration :  38   Loss :  557.0539861
Iteration :  39   Loss :  1487.15658463
Iteration :  40   Loss :  1646.41724846
Iteration :  41   Loss :  1308.16430135
Iteration :  42   Loss :  1734.87327083
Iteration :  43   Loss :  4482.2313478
Iteration :  44   Loss :  1121.92566307
Iteration :  45   Loss :  625.567034993
Iteration :  46   Loss :  181.309397005
Iteration :  47   Loss :  21.300240275
Iteration :  48   Loss :  2.51858676441
Iteration :  49   Loss :  10484.2439861
Iteration :  50   Loss :  2021.16584225
Iteration :  51   Loss :  720.07612322
Iteration :  52   Loss :  354.232129022
Iteration :  53   Loss :  5194.9501948
Iteration :  54   Loss :  1375.72496326
Iteration :  55   Loss :  52.4323776286
Iteration :  56   Loss :  6.16119229512
Iteration :  57   Loss :  0.747060647171
Iteration :  58   Loss :  8251.82857368
Iteration :  59   Loss :  4645.7795938
Iteration :  60   Loss :  273.432888415
Iteration :  61   Loss :  32.1229020636
Iteration :  62   Loss :  3.77521155799
Iteration :  63   Loss :  0.451091253187
Iteration :  64   Loss :  5208.74047504
Iteration :  65   Loss :  7485.72854674
Iteration :  66   Loss :  485.190848879
Iteration :  67   Loss :  876.734729821
Iteration :  68   Loss :  2709.49994021
Iteration :  69   Loss :  1327.30298116
Iteration :  70   Loss :  1766.89753544
Iteration :  71   Loss :  1045.40585171
Iteration :  72   Loss :  3000.33188811
Iteration :  73   Loss :  4935.91091187
Iteration :  74   Loss :  2099.8193572
Iteration :  75   Loss :  129.463577431
Iteration :  76   Loss :  15.2093840728
Iteration :  77   Loss :  1.78704133608
Iteration :  78   Loss :  31.1513460002
Iteration :  79   Loss :  2684.74411559
Iteration :  80   Loss :  405.055270413
Iteration :  81   Loss :  2617.51679405
Iteration :  82   Loss :  5132.13146954
Iteration :  83   Loss :  2163.71319785
Iteration :  84   Loss :  3402.4752253
Iteration :  85   Loss :  1241.82571084
Iteration :  86   Loss :  4674.47132118
Iteration :  87   Loss :  797.849022612
Iteration :  88   Loss :  1344.22544635
Iteration :  89   Loss :  2655.78828957
Iteration :  90   Loss :  343.453276208
Iteration :  91   Loss :  40.3513347929
Iteration :  92   Loss :  4.81798159245
Iteration :  93   Loss :  4911.06173452
Iteration :  94   Loss :  497.720069462
Iteration :  95   Loss :  1675.99260955
Iteration :  96   Loss :  2133.87397356
Iteration :  97   Loss :  650.320243344
Iteration :  98   Loss :  1008.61292256
Iteration :  99   Loss :  1022.32275699
[-0.09511708  0.0101098  -0.10032784 ...,  0.04619208 -0.06283868
  0.0220068 ]
CROSS VALIDATION 11
Iteration :  0   Loss :  2739.63021008
Iteration :  1   Loss :  1416.90163024
Iteration :  2   Loss :  443.006568539
Iteration :  3   Loss :  1635.11120943
Iteration :  4   Loss :  1678.35198416
Iteration :  5   Loss :  503.257528986
Iteration :  6   Loss :  2997.57323418
Iteration :  7   Loss :  1781.43666909
Iteration :  8   Loss :  888.801980463
Iteration :  9   Loss :  1845.2715576
Iteration :  10   Loss :  1060.52199115
Iteration :  11   Loss :  1482.96805478
Iteration :  12   Loss :  3524.40799217
Iteration :  13   Loss :  289.688622887
Iteration :  14   Loss :  34.032626121
Iteration :  15   Loss :  3.99819045386
Iteration :  16   Loss :  0.484597946054
Iteration :  17   Loss :  0.327829979598
Iteration :  18   Loss :  4700.99465519
Iteration :  19   Loss :  2361.30678798
Iteration :  20   Loss :  527.793542183
Iteration :  21   Loss :  2154.37503185
Iteration :  22   Loss :  1814.63919874
Iteration :  23   Loss :  322.685052727
Iteration :  24   Loss :  2017.94584373
Iteration :  25   Loss :  1679.52472629
Iteration :  26   Loss :  2179.43009337
Iteration :  27   Loss :  1287.72120204
Iteration :  28   Loss :  1504.16076866
Iteration :  29   Loss :  877.393019818
Iteration :  30   Loss :  2313.47995725
Iteration :  31   Loss :  3006.78713985
Iteration :  32   Loss :  255.590159757
Iteration :  33   Loss :  30.0272601003
Iteration :  34   Loss :  21.0563703614
Iteration :  35   Loss :  3332.31481432
Iteration :  36   Loss :  1462.27579901
Iteration :  37   Loss :  900.208459058
Iteration :  38   Loss :  923.316406836
Iteration :  39   Loss :  1400.55353337
Iteration :  40   Loss :  922.284531826
Iteration :  41   Loss :  1538.40131132
Iteration :  42   Loss :  128.779938621
Iteration :  43   Loss :  15.1290701708
Iteration :  44   Loss :  1.77831585522
Iteration :  45   Loss :  0.217055947679
Iteration :  46   Loss :  0.648982726513
Iteration :  47   Loss :  3761.25758968
Iteration :  48   Loss :  2088.82581343
Iteration :  49   Loss :  343.634070411
Iteration :  50   Loss :  40.4028740856
Iteration :  51   Loss :  4.74674830297
Iteration :  52   Loss :  27.3167257071
Iteration :  53   Loss :  3763.23369287
Iteration :  54   Loss :  2727.8074849
Iteration :  55   Loss :  2299.12861751
Iteration :  56   Loss :  1509.89171137
Iteration :  57   Loss :  2199.42111611
Iteration :  58   Loss :  1052.31916952
Iteration :  59   Loss :  3582.55572339
Iteration :  60   Loss :  1763.058755
Iteration :  61   Loss :  2769.6157104
Iteration :  62   Loss :  379.27044536
Iteration :  63   Loss :  1521.05755825
Iteration :  64   Loss :  2002.08062211
Iteration :  65   Loss :  1914.67775233
Iteration :  66   Loss :  3946.92554074
Iteration :  67   Loss :  1235.41482349
Iteration :  68   Loss :  1814.91856075
Iteration :  69   Loss :  1956.35465462
Iteration :  70   Loss :  2519.28858849
Iteration :  71   Loss :  2526.43547658
Iteration :  72   Loss :  2805.49996512
Iteration :  73   Loss :  1067.16887646
Iteration :  74   Loss :  2313.89866954
Iteration :  75   Loss :  919.950655474
Iteration :  76   Loss :  1889.77554392
Iteration :  77   Loss :  784.683670086
Iteration :  78   Loss :  2564.05890014
Iteration :  79   Loss :  1748.93248824
Iteration :  80   Loss :  649.437871387
Iteration :  81   Loss :  438.995795172
Iteration :  82   Loss :  2677.23844801
Iteration :  83   Loss :  4612.20865498
Iteration :  84   Loss :  6021.94895184
Iteration :  85   Loss :  2479.28482868
Iteration :  86   Loss :  162.373462444
Iteration :  87   Loss :  3012.64902939
Iteration :  88   Loss :  871.535323176
Iteration :  89   Loss :  2152.67832308
Iteration :  90   Loss :  6730.41303569
Iteration :  91   Loss :  929.146816684
Iteration :  92   Loss :  4088.765904
Iteration :  93   Loss :  3989.70294552
Iteration :  94   Loss :  1112.52573432
Iteration :  95   Loss :  2782.89361537
Iteration :  96   Loss :  1414.85055863
Iteration :  97   Loss :  441.408241103
Iteration :  98   Loss :  370.129949735
Iteration :  99   Loss :  3057.37497933
[-0.04159219 -0.01640474 -0.06048375 ...,  0.02528431 -0.03386807
  0.01311976]
CROSS VALIDATION 12
Iteration :  0   Loss :  2853.48756222
Iteration :  1   Loss :  838.188190035
Iteration :  2   Loss :  1022.06485842
Iteration :  3   Loss :  1204.45533216
Iteration :  4   Loss :  1206.36797026
Iteration :  5   Loss :  1523.29239541
Iteration :  6   Loss :  431.660024963
Iteration :  7   Loss :  50.7668797575
Iteration :  8   Loss :  5.96410456648
Iteration :  9   Loss :  0.70623630156
Iteration :  10   Loss :  0.126661273914
Iteration :  11   Loss :  1821.15476455
Iteration :  12   Loss :  5567.91212609
Iteration :  13   Loss :  200.469250945
Iteration :  14   Loss :  23.5583820957
Iteration :  15   Loss :  2.78364745162
Iteration :  16   Loss :  0.331503407486
Iteration :  17   Loss :  0.0687217195791
Iteration :  18   Loss :  3052.20121672
Iteration :  19   Loss :  1696.76343962
Iteration :  20   Loss :  6776.23849218
Iteration :  21   Loss :  415.550376495
Iteration :  22   Loss :  243.836905132
Iteration :  23   Loss :  28.6459652587
Iteration :  24   Loss :  3.36607185652
Iteration :  25   Loss :  0.406150244249
Iteration :  26   Loss :  9308.81033677
Iteration :  27   Loss :  7946.77147962
Iteration :  28   Loss :  342.4982819
Iteration :  29   Loss :  3351.45847664
Iteration :  30   Loss :  2186.2899607
Iteration :  31   Loss :  128.325281767
Iteration :  32   Loss :  15.0756570718
Iteration :  33   Loss :  1.77108867501
Iteration :  34   Loss :  0.214564660768
Iteration :  35   Loss :  4500.69984372
Iteration :  36   Loss :  1754.19531223
Iteration :  37   Loss :  323.989008687
Iteration :  38   Loss :  39.7528618679
Iteration :  39   Loss :  4.67016721654
Iteration :  40   Loss :  0.549021988865
Iteration :  41   Loss :  837.481680022
Iteration :  42   Loss :  1878.34386553
Iteration :  43   Loss :  2025.11697685
Iteration :  44   Loss :  409.142722419
Iteration :  45   Loss :  3266.87626096
Iteration :  46   Loss :  2129.44096295
Iteration :  47   Loss :  3336.8853664
Iteration :  48   Loss :  882.421460453
Iteration :  49   Loss :  2220.41757702
Iteration :  50   Loss :  3435.71767452
Iteration :  51   Loss :  213.213704051
Iteration :  52   Loss :  25.0483508872
Iteration :  53   Loss :  2.94300293395
Iteration :  54   Loss :  0.37000118982
Iteration :  55   Loss :  0.800458972384
Iteration :  56   Loss :  3328.36784441
Iteration :  57   Loss :  460.6997585
Iteration :  58   Loss :  354.999579889
Iteration :  59   Loss :  2694.76098507
Iteration :  60   Loss :  3160.26562188
Iteration :  61   Loss :  751.347173625
Iteration :  62   Loss :  1627.20995418
Iteration :  63   Loss :  1860.86859408
Iteration :  64   Loss :  499.50565044
Iteration :  65   Loss :  3987.49866497
Iteration :  66   Loss :  1907.00919153
Iteration :  67   Loss :  493.160713492
Iteration :  68   Loss :  1536.95880781
Iteration :  69   Loss :  1226.97805542
Iteration :  70   Loss :  2868.87941136
Iteration :  71   Loss :  941.275258568
Iteration :  72   Loss :  5880.09294558
Iteration :  73   Loss :  2025.66432292
Iteration :  74   Loss :  1317.96083771
Iteration :  75   Loss :  1482.72051002
Iteration :  76   Loss :  2017.05158668
Iteration :  77   Loss :  3727.60396397
Iteration :  78   Loss :  4960.82381903
Iteration :  79   Loss :  1497.34926209
Iteration :  80   Loss :  1439.66900387
Iteration :  81   Loss :  4676.90896173
Iteration :  82   Loss :  1678.63572865
Iteration :  83   Loss :  2884.50354232
Iteration :  84   Loss :  674.329650351
Iteration :  85   Loss :  254.806585488
Iteration :  86   Loss :  2676.8116433
Iteration :  87   Loss :  2764.52809389
Iteration :  88   Loss :  2376.70886411
Iteration :  89   Loss :  4107.39658064
Iteration :  90   Loss :  222.597266832
Iteration :  91   Loss :  1070.03859413
Iteration :  92   Loss :  1132.38633199
Iteration :  93   Loss :  2636.61580352
Iteration :  94   Loss :  2358.59010352
Iteration :  95   Loss :  477.884976109
Iteration :  96   Loss :  5746.70586516
Iteration :  97   Loss :  923.901996897
Iteration :  98   Loss :  2544.72356996
Iteration :  99   Loss :  6688.48277123
[-0.13620212 -0.06381226 -0.07027701 ...,  0.05645556 -0.06740081
  0.00886394]
CROSS VALIDATION 13
Iteration :  0   Loss :  4223.94918738
Iteration :  1   Loss :  713.192192288
Iteration :  2   Loss :  1963.91518357
Iteration :  3   Loss :  1943.02567771
Iteration :  4   Loss :  1739.70943908
Iteration :  5   Loss :  607.121580267
Iteration :  6   Loss :  1405.44026648
Iteration :  7   Loss :  3396.3097263
Iteration :  8   Loss :  2292.27407051
Iteration :  9   Loss :  1419.0643272
Iteration :  10   Loss :  2853.24635948
Iteration :  11   Loss :  2449.6569097
Iteration :  12   Loss :  481.282530105
Iteration :  13   Loss :  1158.80473611
Iteration :  14   Loss :  2495.13871813
Iteration :  15   Loss :  190.647546617
Iteration :  16   Loss :  22.3972781887
Iteration :  17   Loss :  2.63125439656
Iteration :  18   Loss :  0.328448116593
Iteration :  19   Loss :  5693.66494361
Iteration :  20   Loss :  951.62261629
Iteration :  21   Loss :  247.341276135
Iteration :  22   Loss :  1090.85057092
Iteration :  23   Loss :  3925.50768736
Iteration :  24   Loss :  725.904719237
Iteration :  25   Loss :  4426.92391071
Iteration :  26   Loss :  382.020709382
Iteration :  27   Loss :  44.8799475475
Iteration :  28   Loss :  5.37543725343
Iteration :  29   Loss :  4572.85136002
Iteration :  30   Loss :  829.974566185
Iteration :  31   Loss :  162.723169477
Iteration :  32   Loss :  19.1367658189
Iteration :  33   Loss :  2.24943922541
Iteration :  34   Loss :  0.621075761592
Iteration :  35   Loss :  2148.42676989
Iteration :  36   Loss :  2370.57353446
Iteration :  37   Loss :  334.381218985
Iteration :  38   Loss :  39.2831133486
Iteration :  39   Loss :  4.61642675378
Iteration :  40   Loss :  6864.62788066
Iteration :  41   Loss :  3607.19525631
Iteration :  42   Loss :  1381.58235757
Iteration :  43   Loss :  108.914037491
Iteration :  44   Loss :  1578.62284965
Iteration :  45   Loss :  1647.38083562
Iteration :  46   Loss :  1148.75375238
Iteration :  47   Loss :  1549.46534978
Iteration :  48   Loss :  3452.96047227
Iteration :  49   Loss :  1136.6642551
Iteration :  50   Loss :  2919.19253216
Iteration :  51   Loss :  4138.09891926
Iteration :  52   Loss :  1614.46016324
Iteration :  53   Loss :  331.209375472
Iteration :  54   Loss :  3146.34459696
Iteration :  55   Loss :  1385.9901683
Iteration :  56   Loss :  940.390631785
Iteration :  57   Loss :  3918.29248046
Iteration :  58   Loss :  634.723427292
Iteration :  59   Loss :  1875.33076788
Iteration :  60   Loss :  3799.57083974
Iteration :  61   Loss :  6767.36245176
Iteration :  62   Loss :  754.644682541
Iteration :  63   Loss :  3017.31756314
Iteration :  64   Loss :  3939.87015921
Iteration :  65   Loss :  514.349359702
Iteration :  66   Loss :  96.2237964521
Iteration :  67   Loss :  11.3043738569
Iteration :  68   Loss :  1.32985222047
Iteration :  69   Loss :  7611.20650198
Iteration :  70   Loss :  4567.69593755
Iteration :  71   Loss :  2678.56714093
Iteration :  72   Loss :  846.049136633
Iteration :  73   Loss :  277.657149936
Iteration :  74   Loss :  2677.9206233
Iteration :  75   Loss :  1269.87922058
Iteration :  76   Loss :  116.614211298
Iteration :  77   Loss :  13.700068592
Iteration :  78   Loss :  1.61359828463
Iteration :  79   Loss :  0.23135177296
Iteration :  80   Loss :  4559.42921966
Iteration :  81   Loss :  1412.12412137
Iteration :  82   Loss :  1068.45420218
Iteration :  83   Loss :  8404.79525176
Iteration :  84   Loss :  3280.94611987
Iteration :  85   Loss :  836.479111679
Iteration :  86   Loss :  511.146681607
Iteration :  87   Loss :  5220.2780007
Iteration :  88   Loss :  2643.67082358
Iteration :  89   Loss :  3350.11994637
Iteration :  90   Loss :  580.730584084
Iteration :  91   Loss :  973.269662315
Iteration :  92   Loss :  4028.17624169
Iteration :  93   Loss :  1661.27839961
Iteration :  94   Loss :  1108.45755335
Iteration :  95   Loss :  2577.81707275
Iteration :  96   Loss :  1918.53262195
Iteration :  97   Loss :  2674.3405265
Iteration :  98   Loss :  6014.60540486
Iteration :  99   Loss :  1658.20877993
[-0.02868107 -0.00571244  0.02958802 ...,  0.04533541  0.02177672
  0.0166264 ]
CROSS VALIDATION 14
Iteration :  0   Loss :  2739.11406951
Iteration :  1   Loss :  1703.07918814
Iteration :  2   Loss :  661.349307271
Iteration :  3   Loss :  1581.66794527
Iteration :  4   Loss :  1185.43304171
Iteration :  5   Loss :  1690.48022306
Iteration :  6   Loss :  1639.61160529
Iteration :  7   Loss :  1651.12080019
Iteration :  8   Loss :  2892.81091136
Iteration :  9   Loss :  1748.02953043
Iteration :  10   Loss :  3413.62245249
Iteration :  11   Loss :  1541.51400387
Iteration :  12   Loss :  753.594120151
Iteration :  13   Loss :  1595.07541996
Iteration :  14   Loss :  1934.75443648
Iteration :  15   Loss :  1723.79896805
Iteration :  16   Loss :  1800.01483269
Iteration :  17   Loss :  2510.89581644
Iteration :  18   Loss :  230.604663702
Iteration :  19   Loss :  4147.0325781
Iteration :  20   Loss :  1966.18121225
Iteration :  21   Loss :  1633.92626124
Iteration :  22   Loss :  4295.72276853
Iteration :  23   Loss :  1186.43954467
Iteration :  24   Loss :  1405.71977062
Iteration :  25   Loss :  715.704611797
Iteration :  26   Loss :  1566.85203022
Iteration :  27   Loss :  1570.3160825
Iteration :  28   Loss :  1229.28182919
Iteration :  29   Loss :  4171.52587474
Iteration :  30   Loss :  1757.03611789
Iteration :  31   Loss :  115.433361382
Iteration :  32   Loss :  2195.61700777
Iteration :  33   Loss :  624.954933507
Iteration :  34   Loss :  1290.30651249
Iteration :  35   Loss :  2642.02503063
Iteration :  36   Loss :  4448.48483949
Iteration :  37   Loss :  1875.25630065
Iteration :  38   Loss :  139.628103945
Iteration :  39   Loss :  16.4055355075
Iteration :  40   Loss :  1.92855514069
Iteration :  41   Loss :  0.244520351638
Iteration :  42   Loss :  5418.05625945
Iteration :  43   Loss :  1232.77887186
Iteration :  44   Loss :  473.584314849
Iteration :  45   Loss :  2324.16482428
Iteration :  46   Loss :  645.839639348
Iteration :  47   Loss :  1215.94093547
Iteration :  48   Loss :  2507.98500068
Iteration :  49   Loss :  182.54700526
Iteration :  50   Loss :  21.4481804733
Iteration :  51   Loss :  8570.26058292
Iteration :  52   Loss :  1218.19341365
Iteration :  53   Loss :  2239.93854328
Iteration :  54   Loss :  1796.79592955
Iteration :  55   Loss :  3019.75891945
Iteration :  56   Loss :  1660.64121769
Iteration :  57   Loss :  580.560940283
Iteration :  58   Loss :  62.1555850893
Iteration :  59   Loss :  7.30203986746
Iteration :  60   Loss :  0.860484515709
Iteration :  61   Loss :  7859.91476274
Iteration :  62   Loss :  1588.13633192
Iteration :  63   Loss :  1057.18746829
Iteration :  64   Loss :  2077.2296752
Iteration :  65   Loss :  344.253074309
Iteration :  66   Loss :  977.60386727
Iteration :  67   Loss :  75.0664200684
Iteration :  68   Loss :  8.81880477966
Iteration :  69   Loss :  1.03615502803
Iteration :  70   Loss :  0.12539326191
Iteration :  71   Loss :  5478.89164985
Iteration :  72   Loss :  1284.08278484
Iteration :  73   Loss :  447.25594491
Iteration :  74   Loss :  2706.99915842
Iteration :  75   Loss :  1422.70277157
Iteration :  76   Loss :  1184.79474325
Iteration :  77   Loss :  225.857655433
Iteration :  78   Loss :  4114.39302904
Iteration :  79   Loss :  1854.00859381
Iteration :  80   Loss :  918.971894976
Iteration :  81   Loss :  3460.19747946
Iteration :  82   Loss :  230.083386557
Iteration :  83   Loss :  3055.21188732
Iteration :  84   Loss :  2342.52874822
Iteration :  85   Loss :  667.923104176
Iteration :  86   Loss :  3899.40957439
Iteration :  87   Loss :  496.567966266
Iteration :  88   Loss :  1189.6294541
Iteration :  89   Loss :  3227.11960675
Iteration :  90   Loss :  5010.79113074
Iteration :  91   Loss :  3596.03951825
Iteration :  92   Loss :  2580.84194024
Iteration :  93   Loss :  278.552013302
Iteration :  94   Loss :  2492.85352861
Iteration :  95   Loss :  1128.45340707
Iteration :  96   Loss :  2445.35913027
Iteration :  97   Loss :  2063.14692604
Iteration :  98   Loss :  864.801482158
Iteration :  99   Loss :  3488.33518589
[-0.08587144  0.0205718  -0.06480441 ...,  0.06677188 -0.03023623
  0.0082784 ]
CROSS VALIDATION 15
Iteration :  0   Loss :  2739.11406951
Iteration :  1   Loss :  1703.07918814
Iteration :  2   Loss :  661.349307271
Iteration :  3   Loss :  1630.16370677
Iteration :  4   Loss :  1683.96819375
Iteration :  5   Loss :  476.582882319
Iteration :  6   Loss :  2155.72575934
Iteration :  7   Loss :  1904.04012317
Iteration :  8   Loss :  2816.16635661
Iteration :  9   Loss :  1686.0181684
Iteration :  10   Loss :  1191.53515368
Iteration :  11   Loss :  1438.90793892
Iteration :  12   Loss :  757.61477162
Iteration :  13   Loss :  2013.46525833
Iteration :  14   Loss :  1972.7140327
Iteration :  15   Loss :  1626.96798377
Iteration :  16   Loss :  691.373220523
Iteration :  17   Loss :  65.9649874277
Iteration :  18   Loss :  7.75077000373
Iteration :  19   Loss :  0.912021967592
Iteration :  20   Loss :  7591.61277335
Iteration :  21   Loss :  1668.94261172
Iteration :  22   Loss :  4846.37471909
Iteration :  23   Loss :  1303.25685633
Iteration :  24   Loss :  4402.92496413
Iteration :  25   Loss :  1451.02611603
Iteration :  26   Loss :  2729.75389294
Iteration :  27   Loss :  826.491890182
Iteration :  28   Loss :  2822.94436487
Iteration :  29   Loss :  2133.08761379
Iteration :  30   Loss :  2429.14100485
Iteration :  31   Loss :  2332.69249811
Iteration :  32   Loss :  1302.02637538
Iteration :  33   Loss :  1776.38475925
Iteration :  34   Loss :  1425.12946776
Iteration :  35   Loss :  4380.55194486
Iteration :  36   Loss :  488.864724282
Iteration :  37   Loss :  3314.71092348
Iteration :  38   Loss :  536.243572322
Iteration :  39   Loss :  1107.67218147
Iteration :  40   Loss :  2932.13363664
Iteration :  41   Loss :  1494.25766359
Iteration :  42   Loss :  706.623376994
Iteration :  43   Loss :  1552.73913769
Iteration :  44   Loss :  2174.98130088
Iteration :  45   Loss :  431.144167998
Iteration :  46   Loss :  4128.1676268
Iteration :  47   Loss :  339.163420424
Iteration :  48   Loss :  39.8449264883
Iteration :  49   Loss :  4.6826332167
Iteration :  50   Loss :  0.661058723301
Iteration :  51   Loss :  8048.66972597
Iteration :  52   Loss :  2556.92249701
Iteration :  53   Loss :  451.99023446
Iteration :  54   Loss :  2209.37970489
Iteration :  55   Loss :  1658.72112848
Iteration :  56   Loss :  977.987119864
Iteration :  57   Loss :  5816.072799
Iteration :  58   Loss :  1646.49013004
Iteration :  59   Loss :  2778.21516322
Iteration :  60   Loss :  2013.43493808
Iteration :  61   Loss :  2455.38710425
Iteration :  62   Loss :  374.916915709
Iteration :  63   Loss :  5158.82220553
Iteration :  64   Loss :  1380.37150101
Iteration :  65   Loss :  1057.60006157
Iteration :  66   Loss :  2747.0242189
Iteration :  67   Loss :  2025.09851602
Iteration :  68   Loss :  708.528211745
Iteration :  69   Loss :  98.1887262431
Iteration :  70   Loss :  5505.32482277
Iteration :  71   Loss :  1729.57318973
Iteration :  72   Loss :  536.289563565
Iteration :  73   Loss :  2303.86467805
Iteration :  74   Loss :  1206.90970188
Iteration :  75   Loss :  207.443881112
Iteration :  76   Loss :  2085.47732704
Iteration :  77   Loss :  4098.27779986
Iteration :  78   Loss :  504.285120764
Iteration :  79   Loss :  59.2434276579
Iteration :  80   Loss :  6.95991926009
Iteration :  81   Loss :  0.819347900056
Iteration :  82   Loss :  7020.39888077
Iteration :  83   Loss :  2195.59147288
Iteration :  84   Loss :  169.247646499
Iteration :  85   Loss :  19.8832175501
Iteration :  86   Loss :  2.34038179006
Iteration :  87   Loss :  0.321638675672
Iteration :  88   Loss :  4617.5852183
Iteration :  89   Loss :  1352.12296238
Iteration :  90   Loss :  2075.29615523
Iteration :  91   Loss :  2940.99953452
Iteration :  92   Loss :  1697.64248195
Iteration :  93   Loss :  623.9161832
Iteration :  94   Loss :  1234.51218933
Iteration :  95   Loss :  1705.47349182
Iteration :  96   Loss :  4376.25222327
Iteration :  97   Loss :  462.401104269
Iteration :  98   Loss :  95.1575441626
Iteration :  99   Loss :  11.1791192094
[ -2.18158038e-02  -7.83832055e-03   2.45651272e-05 ...,   1.04920508e-02
   2.14287745e-03   2.04009074e-03]
CROSS VALIDATION 16
Iteration :  0   Loss :  2739.11406951
Iteration :  1   Loss :  1703.07918814
Iteration :  2   Loss :  77.1520559235
Iteration :  3   Loss :  9.06480684989
Iteration :  4   Loss :  1.07883988213
Iteration :  5   Loss :  0.15768694239
Iteration :  6   Loss :  5851.25799768
Iteration :  7   Loss :  2726.94740805
Iteration :  8   Loss :  390.934325687
Iteration :  9   Loss :  45.9269774861
Iteration :  10   Loss :  5.39922828894
Iteration :  11   Loss :  0.637419781974
Iteration :  12   Loss :  8641.62290281
Iteration :  13   Loss :  3487.50418167
Iteration :  14   Loss :  1261.14868599
Iteration :  15   Loss :  3364.65628555
Iteration :  16   Loss :  1124.01927886
Iteration :  17   Loss :  1379.20608604
Iteration :  18   Loss :  1847.72892147
Iteration :  19   Loss :  4161.89413231
Iteration :  20   Loss :  995.415243057
Iteration :  21   Loss :  1269.52186747
Iteration :  22   Loss :  3245.39056787
Iteration :  23   Loss :  764.170233399
Iteration :  24   Loss :  1638.07976145
Iteration :  25   Loss :  1775.59365591
Iteration :  26   Loss :  1080.09621653
Iteration :  27   Loss :  1335.32612932
Iteration :  28   Loss :  3226.7653126
Iteration :  29   Loss :  947.347934573
Iteration :  30   Loss :  387.764978564
Iteration :  31   Loss :  2748.95863104
Iteration :  32   Loss :  1742.02501414
Iteration :  33   Loss :  1483.10712542
Iteration :  34   Loss :  1713.55598782
Iteration :  35   Loss :  4918.5212531
Iteration :  36   Loss :  2252.06128235
Iteration :  37   Loss :  1346.59888945
Iteration :  38   Loss :  1225.23054464
Iteration :  39   Loss :  1293.9513711
Iteration :  40   Loss :  1163.56310661
Iteration :  41   Loss :  2350.17555914
Iteration :  42   Loss :  562.837728829
Iteration :  43   Loss :  1069.23082013
Iteration :  44   Loss :  4775.61344398
Iteration :  45   Loss :  6194.72836179
Iteration :  46   Loss :  1002.01905036
Iteration :  47   Loss :  1411.46986471
Iteration :  48   Loss :  2961.46141374
Iteration :  49   Loss :  1574.54083562
Iteration :  50   Loss :  2714.54084772
Iteration :  51   Loss :  4613.60129096
Iteration :  52   Loss :  2172.21183047
Iteration :  53   Loss :  1546.49804943
Iteration :  54   Loss :  2767.68890173
Iteration :  55   Loss :  1676.86551127
Iteration :  56   Loss :  856.542663487
Iteration :  57   Loss :  2173.31977318
Iteration :  58   Loss :  278.393824952
Iteration :  59   Loss :  653.278738877
Iteration :  60   Loss :  2185.30127509
Iteration :  61   Loss :  1144.68351747
Iteration :  62   Loss :  1525.58031628
Iteration :  63   Loss :  875.338097692
Iteration :  64   Loss :  1446.89987645
Iteration :  65   Loss :  3067.7933218
Iteration :  66   Loss :  331.907279934
Iteration :  67   Loss :  38.9924749279
Iteration :  68   Loss :  4.58083690361
Iteration :  69   Loss :  0.542033609711
Iteration :  70   Loss :  4574.90227611
Iteration :  71   Loss :  706.758506055
Iteration :  72   Loss :  558.432720652
Iteration :  73   Loss :  6593.47515851
Iteration :  74   Loss :  1649.4456436
Iteration :  75   Loss :  462.665755983
Iteration :  76   Loss :  893.851429199
Iteration :  77   Loss :  1921.52285983
Iteration :  78   Loss :  3344.00051547
Iteration :  79   Loss :  620.393624476
Iteration :  80   Loss :  980.216879767
Iteration :  81   Loss :  3725.46060791
Iteration :  82   Loss :  1137.72298152
Iteration :  83   Loss :  1087.6426294
Iteration :  84   Loss :  1558.89002516
Iteration :  85   Loss :  1405.69703594
Iteration :  86   Loss :  6910.56344638
Iteration :  87   Loss :  3626.48948049
Iteration :  88   Loss :  333.736021725
Iteration :  89   Loss :  1355.59266346
Iteration :  90   Loss :  3274.22942909
Iteration :  91   Loss :  1195.03104318
Iteration :  92   Loss :  2613.59382061
Iteration :  93   Loss :  2018.41868533
Iteration :  94   Loss :  362.236829591
Iteration :  95   Loss :  306.023586925
Iteration :  96   Loss :  35.951658076
Iteration :  97   Loss :  4.22407307896
Iteration :  98   Loss :  8397.40787484
Iteration :  99   Loss :  7923.47219455
[-0.0368581  -0.03082378 -0.00894535 ...,  0.06306294  0.02306978
  0.03676247]
CROSS VALIDATION 17
Iteration :  0   Loss :  2739.11406951
Iteration :  1   Loss :  1703.07918814
Iteration :  2   Loss :  683.602678833
Iteration :  3   Loss :  1633.33538156
Iteration :  4   Loss :  1671.42965893
Iteration :  5   Loss :  501.724055068
Iteration :  6   Loss :  4022.18402967
Iteration :  7   Loss :  724.253823948
Iteration :  8   Loss :  2037.33604015
Iteration :  9   Loss :  3024.10463816
Iteration :  10   Loss :  688.322059527
Iteration :  11   Loss :  2576.57561771
Iteration :  12   Loss :  2729.00234768
Iteration :  13   Loss :  807.67123852
Iteration :  14   Loss :  794.664909199
Iteration :  15   Loss :  2096.58199833
Iteration :  16   Loss :  424.210377619
Iteration :  17   Loss :  638.063795513
Iteration :  18   Loss :  3807.40471796
Iteration :  19   Loss :  1949.5070317
Iteration :  20   Loss :  955.801526001
Iteration :  21   Loss :  2755.8981245
Iteration :  22   Loss :  3699.46279772
Iteration :  23   Loss :  2998.59960343
Iteration :  24   Loss :  154.593301156
Iteration :  25   Loss :  1586.50035261
Iteration :  26   Loss :  2276.0832156
Iteration :  27   Loss :  1041.74291849
Iteration :  28   Loss :  4390.44188403
Iteration :  29   Loss :  471.937782457
Iteration :  30   Loss :  1034.40881999
Iteration :  31   Loss :  2553.96614708
Iteration :  32   Loss :  2393.55934148
Iteration :  33   Loss :  760.958742201
Iteration :  34   Loss :  1318.73754784
Iteration :  35   Loss :  3114.49164554
Iteration :  36   Loss :  1583.06309016
Iteration :  37   Loss :  145.195126116
Iteration :  38   Loss :  3070.33785489
Iteration :  39   Loss :  3184.90565022
Iteration :  40   Loss :  4250.07647606
Iteration :  41   Loss :  349.507404772
Iteration :  42   Loss :  2921.79664343
Iteration :  43   Loss :  1640.21761883
Iteration :  44   Loss :  509.568016482
Iteration :  45   Loss :  3545.45874207
Iteration :  46   Loss :  2237.76263455
Iteration :  47   Loss :  523.673963497
Iteration :  48   Loss :  962.769676199
Iteration :  49   Loss :  2675.89370176
Iteration :  50   Loss :  2722.48846038
Iteration :  51   Loss :  1863.47529415
Iteration :  52   Loss :  2469.97965973
Iteration :  53   Loss :  2117.57903138
Iteration :  54   Loss :  1725.13805741
Iteration :  55   Loss :  544.252911142
Iteration :  56   Loss :  1452.99029334
Iteration :  57   Loss :  3122.85879482
Iteration :  58   Loss :  1466.43004216
Iteration :  59   Loss :  1842.13455083
Iteration :  60   Loss :  188.846151508
Iteration :  61   Loss :  22.187119072
Iteration :  62   Loss :  2.63676193991
Iteration :  63   Loss :  10028.4715374
Iteration :  64   Loss :  4346.69710183
Iteration :  65   Loss :  2873.77172689
Iteration :  66   Loss :  290.021547018
Iteration :  67   Loss :  4577.64885064
Iteration :  68   Loss :  2002.97258239
Iteration :  69   Loss :  2898.42863497
Iteration :  70   Loss :  1129.0953244
Iteration :  71   Loss :  3085.63990148
Iteration :  72   Loss :  933.203192291
Iteration :  73   Loss :  2432.96785423
Iteration :  74   Loss :  900.278517973
Iteration :  75   Loss :  1667.91390337
Iteration :  76   Loss :  885.293817513
Iteration :  77   Loss :  2108.92211318
Iteration :  78   Loss :  266.708257309
Iteration :  79   Loss :  1911.70157843
Iteration :  80   Loss :  1100.27174853
Iteration :  81   Loss :  1023.11837481
Iteration :  82   Loss :  5964.72665658
Iteration :  83   Loss :  4513.79290781
Iteration :  84   Loss :  2188.80776347
Iteration :  85   Loss :  799.355634069
Iteration :  86   Loss :  1822.59531599
Iteration :  87   Loss :  2550.92090811
Iteration :  88   Loss :  701.352805545
Iteration :  89   Loss :  1364.14290641
Iteration :  90   Loss :  1159.1036912
Iteration :  91   Loss :  977.014390877
Iteration :  92   Loss :  3083.23839688
Iteration :  93   Loss :  624.324282378
Iteration :  94   Loss :  1717.749914
Iteration :  95   Loss :  3815.35336971
Iteration :  96   Loss :  1660.20546559
Iteration :  97   Loss :  240.965462243
Iteration :  98   Loss :  226.586208036
Iteration :  99   Loss :  1715.22497192
[-0.04404147 -0.05293645  0.03135205 ...,  0.01191035 -0.00426469
 -0.00168163]
CROSS VALIDATION 18
Iteration :  0   Loss :  2368.56738647
Iteration :  1   Loss :  459.627774627
Iteration :  2   Loss :  334.237908333
Iteration :  3   Loss :  1116.40281675
Iteration :  4   Loss :  1040.19107211
Iteration :  5   Loss :  2092.41276157
Iteration :  6   Loss :  928.99865378
Iteration :  7   Loss :  679.561522034
Iteration :  8   Loss :  2417.81541247
Iteration :  9   Loss :  5121.9930912
Iteration :  10   Loss :  553.112728226
Iteration :  11   Loss :  1463.49849277
Iteration :  12   Loss :  3668.30152089
Iteration :  13   Loss :  770.47413249
Iteration :  14   Loss :  309.146880635
Iteration :  15   Loss :  36.3185826915
Iteration :  16   Loss :  4.26670793517
Iteration :  17   Loss :  0.501552542948
Iteration :  18   Loss :  6804.18589011
Iteration :  19   Loss :  7568.65414547
Iteration :  20   Loss :  2552.81170082
Iteration :  21   Loss :  2085.10591018
Iteration :  22   Loss :  2011.80765578
Iteration :  23   Loss :  1929.50653246
Iteration :  24   Loss :  2883.50751417
Iteration :  25   Loss :  3696.75428834
Iteration :  26   Loss :  1594.75788772
Iteration :  27   Loss :  312.1586491
Iteration :  28   Loss :  36.6751289566
Iteration :  29   Loss :  4.31088630157
Iteration :  30   Loss :  0.527929127089
Iteration :  31   Loss :  2658.87218715
Iteration :  32   Loss :  3152.59050503
Iteration :  33   Loss :  176.842530256
Iteration :  34   Loss :  20.7756774215
Iteration :  35   Loss :  2.44121927463
Iteration :  36   Loss :  0.51326420859
Iteration :  37   Loss :  6095.11019593
Iteration :  38   Loss :  2390.75969195
Iteration :  39   Loss :  4500.87784029
Iteration :  40   Loss :  1347.32881316
Iteration :  41   Loss :  326.048105972
Iteration :  42   Loss :  242.712891025
Iteration :  43   Loss :  1828.05774038
Iteration :  44   Loss :  690.114065468
Iteration :  45   Loss :  683.819791283
Iteration :  46   Loss :  1596.77636282
Iteration :  47   Loss :  1473.88148642
Iteration :  48   Loss :  887.74944377
Iteration :  49   Loss :  1768.59661862
Iteration :  50   Loss :  1520.11726212
Iteration :  51   Loss :  1103.02781529
Iteration :  52   Loss :  2703.72626693
Iteration :  53   Loss :  1263.293696
Iteration :  54   Loss :  2745.87506659
Iteration :  55   Loss :  1158.30616941
Iteration :  56   Loss :  2911.15361206
Iteration :  57   Loss :  2679.54196031
Iteration :  58   Loss :  3404.57432784
Iteration :  59   Loss :  459.081725573
Iteration :  60   Loss :  1782.24148369
Iteration :  61   Loss :  2648.1625961
Iteration :  62   Loss :  916.255047916
Iteration :  63   Loss :  2898.01944555
Iteration :  64   Loss :  947.351247326
Iteration :  65   Loss :  74.8049726498
Iteration :  66   Loss :  8.7880899196
Iteration :  67   Loss :  1.03361808036
Iteration :  68   Loss :  0.810347816174
Iteration :  69   Loss :  4199.83719494
Iteration :  70   Loss :  1009.45468619
Iteration :  71   Loss :  2258.46156228
Iteration :  72   Loss :  3214.65463782
Iteration :  73   Loss :  1931.31158218
Iteration :  74   Loss :  187.933928925
Iteration :  75   Loss :  1607.97505426
Iteration :  76   Loss :  4547.40008866
Iteration :  77   Loss :  307.867084681
Iteration :  78   Loss :  2029.67256111
Iteration :  79   Loss :  3159.3612057
Iteration :  80   Loss :  1049.81887434
Iteration :  81   Loss :  3640.39811724
Iteration :  82   Loss :  3151.66910677
Iteration :  83   Loss :  1234.25004324
Iteration :  84   Loss :  1798.63526651
Iteration :  85   Loss :  2862.64266943
Iteration :  86   Loss :  2083.33225976
Iteration :  87   Loss :  1387.55519288
Iteration :  88   Loss :  1152.41647152
Iteration :  89   Loss :  1177.82841287
Iteration :  90   Loss :  2263.15302145
Iteration :  91   Loss :  1132.97151181
Iteration :  92   Loss :  1196.90718789
Iteration :  93   Loss :  2311.14996763
Iteration :  94   Loss :  4099.82744707
Iteration :  95   Loss :  2797.10700437
Iteration :  96   Loss :  2240.04759094
Iteration :  97   Loss :  1518.63968719
Iteration :  98   Loss :  248.754366627
Iteration :  99   Loss :  29.2236689205
[ 0.0065695  -0.01908603  0.00516369 ...,  0.00375283  0.00313534
  0.00136157]
CROSS VALIDATION 19
Iteration :  0   Loss :  2731.69490482
Iteration :  1   Loss :  1699.89623596
Iteration :  2   Loss :  492.800434278
Iteration :  3   Loss :  1371.96365033
Iteration :  4   Loss :  1371.845007
Iteration :  5   Loss :  1049.45930939
Iteration :  6   Loss :  3250.06478457
Iteration :  7   Loss :  1536.74908954
Iteration :  8   Loss :  361.244477683
Iteration :  9   Loss :  2756.28635319
Iteration :  10   Loss :  263.334504669
Iteration :  11   Loss :  1881.34008857
Iteration :  12   Loss :  4627.84842114
Iteration :  13   Loss :  334.112943352
Iteration :  14   Loss :  39.2523755389
Iteration :  15   Loss :  4.61346136125
Iteration :  16   Loss :  0.547022482992
Iteration :  17   Loss :  0.166524663187
Iteration :  18   Loss :  4784.10110796
Iteration :  19   Loss :  1151.34618293
Iteration :  20   Loss :  2934.17651338
Iteration :  21   Loss :  601.699281861
Iteration :  22   Loss :  2779.34876745
Iteration :  23   Loss :  872.651739867
Iteration :  24   Loss :  1279.4717322
Iteration :  25   Loss :  1655.93325059
Iteration :  26   Loss :  1563.09102779
Iteration :  27   Loss :  301.080901868
Iteration :  28   Loss :  2244.60188886
Iteration :  29   Loss :  966.733595694
Iteration :  30   Loss :  4699.39492935
Iteration :  31   Loss :  1096.71540654
Iteration :  32   Loss :  3818.24833134
Iteration :  33   Loss :  1354.46754159
Iteration :  34   Loss :  3894.05894828
Iteration :  35   Loss :  1486.8424486
Iteration :  36   Loss :  1173.11635502
Iteration :  37   Loss :  1843.00088092
Iteration :  38   Loss :  905.523764058
Iteration :  39   Loss :  599.928823342
Iteration :  40   Loss :  1437.44184095
Iteration :  41   Loss :  1696.15826574
Iteration :  42   Loss :  1053.28235244
Iteration :  43   Loss :  1563.39744315
Iteration :  44   Loss :  1212.16419958
Iteration :  45   Loss :  2552.21640522
Iteration :  46   Loss :  2236.70141614
Iteration :  47   Loss :  355.516532993
Iteration :  48   Loss :  4238.1470497
Iteration :  49   Loss :  1450.1071409
Iteration :  50   Loss :  1387.20570116
Iteration :  51   Loss :  2532.8869593
Iteration :  52   Loss :  1314.46282164
Iteration :  53   Loss :  1062.55557729
Iteration :  54   Loss :  331.116746101
Iteration :  55   Loss :  38.908420085
Iteration :  56   Loss :  5396.37215042
Iteration :  57   Loss :  5784.87144672
Iteration :  58   Loss :  526.328869534
Iteration :  59   Loss :  481.546569983
Iteration :  60   Loss :  1644.09302022
Iteration :  61   Loss :  2223.94899415
Iteration :  62   Loss :  1167.51802152
Iteration :  63   Loss :  1287.91634557
Iteration :  64   Loss :  1063.08318141
Iteration :  65   Loss :  735.387868855
Iteration :  66   Loss :  1718.99472696
Iteration :  67   Loss :  1691.36493907
Iteration :  68   Loss :  2264.44762807
Iteration :  69   Loss :  776.850831444
Iteration :  70   Loss :  1472.43493938
Iteration :  71   Loss :  1414.98704941
Iteration :  72   Loss :  1711.93900137
Iteration :  73   Loss :  1512.12641737
Iteration :  74   Loss :  2220.0206671
Iteration :  75   Loss :  294.320744963
Iteration :  76   Loss :  2580.82532519
Iteration :  77   Loss :  1783.06583194
Iteration :  78   Loss :  1544.92335241
Iteration :  79   Loss :  1119.73075545
Iteration :  80   Loss :  3879.46737166
Iteration :  81   Loss :  2118.95409186
Iteration :  82   Loss :  335.453565262
Iteration :  83   Loss :  3416.54087904
Iteration :  84   Loss :  3359.98583486
Iteration :  85   Loss :  626.987444378
Iteration :  86   Loss :  3039.63501515
Iteration :  87   Loss :  3416.56731231
Iteration :  88   Loss :  952.195023843
Iteration :  89   Loss :  4153.42931311
Iteration :  90   Loss :  558.400458819
Iteration :  91   Loss :  1885.65316354
Iteration :  92   Loss :  2831.36208037
Iteration :  93   Loss :  612.239811696
Iteration :  94   Loss :  2151.13992731
Iteration :  95   Loss :  1875.66028383
Iteration :  96   Loss :  1288.49694313
Iteration :  97   Loss :  1599.66373612
Iteration :  98   Loss :  2485.72820716
Iteration :  99   Loss :  361.015961457
[-0.0809987  -0.06098917 -0.05165221 ...,  0.02652988  0.0277708
  0.00763971]
Accuracy (Logistic Loss):	0.7
Hinge Loss
CROSS VALIDATION 0
Iteration :  0   Loss :  3219.86579996
Iteration :  1   Loss :  1248.10737345
Iteration :  2   Loss :  2075.73399916
Iteration :  3   Loss :  1869.22351741
Iteration :  4   Loss :  415.101409547
Iteration :  5   Loss :  649.637108367
Iteration :  6   Loss :  95.9973753737
Iteration :  7   Loss :  3142.95581184
Iteration :  8   Loss :  439.704270472
Iteration :  9   Loss :  51.6564678711
Iteration :  10   Loss :  6.06860304098
Iteration :  11   Loss :  0.712939625699
Iteration :  12   Loss :  0.0837561637266
Iteration :  13   Loss :  10868.3715836
Iteration :  14   Loss :  3085.10947594
Iteration :  15   Loss :  3102.11834779
Iteration :  16   Loss :  220.267625582
Iteration :  17   Loss :  25.877045751
Iteration :  18   Loss :  3.0400359337
Iteration :  19   Loss :  0.357143491846
Iteration :  20   Loss :  4538.54934507
Iteration :  21   Loss :  1316.41514345
Iteration :  22   Loss :  761.67222872
Iteration :  23   Loss :  3056.88753549
Iteration :  24   Loss :  2253.60925557
Iteration :  25   Loss :  1767.02027968
Iteration :  26   Loss :  1312.44508098
Iteration :  27   Loss :  412.654295499
Iteration :  28   Loss :  347.578638608
Iteration :  29   Loss :  3607.67022347
Iteration :  30   Loss :  1845.08448142
Iteration :  31   Loss :  3534.73071057
Iteration :  32   Loss :  666.563273571
Iteration :  33   Loss :  2067.5443935
Iteration :  34   Loss :  878.67989229
Iteration :  35   Loss :  3153.71795898
Iteration :  36   Loss :  1564.15249507
Iteration :  37   Loss :  891.999416802
Iteration :  38   Loss :  1059.91460117
Iteration :  39   Loss :  1309.16643409
Iteration :  40   Loss :  248.985958041
Iteration :  41   Loss :  29.250876113
Iteration :  42   Loss :  3.59742084726
Iteration :  43   Loss :  1964.61788267
Iteration :  44   Loss :  651.354821143
Iteration :  45   Loss :  1936.7595768
Iteration :  46   Loss :  1302.73071799
Iteration :  47   Loss :  492.854964149
Iteration :  48   Loss :  3946.57600673
Iteration :  49   Loss :  2337.30512158
Iteration :  50   Loss :  3375.39272912
Iteration :  51   Loss :  1248.58338358
Iteration :  52   Loss :  290.837436539
Iteration :  53   Loss :  1367.25591271
Iteration :  54   Loss :  1576.28149611
Iteration :  55   Loss :  2676.80850036
Iteration :  56   Loss :  2241.90198051
Iteration :  57   Loss :  1787.30442091
Iteration :  58   Loss :  4700.20494662
Iteration :  59   Loss :  2177.86234611
Iteration :  60   Loss :  555.718176208
Iteration :  61   Loss :  2750.29176793
Iteration :  62   Loss :  434.104526005
Iteration :  63   Loss :  850.604629775
Iteration :  64   Loss :  4087.05744457
Iteration :  65   Loss :  1664.58877145
Iteration :  66   Loss :  1679.6022468
Iteration :  67   Loss :  3658.85168098
Iteration :  68   Loss :  774.857976049
Iteration :  69   Loss :  1513.99751724
Iteration :  70   Loss :  5678.04786451
Iteration :  71   Loss :  1352.28121141
Iteration :  72   Loss :  4034.06967562
Iteration :  73   Loss :  2897.26152266
Iteration :  74   Loss :  823.30431708
Iteration :  75   Loss :  3006.32877884
Iteration :  76   Loss :  1592.18681598
Iteration :  77   Loss :  388.877171738
Iteration :  78   Loss :  3902.32080838
Iteration :  79   Loss :  5377.78966769
Iteration :  80   Loss :  1142.21208349
Iteration :  81   Loss :  3258.93465804
Iteration :  82   Loss :  3199.71898213
Iteration :  83   Loss :  480.635066966
Iteration :  84   Loss :  2114.23894858
Iteration :  85   Loss :  2968.43250184
Iteration :  86   Loss :  220.433544229
Iteration :  87   Loss :  25.8965378775
Iteration :  88   Loss :  3.04232586917
Iteration :  89   Loss :  0.357412513519
Iteration :  90   Loss :  0.562510510913
Iteration :  91   Loss :  3652.98760987
Iteration :  92   Loss :  1140.0795873
Iteration :  93   Loss :  736.109607653
Iteration :  94   Loss :  4572.48270148
Iteration :  95   Loss :  1157.48601937
Iteration :  96   Loss :  2192.15548428
Iteration :  97   Loss :  2680.27808122
Iteration :  98   Loss :  310.543299261
Iteration :  99   Loss :  2964.21868715
[-0.04575023  0.02182507 -0.02539069 ...,  0.0327036   0.01233592
  0.03860942]
CROSS VALIDATION 1
Iteration :  0   Loss :  1894.67918062
Iteration :  1   Loss :  1556.65603809
Iteration :  2   Loss :  1517.25215398
Iteration :  3   Loss :  294.264784852
Iteration :  4   Loss :  37.5185277038
Iteration :  5   Loss :  4.40767750293
Iteration :  6   Loss :  0.51781405505
Iteration :  7   Loss :  0.0945166610871
Iteration :  8   Loss :  3581.62733762
Iteration :  9   Loss :  1068.95051979
Iteration :  10   Loss :  129.93581005
Iteration :  11   Loss :  15.2648619718
Iteration :  12   Loss :  3827.59222994
Iteration :  13   Loss :  588.701335971
Iteration :  14   Loss :  69.1606465742
Iteration :  15   Loss :  8.12499436012
Iteration :  16   Loss :  0.954524525463
Iteration :  17   Loss :  0.69195580056
Iteration :  18   Loss :  1601.75049127
Iteration :  19   Loss :  789.901249374
Iteration :  20   Loss :  1623.61481298
Iteration :  21   Loss :  6760.1150065
Iteration :  22   Loss :  331.168510395
Iteration :  23   Loss :  38.9056842654
Iteration :  24   Loss :  4.57064068789
Iteration :  25   Loss :  0.536958973792
Iteration :  26   Loss :  12599.3517126
Iteration :  27   Loss :  3056.362606
Iteration :  28   Loss :  1517.70463205
Iteration :  29   Loss :  704.675853142
Iteration :  30   Loss :  3732.86826021
Iteration :  31   Loss :  926.38847082
Iteration :  32   Loss :  921.764405302
Iteration :  33   Loss :  1171.9885958
Iteration :  34   Loss :  159.223334222
Iteration :  35   Loss :  2532.11786206
Iteration :  36   Loss :  1562.07466884
Iteration :  37   Loss :  3848.67955366
Iteration :  38   Loss :  873.780485491
Iteration :  39   Loss :  2328.52249457
Iteration :  40   Loss :  2197.32690265
Iteration :  41   Loss :  1036.93890112
Iteration :  42   Loss :  1496.22536904
Iteration :  43   Loss :  1468.35079028
Iteration :  44   Loss :  258.434704729
Iteration :  45   Loss :  30.3609150925
Iteration :  46   Loss :  3.56680100771
Iteration :  47   Loss :  0.419027864932
Iteration :  48   Loss :  0.0492274032698
Iteration :  49   Loss :  4625.3193598
Iteration :  50   Loss :  3508.24888135
Iteration :  51   Loss :  409.368717587
Iteration :  52   Loss :  291.309465894
Iteration :  53   Loss :  2928.50370046
Iteration :  54   Loss :  634.927012711
Iteration :  55   Loss :  1698.0121732
Iteration :  56   Loss :  3576.47604928
Iteration :  57   Loss :  176.428015686
Iteration :  58   Loss :  20.7267673658
Iteration :  59   Loss :  2.43498110981
Iteration :  60   Loss :  0.286061637133
Iteration :  61   Loss :  0.0336065277507
Iteration :  62   Loss :  6447.60600392
Iteration :  63   Loss :  1043.7919439
Iteration :  64   Loss :  365.999978724
Iteration :  65   Loss :  42.9976859708
Iteration :  66   Loss :  5.05136914294
Iteration :  67   Loss :  0.660882095964
Iteration :  68   Loss :  5607.88685311
Iteration :  69   Loss :  4455.00007737
Iteration :  70   Loss :  513.362017139
Iteration :  71   Loss :  314.1539423
Iteration :  72   Loss :  3032.64295201
Iteration :  73   Loss :  979.133877815
Iteration :  74   Loss :  5437.15555766
Iteration :  75   Loss :  4181.31303616
Iteration :  76   Loss :  1411.69072042
Iteration :  77   Loss :  1754.01784723
Iteration :  78   Loss :  1867.87853508
Iteration :  79   Loss :  252.924893565
Iteration :  80   Loss :  3325.36438645
Iteration :  81   Loss :  1365.50035901
Iteration :  82   Loss :  1146.0594782
Iteration :  83   Loss :  362.965391226
Iteration :  84   Loss :  42.6411825614
Iteration :  85   Loss :  5.00948711415
Iteration :  86   Loss :  0.588514662105
Iteration :  87   Loss :  0.690586183051
Iteration :  88   Loss :  3680.19196363
Iteration :  89   Loss :  3553.87520161
Iteration :  90   Loss :  292.392134411
Iteration :  91   Loss :  34.3502347174
Iteration :  92   Loss :  4.03546636956
Iteration :  93   Loss :  0.474086682489
Iteration :  94   Loss :  0.0556957144305
Iteration :  95   Loss :  0.038709998644
Iteration :  96   Loss :  3266.98147919
Iteration :  97   Loss :  3768.04562926
Iteration :  98   Loss :  874.421897066
Iteration :  99   Loss :  4561.73983208
[-0.073877    0.02481176 -0.04448277 ...,  0.03342863  0.03479882
  0.04456341]
CROSS VALIDATION 2
Iteration :  0   Loss :  2746.51388923
Iteration :  1   Loss :  1525.0937951
Iteration :  2   Loss :  619.180674403
Iteration :  3   Loss :  72.7413599585
Iteration :  4   Loss :  8.54565665137
Iteration :  5   Loss :  1.00394394117
Iteration :  6   Loss :  0.129565106052
Iteration :  7   Loss :  4469.26844022
Iteration :  8   Loss :  6817.45797662
Iteration :  9   Loss :  524.452551367
Iteration :  10   Loss :  862.88644031
Iteration :  11   Loss :  4147.07327851
Iteration :  12   Loss :  2433.60589993
Iteration :  13   Loss :  2228.11161516
Iteration :  14   Loss :  2990.60370368
Iteration :  15   Loss :  500.32046097
Iteration :  16   Loss :  2056.76761067
Iteration :  17   Loss :  1891.5965681
Iteration :  18   Loss :  354.408840389
Iteration :  19   Loss :  41.6359587709
Iteration :  20   Loss :  4.89139339999
Iteration :  21   Loss :  0.574641009832
Iteration :  22   Loss :  5531.0296514
Iteration :  23   Loss :  3674.193489
Iteration :  24   Loss :  371.114249575
Iteration :  25   Loss :  973.602744012
Iteration :  26   Loss :  1903.74744339
Iteration :  27   Loss :  1487.96747603
Iteration :  28   Loss :  4184.47883472
Iteration :  29   Loss :  2523.81018892
Iteration :  30   Loss :  813.005956465
Iteration :  31   Loss :  1604.41378842
Iteration :  32   Loss :  3041.63917211
Iteration :  33   Loss :  599.091758894
Iteration :  34   Loss :  1891.51182259
Iteration :  35   Loss :  2616.44815373
Iteration :  36   Loss :  394.069218759
Iteration :  37   Loss :  46.2952609397
Iteration :  38   Loss :  5.43876832661
Iteration :  39   Loss :  0.638946628879
Iteration :  40   Loss :  0.457916397131
Iteration :  41   Loss :  3624.86496128
Iteration :  42   Loss :  1073.50929863
Iteration :  43   Loss :  129.982187257
Iteration :  44   Loss :  15.3834309905
Iteration :  45   Loss :  4054.41788085
Iteration :  46   Loss :  974.543620272
Iteration :  47   Loss :  725.991095879
Iteration :  48   Loss :  3140.1409654
Iteration :  49   Loss :  2696.647203
Iteration :  50   Loss :  1846.38451193
Iteration :  51   Loss :  2654.77274522
Iteration :  52   Loss :  435.909564133
Iteration :  53   Loss :  51.3008399681
Iteration :  54   Loss :  4899.98074565
Iteration :  55   Loss :  3068.32963119
Iteration :  56   Loss :  4397.4997049
Iteration :  57   Loss :  409.732906631
Iteration :  58   Loss :  1973.40905506
Iteration :  59   Loss :  2293.20395928
Iteration :  60   Loss :  779.078811862
Iteration :  61   Loss :  4169.56963683
Iteration :  62   Loss :  1750.19511409
Iteration :  63   Loss :  2562.93866511
Iteration :  64   Loss :  2766.06388043
Iteration :  65   Loss :  831.35317605
Iteration :  66   Loss :  1432.49850793
Iteration :  67   Loss :  2392.52892131
Iteration :  68   Loss :  2196.05990428
Iteration :  69   Loss :  345.057838607
Iteration :  70   Loss :  1948.12162777
Iteration :  71   Loss :  951.778166966
Iteration :  72   Loss :  1691.98046858
Iteration :  73   Loss :  2078.03284124
Iteration :  74   Loss :  3164.5009664
Iteration :  75   Loss :  850.026916291
Iteration :  76   Loss :  1650.71319047
Iteration :  77   Loss :  1159.23464471
Iteration :  78   Loss :  692.788159136
Iteration :  79   Loss :  2765.69542908
Iteration :  80   Loss :  645.67890678
Iteration :  81   Loss :  1899.34937822
Iteration :  82   Loss :  1099.38105737
Iteration :  83   Loss :  1110.83017654
Iteration :  84   Loss :  1195.04260428
Iteration :  85   Loss :  3237.25435257
Iteration :  86   Loss :  199.823649631
Iteration :  87   Loss :  23.4752869831
Iteration :  88   Loss :  2.75787725806
Iteration :  89   Loss :  9791.51955747
Iteration :  90   Loss :  3580.12996583
Iteration :  91   Loss :  260.628229343
Iteration :  92   Loss :  30.6186104149
Iteration :  93   Loss :  3.59707506014
Iteration :  94   Loss :  6502.47822466
Iteration :  95   Loss :  3833.33562852
Iteration :  96   Loss :  3462.86538017
Iteration :  97   Loss :  2747.83545138
Iteration :  98   Loss :  2653.57059854
Iteration :  99   Loss :  1196.57044734
[ 0.03045829  0.01542833  0.09215546 ...,  0.06641615  0.03056227
  0.01508318]
CROSS VALIDATION 3
Iteration :  0   Loss :  1882.67246421
Iteration :  1   Loss :  1718.21648236
Iteration :  2   Loss :  1670.53412293
Iteration :  3   Loss :  1031.2654125
Iteration :  4   Loss :  558.348979109
Iteration :  5   Loss :  2605.31712048
Iteration :  6   Loss :  1767.93658217
Iteration :  7   Loss :  2104.83997336
Iteration :  8   Loss :  467.269258888
Iteration :  9   Loss :  3246.07073045
Iteration :  10   Loss :  3143.44577041
Iteration :  11   Loss :  2548.36085067
Iteration :  12   Loss :  614.458886581
Iteration :  13   Loss :  796.932532118
Iteration :  14   Loss :  3082.11398131
Iteration :  15   Loss :  2749.32276077
Iteration :  16   Loss :  1065.99533553
Iteration :  17   Loss :  892.673735048
Iteration :  18   Loss :  2873.42094601
Iteration :  19   Loss :  2424.96752792
Iteration :  20   Loss :  3818.25511843
Iteration :  21   Loss :  237.536611115
Iteration :  22   Loss :  2253.93539471
Iteration :  23   Loss :  3382.87188541
Iteration :  24   Loss :  1805.4773461
Iteration :  25   Loss :  1263.38386862
Iteration :  26   Loss :  421.081844242
Iteration :  27   Loss :  2331.65993741
Iteration :  28   Loss :  1459.43031756
Iteration :  29   Loss :  837.680909744
Iteration :  30   Loss :  182.0768365
Iteration :  31   Loss :  2909.88850124
Iteration :  32   Loss :  1874.87038882
Iteration :  33   Loss :  3825.98933799
Iteration :  34   Loss :  468.970455439
Iteration :  35   Loss :  1397.54924244
Iteration :  36   Loss :  4787.91341222
Iteration :  37   Loss :  238.289073376
Iteration :  38   Loss :  27.9942058549
Iteration :  39   Loss :  3.2887599517
Iteration :  40   Loss :  0.386363595237
Iteration :  41   Loss :  0.429922169423
Iteration :  42   Loss :  4477.00835971
Iteration :  43   Loss :  6828.08149975
Iteration :  44   Loss :  524.801709005
Iteration :  45   Loss :  863.862955167
Iteration :  46   Loss :  2360.84795011
Iteration :  47   Loss :  1982.79760927
Iteration :  48   Loss :  1107.13259793
Iteration :  49   Loss :  922.642696676
Iteration :  50   Loss :  2089.79985451
Iteration :  51   Loss :  804.533769337
Iteration :  52   Loss :  4197.84591468
Iteration :  53   Loss :  1480.2540028
Iteration :  54   Loss :  113.856296243
Iteration :  55   Loss :  13.3758403176
Iteration :  56   Loss :  1.65294498238
Iteration :  57   Loss :  3563.3251934
Iteration :  58   Loss :  1925.6211584
Iteration :  59   Loss :  348.16646774
Iteration :  60   Loss :  5014.47905161
Iteration :  61   Loss :  1314.8108627
Iteration :  62   Loss :  1626.73476182
Iteration :  63   Loss :  2046.9619487
Iteration :  64   Loss :  3082.77845913
Iteration :  65   Loss :  395.74658743
Iteration :  66   Loss :  2877.6191218
Iteration :  67   Loss :  1755.33925472
Iteration :  68   Loss :  121.018642413
Iteration :  69   Loss :  1865.30603144
Iteration :  70   Loss :  1818.77597242
Iteration :  71   Loss :  1956.39336564
Iteration :  72   Loss :  3329.03237376
Iteration :  73   Loss :  347.389513854
Iteration :  74   Loss :  140.781239132
Iteration :  75   Loss :  4865.66112832
Iteration :  76   Loss :  1916.87534541
Iteration :  77   Loss :  4315.56502249
Iteration :  78   Loss :  584.257174157
Iteration :  79   Loss :  3136.09661216
Iteration :  80   Loss :  647.437563839
Iteration :  81   Loss :  2539.25307859
Iteration :  82   Loss :  4864.99003642
Iteration :  83   Loss :  797.993756679
Iteration :  84   Loss :  888.586839229
Iteration :  85   Loss :  949.839439183
Iteration :  86   Loss :  587.086336882
Iteration :  87   Loss :  643.107810342
Iteration :  88   Loss :  2024.05442256
Iteration :  89   Loss :  835.354829288
Iteration :  90   Loss :  3807.88547545
Iteration :  91   Loss :  5530.80437382
Iteration :  92   Loss :  1233.5467929
Iteration :  93   Loss :  1413.16282141
Iteration :  94   Loss :  436.95077196
Iteration :  95   Loss :  2106.9739721
Iteration :  96   Loss :  4770.92580634
Iteration :  97   Loss :  622.544698788
Iteration :  98   Loss :  688.967002577
Iteration :  99   Loss :  2586.65752576
[-0.10508007 -0.03401848 -0.06972628 ...,  0.09643043 -0.02700662
  0.01567931]
CROSS VALIDATION 4
Iteration :  0   Loss :  1882.67246421
Iteration :  1   Loss :  1718.21648236
Iteration :  2   Loss :  3595.55663742
Iteration :  3   Loss :  1226.82499186
Iteration :  4   Loss :  1699.24241769
Iteration :  5   Loss :  2596.88957714
Iteration :  6   Loss :  2113.76332712
Iteration :  7   Loss :  1987.04604445
Iteration :  8   Loss :  1232.35369095
Iteration :  9   Loss :  6517.35772456
Iteration :  10   Loss :  504.834744618
Iteration :  11   Loss :  59.3079974811
Iteration :  12   Loss :  6.96750491664
Iteration :  13   Loss :  0.818542638855
Iteration :  14   Loss :  0.0961624081562
Iteration :  15   Loss :  0.0112971619357
Iteration :  16   Loss :  2917.38721473
Iteration :  17   Loss :  487.139206343
Iteration :  18   Loss :  1299.18182575
Iteration :  19   Loss :  2366.88907229
Iteration :  20   Loss :  470.130565767
Iteration :  21   Loss :  3430.66946047
Iteration :  22   Loss :  4829.68918628
Iteration :  23   Loss :  1398.58057444
Iteration :  24   Loss :  297.140306161
Iteration :  25   Loss :  34.9080500445
Iteration :  26   Loss :  4.1009985271
Iteration :  27   Loss :  0.481785401873
Iteration :  28   Loss :  0.056600160162
Iteration :  29   Loss :  0.43522849311
Iteration :  30   Loss :  3327.75672735
Iteration :  31   Loss :  540.56820328
Iteration :  32   Loss :  4448.53562116
Iteration :  33   Loss :  1951.63342223
Iteration :  34   Loss :  2011.00117191
Iteration :  35   Loss :  960.665784848
Iteration :  36   Loss :  4288.37056455
Iteration :  37   Loss :  1535.51788345
Iteration :  38   Loss :  1182.27408764
Iteration :  39   Loss :  1982.46257359
Iteration :  40   Loss :  2390.67065651
Iteration :  41   Loss :  6187.19984768
Iteration :  42   Loss :  741.477447874
Iteration :  43   Loss :  1014.3976899
Iteration :  44   Loss :  3025.7744729
Iteration :  45   Loss :  1730.354922
Iteration :  46   Loss :  336.367207524
Iteration :  47   Loss :  3064.23922174
Iteration :  48   Loss :  451.890728136
Iteration :  49   Loss :  2276.08505801
Iteration :  50   Loss :  1669.68766579
Iteration :  51   Loss :  1032.82473956
Iteration :  52   Loss :  2572.55779528
Iteration :  53   Loss :  1861.77187576
Iteration :  54   Loss :  2378.86333092
Iteration :  55   Loss :  1170.74662342
Iteration :  56   Loss :  1022.87622657
Iteration :  57   Loss :  1536.18182392
Iteration :  58   Loss :  1041.51477629
Iteration :  59   Loss :  2990.25619194
Iteration :  60   Loss :  1116.10920892
Iteration :  61   Loss :  541.949239226
Iteration :  62   Loss :  2196.24949926
Iteration :  63   Loss :  1553.24703875
Iteration :  64   Loss :  2147.00752336
Iteration :  65   Loss :  466.717533825
Iteration :  66   Loss :  54.829986675
Iteration :  67   Loss :  6.75100875006
Iteration :  68   Loss :  1622.76762973
Iteration :  69   Loss :  1925.33074906
Iteration :  70   Loss :  3081.86468747
Iteration :  71   Loss :  3229.06828281
Iteration :  72   Loss :  745.443907886
Iteration :  73   Loss :  3438.98311231
Iteration :  74   Loss :  550.873758613
Iteration :  75   Loss :  4758.98051347
Iteration :  76   Loss :  448.645469546
Iteration :  77   Loss :  52.7068801453
Iteration :  78   Loss :  6.19200549926
Iteration :  79   Loss :  0.727436949354
Iteration :  80   Loss :  0.0854593096451
Iteration :  81   Loss :  0.602440941163
Iteration :  82   Loss :  4491.49188967
Iteration :  83   Loss :  6073.2107398
Iteration :  84   Loss :  684.266665367
Iteration :  85   Loss :  2755.26325424
Iteration :  86   Loss :  1896.29221932
Iteration :  87   Loss :  2989.41603959
Iteration :  88   Loss :  1464.91167501
Iteration :  89   Loss :  2431.21469447
Iteration :  90   Loss :  6637.70291553
Iteration :  91   Loss :  3179.43440732
Iteration :  92   Loss :  615.991761107
Iteration :  93   Loss :  1194.07319719
Iteration :  94   Loss :  4648.78085531
Iteration :  95   Loss :  306.24295192
Iteration :  96   Loss :  2317.3318014
Iteration :  97   Loss :  2148.10100436
Iteration :  98   Loss :  984.148852668
Iteration :  99   Loss :  2013.66138089
[-0.09747887 -0.07123912  0.02347923 ...,  0.02320241  0.01014825
  0.0116962 ]
CROSS VALIDATION 5
Iteration :  0   Loss :  1411.22907
Iteration :  1   Loss :  152.780358214
Iteration :  2   Loss :  17.9486400188
Iteration :  3   Loss :  2.10860664479
Iteration :  4   Loss :  9774.74748039
Iteration :  5   Loss :  3314.62596378
Iteration :  6   Loss :  273.956226975
Iteration :  7   Loss :  32.1843838852
Iteration :  8   Loss :  3.78102216368
Iteration :  9   Loss :  0.963292964451
Iteration :  10   Loss :  3562.24960938
Iteration :  11   Loss :  751.242499495
Iteration :  12   Loss :  2700.87876502
Iteration :  13   Loss :  4461.41537242
Iteration :  14   Loss :  3350.4099576
Iteration :  15   Loss :  395.832280173
Iteration :  16   Loss :  2138.81053298
Iteration :  17   Loss :  2581.1386244
Iteration :  18   Loss :  1289.32518736
Iteration :  19   Loss :  146.842073633
Iteration :  20   Loss :  17.2510102088
Iteration :  21   Loss :  2.02664907857
Iteration :  22   Loss :  0.238090780653
Iteration :  23   Loss :  5213.48635107
Iteration :  24   Loss :  1944.76693134
Iteration :  25   Loss :  475.507997734
Iteration :  26   Loss :  3049.26373859
Iteration :  27   Loss :  1623.67105892
Iteration :  28   Loss :  1803.75341437
Iteration :  29   Loss :  1764.8163392
Iteration :  30   Loss :  3806.08146764
Iteration :  31   Loss :  1840.83108643
Iteration :  32   Loss :  1111.59551756
Iteration :  33   Loss :  3346.35063219
Iteration :  34   Loss :  441.960882798
Iteration :  35   Loss :  3544.93760494
Iteration :  36   Loss :  1372.34875218
Iteration :  37   Loss :  1179.65449081
Iteration :  38   Loss :  485.580651253
Iteration :  39   Loss :  5369.95146181
Iteration :  40   Loss :  2009.86762948
Iteration :  41   Loss :  2314.64316348
Iteration :  42   Loss :  3494.00022015
Iteration :  43   Loss :  4062.79015503
Iteration :  44   Loss :  450.840255523
Iteration :  45   Loss :  1537.54473153
Iteration :  46   Loss :  3433.03534403
Iteration :  47   Loss :  957.436826665
Iteration :  48   Loss :  3028.24746487
Iteration :  49   Loss :  3899.9877428
Iteration :  50   Loss :  2351.16612664
Iteration :  51   Loss :  1700.25917123
Iteration :  52   Loss :  1422.3103898
Iteration :  53   Loss :  1633.87817929
Iteration :  54   Loss :  817.465571343
Iteration :  55   Loss :  3166.88548834
Iteration :  56   Loss :  1617.36362132
Iteration :  57   Loss :  5371.97362333
Iteration :  58   Loss :  598.170496631
Iteration :  59   Loss :  70.2730837877
Iteration :  60   Loss :  8.2556835097
Iteration :  61   Loss :  0.96987788978
Iteration :  62   Loss :  0.113941276937
Iteration :  63   Loss :  0.223963713402
Iteration :  64   Loss :  2130.05052839
Iteration :  65   Loss :  2110.99143726
Iteration :  66   Loss :  357.69353536
Iteration :  67   Loss :  3515.10450719
Iteration :  68   Loss :  1537.51005415
Iteration :  69   Loss :  388.731000524
Iteration :  70   Loss :  45.6681269379
Iteration :  71   Loss :  5.36509260956
Iteration :  72   Loss :  0.630291203935
Iteration :  73   Loss :  0.0740466252251
Iteration :  74   Loss :  7765.72245866
Iteration :  75   Loss :  1738.70952713
Iteration :  76   Loss :  1210.95760739
Iteration :  77   Loss :  2245.77738871
Iteration :  78   Loss :  2345.42555795
Iteration :  79   Loss :  688.76903016
Iteration :  80   Loss :  1362.98618737
Iteration :  81   Loss :  2245.6776049
Iteration :  82   Loss :  1922.51613325
Iteration :  83   Loss :  1571.70835745
Iteration :  84   Loss :  3048.94841831
Iteration :  85   Loss :  1256.76482106
Iteration :  86   Loss :  839.260548886
Iteration :  87   Loss :  1617.42745806
Iteration :  88   Loss :  4392.27858062
Iteration :  89   Loss :  2340.30263815
Iteration :  90   Loss :  2246.86945485
Iteration :  91   Loss :  2121.05056123
Iteration :  92   Loss :  1546.91777973
Iteration :  93   Loss :  4762.73653336
Iteration :  94   Loss :  247.877320178
Iteration :  95   Loss :  1450.27387415
Iteration :  96   Loss :  2752.1835343
Iteration :  97   Loss :  4222.12761377
Iteration :  98   Loss :  2016.32230979
Iteration :  99   Loss :  91.5630275222
[-0.07085472 -0.03003943 -0.01812999 ...,  0.0167581   0.01442267
  0.00076138]
CROSS VALIDATION 6
Iteration :  0   Loss :  1874.94183526
Iteration :  1   Loss :  1713.78626704
Iteration :  2   Loss :  3596.2098499
Iteration :  3   Loss :  1221.41832966
Iteration :  4   Loss :  1699.15584708
Iteration :  5   Loss :  2596.90829766
Iteration :  6   Loss :  1783.83097242
Iteration :  7   Loss :  460.700086717
Iteration :  8   Loss :  763.076392898
Iteration :  9   Loss :  1807.40613334
Iteration :  10   Loss :  2153.27785632
Iteration :  11   Loss :  1630.50393797
Iteration :  12   Loss :  3208.12991925
Iteration :  13   Loss :  6388.6378613
Iteration :  14   Loss :  1430.21580813
Iteration :  15   Loss :  109.312577161
Iteration :  16   Loss :  12.8420440946
Iteration :  17   Loss :  1.50868363744
Iteration :  18   Loss :  7039.47736366
Iteration :  19   Loss :  856.846430554
Iteration :  20   Loss :  538.741734364
Iteration :  21   Loss :  1301.88781528
Iteration :  22   Loss :  1885.16091986
Iteration :  23   Loss :  6058.87679265
Iteration :  24   Loss :  484.490900338
Iteration :  25   Loss :  1501.14020922
Iteration :  26   Loss :  1147.49225845
Iteration :  27   Loss :  317.126286662
Iteration :  28   Loss :  1328.71749623
Iteration :  29   Loss :  930.189070719
Iteration :  30   Loss :  460.6770228
Iteration :  31   Loss :  2997.80721085
Iteration :  32   Loss :  1397.36706413
Iteration :  33   Loss :  599.141560506
Iteration :  34   Loss :  3807.56948365
Iteration :  35   Loss :  206.068573203
Iteration :  36   Loss :  3345.14344975
Iteration :  37   Loss :  2542.52237123
Iteration :  38   Loss :  370.801070823
Iteration :  39   Loss :  3239.42899862
Iteration :  40   Loss :  4136.69306685
Iteration :  41   Loss :  1875.16506034
Iteration :  42   Loss :  2472.42313412
Iteration :  43   Loss :  702.692698297
Iteration :  44   Loss :  1167.20820351
Iteration :  45   Loss :  785.416350571
Iteration :  46   Loss :  1924.19211846
Iteration :  47   Loss :  1501.19452881
Iteration :  48   Loss :  1188.57503689
Iteration :  49   Loss :  943.288296109
Iteration :  50   Loss :  4585.03787143
Iteration :  51   Loss :  526.783155934
Iteration :  52   Loss :  2180.90031462
Iteration :  53   Loss :  2939.66992041
Iteration :  54   Loss :  1044.0034897
Iteration :  55   Loss :  549.552093228
Iteration :  56   Loss :  3905.22062649
Iteration :  57   Loss :  3775.98645282
Iteration :  58   Loss :  553.443106115
Iteration :  59   Loss :  2391.59524624
Iteration :  60   Loss :  4337.58846421
Iteration :  61   Loss :  1543.966821
Iteration :  62   Loss :  2405.92204911
Iteration :  63   Loss :  1396.30775126
Iteration :  64   Loss :  1216.83012133
Iteration :  65   Loss :  4656.8146147
Iteration :  66   Loss :  569.734145434
Iteration :  67   Loss :  2662.83162682
Iteration :  68   Loss :  2191.02245858
Iteration :  69   Loss :  3360.83597417
Iteration :  70   Loss :  307.223349021
Iteration :  71   Loss :  36.092606153
Iteration :  72   Loss :  4.24016020615
Iteration :  73   Loss :  5171.35109846
Iteration :  74   Loss :  2442.8866738
Iteration :  75   Loss :  1266.73120777
Iteration :  76   Loss :  2452.42062832
Iteration :  77   Loss :  2113.22848008
Iteration :  78   Loss :  694.52015232
Iteration :  79   Loss :  1385.68743503
Iteration :  80   Loss :  3705.11281802
Iteration :  81   Loss :  438.974012991
Iteration :  82   Loss :  1774.31031519
Iteration :  83   Loss :  5192.88116681
Iteration :  84   Loss :  186.967797305
Iteration :  85   Loss :  4950.35514351
Iteration :  86   Loss :  1426.20439327
Iteration :  87   Loss :  3400.2053396
Iteration :  88   Loss :  274.791045498
Iteration :  89   Loss :  377.211284531
Iteration :  90   Loss :  4200.04950088
Iteration :  91   Loss :  5232.72592822
Iteration :  92   Loss :  307.504448006
Iteration :  93   Loss :  36.1256296683
Iteration :  94   Loss :  4.24403980948
Iteration :  95   Loss :  0.498589895038
Iteration :  96   Loss :  10492.3674856
Iteration :  97   Loss :  3722.37996882
Iteration :  98   Loss :  238.067422728
Iteration :  99   Loss :  27.9681663317
[-0.02488947 -0.00816    -0.0188872  ...,  0.01553762  0.01134306
  0.00338132]
CROSS VALIDATION 7
Iteration :  0   Loss :  1874.94183526
Iteration :  1   Loss :  1713.78626704
Iteration :  2   Loss :  3004.52596254
Iteration :  3   Loss :  1562.00172199
Iteration :  4   Loss :  1063.85380957
Iteration :  5   Loss :  1721.21291786
Iteration :  6   Loss :  808.000084676
Iteration :  7   Loss :  599.362343911
Iteration :  8   Loss :  3692.71869574
Iteration :  9   Loss :  1353.84466888
Iteration :  10   Loss :  591.754772973
Iteration :  11   Loss :  3530.3193522
Iteration :  12   Loss :  2260.4831442
Iteration :  13   Loss :  1736.31549527
Iteration :  14   Loss :  3091.0667793
Iteration :  15   Loss :  201.783472167
Iteration :  16   Loss :  23.70552698
Iteration :  17   Loss :  2.78492585822
Iteration :  18   Loss :  0.327173154275
Iteration :  19   Loss :  6007.01105094
Iteration :  20   Loss :  1505.99017168
Iteration :  21   Loss :  1762.40557563
Iteration :  22   Loss :  1351.79586093
Iteration :  23   Loss :  350.193679112
Iteration :  24   Loss :  3427.88170075
Iteration :  25   Loss :  544.188192837
Iteration :  26   Loss :  1561.93037993
Iteration :  27   Loss :  534.728947314
Iteration :  28   Loss :  1855.96503274
Iteration :  29   Loss :  1048.5403929
Iteration :  30   Loss :  576.045180521
Iteration :  31   Loss :  2888.70943637
Iteration :  32   Loss :  3367.48136097
Iteration :  33   Loss :  2109.11678837
Iteration :  34   Loss :  2180.26468784
Iteration :  35   Loss :  523.690624485
Iteration :  36   Loss :  2732.31149873
Iteration :  37   Loss :  3802.16776798
Iteration :  38   Loss :  781.532899896
Iteration :  39   Loss :  955.859836206
Iteration :  40   Loss :  3043.41525221
Iteration :  41   Loss :  419.280840481
Iteration :  42   Loss :  2331.3653228
Iteration :  43   Loss :  1612.54506902
Iteration :  44   Loss :  1320.33389443
Iteration :  45   Loss :  72.2048587992
Iteration :  46   Loss :  8.48262848277
Iteration :  47   Loss :  0.996539390469
Iteration :  48   Loss :  0.117073470655
Iteration :  49   Loss :  0.489827870321
Iteration :  50   Loss :  3393.26887171
Iteration :  51   Loss :  1698.69994069
Iteration :  52   Loss :  258.30993751
Iteration :  53   Loss :  2400.33248805
Iteration :  54   Loss :  2324.19061695
Iteration :  55   Loss :  1918.13195666
Iteration :  56   Loss :  251.264454127
Iteration :  57   Loss :  3774.65841924
Iteration :  58   Loss :  2338.10777998
Iteration :  59   Loss :  272.58992901
Iteration :  60   Loss :  32.0238711686
Iteration :  61   Loss :  3.76216512601
Iteration :  62   Loss :  0.697567948653
Iteration :  63   Loss :  5101.85827364
Iteration :  64   Loss :  2464.40215714
Iteration :  65   Loss :  2180.52805555
Iteration :  66   Loss :  235.259225592
Iteration :  67   Loss :  27.6382592672
Iteration :  68   Loss :  3.24694333834
Iteration :  69   Loss :  0.381450978532
Iteration :  70   Loss :  0.0448128697858
Iteration :  71   Loss :  9726.32866018
Iteration :  72   Loss :  980.589700781
Iteration :  73   Loss :  318.946629194
Iteration :  74   Loss :  37.4698573791
Iteration :  75   Loss :  4.40195971205
Iteration :  76   Loss :  0.517142328846
Iteration :  77   Loss :  0.0607538927611
Iteration :  78   Loss :  12976.0647102
Iteration :  79   Loss :  4194.40629376
Iteration :  80   Loss :  931.382410787
Iteration :  81   Loss :  1448.59811589
Iteration :  82   Loss :  1307.38374819
Iteration :  83   Loss :  1148.32499473
Iteration :  84   Loss :  1290.2080902
Iteration :  85   Loss :  5271.80290757
Iteration :  86   Loss :  1238.31560805
Iteration :  87   Loss :  862.775126215
Iteration :  88   Loss :  1692.47951682
Iteration :  89   Loss :  2311.05142591
Iteration :  90   Loss :  2385.38962042
Iteration :  91   Loss :  3053.54131328
Iteration :  92   Loss :  662.652650716
Iteration :  93   Loss :  2256.08664253
Iteration :  94   Loss :  3506.67591465
Iteration :  95   Loss :  759.331610547
Iteration :  96   Loss :  542.535749049
Iteration :  97   Loss :  2408.62717134
Iteration :  98   Loss :  2141.01126254
Iteration :  99   Loss :  3577.10227293
[-0.14065415 -0.0508221  -0.00459518 ...,  0.03347294  0.00315829
 -0.00162819]
CROSS VALIDATION 8
Iteration :  0   Loss :  3243.06514689
Iteration :  1   Loss :  1455.28531797
Iteration :  2   Loss :  442.654378779
Iteration :  3   Loss :  2033.08710747
Iteration :  4   Loss :  1053.40560641
Iteration :  5   Loss :  1902.20431784
Iteration :  6   Loss :  1383.44101803
Iteration :  7   Loss :  2346.00572553
Iteration :  8   Loss :  1462.38318509
Iteration :  9   Loss :  1144.1642911
Iteration :  10   Loss :  562.473073363
Iteration :  11   Loss :  1090.93675748
Iteration :  12   Loss :  429.331892635
Iteration :  13   Loss :  1717.42674403
Iteration :  14   Loss :  2592.13616608
Iteration :  15   Loss :  1326.4858771
Iteration :  16   Loss :  2927.00893041
Iteration :  17   Loss :  1407.72811853
Iteration :  18   Loss :  1306.17415729
Iteration :  19   Loss :  2254.65524844
Iteration :  20   Loss :  1294.78373596
Iteration :  21   Loss :  685.428267317
Iteration :  22   Loss :  2317.66511484
Iteration :  23   Loss :  195.777478007
Iteration :  24   Loss :  3461.50297194
Iteration :  25   Loss :  5463.03625586
Iteration :  26   Loss :  522.450599345
Iteration :  27   Loss :  2331.88548283
Iteration :  28   Loss :  1494.72778443
Iteration :  29   Loss :  1189.5397474
Iteration :  30   Loss :  1540.11880588
Iteration :  31   Loss :  4465.32299593
Iteration :  32   Loss :  5700.02283372
Iteration :  33   Loss :  688.150943033
Iteration :  34   Loss :  1611.54899788
Iteration :  35   Loss :  864.013158315
Iteration :  36   Loss :  372.051577965
Iteration :  37   Loss :  3347.10096116
Iteration :  38   Loss :  2723.36558909
Iteration :  39   Loss :  888.447496162
Iteration :  40   Loss :  1500.33727056
Iteration :  41   Loss :  748.290131382
Iteration :  42   Loss :  1833.6571839
Iteration :  43   Loss :  2123.29855142
Iteration :  44   Loss :  3205.30462921
Iteration :  45   Loss :  560.233845056
Iteration :  46   Loss :  65.8162850826
Iteration :  47   Loss :  7.73209869468
Iteration :  48   Loss :  0.908367133595
Iteration :  49   Loss :  13204.4251134
Iteration :  50   Loss :  8492.55731425
Iteration :  51   Loss :  1842.38505333
Iteration :  52   Loss :  802.400185928
Iteration :  53   Loss :  2465.61898806
Iteration :  54   Loss :  1295.02597134
Iteration :  55   Loss :  2216.01080121
Iteration :  56   Loss :  2255.3776708
Iteration :  57   Loss :  3095.10383236
Iteration :  58   Loss :  452.402230655
Iteration :  59   Loss :  3226.54858025
Iteration :  60   Loss :  363.387871693
Iteration :  61   Loss :  802.831948486
Iteration :  62   Loss :  1547.20287507
Iteration :  63   Loss :  2354.9879645
Iteration :  64   Loss :  901.798183191
Iteration :  65   Loss :  1437.88894048
Iteration :  66   Loss :  3024.65504879
Iteration :  67   Loss :  2074.062692
Iteration :  68   Loss :  1229.13756364
Iteration :  69   Loss :  279.361871875
Iteration :  70   Loss :  582.141076588
Iteration :  71   Loss :  3324.68968733
Iteration :  72   Loss :  1521.31120408
Iteration :  73   Loss :  856.48239412
Iteration :  74   Loss :  1463.73050635
Iteration :  75   Loss :  3851.95188153
Iteration :  76   Loss :  2385.97165876
Iteration :  77   Loss :  507.481377046
Iteration :  78   Loss :  3529.32490654
Iteration :  79   Loss :  3129.40545048
Iteration :  80   Loss :  339.047975306
Iteration :  81   Loss :  39.8313639856
Iteration :  82   Loss :  4.67938956284
Iteration :  83   Loss :  0.549734794137
Iteration :  84   Loss :  4858.52898817
Iteration :  85   Loss :  2271.41215823
Iteration :  86   Loss :  636.046758735
Iteration :  87   Loss :  660.487041825
Iteration :  88   Loss :  3197.74336482
Iteration :  89   Loss :  1951.10612665
Iteration :  90   Loss :  1324.23176009
Iteration :  91   Loss :  1227.64510688
Iteration :  92   Loss :  326.80325052
Iteration :  93   Loss :  38.3928534343
Iteration :  94   Loss :  4.51039330998
Iteration :  95   Loss :  0.52988111044
Iteration :  96   Loss :  0.0622504451175
Iteration :  97   Loss :  6814.48325902
Iteration :  98   Loss :  519.70393438
Iteration :  99   Loss :  1077.030949
[-0.02739671 -0.04342311 -0.00219694 ...,  0.01446999  0.03605287
  0.00305192]
CROSS VALIDATION 9
Iteration :  0   Loss :  1674.11283131
Iteration :  1   Loss :  1754.09293225
Iteration :  2   Loss :  273.630191898
Iteration :  3   Loss :  32.1460812768
Iteration :  4   Loss :  3.77652237236
Iteration :  5   Loss :  0.443665935704
Iteration :  6   Loss :  0.0521218843943
Iteration :  7   Loss :  0.22303457716
Iteration :  8   Loss :  3202.08166384
Iteration :  9   Loss :  1235.11041772
Iteration :  10   Loss :  86.1181207808
Iteration :  11   Loss :  10.1171588224
Iteration :  12   Loss :  2806.8406065
Iteration :  13   Loss :  2232.13920857
Iteration :  14   Loss :  931.866434969
Iteration :  15   Loss :  2400.25827753
Iteration :  16   Loss :  328.131236442
Iteration :  17   Loss :  38.5488652511
Iteration :  18   Loss :  4.52872158184
Iteration :  19   Loss :  0.532034316242
Iteration :  20   Loss :  5307.81580895
Iteration :  21   Loss :  1057.96819568
Iteration :  22   Loss :  776.790101874
Iteration :  23   Loss :  3031.52940562
Iteration :  24   Loss :  1994.56310384
Iteration :  25   Loss :  1130.45191228
Iteration :  26   Loss :  3998.1831393
Iteration :  27   Loss :  212.970991367
Iteration :  28   Loss :  25.0198369946
Iteration :  29   Loss :  2.93933102916
Iteration :  30   Loss :  0.34531267733
Iteration :  31   Loss :  0.6285866515
Iteration :  32   Loss :  3013.54535055
Iteration :  33   Loss :  1480.98282965
Iteration :  34   Loss :  1080.44326239
Iteration :  35   Loss :  2366.59370383
Iteration :  36   Loss :  2108.23444434
Iteration :  37   Loss :  1472.9636475
Iteration :  38   Loss :  3849.4108817
Iteration :  39   Loss :  3978.85697795
Iteration :  40   Loss :  1725.54358658
Iteration :  41   Loss :  556.425602213
Iteration :  42   Loss :  1949.03664008
Iteration :  43   Loss :  1904.3756064
Iteration :  44   Loss :  1327.85837672
Iteration :  45   Loss :  1266.27085703
Iteration :  46   Loss :  1559.19009789
Iteration :  47   Loss :  170.816858367
Iteration :  48   Loss :  4530.89278581
Iteration :  49   Loss :  1426.74427606
Iteration :  50   Loss :  1384.20239747
Iteration :  51   Loss :  7138.14458821
Iteration :  52   Loss :  2089.40912472
Iteration :  53   Loss :  862.002880467
Iteration :  54   Loss :  1393.50346674
Iteration :  55   Loss :  1949.59503594
Iteration :  56   Loss :  994.243597689
Iteration :  57   Loss :  3306.72595523
Iteration :  58   Loss :  737.831463105
Iteration :  59   Loss :  3413.38322393
Iteration :  60   Loss :  734.43177517
Iteration :  61   Loss :  2106.42513304
Iteration :  62   Loss :  1193.25618252
Iteration :  63   Loss :  3375.14096121
Iteration :  64   Loss :  5466.04237129
Iteration :  65   Loss :  1667.32295846
Iteration :  66   Loss :  237.712169864
Iteration :  67   Loss :  1533.26615252
Iteration :  68   Loss :  2834.43882471
Iteration :  69   Loss :  639.560956142
Iteration :  70   Loss :  2044.25822121
Iteration :  71   Loss :  4786.7625088
Iteration :  72   Loss :  1024.41479806
Iteration :  73   Loss :  140.808103565
Iteration :  74   Loss :  16.5421392655
Iteration :  75   Loss :  1.94337090375
Iteration :  76   Loss :  9808.35712461
Iteration :  77   Loss :  1248.40724693
Iteration :  78   Loss :  563.070480197
Iteration :  79   Loss :  4450.54862879
Iteration :  80   Loss :  2087.34322636
Iteration :  81   Loss :  764.643218174
Iteration :  82   Loss :  89.8303029671
Iteration :  83   Loss :  10.5532660715
Iteration :  84   Loss :  1.23979794232
Iteration :  85   Loss :  0.155781751876
Iteration :  86   Loss :  6718.77060007
Iteration :  87   Loss :  1293.82253029
Iteration :  88   Loss :  677.842813511
Iteration :  89   Loss :  1276.37345075
Iteration :  90   Loss :  1411.62899358
Iteration :  91   Loss :  532.044362279
Iteration :  92   Loss :  1375.26102444
Iteration :  93   Loss :  2578.31224502
Iteration :  94   Loss :  1667.04781833
Iteration :  95   Loss :  336.044806375
Iteration :  96   Loss :  2814.78623719
Iteration :  97   Loss :  659.083307866
Iteration :  98   Loss :  2941.76852164
Iteration :  99   Loss :  3308.78157279
[-0.14190029 -0.08532771 -0.01956561 ...,  0.02935598  0.02008061
  0.0107686 ]
CROSS VALIDATION 10
Iteration :  0   Loss :  2344.97836402
Iteration :  1   Loss :  1636.28367741
Iteration :  2   Loss :  64.2585836014
Iteration :  3   Loss :  7.54909988863
Iteration :  4   Loss :  0.886868429625
Iteration :  5   Loss :  0.295410256426
Iteration :  6   Loss :  4086.54659666
Iteration :  7   Loss :  243.85747114
Iteration :  8   Loss :  272.964466205
Iteration :  9   Loss :  32.067871807
Iteration :  10   Loss :  3.76733432203
Iteration :  11   Loss :  0.442586523339
Iteration :  12   Loss :  5393.23651226
Iteration :  13   Loss :  4143.0958758
Iteration :  14   Loss :  2204.39453055
Iteration :  15   Loss :  2021.78464331
Iteration :  16   Loss :  249.274669914
Iteration :  17   Loss :  1520.93479416
Iteration :  18   Loss :  4790.97611617
Iteration :  19   Loss :  572.906836954
Iteration :  20   Loss :  2220.67401177
Iteration :  21   Loss :  4983.6771953
Iteration :  22   Loss :  447.584229526
Iteration :  23   Loss :  4639.95015536
Iteration :  24   Loss :  701.291933242
Iteration :  25   Loss :  426.623450305
Iteration :  26   Loss :  3255.03242036
Iteration :  27   Loss :  549.535639636
Iteration :  28   Loss :  1224.67142291
Iteration :  29   Loss :  2387.2573448
Iteration :  30   Loss :  2072.56792847
Iteration :  31   Loss :  375.595886382
Iteration :  32   Loss :  44.1250134246
Iteration :  33   Loss :  5.1838075983
Iteration :  34   Loss :  0.608993836617
Iteration :  35   Loss :  0.0715446100197
Iteration :  36   Loss :  12578.4154726
Iteration :  37   Loss :  6601.13919209
Iteration :  38   Loss :  521.475676345
Iteration :  39   Loss :  61.2629745255
Iteration :  40   Loss :  7.19717566506
Iteration :  41   Loss :  0.845524363695
Iteration :  42   Loss :  0.596970230679
Iteration :  43   Loss :  3138.00109372
Iteration :  44   Loss :  577.821457811
Iteration :  45   Loss :  429.9978597
Iteration :  46   Loss :  3663.62664789
Iteration :  47   Loss :  1787.27685352
Iteration :  48   Loss :  2308.53929885
Iteration :  49   Loss :  2729.48357403
Iteration :  50   Loss :  1138.68190957
Iteration :  51   Loss :  477.796762934
Iteration :  52   Loss :  2283.87072418
Iteration :  53   Loss :  1730.45954266
Iteration :  54   Loss :  657.817665676
Iteration :  55   Loss :  4555.73619336
Iteration :  56   Loss :  2566.75061732
Iteration :  57   Loss :  1065.84367396
Iteration :  58   Loss :  1371.41107216
Iteration :  59   Loss :  1205.09554314
Iteration :  60   Loss :  2225.59885918
Iteration :  61   Loss :  1696.23845876
Iteration :  62   Loss :  1146.06676248
Iteration :  63   Loss :  3829.05125272
Iteration :  64   Loss :  2539.81592004
Iteration :  65   Loss :  1855.68321559
Iteration :  66   Loss :  5780.18154707
Iteration :  67   Loss :  521.903562872
Iteration :  68   Loss :  2416.04708221
Iteration :  69   Loss :  3705.72295415
Iteration :  70   Loss :  322.410046521
Iteration :  71   Loss :  37.8767397268
Iteration :  72   Loss :  4.44976025969
Iteration :  73   Loss :  0.808172278165
Iteration :  74   Loss :  4166.02378667
Iteration :  75   Loss :  6722.5571836
Iteration :  76   Loss :  520.515447811
Iteration :  77   Loss :  850.097654853
Iteration :  78   Loss :  2359.43499448
Iteration :  79   Loss :  1853.36107763
Iteration :  80   Loss :  5806.62140202
Iteration :  81   Loss :  697.268975281
Iteration :  82   Loss :  316.111166971
Iteration :  83   Loss :  5116.59174405
Iteration :  84   Loss :  3640.36795946
Iteration :  85   Loss :  1501.59688921
Iteration :  86   Loss :  2531.12422857
Iteration :  87   Loss :  2826.13279902
Iteration :  88   Loss :  941.274466051
Iteration :  89   Loss :  10741.60125
Iteration :  90   Loss :  868.882668088
Iteration :  91   Loss :  292.91499549
Iteration :  92   Loss :  2456.80463551
Iteration :  93   Loss :  2686.85405606
Iteration :  94   Loss :  205.199671988
Iteration :  95   Loss :  24.1068622141
Iteration :  96   Loss :  2.83207473082
Iteration :  97   Loss :  0.332849430935
Iteration :  98   Loss :  3630.68697588
Iteration :  99   Loss :  2078.31524922
[-0.01180373  0.0145515   0.06268851 ...,  0.03119047 -0.01444538
  0.01582785]
CROSS VALIDATION 11
Iteration :  0   Loss :  1815.04456979
Iteration :  1   Loss :  1703.8366864
Iteration :  2   Loss :  2926.02027705
Iteration :  3   Loss :  1113.6733463
Iteration :  4   Loss :  1404.18014053
Iteration :  5   Loss :  271.541097968
Iteration :  6   Loss :  1007.021703
Iteration :  7   Loss :  2741.97561008
Iteration :  8   Loss :  613.065966298
Iteration :  9   Loss :  1873.46569434
Iteration :  10   Loss :  677.739201045
Iteration :  11   Loss :  4624.27308458
Iteration :  12   Loss :  1680.27146899
Iteration :  13   Loss :  368.695458317
Iteration :  14   Loss :  3151.82954109
Iteration :  15   Loss :  1293.85131748
Iteration :  16   Loss :  1832.38758355
Iteration :  17   Loss :  2303.40714076
Iteration :  18   Loss :  2924.2997024
Iteration :  19   Loss :  2442.4815563
Iteration :  20   Loss :  1652.12745708
Iteration :  21   Loss :  760.950882682
Iteration :  22   Loss :  1018.49276989
Iteration :  23   Loss :  477.257349739
Iteration :  24   Loss :  1771.67222676
Iteration :  25   Loss :  1557.99480547
Iteration :  26   Loss :  1746.61864731
Iteration :  27   Loss :  2178.06365927
Iteration :  28   Loss :  225.800972446
Iteration :  29   Loss :  4419.46835708
Iteration :  30   Loss :  974.60513695
Iteration :  31   Loss :  2065.89128381
Iteration :  32   Loss :  2864.98399014
Iteration :  33   Loss :  2283.78550367
Iteration :  34   Loss :  1626.60844144
Iteration :  35   Loss :  759.550997452
Iteration :  36   Loss :  1662.91245478
Iteration :  37   Loss :  1122.51125438
Iteration :  38   Loss :  574.748896115
Iteration :  39   Loss :  1591.747977
Iteration :  40   Loss :  2077.63166598
Iteration :  41   Loss :  117.03305606
Iteration :  42   Loss :  13.8141474771
Iteration :  43   Loss :  1854.16594591
Iteration :  44   Loss :  2708.12124902
Iteration :  45   Loss :  249.27285834
Iteration :  46   Loss :  29.2845811666
Iteration :  47   Loss :  3.44035327319
Iteration :  48   Loss :  0.404172782154
Iteration :  49   Loss :  0.0474822278012
Iteration :  50   Loss :  10368.7193061
Iteration :  51   Loss :  2125.60809574
Iteration :  52   Loss :  1838.45653219
Iteration :  53   Loss :  799.284069661
Iteration :  54   Loss :  79.7247026269
Iteration :  55   Loss :  9.36605990967
Iteration :  56   Loss :  5152.36226823
Iteration :  57   Loss :  2707.46758754
Iteration :  58   Loss :  343.274577482
Iteration :  59   Loss :  40.327905307
Iteration :  60   Loss :  4.73772324877
Iteration :  61   Loss :  0.557775700598
Iteration :  62   Loss :  8269.13923661
Iteration :  63   Loss :  1969.58911206
Iteration :  64   Loss :  496.80726145
Iteration :  65   Loss :  58.3649285727
Iteration :  66   Loss :  6.85671315946
Iteration :  67   Loss :  0.805526820658
Iteration :  68   Loss :  0.0946333095331
Iteration :  69   Loss :  7299.77452579
Iteration :  70   Loss :  2574.26102828
Iteration :  71   Loss :  2630.73706501
Iteration :  72   Loss :  3263.92242889
Iteration :  73   Loss :  2025.6994936
Iteration :  74   Loss :  396.865770939
Iteration :  75   Loss :  1382.49272704
Iteration :  76   Loss :  2320.09704204
Iteration :  77   Loss :  812.185293473
Iteration :  78   Loss :  1582.16980644
Iteration :  79   Loss :  2449.26548645
Iteration :  80   Loss :  308.499522857
Iteration :  81   Loss :  2741.9003523
Iteration :  82   Loss :  1124.52820414
Iteration :  83   Loss :  1480.37524012
Iteration :  84   Loss :  1445.95263071
Iteration :  85   Loss :  392.15954671
Iteration :  86   Loss :  3663.36132693
Iteration :  87   Loss :  3329.46135941
Iteration :  88   Loss :  461.206044617
Iteration :  89   Loss :  2779.13677307
Iteration :  90   Loss :  1453.56558836
Iteration :  91   Loss :  484.35275835
Iteration :  92   Loss :  56.9017732603
Iteration :  93   Loss :  6.68482163949
Iteration :  94   Loss :  0.785333000914
Iteration :  95   Loss :  0.0922609391223
Iteration :  96   Loss :  0.731406919728
Iteration :  97   Loss :  4494.38267921
Iteration :  98   Loss :  6044.26263788
Iteration :  99   Loss :  530.780372435
[-0.10625632 -0.00354293 -0.03383218 ...,  0.01333173  0.00762751
  0.00177902]
CROSS VALIDATION 12
Iteration :  0   Loss :  2577.01947566
Iteration :  1   Loss :  475.818542882
Iteration :  2   Loss :  790.257536971
Iteration :  3   Loss :  5065.68607508
Iteration :  4   Loss :  729.578191691
Iteration :  5   Loss :  1985.19950841
Iteration :  6   Loss :  1433.60698612
Iteration :  7   Loss :  389.74565728
Iteration :  8   Loss :  945.450591153
Iteration :  9   Loss :  2238.74729844
Iteration :  10   Loss :  1447.66564696
Iteration :  11   Loss :  1569.91762884
Iteration :  12   Loss :  655.235129382
Iteration :  13   Loss :  208.445782217
Iteration :  14   Loss :  24.4882153189
Iteration :  15   Loss :  2.87687610239
Iteration :  16   Loss :  0.337975471088
Iteration :  17   Loss :  0.807839975522
Iteration :  18   Loss :  1723.05733288
Iteration :  19   Loss :  578.198707312
Iteration :  20   Loss :  1179.51085606
Iteration :  21   Loss :  1539.85133335
Iteration :  22   Loss :  1516.35114129
Iteration :  23   Loss :  4468.02909286
Iteration :  24   Loss :  242.491988271
Iteration :  25   Loss :  28.487964394
Iteration :  26   Loss :  3.34676671631
Iteration :  27   Loss :  0.393178231286
Iteration :  28   Loss :  0.0461905877107
Iteration :  29   Loss :  1.17581503875
Iteration :  30   Loss :  1700.90145363
Iteration :  31   Loss :  578.400098979
Iteration :  32   Loss :  1182.93332218
Iteration :  33   Loss :  1540.46833486
Iteration :  34   Loss :  1516.3335808
Iteration :  35   Loss :  4467.98773693
Iteration :  36   Loss :  242.491674621
Iteration :  37   Loss :  28.4879275464
Iteration :  38   Loss :  3.34676238745
Iteration :  39   Loss :  0.393177722731
Iteration :  40   Loss :  0.0461905279657
Iteration :  41   Loss :  1.17595317517
Iteration :  42   Loss :  1700.90139539
Iteration :  43   Loss :  578.400099509
Iteration :  44   Loss :  1182.93333118
Iteration :  45   Loss :  1540.46833648
Iteration :  46   Loss :  1516.33358076
Iteration :  47   Loss :  4467.98773682
Iteration :  48   Loss :  242.49167462
Iteration :  49   Loss :  28.4879275463
Iteration :  50   Loss :  3.34676238744
Iteration :  51   Loss :  0.39317772273
Iteration :  52   Loss :  0.0461905279656
Iteration :  53   Loss :  1.17595317554
Iteration :  54   Loss :  1700.90139539
Iteration :  55   Loss :  578.400099509
Iteration :  56   Loss :  1182.93333118
Iteration :  57   Loss :  1540.46833648
Iteration :  58   Loss :  1516.33358076
Iteration :  59   Loss :  4467.98773682
Iteration :  60   Loss :  242.49167462
Iteration :  61   Loss :  28.4879275463
Iteration :  62   Loss :  3.34676238744
Iteration :  63   Loss :  0.39317772273
Iteration :  64   Loss :  0.0461905279656
Iteration :  65   Loss :  1.17595317554
Iteration :  66   Loss :  1700.90139539
Iteration :  67   Loss :  578.400099509
Iteration :  68   Loss :  1182.93333118
Iteration :  69   Loss :  1540.46833648
Iteration :  70   Loss :  1516.33358076
Iteration :  71   Loss :  4467.98773682
Iteration :  72   Loss :  242.49167462
Iteration :  73   Loss :  28.4879275463
Iteration :  74   Loss :  3.34676238744
Iteration :  75   Loss :  0.39317772273
Iteration :  76   Loss :  0.0461905279656
Iteration :  77   Loss :  1.17595317554
Iteration :  78   Loss :  1700.90139539
Iteration :  79   Loss :  578.400099509
Iteration :  80   Loss :  1182.93333118
Iteration :  81   Loss :  1540.46833648
Iteration :  82   Loss :  1516.33358076
Iteration :  83   Loss :  4467.98773682
Iteration :  84   Loss :  242.49167462
Iteration :  85   Loss :  28.4879275463
Iteration :  86   Loss :  3.34676238744
Iteration :  87   Loss :  0.39317772273
Iteration :  88   Loss :  0.0461905279656
Iteration :  89   Loss :  1.17595317554
Iteration :  90   Loss :  1700.90139539
Iteration :  91   Loss :  578.400099509
Iteration :  92   Loss :  1182.93333118
Iteration :  93   Loss :  1540.46833648
Iteration :  94   Loss :  1516.33358076
Iteration :  95   Loss :  4467.98773682
Iteration :  96   Loss :  242.49167462
Iteration :  97   Loss :  28.4879275463
Iteration :  98   Loss :  3.34676238744
Iteration :  99   Loss :  0.39317772273
[-0.00278127 -0.00042054 -0.00125586 ...,  0.00203788  0.00128586
  0.00014871]
CROSS VALIDATION 13
Iteration :  0   Loss :  2442.14263852
Iteration :  1   Loss :  176.286105549
Iteration :  2   Loss :  20.7100957597
Iteration :  3   Loss :  2.47467086881
Iteration :  4   Loss :  4696.48258683
Iteration :  5   Loss :  477.765218427
Iteration :  6   Loss :  56.1278689177
Iteration :  7   Loss :  6.59390333942
Iteration :  8   Loss :  2078.12167872
Iteration :  9   Loss :  3459.25559932
Iteration :  10   Loss :  2321.55612175
Iteration :  11   Loss :  291.734790394
Iteration :  12   Loss :  480.262351203
Iteration :  13   Loss :  5195.77669628
Iteration :  14   Loss :  2517.26993358
Iteration :  15   Loss :  1566.53946272
Iteration :  16   Loss :  1781.51625354
Iteration :  17   Loss :  1063.06853898
Iteration :  18   Loss :  4169.67168238
Iteration :  19   Loss :  2521.4844954
Iteration :  20   Loss :  5282.72437483
Iteration :  21   Loss :  1137.11140913
Iteration :  22   Loss :  2503.60797865
Iteration :  23   Loss :  2364.4859651
Iteration :  24   Loss :  463.639783821
Iteration :  25   Loss :  1723.35065933
Iteration :  26   Loss :  1157.08809968
Iteration :  27   Loss :  1574.39494235
Iteration :  28   Loss :  1166.19418598
Iteration :  29   Loss :  1723.91570219
Iteration :  30   Loss :  491.293634834
Iteration :  31   Loss :  2520.32595869
Iteration :  32   Loss :  1118.13452391
Iteration :  33   Loss :  1347.93420436
Iteration :  34   Loss :  1693.76360361
Iteration :  35   Loss :  3735.83268315
Iteration :  36   Loss :  676.779805037
Iteration :  37   Loss :  2551.66099374
Iteration :  38   Loss :  1672.05871053
Iteration :  39   Loss :  429.995375071
Iteration :  40   Loss :  2401.28543232
Iteration :  41   Loss :  4005.78931022
Iteration :  42   Loss :  877.364489644
Iteration :  43   Loss :  3275.53579415
Iteration :  44   Loss :  1339.88867815
Iteration :  45   Loss :  508.679196397
Iteration :  46   Loss :  4581.91043871
Iteration :  47   Loss :  283.896147065
Iteration :  48   Loss :  33.3521259274
Iteration :  49   Loss :  3.91820852582
Iteration :  50   Loss :  0.460311228292
Iteration :  51   Loss :  0.659011534809
Iteration :  52   Loss :  3518.36748939
Iteration :  53   Loss :  722.654797741
Iteration :  54   Loss :  113.460825482
Iteration :  55   Loss :  13.3293804034
Iteration :  56   Loss :  1.56593591827
Iteration :  57   Loss :  10229.743413
Iteration :  58   Loss :  6720.67820786
Iteration :  59   Loss :  1222.76531857
Iteration :  60   Loss :  156.237291281
Iteration :  61   Loss :  18.3547605955
Iteration :  62   Loss :  2.15631769954
Iteration :  63   Loss :  0.253324253245
Iteration :  64   Loss :  0.0297605391339
Iteration :  65   Loss :  5935.94533142
Iteration :  66   Loss :  1882.32640034
Iteration :  67   Loss :  1874.51158414
Iteration :  68   Loss :  1532.25542679
Iteration :  69   Loss :  2419.49796516
Iteration :  70   Loss :  1727.38264008
Iteration :  71   Loss :  2028.87393086
Iteration :  72   Loss :  2047.14578171
Iteration :  73   Loss :  4075.61437095
Iteration :  74   Loss :  557.083847137
Iteration :  75   Loss :  2530.61797802
Iteration :  76   Loss :  1990.60294585
Iteration :  77   Loss :  2554.6132771
Iteration :  78   Loss :  2045.74606284
Iteration :  79   Loss :  1867.55212978
Iteration :  80   Loss :  3182.93673096
Iteration :  81   Loss :  485.644920532
Iteration :  82   Loss :  1972.22778758
Iteration :  83   Loss :  3949.25146746
Iteration :  84   Loss :  718.807230678
Iteration :  85   Loss :  4252.22992104
Iteration :  86   Loss :  1933.0955481
Iteration :  87   Loss :  2613.26547617
Iteration :  88   Loss :  1030.54155379
Iteration :  89   Loss :  2476.35384175
Iteration :  90   Loss :  598.282663528
Iteration :  91   Loss :  200.304671902
Iteration :  92   Loss :  2600.47071773
Iteration :  93   Loss :  1485.65085078
Iteration :  94   Loss :  1141.3719145
Iteration :  95   Loss :  3363.84640948
Iteration :  96   Loss :  2264.16992151
Iteration :  97   Loss :  1239.92237464
Iteration :  98   Loss :  1849.21577527
Iteration :  99   Loss :  1287.93723867
[-0.08614083 -0.03036107 -0.01469004 ...,  0.06418226 -0.03786401
  0.01135741]
CROSS VALIDATION 14
Iteration :  0   Loss :  1647.05054506
Iteration :  1   Loss :  1894.90811787
Iteration :  2   Loss :  3675.93880371
Iteration :  3   Loss :  647.103444389
Iteration :  4   Loss :  1516.40164924
Iteration :  5   Loss :  2868.82478446
Iteration :  6   Loss :  1854.02611988
Iteration :  7   Loss :  2699.33012529
Iteration :  8   Loss :  2381.99343046
Iteration :  9   Loss :  2171.57488751
Iteration :  10   Loss :  692.478861167
Iteration :  11   Loss :  980.944174543
Iteration :  12   Loss :  2554.30106396
Iteration :  13   Loss :  2163.84874825
Iteration :  14   Loss :  1407.46520024
Iteration :  15   Loss :  2126.16771088
Iteration :  16   Loss :  2132.19052108
Iteration :  17   Loss :  384.779380589
Iteration :  18   Loss :  45.2038905365
Iteration :  19   Loss :  5.31055410638
Iteration :  20   Loss :  0.62388401932
Iteration :  21   Loss :  0.0732939090282
Iteration :  22   Loss :  0.647233466672
Iteration :  23   Loss :  4445.87321926
Iteration :  24   Loss :  6825.17152509
Iteration :  25   Loss :  281.663812811
Iteration :  26   Loss :  2469.56228119
Iteration :  27   Loss :  1743.65137949
Iteration :  28   Loss :  165.427550875
Iteration :  29   Loss :  2012.88498215
Iteration :  30   Loss :  3421.17770688
Iteration :  31   Loss :  158.191576207
Iteration :  32   Loss :  18.5843500338
Iteration :  33   Loss :  2.18328987207
Iteration :  34   Loss :  0.256492944698
Iteration :  35   Loss :  0.0301327970792
Iteration :  36   Loss :  10664.1119504
Iteration :  37   Loss :  3747.20199333
Iteration :  38   Loss :  1681.27224858
Iteration :  39   Loss :  231.072256128
Iteration :  40   Loss :  27.1463739976
Iteration :  41   Loss :  3.18915664548
Iteration :  42   Loss :  0.374662196518
Iteration :  43   Loss :  0.566768049359
Iteration :  44   Loss :  3771.16105023
Iteration :  45   Loss :  270.033401417
Iteration :  46   Loss :  31.7235302478
Iteration :  47   Loss :  3.72688106769
Iteration :  48   Loss :  0.437834074084
Iteration :  49   Loss :  0.0514367571561
Iteration :  50   Loss :  0.056734758606
Iteration :  51   Loss :  3758.7475086
Iteration :  52   Loss :  2080.21944591
Iteration :  53   Loss :  134.738222891
Iteration :  54   Loss :  15.8290495435
Iteration :  55   Loss :  1.8595971067
Iteration :  56   Loss :  0.218465511131
Iteration :  57   Loss :  0.0256653333036
Iteration :  58   Loss :  4237.97258704
Iteration :  59   Loss :  3633.83165541
Iteration :  60   Loss :  300.943563958
Iteration :  61   Loss :  35.3548568585
Iteration :  62   Loss :  4.15348940196
Iteration :  63   Loss :  0.487952031066
Iteration :  64   Loss :  0.261747406517
Iteration :  65   Loss :  6290.85693506
Iteration :  66   Loss :  1312.02044231
Iteration :  67   Loss :  420.699592944
Iteration :  68   Loss :  1374.07380042
Iteration :  69   Loss :  2944.49627742
Iteration :  70   Loss :  1272.19504811
Iteration :  71   Loss :  2968.01341727
Iteration :  72   Loss :  2051.53865113
Iteration :  73   Loss :  2618.02761056
Iteration :  74   Loss :  2967.84521238
Iteration :  75   Loss :  4640.36387898
Iteration :  76   Loss :  578.682769958
Iteration :  77   Loss :  541.492504343
Iteration :  78   Loss :  1997.92619229
Iteration :  79   Loss :  893.802924454
Iteration :  80   Loss :  2072.04825088
Iteration :  81   Loss :  2829.4083023
Iteration :  82   Loss :  1225.89592888
Iteration :  83   Loss :  1887.15860793
Iteration :  84   Loss :  891.669524902
Iteration :  85   Loss :  605.132239973
Iteration :  86   Loss :  1845.83535885
Iteration :  87   Loss :  1248.64991121
Iteration :  88   Loss :  695.833597117
Iteration :  89   Loss :  4600.12492574
Iteration :  90   Loss :  5754.69562886
Iteration :  91   Loss :  379.168473494
Iteration :  92   Loss :  44.9663498424
Iteration :  93   Loss :  3403.00427886
Iteration :  94   Loss :  3586.11530948
Iteration :  95   Loss :  4100.59681451
Iteration :  96   Loss :  1743.49300072
Iteration :  97   Loss :  877.65651458
Iteration :  98   Loss :  1647.30619465
Iteration :  99   Loss :  2810.63807322
[-0.07570096  0.01138828  0.04570176 ...,  0.03515614  0.06044391
  0.00034354]
CROSS VALIDATION 15
Iteration :  0   Loss :  1888.09429227
Iteration :  1   Loss :  1710.02623277
Iteration :  2   Loss :  2911.25867612
Iteration :  3   Loss :  1202.09375856
Iteration :  4   Loss :  1705.18217294
Iteration :  5   Loss :  2604.40626803
Iteration :  6   Loss :  2096.80855488
Iteration :  7   Loss :  1989.26769839
Iteration :  8   Loss :  1236.51659565
Iteration :  9   Loss :  6329.27806854
Iteration :  10   Loss :  509.337965073
Iteration :  11   Loss :  59.8370359244
Iteration :  12   Loss :  7.0296563652
Iteration :  13   Loss :  0.825844192471
Iteration :  14   Loss :  0.0970201948439
Iteration :  15   Loss :  0.0113979347356
Iteration :  16   Loss :  2928.74546564
Iteration :  17   Loss :  485.22478945
Iteration :  18   Loss :  1290.14449589
Iteration :  19   Loss :  2358.26727075
Iteration :  20   Loss :  1339.69098957
Iteration :  21   Loss :  4639.66476039
Iteration :  22   Loss :  1625.75752214
Iteration :  23   Loss :  776.305905004
Iteration :  24   Loss :  4709.77160943
Iteration :  25   Loss :  1370.36202232
Iteration :  26   Loss :  359.949053285
Iteration :  27   Loss :  2520.97179506
Iteration :  28   Loss :  1569.90047128
Iteration :  29   Loss :  2594.10150348
Iteration :  30   Loss :  1841.81847956
Iteration :  31   Loss :  2757.36647311
Iteration :  32   Loss :  1964.35827243
Iteration :  33   Loss :  2665.37976603
Iteration :  34   Loss :  1891.55761375
Iteration :  35   Loss :  3639.75555685
Iteration :  36   Loss :  273.398060014
Iteration :  37   Loss :  236.521624926
Iteration :  38   Loss :  27.7865659701
Iteration :  39   Loss :  3.26436641323
Iteration :  40   Loss :  9781.1659958
Iteration :  41   Loss :  6233.32125971
Iteration :  42   Loss :  925.379892428
Iteration :  43   Loss :  107.816409559
Iteration :  44   Loss :  12.6662742902
Iteration :  45   Loss :  1.48803419675
Iteration :  46   Loss :  0.210412411975
Iteration :  47   Loss :  4255.83808869
Iteration :  48   Loss :  200.292937622
Iteration :  49   Loss :  23.5304189472
Iteration :  50   Loss :  2.76435416248
Iteration :  51   Loss :  6861.94975696
Iteration :  52   Loss :  417.682653494
Iteration :  53   Loss :  836.014538516
Iteration :  54   Loss :  1087.7224114
Iteration :  55   Loss :  4806.58741003
Iteration :  56   Loss :  3696.29547865
Iteration :  57   Loss :  2711.46982776
Iteration :  58   Loss :  439.932180666
Iteration :  59   Loss :  51.683242766
Iteration :  60   Loss :  6.07174855625
Iteration :  61   Loss :  0.713309160906
Iteration :  62   Loss :  0.0837995767313
Iteration :  63   Loss :  10600.0287898
Iteration :  64   Loss :  3708.59084475
Iteration :  65   Loss :  240.964218097
Iteration :  66   Loss :  28.3084819186
Iteration :  67   Loss :  3.32568111094
Iteration :  68   Loss :  0.390701093879
Iteration :  69   Loss :  0.210154742226
Iteration :  70   Loss :  6662.25046675
Iteration :  71   Loss :  1328.43338954
Iteration :  72   Loss :  1838.52477102
Iteration :  73   Loss :  2326.21737463
Iteration :  74   Loss :  451.388636721
Iteration :  75   Loss :  96.9761439434
Iteration :  76   Loss :  4222.94111335
Iteration :  77   Loss :  1796.48930611
Iteration :  78   Loss :  1489.02831072
Iteration :  79   Loss :  1222.43910365
Iteration :  80   Loss :  937.8279301
Iteration :  81   Loss :  3265.85260305
Iteration :  82   Loss :  999.275866622
Iteration :  83   Loss :  4031.3777604
Iteration :  84   Loss :  184.560543211
Iteration :  85   Loss :  21.6821768877
Iteration :  86   Loss :  2.54722264255
Iteration :  87   Loss :  0.299247774997
Iteration :  88   Loss :  0.106793092228
Iteration :  89   Loss :  4139.27760732
Iteration :  90   Loss :  666.416851673
Iteration :  91   Loss :  1104.28636737
Iteration :  92   Loss :  3486.18375759
Iteration :  93   Loss :  1695.35909369
Iteration :  94   Loss :  1019.91544559
Iteration :  95   Loss :  2281.08537318
Iteration :  96   Loss :  1781.0753011
Iteration :  97   Loss :  1830.87346156
Iteration :  98   Loss :  869.339738242
Iteration :  99   Loss :  1465.43919778
[ 0.00168106 -0.02401364 -0.05911191 ...,  0.04793845  0.00407809
  0.02101857]
CROSS VALIDATION 16
Iteration :  0   Loss :  1888.09429227
Iteration :  1   Loss :  1710.02623277
Iteration :  2   Loss :  593.515232862
Iteration :  3   Loss :  1835.10530675
Iteration :  4   Loss :  1609.7276606
Iteration :  5   Loss :  357.336526031
Iteration :  6   Loss :  4265.06456101
Iteration :  7   Loss :  3292.8069988
Iteration :  8   Loss :  1955.32285085
Iteration :  9   Loss :  1884.21099934
Iteration :  10   Loss :  5713.47576522
Iteration :  11   Loss :  2064.22572957
Iteration :  12   Loss :  1555.71674794
Iteration :  13   Loss :  798.704754824
Iteration :  14   Loss :  1608.908165
Iteration :  15   Loss :  1599.55469804
Iteration :  16   Loss :  2623.1838221
Iteration :  17   Loss :  723.996446338
Iteration :  18   Loss :  1941.13652439
Iteration :  19   Loss :  2273.5444439
Iteration :  20   Loss :  499.463726213
Iteration :  21   Loss :  240.161949391
Iteration :  22   Loss :  197.30808863
Iteration :  23   Loss :  4129.88106973
Iteration :  24   Loss :  1646.58021112
Iteration :  25   Loss :  820.692005704
Iteration :  26   Loss :  4625.07897387
Iteration :  27   Loss :  2047.70440536
Iteration :  28   Loss :  2446.97937192
Iteration :  29   Loss :  298.639075498
Iteration :  30   Loss :  2372.46394368
Iteration :  31   Loss :  2023.14833376
Iteration :  32   Loss :  805.009607249
Iteration :  33   Loss :  3565.36028751
Iteration :  34   Loss :  2481.59961273
Iteration :  35   Loss :  544.84051935
Iteration :  36   Loss :  2666.77781271
Iteration :  37   Loss :  1009.90237782
Iteration :  38   Loss :  4015.38827357
Iteration :  39   Loss :  6359.11630779
Iteration :  40   Loss :  295.215864444
Iteration :  41   Loss :  34.6819665871
Iteration :  42   Loss :  4.07443823729
Iteration :  43   Loss :  0.478665098412
Iteration :  44   Loss :  0.21500509763
Iteration :  45   Loss :  6723.15124201
Iteration :  46   Loss :  1013.33196226
Iteration :  47   Loss :  2116.98579328
Iteration :  48   Loss :  2731.94602569
Iteration :  49   Loss :  832.290237472
Iteration :  50   Loss :  2276.17766596
Iteration :  51   Loss :  1495.2602164
Iteration :  52   Loss :  1476.5745427
Iteration :  53   Loss :  2291.84109523
Iteration :  54   Loss :  3233.0915574
Iteration :  55   Loss :  298.481476042
Iteration :  56   Loss :  35.0656107131
Iteration :  57   Loss :  4.1195087581
Iteration :  58   Loss :  0.483959984239
Iteration :  59   Loss :  0.0568556301486
Iteration :  60   Loss :  5252.07586713
Iteration :  61   Loss :  574.188804093
Iteration :  62   Loss :  67.4557139933
Iteration :  63   Loss :  7.92469884105
Iteration :  64   Loss :  0.930993803247
Iteration :  65   Loss :  0.271652320963
Iteration :  66   Loss :  2472.96564463
Iteration :  67   Loss :  1420.69737407
Iteration :  68   Loss :  77.5672540132
Iteration :  69   Loss :  9.1126027966
Iteration :  70   Loss :  1.07054878744
Iteration :  71   Loss :  0.125768096324
Iteration :  72   Loss :  0.120000935276
Iteration :  73   Loss :  3791.37564671
Iteration :  74   Loss :  3494.0268047
Iteration :  75   Loss :  763.84429346
Iteration :  76   Loss :  2737.59535695
Iteration :  77   Loss :  3472.23157533
Iteration :  78   Loss :  451.886002769
Iteration :  79   Loss :  208.433161768
Iteration :  80   Loss :  3912.21647295
Iteration :  81   Loss :  3553.02461937
Iteration :  82   Loss :  6458.65899418
Iteration :  83   Loss :  674.801610899
Iteration :  84   Loss :  1480.98621811
Iteration :  85   Loss :  1733.00615561
Iteration :  86   Loss :  803.58797314
Iteration :  87   Loss :  319.291197811
Iteration :  88   Loss :  5670.96260954
Iteration :  89   Loss :  2106.23171979
Iteration :  90   Loss :  410.800603713
Iteration :  91   Loss :  1684.75648733
Iteration :  92   Loss :  2323.21299176
Iteration :  93   Loss :  2332.3409661
Iteration :  94   Loss :  4087.16813608
Iteration :  95   Loss :  993.974636579
Iteration :  96   Loss :  101.189455751
Iteration :  97   Loss :  11.8877396034
Iteration :  98   Loss :  1.39657192372
Iteration :  99   Loss :  0.16406930192
[ 0.0004761  -0.00028318  0.00070618 ...,  0.00192444  0.00035393
 -0.00019592]
CROSS VALIDATION 17
Iteration :  0   Loss :  1888.09429227
Iteration :  1   Loss :  1695.95378189
Iteration :  2   Loss :  327.253414642
Iteration :  3   Loss :  38.4457387258
Iteration :  4   Loss :  4.51660627525
Iteration :  5   Loss :  0.530611009744
Iteration :  6   Loss :  0.0623361937046
Iteration :  7   Loss :  8379.5481618
Iteration :  8   Loss :  1801.57562619
Iteration :  9   Loss :  2728.65692174
Iteration :  10   Loss :  3836.15849545
Iteration :  11   Loss :  4830.92105823
Iteration :  12   Loss :  816.388236552
Iteration :  13   Loss :  355.700477074
Iteration :  14   Loss :  41.7877002786
Iteration :  15   Loss :  4.90921999581
Iteration :  16   Loss :  0.576735278722
Iteration :  17   Loss :  0.0677548738916
Iteration :  18   Loss :  6212.2058945
Iteration :  19   Loss :  1520.55244175
Iteration :  20   Loss :  1811.76780228
Iteration :  21   Loss :  1337.01893498
Iteration :  22   Loss :  375.822966161
Iteration :  23   Loss :  1181.27461425
Iteration :  24   Loss :  2976.53913646
Iteration :  25   Loss :  292.329936004
Iteration :  26   Loss :  2020.25720184
Iteration :  27   Loss :  2801.62152559
Iteration :  28   Loss :  957.500458008
Iteration :  29   Loss :  1277.30138655
Iteration :  30   Loss :  5506.87403401
Iteration :  31   Loss :  524.89353612
Iteration :  32   Loss :  1429.0842137
Iteration :  33   Loss :  3565.90973774
Iteration :  34   Loss :  253.135956367
Iteration :  35   Loss :  29.7384180123
Iteration :  36   Loss :  3.49367003631
Iteration :  37   Loss :  0.41043643672
Iteration :  38   Loss :  0.259344876477
Iteration :  39   Loss :  4212.49801518
Iteration :  40   Loss :  1409.51614568
Iteration :  41   Loss :  2553.31693565
Iteration :  42   Loss :  3468.94772727
Iteration :  43   Loss :  501.117386905
Iteration :  44   Loss :  58.8712822109
Iteration :  45   Loss :  6.91619959659
Iteration :  46   Loss :  0.812515288668
Iteration :  47   Loss :  6760.49477545
Iteration :  48   Loss :  1974.05554959
Iteration :  49   Loss :  2243.86093235
Iteration :  50   Loss :  771.444779964
Iteration :  51   Loss :  82.9217682275
Iteration :  52   Loss :  9.74165125042
Iteration :  53   Loss :  1.14444941435
Iteration :  54   Loss :  10230.2210786
Iteration :  55   Loss :  3732.26673013
Iteration :  56   Loss :  242.75486984
Iteration :  57   Loss :  28.5188477269
Iteration :  58   Loss :  3.35039489096
Iteration :  59   Loss :  0.393604469328
Iteration :  60   Loss :  0.120467465564
Iteration :  61   Loss :  6662.20141274
Iteration :  62   Loss :  1362.24773314
Iteration :  63   Loss :  3365.44762209
Iteration :  64   Loss :  2394.87497945
Iteration :  65   Loss :  874.054996548
Iteration :  66   Loss :  67.9473321778
Iteration :  67   Loss :  7.98245415674
Iteration :  68   Loss :  1.2063255735
Iteration :  69   Loss :  5383.32019172
Iteration :  70   Loss :  755.229767269
Iteration :  71   Loss :  56.2596728995
Iteration :  72   Loss :  1433.74287223
Iteration :  73   Loss :  1804.90365253
Iteration :  74   Loss :  1386.7592529
Iteration :  75   Loss :  9220.16118844
Iteration :  76   Loss :  643.217914549
Iteration :  77   Loss :  2465.00110783
Iteration :  78   Loss :  849.729467852
Iteration :  79   Loss :  1900.48681984
Iteration :  80   Loss :  2197.59056172
Iteration :  81   Loss :  693.766418016
Iteration :  82   Loss :  6655.9213424
Iteration :  83   Loss :  3827.47282454
Iteration :  84   Loss :  3019.06167566
Iteration :  85   Loss :  333.042068015
Iteration :  86   Loss :  4058.76964797
Iteration :  87   Loss :  1995.94412579
Iteration :  88   Loss :  2875.3432908
Iteration :  89   Loss :  1994.1702822
Iteration :  90   Loss :  114.55390016
Iteration :  91   Loss :  13.4577948419
Iteration :  92   Loss :  1.82609157085
Iteration :  93   Loss :  3588.39003482
Iteration :  94   Loss :  773.113912778
Iteration :  95   Loss :  1532.42085391
Iteration :  96   Loss :  5060.79989174
Iteration :  97   Loss :  873.840005866
Iteration :  98   Loss :  3549.48407487
Iteration :  99   Loss :  1767.24974877
[-0.13830974 -0.06690328  0.00663731 ...,  0.03218926  0.02009872
 -0.00405407]
CROSS VALIDATION 18
Iteration :  0   Loss :  3189.18318531
Iteration :  1   Loss :  1117.44234644
Iteration :  2   Loss :  72.8746564965
Iteration :  3   Loss :  8.56131633175
Iteration :  4   Loss :  1.50781258233
Iteration :  5   Loss :  3562.19007844
Iteration :  6   Loss :  866.995123324
Iteration :  7   Loss :  606.769865823
Iteration :  8   Loss :  4449.59195325
Iteration :  9   Loss :  1408.79866731
Iteration :  10   Loss :  2303.67745001
Iteration :  11   Loss :  1430.77529779
Iteration :  12   Loss :  682.603604784
Iteration :  13   Loss :  2384.48333817
Iteration :  14   Loss :  2284.3108795
Iteration :  15   Loss :  425.891760321
Iteration :  16   Loss :  2527.51185027
Iteration :  17   Loss :  3867.82948353
Iteration :  18   Loss :  675.064094919
Iteration :  19   Loss :  1527.39008239
Iteration :  20   Loss :  1604.28243337
Iteration :  21   Loss :  201.056453025
Iteration :  22   Loss :  23.6201167544
Iteration :  23   Loss :  2.77489186295
Iteration :  24   Loss :  0.325994360278
Iteration :  25   Loss :  0.0382978249899
Iteration :  26   Loss :  7743.94424645
Iteration :  27   Loss :  1118.8604361
Iteration :  28   Loss :  608.047305569
Iteration :  29   Loss :  3293.56296309
Iteration :  30   Loss :  1215.64460898
Iteration :  31   Loss :  1899.54012801
Iteration :  32   Loss :  658.743165009
Iteration :  33   Loss :  1417.68010695
Iteration :  34   Loss :  1326.18152682
Iteration :  35   Loss :  954.244255334
Iteration :  36   Loss :  1936.62279594
Iteration :  37   Loss :  357.582908146
Iteration :  38   Loss :  2506.39732108
Iteration :  39   Loss :  934.262399125
Iteration :  40   Loss :  136.495480194
Iteration :  41   Loss :  16.0354921721
Iteration :  42   Loss :  1.88384999148
Iteration :  43   Loss :  0.283031226763
Iteration :  44   Loss :  5348.65979006
Iteration :  45   Loss :  1098.72948004
Iteration :  46   Loss :  2845.3852384
Iteration :  47   Loss :  152.996535735
Iteration :  48   Loss :  17.974036559
Iteration :  49   Loss :  2.11159023092
Iteration :  50   Loss :  0.248069669197
Iteration :  51   Loss :  0.504982090196
Iteration :  52   Loss :  2778.60719562
Iteration :  53   Loss :  1355.86658906
Iteration :  54   Loss :  88.4705571533
Iteration :  55   Loss :  10.3935231018
Iteration :  56   Loss :  1.22103133454
Iteration :  57   Loss :  0.14344678944
Iteration :  58   Loss :  0.346495311051
Iteration :  59   Loss :  2656.69783462
Iteration :  60   Loss :  452.688368331
Iteration :  61   Loss :  53.181839988
Iteration :  62   Loss :  6.24780379258
Iteration :  63   Loss :  0.733992134144
Iteration :  64   Loss :  3873.16369682
Iteration :  65   Loss :  607.587922999
Iteration :  66   Loss :  452.88500321
Iteration :  67   Loss :  1900.42918517
Iteration :  68   Loss :  1984.35475854
Iteration :  69   Loss :  564.181514998
Iteration :  70   Loss :  1435.55857794
Iteration :  71   Loss :  4239.55342016
Iteration :  72   Loss :  556.094136335
Iteration :  73   Loss :  1405.02105275
Iteration :  74   Loss :  2245.95807556
Iteration :  75   Loss :  1106.97073078
Iteration :  76   Loss :  1217.43628249
Iteration :  77   Loss :  2481.50585853
Iteration :  78   Loss :  1937.6165239
Iteration :  79   Loss :  2490.18321761
Iteration :  80   Loss :  2999.81583887
Iteration :  81   Loss :  2749.6904849
Iteration :  82   Loss :  548.836314314
Iteration :  83   Loss :  2792.92044072
Iteration :  84   Loss :  1629.0100406
Iteration :  85   Loss :  808.684107937
Iteration :  86   Loss :  313.54747179
Iteration :  87   Loss :  37.1683359482
Iteration :  88   Loss :  1614.18598278
Iteration :  89   Loss :  2392.18202833
Iteration :  90   Loss :  1106.54308746
Iteration :  91   Loss :  1754.18774584
Iteration :  92   Loss :  1936.76817038
Iteration :  93   Loss :  1002.82360518
Iteration :  94   Loss :  1928.99690578
Iteration :  95   Loss :  260.470482318
Iteration :  96   Loss :  2262.06357786
Iteration :  97   Loss :  4176.40506165
Iteration :  98   Loss :  244.461257954
Iteration :  99   Loss :  28.719314242
[-0.00925877 -0.01467906 -0.00278394 ...,  0.01822081  0.00315995
  0.00549214]
CROSS VALIDATION 19
Iteration :  0   Loss :  2756.12784786
Iteration :  1   Loss :  739.818483104
Iteration :  2   Loss :  60.7341920391
Iteration :  3   Loss :  7.13505428632
Iteration :  4   Loss :  0.838226342683
Iteration :  5   Loss :  0.241713163272
Iteration :  6   Loss :  4933.98580114
Iteration :  7   Loss :  2594.20431179
Iteration :  8   Loss :  1216.29103002
Iteration :  9   Loss :  702.108374532
Iteration :  10   Loss :  264.640496177
Iteration :  11   Loss :  446.989176288
Iteration :  12   Loss :  3501.99548101
Iteration :  13   Loss :  1182.53513238
Iteration :  14   Loss :  3491.55067214
Iteration :  15   Loss :  320.548265977
Iteration :  16   Loss :  3668.44679342
Iteration :  17   Loss :  2338.62675193
Iteration :  18   Loss :  605.766616749
Iteration :  19   Loss :  194.606441124
Iteration :  20   Loss :  3084.50842742
Iteration :  21   Loss :  1520.75903496
Iteration :  22   Loss :  1919.51649004
Iteration :  23   Loss :  2354.46343158
Iteration :  24   Loss :  389.440008742
Iteration :  25   Loss :  1473.19256726
Iteration :  26   Loss :  2359.65815183
Iteration :  27   Loss :  1437.52577571
Iteration :  28   Loss :  536.462409224
Iteration :  29   Loss :  2753.31004061
Iteration :  30   Loss :  614.702439847
Iteration :  31   Loss :  1960.91720289
Iteration :  32   Loss :  368.762540077
Iteration :  33   Loss :  1869.25404305
Iteration :  34   Loss :  2059.94195989
Iteration :  35   Loss :  1652.30284667
Iteration :  36   Loss :  4751.17955729
Iteration :  37   Loss :  1211.53717397
Iteration :  38   Loss :  1578.52140952
Iteration :  39   Loss :  3146.2884093
Iteration :  40   Loss :  2672.68543518
Iteration :  41   Loss :  998.872686134
Iteration :  42   Loss :  2418.46514045
Iteration :  43   Loss :  1267.9361915
Iteration :  44   Loss :  286.836484269
Iteration :  45   Loss :  3345.76215856
Iteration :  46   Loss :  1201.75411912
Iteration :  47   Loss :  1960.88526107
Iteration :  48   Loss :  1902.78926577
Iteration :  49   Loss :  1446.6052854
Iteration :  50   Loss :  600.999025673
Iteration :  51   Loss :  937.607469006
Iteration :  52   Loss :  2495.64606915
Iteration :  53   Loss :  3697.37364826
Iteration :  54   Loss :  724.38289996
Iteration :  55   Loss :  783.488065746
Iteration :  56   Loss :  1211.88538525
Iteration :  57   Loss :  3057.00601738
Iteration :  58   Loss :  984.316960087
Iteration :  59   Loss :  2169.36584936
Iteration :  60   Loss :  356.65616791
Iteration :  61   Loss :  41.8999748602
Iteration :  62   Loss :  4.92241001626
Iteration :  63   Loss :  0.665694721715
Iteration :  64   Loss :  10459.0776677
Iteration :  65   Loss :  1160.28198523
Iteration :  66   Loss :  433.095039027
Iteration :  67   Loss :  1653.68456281
Iteration :  68   Loss :  2026.22701048
Iteration :  69   Loss :  1460.74108936
Iteration :  70   Loss :  357.842459824
Iteration :  71   Loss :  4784.54087759
Iteration :  72   Loss :  160.500092285
Iteration :  73   Loss :  18.8555545561
Iteration :  74   Loss :  2.21515098562
Iteration :  75   Loss :  0.790875959026
Iteration :  76   Loss :  3611.2341346
Iteration :  77   Loss :  670.304628683
Iteration :  78   Loss :  97.4286822769
Iteration :  79   Loss :  2310.78208284
Iteration :  80   Loss :  2003.3135749
Iteration :  81   Loss :  846.215940957
Iteration :  82   Loss :  2204.612328
Iteration :  83   Loss :  268.156418278
Iteration :  84   Loss :  1288.4891648
Iteration :  85   Loss :  4297.79868968
Iteration :  86   Loss :  5941.84374223
Iteration :  87   Loss :  2380.23965996
Iteration :  88   Loss :  1515.70689154
Iteration :  89   Loss :  749.933959115
Iteration :  90   Loss :  254.309985299
Iteration :  91   Loss :  1557.15867999
Iteration :  92   Loss :  1770.58512017
Iteration :  93   Loss :  2807.19540939
Iteration :  94   Loss :  1851.11785212
Iteration :  95   Loss :  2023.96548725
Iteration :  96   Loss :  3997.00869874
Iteration :  97   Loss :  691.119932347
Iteration :  98   Loss :  381.03956504
Iteration :  99   Loss :  2312.3497327
[-0.11362648 -0.0840334  -0.01457079 ...,  0.02649348 -0.00407099
  0.00144853]
Accuracy (Hinge Loss):	0.65
lmda : 0.3  eta : 0.01
Logistic Loss
CROSS VALIDATION 0
Iteration :  0   Loss :  1422.63046967
Iteration :  1   Loss :  402.311438487
Iteration :  2   Loss :  212.578396444
Iteration :  3   Loss :  112.324856595
Iteration :  4   Loss :  59.3516256601
Iteration :  5   Loss :  31.3609611912
Iteration :  6   Loss :  16.5709005591
Iteration :  7   Loss :  8.75594162011
Iteration :  8   Loss :  4.62657496381
Iteration :  9   Loss :  2.44464808293
Iteration :  10   Loss :  1.2917340158
Iteration :  11   Loss :  0.682543188661
Iteration :  12   Loss :  0.360679310407
Iteration :  13   Loss :  0.190990577778
Iteration :  14   Loss :  0.101834269789
Iteration :  15   Loss :  0.0541792539083
Iteration :  16   Loss :  0.0302370922184
Iteration :  17   Loss :  0.0165922386834
Iteration :  18   Loss :  0.0105739148422
Iteration :  19   Loss :  0.00666963651709
Iteration :  20   Loss :  0.00541795947451
Iteration :  21   Loss :  0.00778004964807
Iteration :  22   Loss :  9580.6193707
Iteration :  23   Loss :  5052.54102942
Iteration :  24   Loss :  424.856150127
Iteration :  25   Loss :  224.490855773
Iteration :  26   Loss :  118.619312233
Iteration :  27   Loss :  62.6775695878
Iteration :  28   Loss :  33.1183654287
Iteration :  29   Loss :  17.4994999947
Iteration :  30   Loss :  9.24660671208
Iteration :  31   Loss :  4.8858387871
Iteration :  32   Loss :  2.58164248487
Iteration :  33   Loss :  1.36416266692
Iteration :  34   Loss :  0.720817062068
Iteration :  35   Loss :  0.380965829709
Iteration :  36   Loss :  0.201373772432
Iteration :  37   Loss :  0.106524999657
Iteration :  38   Loss :  0.0567770765526
Iteration :  39   Loss :  0.0308898503519
Iteration :  40   Loss :  0.0172110384069
Iteration :  41   Loss :  0.0104987810016
Iteration :  42   Loss :  0.00657088342869
Iteration :  43   Loss :  0.00540408695345
Iteration :  44   Loss :  0.0620125435949
Iteration :  45   Loss :  6462.77342633
Iteration :  46   Loss :  518.956722568
Iteration :  47   Loss :  1239.35108523
Iteration :  48   Loss :  231.326510747
Iteration :  49   Loss :  122.231221897
Iteration :  50   Loss :  64.5860760114
Iteration :  51   Loss :  34.1268061449
Iteration :  52   Loss :  18.0323526304
Iteration :  53   Loss :  9.52816215231
Iteration :  54   Loss :  5.03461462946
Iteration :  55   Loss :  2.66036799569
Iteration :  56   Loss :  1.40631329411
Iteration :  57   Loss :  0.743011928453
Iteration :  58   Loss :  0.39322231086
Iteration :  59   Loss :  0.207959123797
Iteration :  60   Loss :  0.110687818986
Iteration :  61   Loss :  0.0591995491598
Iteration :  62   Loss :  0.0325576251097
Iteration :  63   Loss :  0.0183828131536
Iteration :  64   Loss :  0.0115702909598
Iteration :  65   Loss :  0.00725890155903
Iteration :  66   Loss :  0.0209036255071
Iteration :  67   Loss :  8606.40271553
Iteration :  68   Loss :  1941.41383533
Iteration :  69   Loss :  2838.11447091
Iteration :  70   Loss :  311.470450201
Iteration :  71   Loss :  164.578688342
Iteration :  72   Loss :  86.9621673603
Iteration :  73   Loss :  45.9502196289
Iteration :  74   Loss :  24.2797531758
Iteration :  75   Loss :  12.8292988165
Iteration :  76   Loss :  6.77890136306
Iteration :  77   Loss :  3.58197260111
Iteration :  78   Loss :  1.89268426887
Iteration :  79   Loss :  1.00013749648
Iteration :  80   Loss :  0.528463853209
Iteration :  81   Loss :  0.279423067016
Iteration :  82   Loss :  0.147816568144
Iteration :  83   Loss :  0.0782115123584
Iteration :  84   Loss :  0.0420114495957
Iteration :  85   Loss :  0.0231679320476
Iteration :  86   Loss :  0.0130045574653
Iteration :  87   Loss :  0.00849360422863
Iteration :  88   Loss :  0.0064353899912
Iteration :  89   Loss :  0.17223138242
Iteration :  90   Loss :  6372.33304741
Iteration :  91   Loss :  523.339451997
Iteration :  92   Loss :  1228.21391896
Iteration :  93   Loss :  231.410302903
Iteration :  94   Loss :  122.275497054
Iteration :  95   Loss :  64.6094706777
Iteration :  96   Loss :  34.139167714
Iteration :  97   Loss :  18.0388843923
Iteration :  98   Loss :  9.53161346521
Iteration :  99   Loss :  5.03643641431
[-0.02266378 -0.00307065 -0.0043004  ...,  0.01475457  0.00088541
 -0.00131341]
CROSS VALIDATION 1
Iteration :  0   Loss :  1647.93804487
Iteration :  1   Loss :  423.215542203
Iteration :  2   Loss :  368.29814324
Iteration :  3   Loss :  164.53071914
Iteration :  4   Loss :  86.936818332
Iteration :  5   Loss :  45.9367735167
Iteration :  6   Loss :  24.2726522734
Iteration :  7   Loss :  12.8254904139
Iteration :  8   Loss :  6.7768945349
Iteration :  9   Loss :  3.58086231981
Iteration :  10   Loss :  1.89214939843
Iteration :  11   Loss :  0.999972809606
Iteration :  12   Loss :  0.528306530763
Iteration :  13   Loss :  0.279365889146
Iteration :  14   Loss :  0.147755955728
Iteration :  15   Loss :  0.078280936367
Iteration :  16   Loss :  0.0418823268738
Iteration :  17   Loss :  0.022343819354
Iteration :  18   Loss :  0.0126204207865
Iteration :  19   Loss :  0.00837690708838
Iteration :  20   Loss :  0.00619928290796
Iteration :  21   Loss :  0.0611615418091
Iteration :  22   Loss :  6447.49364848
Iteration :  23   Loss :  788.806656508
Iteration :  24   Loss :  281.861528309
Iteration :  25   Loss :  148.933552405
Iteration :  26   Loss :  78.6953904818
Iteration :  27   Loss :  41.5820638336
Iteration :  28   Loss :  21.9716557994
Iteration :  29   Loss :  11.6096608504
Iteration :  30   Loss :  6.1344591546
Iteration :  31   Loss :  3.24140296642
Iteration :  32   Loss :  1.71273341736
Iteration :  33   Loss :  0.904995770345
Iteration :  34   Loss :  0.47819940239
Iteration :  35   Loss :  0.252846915297
Iteration :  36   Loss :  0.134289720789
Iteration :  37   Loss :  0.0711700231327
Iteration :  38   Loss :  0.038860217058
Iteration :  39   Loss :  0.0215605965
Iteration :  40   Loss :  0.0132867012194
Iteration :  41   Loss :  0.00798886315394
Iteration :  42   Loss :  0.00658699708243
Iteration :  43   Loss :  0.0276537008088
Iteration :  44   Loss :  7317.83825002
Iteration :  45   Loss :  1516.62177545
Iteration :  46   Loss :  385.945049464
Iteration :  47   Loss :  548.420251331
Iteration :  48   Loss :  180.910356011
Iteration :  49   Loss :  95.5916976301
Iteration :  50   Loss :  50.5099478951
Iteration :  51   Loss :  26.6890838809
Iteration :  52   Loss :  14.1023150505
Iteration :  53   Loss :  7.45155924687
Iteration :  54   Loss :  3.93734893958
Iteration :  55   Loss :  2.08046613602
Iteration :  56   Loss :  1.09930305674
Iteration :  57   Loss :  0.580871049574
Iteration :  58   Loss :  0.30695595593
Iteration :  59   Loss :  0.162248593424
Iteration :  60   Loss :  0.0859406479824
Iteration :  61   Loss :  0.0458091420314
Iteration :  62   Loss :  0.0249390463278
Iteration :  63   Loss :  0.0139913219575
Iteration :  64   Loss :  0.00941925117832
Iteration :  65   Loss :  0.00643819397135
Iteration :  66   Loss :  0.0120314467446
Iteration :  67   Loss :  0.00673226500839
Iteration :  68   Loss :  0.0045781449211
Iteration :  69   Loss :  0.00582098413372
Iteration :  70   Loss :  0.00611339900365
Iteration :  71   Loss :  0.00648696044322
Iteration :  72   Loss :  0.00421235077591
Iteration :  73   Loss :  0.00556873035082
Iteration :  74   Loss :  0.0202811146853
Iteration :  75   Loss :  5804.35322969
Iteration :  76   Loss :  341.636550101
Iteration :  77   Loss :  180.51823299
Iteration :  78   Loss :  95.384502718
Iteration :  79   Loss :  50.4004676324
Iteration :  80   Loss :  26.6312354895
Iteration :  81   Loss :  14.0717605787
Iteration :  82   Loss :  7.4356551313
Iteration :  83   Loss :  3.92921264695
Iteration :  84   Loss :  2.07640170811
Iteration :  85   Loss :  1.09740365896
Iteration :  86   Loss :  0.58074536435
Iteration :  87   Loss :  0.306783028336
Iteration :  88   Loss :  0.163707263441
Iteration :  89   Loss :  0.0869813871156
Iteration :  90   Loss :  0.0470847373476
Iteration :  91   Loss :  0.0363365863012
Iteration :  92   Loss :  10268.4058016
Iteration :  93   Loss :  1129.08455029
Iteration :  94   Loss :  1041.85771907
Iteration :  95   Loss :  238.725031434
Iteration :  96   Loss :  126.140545653
Iteration :  97   Loss :  66.6517338467
Iteration :  98   Loss :  35.2182845077
Iteration :  99   Loss :  18.609081746
[-0.02080837 -0.02048832 -0.00551164 ...,  0.01476307  0.00041809
  0.00240845]
CROSS VALIDATION 2
Iteration :  0   Loss :  5553.85744342
Iteration :  1   Loss :  386.691363241
Iteration :  2   Loss :  204.324863907
Iteration :  3   Loss :  107.963751277
Iteration :  4   Loss :  57.0472710343
Iteration :  5   Loss :  30.1434959649
Iteration :  6   Loss :  15.9276228209
Iteration :  7   Loss :  8.41617384763
Iteration :  8   Loss :  4.44707643556
Iteration :  9   Loss :  2.34999724566
Iteration :  10   Loss :  1.2418350852
Iteration :  11   Loss :  0.656419364552
Iteration :  12   Loss :  0.350752498051
Iteration :  13   Loss :  6556.01972427
Iteration :  14   Loss :  576.517219735
Iteration :  15   Loss :  391.552637373
Iteration :  16   Loss :  1255.57294524
Iteration :  17   Loss :  145.011451269
Iteration :  18   Loss :  76.6229811727
Iteration :  19   Loss :  40.4870180418
Iteration :  20   Loss :  21.3930416898
Iteration :  21   Loss :  11.3039268612
Iteration :  22   Loss :  5.97296634913
Iteration :  23   Loss :  3.15639588244
Iteration :  24   Loss :  1.6676665782
Iteration :  25   Loss :  0.881456484714
Iteration :  26   Loss :  0.465665485845
Iteration :  27   Loss :  0.24649548353
Iteration :  28   Loss :  0.130698532765
Iteration :  29   Loss :  0.0696497467057
Iteration :  30   Loss :  0.0376726161678
Iteration :  31   Loss :  0.021364710141
Iteration :  32   Loss :  0.012572286778
Iteration :  33   Loss :  0.00884789108499
Iteration :  34   Loss :  0.00944786833939
Iteration :  35   Loss :  38.1485661385
Iteration :  36   Loss :  2314.19480789
Iteration :  37   Loss :  1344.07574496
Iteration :  38   Loss :  298.805564782
Iteration :  39   Loss :  157.88665629
Iteration :  40   Loss :  83.4261445319
Iteration :  41   Loss :  44.0817593772
Iteration :  42   Loss :  23.2924764856
Iteration :  43   Loss :  12.3075727579
Iteration :  44   Loss :  6.5032306584
Iteration :  45   Loss :  3.43625910879
Iteration :  46   Loss :  1.81569396679
Iteration :  47   Loss :  0.959399648862
Iteration :  48   Loss :  0.506960717106
Iteration :  49   Loss :  0.268246657793
Iteration :  50   Loss :  0.142166539043
Iteration :  51   Loss :  0.0754146538622
Iteration :  52   Loss :  0.0405117802651
Iteration :  53   Loss :  0.0223171741642
Iteration :  54   Loss :  0.0129399691893
Iteration :  55   Loss :  0.00839095767355
Iteration :  56   Loss :  0.00538350381925
Iteration :  57   Loss :  0.0227788426528
Iteration :  58   Loss :  5277.51380465
Iteration :  59   Loss :  1716.84188512
Iteration :  60   Loss :  509.08856474
Iteration :  61   Loss :  255.373607722
Iteration :  62   Loss :  134.937530556
Iteration :  63   Loss :  71.2999957782
Iteration :  64   Loss :  37.6743918244
Iteration :  65   Loss :  19.9068707446
Iteration :  66   Loss :  10.5186451813
Iteration :  67   Loss :  5.55802566113
Iteration :  68   Loss :  2.936845091
Iteration :  69   Loss :  1.55190139667
Iteration :  70   Loss :  0.820875536946
Iteration :  71   Loss :  0.434018113048
Iteration :  72   Loss :  0.230071966044
Iteration :  73   Loss :  0.122351350044
Iteration :  74   Loss :  0.0651840907231
Iteration :  75   Loss :  0.0355690011919
Iteration :  76   Loss :  0.019486693008
Iteration :  77   Loss :  0.012587894063
Iteration :  78   Loss :  0.00744758872385
Iteration :  79   Loss :  0.00594063546347
Iteration :  80   Loss :  0.0170090118613
Iteration :  81   Loss :  651.850110668
Iteration :  82   Loss :  2621.93456974
Iteration :  83   Loss :  1754.09150849
Iteration :  84   Loss :  307.280474788
Iteration :  85   Loss :  162.364736222
Iteration :  86   Loss :  85.7923289352
Iteration :  87   Loss :  45.3320337618
Iteration :  88   Loss :  23.9531122477
Iteration :  89   Loss :  12.6566478216
Iteration :  90   Loss :  6.68767942134
Iteration :  91   Loss :  3.53372720146
Iteration :  92   Loss :  1.86736942562
Iteration :  93   Loss :  0.987498678005
Iteration :  94   Loss :  0.52160558769
Iteration :  95   Loss :  0.276644037879
Iteration :  96   Loss :  0.145987195461
Iteration :  97   Loss :  0.0781188591534
Iteration :  98   Loss :  0.0413287548915
Iteration :  99   Loss :  0.0232651253611
[ -1.47166286e-03  -1.84276669e-04  -4.91888085e-04 ...,   1.52923124e-03
   5.70418643e-04   8.41549843e-05]
CROSS VALIDATION 3
Iteration :  0   Loss :  2348.80424702
Iteration :  1   Loss :  359.083467725
Iteration :  2   Loss :  189.737055565
Iteration :  3   Loss :  100.255660564
Iteration :  4   Loss :  52.974351506
Iteration :  5   Loss :  27.9912567271
Iteration :  6   Loss :  14.790384591
Iteration :  7   Loss :  7.81522437371
Iteration :  8   Loss :  4.12948060298
Iteration :  9   Loss :  2.18206159608
Iteration :  10   Loss :  1.15297112755
Iteration :  11   Loss :  0.609318702937
Iteration :  12   Loss :  0.32200368855
Iteration :  13   Loss :  0.170635080779
Iteration :  14   Loss :  0.0901591520527
Iteration :  15   Loss :  0.0487579344386
Iteration :  16   Loss :  0.0261927891846
Iteration :  17   Loss :  0.0155620857367
Iteration :  18   Loss :  0.00885213499512
Iteration :  19   Loss :  0.00658886524509
Iteration :  20   Loss :  0.00475659474826
Iteration :  21   Loss :  0.0058274756998
Iteration :  22   Loss :  0.0337892760667
Iteration :  23   Loss :  0.00677326798181
Iteration :  24   Loss :  0.00566271456222
Iteration :  25   Loss :  0.00507999280955
Iteration :  26   Loss :  0.0044593769216
Iteration :  27   Loss :  0.00858240806555
Iteration :  28   Loss :  0.00667723009626
Iteration :  29   Loss :  0.0055661933285
Iteration :  30   Loss :  0.00502510986556
Iteration :  31   Loss :  0.00513539759973
Iteration :  32   Loss :  0.00614819473875
Iteration :  33   Loss :  0.025096412574
Iteration :  34   Loss :  0.0119816499063
Iteration :  35   Loss :  0.00772414454699
Iteration :  36   Loss :  0.0215122916742
Iteration :  37   Loss :  4279.70901754
Iteration :  38   Loss :  3303.67601959
Iteration :  39   Loss :  1576.73738818
Iteration :  40   Loss :  653.937599834
Iteration :  41   Loss :  234.319554233
Iteration :  42   Loss :  123.812723997
Iteration :  43   Loss :  65.421730055
Iteration :  44   Loss :  34.5683595773
Iteration :  45   Loss :  18.2656662094
Iteration :  46   Loss :  9.6514438352
Iteration :  47   Loss :  5.09977338662
Iteration :  48   Loss :  2.6946920218
Iteration :  49   Loss :  1.42389007466
Iteration :  50   Loss :  0.752380362334
Iteration :  51   Loss :  0.397601362705
Iteration :  52   Loss :  0.210119627826
Iteration :  53   Loss :  0.111326464618
Iteration :  54   Loss :  0.0594199980786
Iteration :  55   Loss :  0.0319767695231
Iteration :  56   Loss :  0.0181687528692
Iteration :  57   Loss :  0.0105396005672
Iteration :  58   Loss :  0.00714003952372
Iteration :  59   Loss :  0.00821706516163
Iteration :  60   Loss :  10162.8240676
Iteration :  61   Loss :  1060.92200486
Iteration :  62   Loss :  374.686917138
Iteration :  63   Loss :  197.981803136
Iteration :  64   Loss :  104.612124363
Iteration :  65   Loss :  55.2762748415
Iteration :  66   Loss :  29.2075758806
Iteration :  67   Loss :  15.4330676791
Iteration :  68   Loss :  8.154722806
Iteration :  69   Loss :  4.30899087591
Iteration :  70   Loss :  2.27693559055
Iteration :  71   Loss :  1.2032857837
Iteration :  72   Loss :  0.635899816231
Iteration :  73   Loss :  0.336482934532
Iteration :  74   Loss :  0.182359105633
Iteration :  75   Loss :  10071.0351912
Iteration :  76   Loss :  4253.29280241
Iteration :  77   Loss :  926.918325434
Iteration :  78   Loss :  277.797026656
Iteration :  79   Loss :  146.785899714
Iteration :  80   Loss :  77.5605866417
Iteration :  81   Loss :  40.9824418555
Iteration :  82   Loss :  21.654820009
Iteration :  83   Loss :  11.4422513105
Iteration :  84   Loss :  6.04609888944
Iteration :  85   Loss :  3.19470433358
Iteration :  86   Loss :  1.68819061143
Iteration :  87   Loss :  0.892032975131
Iteration :  88   Loss :  0.471665033537
Iteration :  89   Loss :  0.249252160252
Iteration :  90   Loss :  0.132118695506
Iteration :  91   Loss :  0.0701396859987
Iteration :  92   Loss :  0.0376218205552
Iteration :  93   Loss :  0.0207421002153
Iteration :  94   Loss :  0.0136118111774
Iteration :  95   Loss :  0.00751472661489
Iteration :  96   Loss :  0.0221303092956
Iteration :  97   Loss :  7446.56821024
Iteration :  98   Loss :  290.048939459
Iteration :  99   Loss :  153.259720066
[ 0.03921076 -0.02614416  0.00114514 ...,  0.05557615 -0.02038474
  0.00254594]
CROSS VALIDATION 4
Iteration :  0   Loss :  2348.80424702
Iteration :  1   Loss :  359.083467725
Iteration :  2   Loss :  189.737055565
Iteration :  3   Loss :  100.255660564
Iteration :  4   Loss :  52.974351506
Iteration :  5   Loss :  27.9912567271
Iteration :  6   Loss :  14.790384591
Iteration :  7   Loss :  7.81522437371
Iteration :  8   Loss :  4.12948060298
Iteration :  9   Loss :  2.18206159608
Iteration :  10   Loss :  1.15297112758
Iteration :  11   Loss :  0.609318724176
Iteration :  12   Loss :  0.322005510288
Iteration :  13   Loss :  0.170680617584
Iteration :  14   Loss :  0.0903226349348
Iteration :  15   Loss :  0.0488752407399
Iteration :  16   Loss :  0.0262984396834
Iteration :  17   Loss :  0.0157150734829
Iteration :  18   Loss :  0.00894159455886
Iteration :  19   Loss :  0.0064422701498
Iteration :  20   Loss :  0.00477628753984
Iteration :  21   Loss :  0.00569655479779
Iteration :  22   Loss :  0.02383512682
Iteration :  23   Loss :  4020.46268938
Iteration :  24   Loss :  761.783481385
Iteration :  25   Loss :  1508.37607913
Iteration :  26   Loss :  368.177666025
Iteration :  27   Loss :  163.427625947
Iteration :  28   Loss :  86.3539520255
Iteration :  29   Loss :  45.62879126
Iteration :  30   Loss :  24.1099167594
Iteration :  31   Loss :  12.7395062373
Iteration :  32   Loss :  6.73156553422
Iteration :  33   Loss :  3.55696372899
Iteration :  34   Loss :  1.87975412755
Iteration :  35   Loss :  0.993221225025
Iteration :  36   Loss :  0.525128957436
Iteration :  37   Loss :  0.277433923764
Iteration :  38   Loss :  0.146962735674
Iteration :  39   Loss :  0.0780077518597
Iteration :  40   Loss :  0.0426853033827
Iteration :  41   Loss :  0.0229298427987
Iteration :  42   Loss :  0.0141674441196
Iteration :  43   Loss :  0.00920464047863
Iteration :  44   Loss :  6.16528875082
Iteration :  45   Loss :  7943.33764542
Iteration :  46   Loss :  1083.2745221
Iteration :  47   Loss :  379.437891581
Iteration :  48   Loss :  200.492182986
Iteration :  49   Loss :  105.938590558
Iteration :  50   Loss :  55.9771697942
Iteration :  51   Loss :  29.5779236034
Iteration :  52   Loss :  15.6287566703
Iteration :  53   Loss :  8.25812226194
Iteration :  54   Loss :  4.36360973884
Iteration :  55   Loss :  2.30585606551
Iteration :  56   Loss :  1.21850843098
Iteration :  57   Loss :  0.644793341734
Iteration :  58   Loss :  0.340900345285
Iteration :  59   Loss :  0.180678121561
Iteration :  60   Loss :  0.0960979653731
Iteration :  61   Loss :  0.0516660733196
Iteration :  62   Loss :  0.0284226111099
Iteration :  63   Loss :  0.0165425521575
Iteration :  64   Loss :  0.0102999036215
Iteration :  65   Loss :  0.0123324613385
Iteration :  66   Loss :  9974.00303938
Iteration :  67   Loss :  1297.9848202
Iteration :  68   Loss :  1898.49546754
Iteration :  69   Loss :  213.361873085
Iteration :  70   Loss :  112.738839873
Iteration :  71   Loss :  59.5703713702
Iteration :  72   Loss :  31.4765448109
Iteration :  73   Loss :  16.6319740912
Iteration :  74   Loss :  8.78821242392
Iteration :  75   Loss :  4.6436266185
Iteration :  76   Loss :  2.4536580529
Iteration :  77   Loss :  1.29649518528
Iteration :  78   Loss :  0.685078734641
Iteration :  79   Loss :  0.362096430367
Iteration :  80   Loss :  0.191406120895
Iteration :  81   Loss :  0.101356995537
Iteration :  82   Loss :  0.0542481511143
Iteration :  83   Loss :  0.0292547174989
Iteration :  84   Loss :  0.0167739749588
Iteration :  85   Loss :  0.0105621395768
Iteration :  86   Loss :  0.00669976650579
Iteration :  87   Loss :  0.00511488734459
Iteration :  88   Loss :  0.0050397516077
[ -4.14919591e-04  -2.52480790e-04  -3.47070309e-05 ...,   2.01187240e-04
   1.71549860e-04   1.64862844e-05]
CROSS VALIDATION 5
Iteration :  0   Loss :  1985.57739074
Iteration :  1   Loss :  359.083467725
Iteration :  2   Loss :  189.737055565
Iteration :  3   Loss :  100.255660564
Iteration :  4   Loss :  52.974351506
Iteration :  5   Loss :  27.9912567271
Iteration :  6   Loss :  14.790384591
Iteration :  7   Loss :  7.81522437371
Iteration :  8   Loss :  4.12948060298
Iteration :  9   Loss :  2.18206159607
Iteration :  10   Loss :  1.15297112542
Iteration :  11   Loss :  0.609318216082
Iteration :  12   Loss :  0.321976789345
Iteration :  13   Loss :  0.170306056333
Iteration :  14   Loss :  0.0903056877312
Iteration :  15   Loss :  0.0485011164304
Iteration :  16   Loss :  0.0262803390041
Iteration :  17   Loss :  0.0153353664067
Iteration :  18   Loss :  0.00888235810701
Iteration :  19   Loss :  0.00643836600467
Iteration :  20   Loss :  0.00487151211607
Iteration :  21   Loss :  0.00534866439469
Iteration :  22   Loss :  0.0209037785007
Iteration :  23   Loss :  0.0057044289297
Iteration :  24   Loss :  0.00372471133915
Iteration :  25   Loss :  0.00579080290768
Iteration :  26   Loss :  0.00493986605466
Iteration :  27   Loss :  0.00537313411809
Iteration :  28   Loss :  0.00884795516684
Iteration :  29   Loss :  9810.30397088
Iteration :  30   Loss :  4855.48730137
Iteration :  31   Loss :  876.948147776
Iteration :  32   Loss :  1813.41257779
Iteration :  33   Loss :  248.313058722
Iteration :  34   Loss :  131.206788546
Iteration :  35   Loss :  69.3286992203
Iteration :  36   Loss :  36.6327732645
Iteration :  37   Loss :  19.3564871711
Iteration :  38   Loss :  10.227825038
Iteration :  39   Loss :  5.40430730446
Iteration :  40   Loss :  2.85559611787
Iteration :  41   Loss :  1.5088765771
Iteration :  42   Loss :  0.797308022636
Iteration :  43   Loss :  0.421331000839
Iteration :  44   Loss :  0.222645023876
Iteration :  45   Loss :  0.117937557801
Iteration :  46   Loss :  0.0626693801016
Iteration :  47   Loss :  0.0334643499685
Iteration :  48   Loss :  0.0193131451721
Iteration :  49   Loss :  0.0108965934035
Iteration :  50   Loss :  0.007885113124
Iteration :  51   Loss :  0.011574225587
Iteration :  52   Loss :  9025.92524248
Iteration :  53   Loss :  6321.74425388
Iteration :  54   Loss :  531.900310152
Iteration :  55   Loss :  1515.82182966
Iteration :  56   Loss :  212.789090941
Iteration :  57   Loss :  154.040335348
Iteration :  58   Loss :  81.3937769187
Iteration :  59   Loss :  43.0078713222
Iteration :  60   Loss :  22.7250419589
Iteration :  61   Loss :  12.0077445397
Iteration :  62   Loss :  6.34480363957
Iteration :  63   Loss :  3.35254744067
Iteration :  64   Loss :  1.77146133757
Iteration :  65   Loss :  0.936027108244
Iteration :  66   Loss :  0.494590777992
Iteration :  67   Loss :  0.261381142052
Iteration :  68   Loss :  0.138295391496
Iteration :  69   Loss :  0.0732456085286
Iteration :  70   Loss :  0.0389668259813
Iteration :  71   Loss :  0.0214820023863
Iteration :  72   Loss :  0.0121531445175
Iteration :  73   Loss :  0.007953175578
Iteration :  74   Loss :  0.00641370505699
Iteration :  75   Loss :  0.00478910582546
Iteration :  76   Loss :  0.0133426162716
Iteration :  77   Loss :  0.00845458087773
Iteration :  78   Loss :  0.00400671730088
Iteration :  79   Loss :  0.0051315378842
Iteration :  80   Loss :  0.0100409056592
Iteration :  81   Loss :  9425.8279305
Iteration :  82   Loss :  6304.34072213
Iteration :  83   Loss :  513.859390653
Iteration :  84   Loss :  1523.08889522
Iteration :  85   Loss :  214.152606059
Iteration :  86   Loss :  153.944078986
Iteration :  87   Loss :  81.3429157671
Iteration :  88   Loss :  42.9809966649
Iteration :  89   Loss :  22.7108415882
Iteration :  90   Loss :  12.0002411685
Iteration :  91   Loss :  6.34083891362
Iteration :  92   Loss :  3.35045250872
Iteration :  93   Loss :  1.77035439098
Iteration :  94   Loss :  0.935442205388
Iteration :  95   Loss :  0.494281657366
Iteration :  96   Loss :  0.261216190773
Iteration :  97   Loss :  0.138210149297
Iteration :  98   Loss :  0.0731925015478
Iteration :  99   Loss :  0.0389505303634
[-0.00281467 -0.00057447 -0.00160573 ...,  0.00081377  0.00087515
  0.00021286]
CROSS VALIDATION 6
Iteration :  0   Loss :  2348.80424702
Iteration :  1   Loss :  359.083467725
Iteration :  2   Loss :  189.737055565
Iteration :  3   Loss :  100.255660564
Iteration :  4   Loss :  52.974351506
Iteration :  5   Loss :  27.9912567271
Iteration :  6   Loss :  14.790384591
Iteration :  7   Loss :  7.81522437371
Iteration :  8   Loss :  4.12948060298
Iteration :  9   Loss :  2.18206159608
Iteration :  10   Loss :  1.15297112758
Iteration :  11   Loss :  0.609318724204
Iteration :  12   Loss :  0.322005491323
Iteration :  13   Loss :  0.170669414601
Iteration :  14   Loss :  0.0903200630583
Iteration :  15   Loss :  0.0488381130764
Iteration :  16   Loss :  0.026213062762
Iteration :  17   Loss :  0.0155785616854
Iteration :  18   Loss :  0.00886732682292
Iteration :  19   Loss :  0.00650983650961
Iteration :  20   Loss :  0.00484017950624
Iteration :  21   Loss :  0.00591015472572
Iteration :  22   Loss :  0.0308196657298
Iteration :  23   Loss :  0.00697296278518
Iteration :  24   Loss :  0.00614713536825
Iteration :  25   Loss :  0.0125084238757
Iteration :  26   Loss :  9046.01921493
Iteration :  27   Loss :  3535.45170807
Iteration :  28   Loss :  1154.36814992
Iteration :  29   Loss :  247.802184577
Iteration :  30   Loss :  130.936846416
Iteration :  31   Loss :  69.18606379
Iteration :  32   Loss :  36.5574057556
Iteration :  33   Loss :  19.3166636559
Iteration :  34   Loss :  10.2067918479
Iteration :  35   Loss :  5.39335519715
Iteration :  36   Loss :  2.84977245154
Iteration :  37   Loss :  1.50619102087
Iteration :  38   Loss :  0.796322160967
Iteration :  39   Loss :  0.421141511279
Iteration :  40   Loss :  0.222945098371
Iteration :  41   Loss :  0.118248328832
Iteration :  42   Loss :  0.0630838314098
Iteration :  43   Loss :  0.0342205398613
Iteration :  44   Loss :  0.0190881659939
Iteration :  45   Loss :  0.0111305119291
Iteration :  46   Loss :  0.0077737673705
Iteration :  47   Loss :  0.00499843066993
Iteration :  48   Loss :  0.0131984444391
Iteration :  49   Loss :  9582.63169808
Iteration :  50   Loss :  2113.91869401
Iteration :  51   Loss :  4598.08341682
Iteration :  52   Loss :  312.310375387
Iteration :  53   Loss :  165.022498595
Iteration :  54   Loss :  87.1966709683
Iteration :  55   Loss :  46.0740777329
Iteration :  56   Loss :  24.3452028084
Iteration :  57   Loss :  12.8638255814
Iteration :  58   Loss :  6.79715054715
Iteration :  59   Loss :  3.5915642099
Iteration :  60   Loss :  1.89775603534
Iteration :  61   Loss :  1.00276030811
Iteration :  62   Loss :  0.529853474517
Iteration :  63   Loss :  0.280044249699
Iteration :  64   Loss :  0.148099053733
Iteration :  65   Loss :  0.0785174126415
Iteration :  66   Loss :  0.0420635123577
Iteration :  67   Loss :  0.0226437717824
Iteration :  68   Loss :  0.0132551024573
Iteration :  69   Loss :  0.00801877327207
Iteration :  70   Loss :  0.00584453072844
Iteration :  71   Loss :  0.0052033674867
Iteration :  72   Loss :  0.00810948444025
Iteration :  73   Loss :  0.0071830690453
Iteration :  74   Loss :  0.00505999691362
Iteration :  75   Loss :  0.00395322828322
Iteration :  76   Loss :  0.00994907205074
Iteration :  77   Loss :  12.7309361159
Iteration :  78   Loss :  1219.35793881
Iteration :  79   Loss :  1303.96689485
Iteration :  80   Loss :  397.210416435
Iteration :  81   Loss :  184.064432062
Iteration :  82   Loss :  97.2582881495
Iteration :  83   Loss :  51.3905620319
Iteration :  84   Loss :  27.1543938962
Iteration :  85   Loss :  14.3481817692
Iteration :  86   Loss :  7.58149286912
Iteration :  87   Loss :  4.0062135668
Iteration :  88   Loss :  2.11684575575
Iteration :  89   Loss :  1.11895696204
Iteration :  90   Loss :  0.591644204315
Iteration :  91   Loss :  0.312757076504
Iteration :  92   Loss :  0.165747234279
Iteration :  93   Loss :  0.0877231576499
Iteration :  94   Loss :  0.0471479092096
Iteration :  95   Loss :  0.0261924325997
Iteration :  96   Loss :  0.0146201227495
Iteration :  97   Loss :  0.00960513514785
Iteration :  98   Loss :  0.00711461217967
Iteration :  99   Loss :  0.0130402892448
[ -5.07670455e-04  -2.26081191e-04  -8.34391664e-06 ...,   2.13906288e-04
   1.93723166e-04   4.05614361e-05]
CROSS VALIDATION 7
Iteration :  0   Loss :  2348.80424702
Iteration :  1   Loss :  359.083467725
Iteration :  2   Loss :  189.737055565
Iteration :  3   Loss :  100.255660564
Iteration :  4   Loss :  52.974351506
Iteration :  5   Loss :  27.9912567271
Iteration :  6   Loss :  14.790384591
Iteration :  7   Loss :  7.81522437371
Iteration :  8   Loss :  4.12948060298
Iteration :  9   Loss :  2.18206159608
Iteration :  10   Loss :  1.15297112758
Iteration :  11   Loss :  0.609318724204
Iteration :  12   Loss :  0.322005491323
Iteration :  13   Loss :  0.170669415602
Iteration :  14   Loss :  0.0903203295357
Iteration :  15   Loss :  0.0488500086987
Iteration :  16   Loss :  0.0262937915472
Iteration :  17   Loss :  0.0155849745141
Iteration :  18   Loss :  0.00882868787128
Iteration :  19   Loss :  0.00652075029652
Iteration :  20   Loss :  0.00475757231716
Iteration :  21   Loss :  0.00561614352512
Iteration :  22   Loss :  0.0161251192789
Iteration :  23   Loss :  0.0058289310792
Iteration :  24   Loss :  0.00371688455464
Iteration :  25   Loss :  0.00560079659191
Iteration :  26   Loss :  0.0485173195122
Iteration :  27   Loss :  5854.45893768
Iteration :  28   Loss :  467.6805245
Iteration :  29   Loss :  247.118939298
Iteration :  30   Loss :  130.575824651
Iteration :  31   Loss :  68.9953025522
Iteration :  32   Loss :  36.4566089244
Iteration :  33   Loss :  19.2634032333
Iteration :  34   Loss :  10.1786401719
Iteration :  35   Loss :  5.37831838881
Iteration :  36   Loss :  2.84186470582
Iteration :  37   Loss :  1.50166602229
Iteration :  38   Loss :  0.794015771079
Iteration :  39   Loss :  0.419458563826
Iteration :  40   Loss :  0.222249036397
Iteration :  41   Loss :  0.118001231955
Iteration :  42   Loss :  0.0634994654603
Iteration :  43   Loss :  0.034368327184
Iteration :  44   Loss :  0.0201642794858
Iteration :  45   Loss :  0.0119396632913
Iteration :  46   Loss :  0.00825506081743
Iteration :  47   Loss :  0.00602457039458
Iteration :  48   Loss :  0.0281087648436
Iteration :  49   Loss :  7612.83885256
Iteration :  50   Loss :  3291.44154298
Iteration :  51   Loss :  1082.59480899
Iteration :  52   Loss :  250.155322645
Iteration :  53   Loss :  132.180227213
Iteration :  54   Loss :  69.8430570314
Iteration :  55   Loss :  36.9045561377
Iteration :  56   Loss :  19.5000952571
Iteration :  57   Loss :  10.303709045
Iteration :  58   Loss :  5.44444421757
Iteration :  59   Loss :  2.8768071303
Iteration :  60   Loss :  1.52011817033
Iteration :  61   Loss :  0.803280537377
Iteration :  62   Loss :  0.424576516042
Iteration :  63   Loss :  0.224457910164
Iteration :  64   Loss :  0.118819218358
Iteration :  65   Loss :  0.0628730398574
Iteration :  66   Loss :  0.0337479889073
Iteration :  67   Loss :  0.018964812096
Iteration :  68   Loss :  0.0113212035922
Iteration :  69   Loss :  0.00686834007226
Iteration :  70   Loss :  0.00609475274377
Iteration :  71   Loss :  0.0112708334468
Iteration :  72   Loss :  0.0691838497049
Iteration :  73   Loss :  3925.20487243
Iteration :  74   Loss :  3685.87850503
Iteration :  75   Loss :  593.61275572
Iteration :  76   Loss :  694.489754233
Iteration :  77   Loss :  371.852120043
Iteration :  78   Loss :  139.506735147
Iteration :  79   Loss :  73.7143297793
Iteration :  80   Loss :  38.9501081017
Iteration :  81   Loss :  20.580949971
Iteration :  82   Loss :  10.8748338403
Iteration :  83   Loss :  5.74640430295
Iteration :  84   Loss :  3.03641168205
Iteration :  85   Loss :  1.60470072499
Iteration :  86   Loss :  0.847900110959
Iteration :  87   Loss :  0.44848414493
Iteration :  88   Loss :  0.237303300817
Iteration :  89   Loss :  0.12603834099
Iteration :  90   Loss :  0.0672251876673
Iteration :  91   Loss :  0.0358515421202
Iteration :  92   Loss :  0.0202886486345
Iteration :  93   Loss :  0.0120855157813
Iteration :  94   Loss :  0.0113663471305
Iteration :  95   Loss :  0.0308751386899
Iteration :  96   Loss :  6283.58124655
Iteration :  97   Loss :  916.595575655
Iteration :  98   Loss :  538.783644511
Iteration :  99   Loss :  261.358769755
[-0.20399798 -0.03241877 -0.06448168 ...,  0.12487107  0.0314206
 -0.00300733]
CROSS VALIDATION 8
Iteration :  0   Loss :  2348.80424702
Iteration :  1   Loss :  359.083467725
Iteration :  2   Loss :  189.737055565
Iteration :  3   Loss :  100.255660564
Iteration :  4   Loss :  52.974351506
Iteration :  5   Loss :  27.9912567271
Iteration :  6   Loss :  14.790384591
Iteration :  7   Loss :  7.81522437371
Iteration :  8   Loss :  4.12948060298
Iteration :  9   Loss :  2.18206159608
Iteration :  10   Loss :  1.15297112758
Iteration :  11   Loss :  0.609318724204
Iteration :  12   Loss :  0.322005491323
Iteration :  13   Loss :  0.170669415225
Iteration :  14   Loss :  0.0903201307844
Iteration :  15   Loss :  0.0488348521062
Iteration :  16   Loss :  0.0261045951966
Iteration :  17   Loss :  0.0156627802174
Iteration :  18   Loss :  0.008735165563
Iteration :  19   Loss :  0.00664755186849
Iteration :  20   Loss :  0.00470250787502
Iteration :  21   Loss :  0.00591613207237
Iteration :  22   Loss :  0.0263727303552
Iteration :  23   Loss :  0.00554512133916
Iteration :  24   Loss :  0.00481854271948
Iteration :  25   Loss :  0.807572318219
Iteration :  26   Loss :  4339.91059098
Iteration :  27   Loss :  1923.11545883
Iteration :  28   Loss :  1666.40607435
Iteration :  29   Loss :  814.499603609
Iteration :  30   Loss :  191.704725889
Iteration :  31   Loss :  101.295363049
Iteration :  32   Loss :  53.5237226292
Iteration :  33   Loss :  28.2815402197
Iteration :  34   Loss :  14.9437573932
Iteration :  35   Loss :  7.89618171719
Iteration :  36   Loss :  4.17230950234
Iteration :  37   Loss :  2.20462686077
Iteration :  38   Loss :  1.16502423923
Iteration :  39   Loss :  0.616306532101
Iteration :  40   Loss :  0.325781406818
Iteration :  41   Loss :  0.172804986816
Iteration :  42   Loss :  0.0918289456491
Iteration :  43   Loss :  0.049188569943
Iteration :  44   Loss :  0.0268308916371
Iteration :  45   Loss :  0.0152803986307
Iteration :  46   Loss :  0.00961926749554
Iteration :  47   Loss :  0.00804501865592
Iteration :  48   Loss :  0.179406365555
Iteration :  49   Loss :  6141.73902445
Iteration :  50   Loss :  808.907596982
Iteration :  51   Loss :  543.606615907
Iteration :  52   Loss :  262.370705347
Iteration :  53   Loss :  138.634745326
Iteration :  54   Loss :  73.2535767905
Iteration :  55   Loss :  38.7066496207
Iteration :  56   Loss :  20.4523081398
Iteration :  57   Loss :  10.8068487596
Iteration :  58   Loss :  5.71026010072
Iteration :  59   Loss :  3.01730001438
Iteration :  60   Loss :  1.59478803303
Iteration :  61   Loss :  0.842635690049
Iteration :  62   Loss :  0.445723195953
Iteration :  63   Loss :  0.235411165883
Iteration :  64   Loss :  0.125400730968
Iteration :  65   Loss :  0.066212761797
Iteration :  66   Loss :  0.036484982019
Iteration :  67   Loss :  0.0201334903946
Iteration :  68   Loss :  0.0123218459589
Iteration :  69   Loss :  0.0076716028947
Iteration :  70   Loss :  0.0112854472354
Iteration :  71   Loss :  7953.71372951
Iteration :  72   Loss :  3828.89876699
Iteration :  73   Loss :  1854.47551985
Iteration :  74   Loss :  323.068149986
Iteration :  75   Loss :  170.706827339
Iteration :  76   Loss :  90.2002283459
Iteration :  77   Loss :  47.6611352955
Iteration :  78   Loss :  25.1837923175
Iteration :  79   Loss :  13.3069300922
Iteration :  80   Loss :  7.03128370208
Iteration :  81   Loss :  3.71527844212
Iteration :  82   Loss :  1.96312583507
Iteration :  83   Loss :  1.0373115181
Iteration :  84   Loss :  0.548335705291
Iteration :  85   Loss :  0.290271999258
Iteration :  86   Loss :  0.153469522412
Iteration :  87   Loss :  0.0815279055678
Iteration :  88   Loss :  0.0435952089006
Iteration :  89   Loss :  0.0239676647006
Iteration :  90   Loss :  0.0137886833862
Iteration :  91   Loss :  0.00822233865515
Iteration :  92   Loss :  0.00580353880595
Iteration :  93   Loss :  0.00533238075707
Iteration :  94   Loss :  0.00650638019158
Iteration :  95   Loss :  0.00429408018732
Iteration :  96   Loss :  0.00458146073665
Iteration :  97   Loss :  0.00674339186612
Iteration :  98   Loss :  0.00654928243059
Iteration :  99   Loss :  0.00513663538841
[ -3.60142415e-04  -2.29855641e-04  -3.88227726e-05 ...,   2.02647702e-04
   9.52694854e-05   1.52814592e-05]
CROSS VALIDATION 9
Iteration :  0   Loss :  2724.17469863
Iteration :  1   Loss :  1836.0240545
Iteration :  2   Loss :  748.952769368
Iteration :  3   Loss :  205.954811732
Iteration :  4   Loss :  108.825003292
Iteration :  5   Loss :  57.5023289914
Iteration :  6   Loss :  30.3838064728
Iteration :  7   Loss :  16.0545792139
Iteration :  8   Loss :  8.48312122995
Iteration :  9   Loss :  4.48241868805
Iteration :  10   Loss :  2.36847801494
Iteration :  11   Loss :  1.2515291961
Iteration :  12   Loss :  0.661486337539
Iteration :  13   Loss :  0.349451282489
Iteration :  14   Loss :  0.184973948384
Iteration :  15   Loss :  0.0977047560523
Iteration :  16   Loss :  0.0520111292205
Iteration :  17   Loss :  0.0278816558639
Iteration :  18   Loss :  0.0158342132395
Iteration :  19   Loss :  0.00912377973284
Iteration :  20   Loss :  0.00665737966786
Iteration :  21   Loss :  0.00468072809125
Iteration :  22   Loss :  0.0448249868712
Iteration :  23   Loss :  0.00873690021137
Iteration :  24   Loss :  8378.47698834
Iteration :  25   Loss :  2820.21592706
Iteration :  26   Loss :  375.878631133
Iteration :  27   Loss :  198.61149602
Iteration :  28   Loss :  104.944849438
Iteration :  29   Loss :  55.4520843169
Iteration :  30   Loss :  29.3004723104
Iteration :  31   Loss :  15.4821534373
Iteration :  32   Loss :  8.1806563985
Iteration :  33   Loss :  4.32263184736
Iteration :  34   Loss :  2.28425629949
Iteration :  35   Loss :  1.20702645183
Iteration :  36   Loss :  0.63831972245
Iteration :  37   Loss :  0.337344803944
Iteration :  38   Loss :  0.17954616565
Iteration :  39   Loss :  0.0949957345967
Iteration :  40   Loss :  0.0511250084351
Iteration :  41   Loss :  0.0279707710248
Iteration :  42   Loss :  0.0158131941867
Iteration :  43   Loss :  0.0100981778482
Iteration :  44   Loss :  0.00648662041144
Iteration :  45   Loss :  0.00528197125944
Iteration :  46   Loss :  0.922677701229
Iteration :  47   Loss :  6667.73651133
Iteration :  48   Loss :  669.460437266
Iteration :  49   Loss :  1129.24624941
Iteration :  50   Loss :  347.867257466
Iteration :  51   Loss :  183.8104928
Iteration :  52   Loss :  97.1241085165
Iteration :  53   Loss :  51.3196625036
Iteration :  54   Loss :  27.1169311071
Iteration :  55   Loss :  14.3283863688
Iteration :  56   Loss :  7.57101366387
Iteration :  57   Loss :  4.00046777343
Iteration :  58   Loss :  2.11381796282
Iteration :  59   Loss :  1.11694363493
Iteration :  60   Loss :  0.590382480937
Iteration :  61   Loss :  0.311993026234
Iteration :  62   Loss :  0.165210981968
Iteration :  63   Loss :  0.0877411942514
Iteration :  64   Loss :  0.0472453959997
Iteration :  65   Loss :  0.0250257548927
Iteration :  66   Loss :  0.0156918589195
Iteration :  67   Loss :  6.45198621111
Iteration :  68   Loss :  7823.61047963
Iteration :  69   Loss :  606.895425537
Iteration :  70   Loss :  329.087697845
Iteration :  71   Loss :  173.887512351
Iteration :  72   Loss :  91.8808977468
Iteration :  73   Loss :  48.5493791467
Iteration :  74   Loss :  25.6530565616
Iteration :  75   Loss :  13.5550464907
Iteration :  76   Loss :  7.16233411704
Iteration :  77   Loss :  3.78471630003
Iteration :  78   Loss :  1.99973777461
Iteration :  79   Loss :  1.05680495797
Iteration :  80   Loss :  0.558357858402
Iteration :  81   Loss :  0.295242141169
Iteration :  82   Loss :  0.156162505039
Iteration :  83   Loss :  0.0837189917995
Iteration :  84   Loss :  0.044199372113
Iteration :  85   Loss :  0.0244661087034
Iteration :  86   Loss :  0.0144407879082
Iteration :  87   Loss :  0.00908623328769
Iteration :  88   Loss :  0.142987213187
Iteration :  89   Loss :  6006.1218262
Iteration :  90   Loss :  814.995289889
Iteration :  91   Loss :  537.575358165
Iteration :  92   Loss :  262.527794672
Iteration :  93   Loss :  138.717750167
Iteration :  94   Loss :  73.297435936
Iteration :  95   Loss :  38.7298244696
Iteration :  96   Loss :  20.464553559
Iteration :  97   Loss :  10.8133191437
Iteration :  98   Loss :  5.71367807102
Iteration :  99   Loss :  3.01906534575
[-0.02217615 -0.00359407 -0.00709001 ...,  0.01343098  0.00329971
 -0.00034494]
CROSS VALIDATION 10
Iteration :  0   Loss :  2352.67735773
Iteration :  1   Loss :  359.483425906
Iteration :  2   Loss :  189.948390517
Iteration :  3   Loss :  100.367328394
Iteration :  4   Loss :  53.0333559634
Iteration :  5   Loss :  28.0224341364
Iteration :  6   Loss :  14.8068521056
Iteration :  7   Loss :  7.82391441153
Iteration :  8   Loss :  4.13407899335
Iteration :  9   Loss :  2.18450677777
Iteration :  10   Loss :  1.15425369516
Iteration :  11   Loss :  0.609983515516
Iteration :  12   Loss :  0.322370936663
Iteration :  13   Loss :  0.170892223738
Iteration :  14   Loss :  0.0903706385263
Iteration :  15   Loss :  0.0485951903792
Iteration :  16   Loss :  0.0263191362183
Iteration :  17   Loss :  0.0155239206905
Iteration :  18   Loss :  0.00896154072142
Iteration :  19   Loss :  0.00632656249952
Iteration :  20   Loss :  0.0049612268119
Iteration :  21   Loss :  0.00558313336444
Iteration :  22   Loss :  0.154661587897
Iteration :  23   Loss :  1957.36730213
Iteration :  24   Loss :  1334.02278084
Iteration :  25   Loss :  1022.23427784
Iteration :  26   Loss :  145.328928221
Iteration :  27   Loss :  76.7907336524
Iteration :  28   Loss :  40.5756572147
Iteration :  29   Loss :  21.4398779657
Iteration :  30   Loss :  11.3286733657
Iteration :  31   Loss :  5.98598744041
Iteration :  32   Loss :  3.16295160784
Iteration :  33   Loss :  1.67128029813
Iteration :  34   Loss :  0.883092233313
Iteration :  35   Loss :  0.466625507441
Iteration :  36   Loss :  0.246702250453
Iteration :  37   Loss :  0.13110464835
Iteration :  38   Loss :  0.0696826882872
Iteration :  39   Loss :  0.0378836823947
Iteration :  40   Loss :  0.0217370161714
Iteration :  41   Loss :  0.0124813179722
Iteration :  42   Loss :  0.0084078540382
Iteration :  43   Loss :  0.0065387198557
Iteration :  44   Loss :  0.00501131825468
Iteration :  45   Loss :  0.00829012677155
Iteration :  46   Loss :  0.0590879230261
Iteration :  47   Loss :  6090.21299887
Iteration :  48   Loss :  638.076486981
Iteration :  49   Loss :  388.413052705
Iteration :  50   Loss :  205.234591918
Iteration :  51   Loss :  108.444444456
Iteration :  52   Loss :  57.3012445093
Iteration :  53   Loss :  30.2775549158
Iteration :  54   Loss :  15.998436675
Iteration :  55   Loss :  8.45345592649
Iteration :  56   Loss :  4.46674375461
Iteration :  57   Loss :  2.36019446728
Iteration :  58   Loss :  1.24711152333
Iteration :  59   Loss :  0.659038982306
Iteration :  60   Loss :  0.348793611734
Iteration :  61   Loss :  0.184277145471
Iteration :  62   Loss :  0.0981244569351
Iteration :  63   Loss :  0.0518455828641
Iteration :  64   Loss :  0.0292540555847
Iteration :  65   Loss :  0.0160140101686
Iteration :  66   Loss :  0.0110144013915
Iteration :  67   Loss :  0.00666098553247
Iteration :  68   Loss :  0.0231580978332
Iteration :  69   Loss :  8568.15317048
Iteration :  70   Loss :  2428.19257788
Iteration :  71   Loss :  2052.41771217
Iteration :  72   Loss :  256.443077516
Iteration :  73   Loss :  177.357721087
Iteration :  74   Loss :  312.763871922
Iteration :  75   Loss :  134.297665276
Iteration :  76   Loss :  70.9618956843
Iteration :  77   Loss :  37.4957422288
Iteration :  78   Loss :  19.8124736062
Iteration :  79   Loss :  10.4687649067
Iteration :  80   Loss :  5.53161815386
Iteration :  81   Loss :  2.9228662705
Iteration :  82   Loss :  1.54442525416
Iteration :  83   Loss :  0.816108784242
Iteration :  84   Loss :  0.431290595281
Iteration :  85   Loss :  0.227920677854
Iteration :  86   Loss :  0.120610295447
Iteration :  87   Loss :  0.0638681120821
Iteration :  88   Loss :  0.0340540760964
Iteration :  89   Loss :  0.018401215268
Iteration :  90   Loss :  0.0121730084456
Iteration :  91   Loss :  0.0212954965517
Iteration :  92   Loss :  4313.04577868
Iteration :  93   Loss :  2164.95497865
Iteration :  94   Loss :  543.252603787
Iteration :  95   Loss :  280.275775445
Iteration :  96   Loss :  148.09565229
Iteration :  97   Loss :  78.2526502414
Iteration :  98   Loss :  41.3481231565
Iteration :  99   Loss :  21.8480432712
[-0.03183199 -0.02717522 -0.01396514 ...,  0.02053557  0.0180777
  0.00162784]
CROSS VALIDATION 11
Iteration :  0   Loss :  2352.67735773
Iteration :  1   Loss :  359.483425906
Iteration :  2   Loss :  189.948390517
Iteration :  3   Loss :  100.367328394
Iteration :  4   Loss :  53.0333559634
Iteration :  5   Loss :  28.0224341364
Iteration :  6   Loss :  14.8068521056
Iteration :  7   Loss :  7.82391441153
Iteration :  8   Loss :  4.13407899335
Iteration :  9   Loss :  2.18450677777
Iteration :  10   Loss :  1.15425369516
Iteration :  11   Loss :  0.609983515517
Iteration :  12   Loss :  0.322370937658
Iteration :  13   Loss :  0.170892515425
Iteration :  14   Loss :  0.090391641583
Iteration :  15   Loss :  0.0488449884781
Iteration :  16   Loss :  0.0263146347108
Iteration :  17   Loss :  0.0154977203061
Iteration :  18   Loss :  0.00885775011175
Iteration :  19   Loss :  0.00644634421844
Iteration :  20   Loss :  0.00478089230322
Iteration :  21   Loss :  0.00568387556896
Iteration :  22   Loss :  0.0155503913225
Iteration :  23   Loss :  0.00553649215961
Iteration :  24   Loss :  0.00611372652228
Iteration :  25   Loss :  0.0158215274441
Iteration :  26   Loss :  13322.7793314
Iteration :  27   Loss :  5695.62421863
Iteration :  28   Loss :  412.847657497
Iteration :  29   Loss :  218.145656849
Iteration :  30   Loss :  115.266555966
Iteration :  31   Loss :  60.9060020879
Iteration :  32   Loss :  32.1823960244
Iteration :  33   Loss :  17.0051523802
Iteration :  34   Loss :  8.9857473599
Iteration :  35   Loss :  4.74806701547
Iteration :  36   Loss :  2.50910286047
Iteration :  37   Loss :  1.32638524795
Iteration :  38   Loss :  0.701103839914
Iteration :  39   Loss :  0.370995343076
Iteration :  40   Loss :  0.196562604687
Iteration :  41   Loss :  0.104832284687
Iteration :  42   Loss :  0.0561677614405
Iteration :  43   Loss :  0.0309975636272
Iteration :  44   Loss :  0.0168973140748
Iteration :  45   Loss :  0.0110721577983
Iteration :  46   Loss :  0.00653354257345
Iteration :  47   Loss :  0.00549407797866
Iteration :  48   Loss :  0.0294530083881
Iteration :  49   Loss :  6058.9547378
Iteration :  50   Loss :  470.339736242
Iteration :  51   Loss :  248.524047167
Iteration :  52   Loss :  131.318273284
Iteration :  53   Loss :  69.3876069333
Iteration :  54   Loss :  36.6638996655
Iteration :  55   Loss :  19.3729341318
Iteration :  56   Loss :  10.2365154943
Iteration :  57   Loss :  5.40890000052
Iteration :  58   Loss :  2.85805673149
Iteration :  59   Loss :  1.51053147913
Iteration :  60   Loss :  0.798192082595
Iteration :  61   Loss :  0.422255836748
Iteration :  62   Loss :  0.223336845879
Iteration :  63   Loss :  0.118786013705
Iteration :  64   Loss :  0.0638244580279
Iteration :  65   Loss :  0.0344410401597
Iteration :  66   Loss :  0.0202152379197
Iteration :  67   Loss :  0.0115841658509
Iteration :  68   Loss :  0.00869165642171
Iteration :  69   Loss :  0.00674724093745
Iteration :  70   Loss :  0.0830404573825
Iteration :  71   Loss :  6313.67696169
Iteration :  72   Loss :  860.314881892
Iteration :  73   Loss :  548.092999023
Iteration :  74   Loss :  261.847063489
Iteration :  75   Loss :  138.358056831
Iteration :  76   Loss :  73.1073766308
Iteration :  77   Loss :  38.6293985348
Iteration :  78   Loss :  20.4114892358
Iteration :  79   Loss :  10.7852803665
Iteration :  80   Loss :  5.69886508882
Iteration :  81   Loss :  3.01132026349
Iteration :  82   Loss :  1.59172083767
Iteration :  83   Loss :  0.840865629859
Iteration :  84   Loss :  0.444875563181
Iteration :  85   Loss :  0.234937912564
Iteration :  86   Loss :  0.125129990004
Iteration :  87   Loss :  0.0660706997409
Iteration :  88   Loss :  0.0361629087494
Iteration :  89   Loss :  0.0200115853304
Iteration :  90   Loss :  0.0121846596672
Iteration :  91   Loss :  0.0075818540427
Iteration :  92   Loss :  0.0168730178287
Iteration :  93   Loss :  9058.30725971
Iteration :  94   Loss :  6241.51151672
Iteration :  95   Loss :  1553.90357283
Iteration :  96   Loss :  356.934222883
Iteration :  97   Loss :  681.200909797
Iteration :  98   Loss :  228.429969161
Iteration :  99   Loss :  120.70071069
[-0.14516116 -0.02378941 -0.0185756  ...,  0.03512116  0.02909741
  0.00878345]
CROSS VALIDATION 12
Iteration :  0   Loss :  2168.23911175
Iteration :  1   Loss :  814.064387296
Iteration :  2   Loss :  197.688253097
Iteration :  3   Loss :  104.457014688
Iteration :  4   Loss :  55.1943160334
Iteration :  5   Loss :  29.1642694509
Iteration :  6   Loss :  15.4101848475
Iteration :  7   Loss :  8.14262800009
Iteration :  8   Loss :  4.30250457111
Iteration :  9   Loss :  2.27341167795
Iteration :  10   Loss :  1.20125398161
Iteration :  11   Loss :  0.634736385467
Iteration :  12   Loss :  0.335454884787
Iteration :  13   Loss :  0.177344245116
Iteration :  14   Loss :  0.0940487578817
Iteration :  15   Loss :  0.0504842933591
Iteration :  16   Loss :  0.0273412588644
Iteration :  17   Loss :  0.0159937679042
Iteration :  18   Loss :  0.0091939046718
Iteration :  19   Loss :  0.0066436422088
Iteration :  20   Loss :  0.00488739575755
Iteration :  21   Loss :  0.00514022296394
Iteration :  22   Loss :  0.0286393873508
Iteration :  23   Loss :  2012.25061657
Iteration :  24   Loss :  645.529049499
Iteration :  25   Loss :  234.951859168
Iteration :  26   Loss :  124.146829261
Iteration :  27   Loss :  65.5982688116
Iteration :  28   Loss :  34.661641354
Iteration :  29   Loss :  18.3149556097
Iteration :  30   Loss :  9.67748744379
Iteration :  31   Loss :  5.11351297926
Iteration :  32   Loss :  2.70194253839
Iteration :  33   Loss :  1.42768650639
Iteration :  34   Loss :  0.754379019332
Iteration :  35   Loss :  0.398608571058
Iteration :  36   Loss :  0.210641663286
Iteration :  37   Loss :  0.111564586676
Iteration :  38   Loss :  0.0591086837206
Iteration :  39   Loss :  0.0327975047883
Iteration :  40   Loss :  0.0177356224816
Iteration :  41   Loss :  0.0109046126734
Iteration :  42   Loss :  0.00703323126614
Iteration :  43   Loss :  0.00608785913987
Iteration :  44   Loss :  0.00445727439948
Iteration :  45   Loss :  0.00672755587644
Iteration :  46   Loss :  3.59073429944
Iteration :  47   Loss :  5885.09646348
Iteration :  48   Loss :  919.88564894
Iteration :  49   Loss :  308.019420663
Iteration :  50   Loss :  162.755189782
Iteration :  51   Loss :  85.9986435688
Iteration :  52   Loss :  45.4411281698
Iteration :  53   Loss :  24.0114228544
Iteration :  54   Loss :  12.6871802492
Iteration :  55   Loss :  6.70447314346
Iteration :  56   Loss :  3.54286384983
Iteration :  57   Loss :  1.87255670784
Iteration :  58   Loss :  0.989719617225
Iteration :  59   Loss :  0.523365404698
Iteration :  60   Loss :  0.277026455335
Iteration :  61   Loss :  0.14701169876
Iteration :  62   Loss :  0.0782032658653
Iteration :  63   Loss :  0.0425401070177
Iteration :  64   Loss :  0.0234930695363
Iteration :  65   Loss :  0.0139547857547
Iteration :  66   Loss :  0.00902905801722
Iteration :  67   Loss :  0.00710651649749
Iteration :  68   Loss :  0.00532345383523
Iteration :  69   Loss :  0.00554250353153
Iteration :  70   Loss :  1.30409443184
Iteration :  71   Loss :  6004.14662291
Iteration :  72   Loss :  946.454893779
Iteration :  73   Loss :  307.751308696
Iteration :  74   Loss :  162.613521387
Iteration :  75   Loss :  85.9237856715
Iteration :  76   Loss :  45.4015251316
Iteration :  77   Loss :  23.990300068
Iteration :  78   Loss :  12.6763346557
Iteration :  79   Loss :  6.69913312505
Iteration :  80   Loss :  3.53942358451
Iteration :  81   Loss :  1.87140091246
Iteration :  82   Loss :  0.988376990523
Iteration :  83   Loss :  0.52342857763
Iteration :  84   Loss :  0.27629829078
Iteration :  85   Loss :  0.147351805885
Iteration :  86   Loss :  0.0779633592663
Iteration :  87   Loss :  0.0424855540306
Iteration :  88   Loss :  0.0236446670622
Iteration :  89   Loss :  0.0137492389063
Iteration :  90   Loss :  0.00930334259532
Iteration :  91   Loss :  0.0478111548058
Iteration :  92   Loss :  8500.87280643
Iteration :  93   Loss :  758.293436807
Iteration :  94   Loss :  293.188835898
Iteration :  95   Loss :  154.918818179
Iteration :  96   Loss :  81.8579607665
Iteration :  97   Loss :  43.2531426758
Iteration :  98   Loss :  22.8546451039
Iteration :  99   Loss :  12.0762901383
[-0.03974319 -0.02708206 -0.00993909 ...,  0.02755069  0.00588144
 -0.00110115]
CROSS VALIDATION 13
Iteration :  0   Loss :  3225.97472367
Iteration :  1   Loss :  455.211409245
Iteration :  2   Loss :  240.530350776
Iteration :  3   Loss :  127.094463077
Iteration :  4   Loss :  67.1557767766
Iteration :  5   Loss :  35.4846170773
Iteration :  6   Loss :  18.7498106218
Iteration :  7   Loss :  9.9072944334
Iteration :  8   Loss :  5.23514715975
Iteration :  9   Loss :  2.76612466981
Iteration :  10   Loss :  1.46177211856
Iteration :  11   Loss :  0.772334305183
Iteration :  12   Loss :  0.408312658049
Iteration :  13   Loss :  0.215804695828
Iteration :  14   Loss :  0.114328029351
Iteration :  15   Loss :  0.0606672302904
Iteration :  16   Loss :  0.0326838144442
Iteration :  17   Loss :  0.0186347404896
Iteration :  18   Loss :  0.0104886923399
Iteration :  19   Loss :  0.00673838797869
Iteration :  20   Loss :  0.00526025839614
Iteration :  21   Loss :  0.0137976936564
Iteration :  22   Loss :  7660.01050583
Iteration :  23   Loss :  2210.84327603
Iteration :  24   Loss :  826.080134162
Iteration :  25   Loss :  1702.21685724
Iteration :  26   Loss :  2357.92023919
Iteration :  27   Loss :  247.073469649
Iteration :  28   Loss :  175.679501899
Iteration :  29   Loss :  92.8277529029
Iteration :  30   Loss :  49.0494998895
Iteration :  31   Loss :  25.9173939277
Iteration :  32   Loss :  13.6945597716
Iteration :  33   Loss :  7.23610436532
Iteration :  34   Loss :  3.82350416947
Iteration :  35   Loss :  2.02031139912
Iteration :  36   Loss :  1.06751768794
Iteration :  37   Loss :  0.564073802327
Iteration :  38   Loss :  0.298174979726
Iteration :  39   Loss :  0.157622489493
Iteration :  40   Loss :  0.0838108275664
Iteration :  41   Loss :  0.0448136227591
Iteration :  42   Loss :  0.024733205483
Iteration :  43   Loss :  0.0148199595029
Iteration :  44   Loss :  0.0852200538015
Iteration :  45   Loss :  6391.14326631
Iteration :  46   Loss :  788.781679534
Iteration :  47   Loss :  281.884849567
Iteration :  48   Loss :  148.945827495
Iteration :  49   Loss :  78.7020693596
Iteration :  50   Loss :  41.5855131961
Iteration :  51   Loss :  21.9736339128
Iteration :  52   Loss :  11.6106583864
Iteration :  53   Loss :  6.13517905665
Iteration :  54   Loss :  3.24170364899
Iteration :  55   Loss :  1.71304778898
Iteration :  56   Loss :  0.905114151352
Iteration :  57   Loss :  0.47845114508
Iteration :  58   Loss :  0.252835632972
Iteration :  59   Loss :  0.13444994987
Iteration :  60   Loss :  0.0712219239729
Iteration :  61   Loss :  0.0394898550806
Iteration :  62   Loss :  0.0219021599006
Iteration :  63   Loss :  0.0130675875625
Iteration :  64   Loss :  0.00834070820787
Iteration :  65   Loss :  0.00645196732868
Iteration :  66   Loss :  0.0224855691775
Iteration :  67   Loss :  7575.34101093
Iteration :  68   Loss :  417.171201698
Iteration :  69   Loss :  2404.89426486
Iteration :  70   Loss :  294.823847554
Iteration :  71   Loss :  155.782746278
Iteration :  72   Loss :  82.3144540011
Iteration :  73   Loss :  43.4943503012
Iteration :  74   Loss :  22.9820938628
Iteration :  75   Loss :  12.1435688696
Iteration :  76   Loss :  6.41657221358
Iteration :  77   Loss :  3.39046942578
Iteration :  78   Loss :  1.79149903477
Iteration :  79   Loss :  0.946614878359
Iteration :  80   Loss :  0.500185134289
Iteration :  81   Loss :  0.264340141964
Iteration :  82   Loss :  0.140326620232
Iteration :  83   Loss :  0.0745880193551
Iteration :  84   Loss :  0.0398775246258
Iteration :  85   Loss :  0.0218526609567
Iteration :  86   Loss :  0.0129186076744
Iteration :  87   Loss :  0.00788912676794
Iteration :  88   Loss :  0.00588720732083
Iteration :  89   Loss :  0.0163610616126
Iteration :  90   Loss :  4859.02658268
Iteration :  91   Loss :  1649.91389985
Iteration :  92   Loss :  300.349531209
Iteration :  93   Loss :  158.702476761
Iteration :  94   Loss :  83.8572180508
Iteration :  95   Loss :  44.3095354448
Iteration :  96   Loss :  23.4128316795
Iteration :  97   Loss :  12.3711675546
Iteration :  98   Loss :  6.53683367985
Iteration :  99   Loss :  3.45401469745
[-0.01933718 -0.00932689 -0.0103989  ...,  0.01411867 -0.00366328
  0.0009088 ]
CROSS VALIDATION 14
Iteration :  0   Loss :  1174.46304686
Iteration :  1   Loss :  327.419709025
Iteration :  2   Loss :  173.006159036
Iteration :  3   Loss :  91.4151782535
Iteration :  4   Loss :  48.3031058646
Iteration :  5   Loss :  25.5230048307
Iteration :  6   Loss :  13.486167482
Iteration :  7   Loss :  7.12599141669
Iteration :  8   Loss :  3.7653212999
Iteration :  9   Loss :  1.98956810119
Iteration :  10   Loss :  1.05128087762
Iteration :  11   Loss :  0.555524027942
Iteration :  12   Loss :  0.293575837783
Iteration :  13   Loss :  0.155242542121
Iteration :  14   Loss :  0.0828241708366
Iteration :  15   Loss :  0.0451139095282
Iteration :  16   Loss :  0.0242739733407
Iteration :  17   Loss :  0.0141982654898
Iteration :  18   Loss :  0.00858039561851
Iteration :  19   Loss :  0.00653233088731
Iteration :  20   Loss :  0.00440750921497
Iteration :  21   Loss :  0.0132829614771
Iteration :  22   Loss :  9319.69894397
Iteration :  23   Loss :  6949.66350149
Iteration :  24   Loss :  1590.98043241
Iteration :  25   Loss :  354.512296266
Iteration :  26   Loss :  591.892222008
Iteration :  27   Loss :  228.152914877
Iteration :  28   Loss :  120.55431725
Iteration :  29   Loss :  63.7000119653
Iteration :  30   Loss :  33.658616439
Iteration :  31   Loss :  17.7849646434
Iteration :  32   Loss :  9.39744412668
Iteration :  33   Loss :  4.96554015624
Iteration :  34   Loss :  2.62375479022
Iteration :  35   Loss :  1.38637267701
Iteration :  36   Loss :  0.732549095948
Iteration :  37   Loss :  0.387073539065
Iteration :  38   Loss :  0.204526971422
Iteration :  39   Loss :  0.108083915279
Iteration :  40   Loss :  0.0573243778258
Iteration :  41   Loss :  0.0310331454103
Iteration :  42   Loss :  0.016987985923
Iteration :  43   Loss :  0.0106620156457
Iteration :  44   Loss :  0.00744598315247
Iteration :  45   Loss :  0.0406936662323
Iteration :  46   Loss :  7609.77331532
Iteration :  47   Loss :  765.053036603
Iteration :  48   Loss :  879.01236195
Iteration :  49   Loss :  272.406381656
Iteration :  50   Loss :  143.93752266
Iteration :  51   Loss :  76.0555252175
Iteration :  52   Loss :  40.1871785009
Iteration :  53   Loss :  21.2346086789
Iteration :  54   Loss :  11.2202105888
Iteration :  55   Loss :  5.92867650915
Iteration :  56   Loss :  3.13266893636
Iteration :  57   Loss :  1.65527960569
Iteration :  58   Loss :  0.874661864606
Iteration :  59   Loss :  0.462506840027
Iteration :  60   Loss :  0.244378667989
Iteration :  61   Loss :  0.129678152278
Iteration :  62   Loss :  0.0694421470421
Iteration :  63   Loss :  0.0371493264615
Iteration :  64   Loss :  0.0209171439999
Iteration :  65   Loss :  0.012505003174
Iteration :  66   Loss :  0.00852560653213
Iteration :  67   Loss :  0.00708365399884
Iteration :  68   Loss :  35.6034729103
Iteration :  69   Loss :  1178.42437313
Iteration :  70   Loss :  331.09414621
Iteration :  71   Loss :  174.94770454
Iteration :  72   Loss :  92.441076576
Iteration :  73   Loss :  48.8451829707
Iteration :  74   Loss :  25.8094343749
Iteration :  75   Loss :  13.6375147403
Iteration :  76   Loss :  7.20596219157
Iteration :  77   Loss :  3.80757726722
Iteration :  78   Loss :  2.01189575601
Iteration :  79   Loss :  1.06307309245
Iteration :  80   Loss :  0.561770196308
Iteration :  81   Loss :  0.296865511576
Iteration :  82   Loss :  0.157045329439
Iteration :  83   Loss :  0.0842235889209
Iteration :  84   Loss :  0.0452038729951
Iteration :  85   Loss :  0.0247659432357
Iteration :  86   Loss :  0.0141457829592
Iteration :  87   Loss :  0.008927322524
Iteration :  88   Loss :  0.00610816368589
Iteration :  89   Loss :  0.00494822535994
Iteration :  90   Loss :  0.0120980882624
Iteration :  91   Loss :  9620.74048217
Iteration :  92   Loss :  6902.85347838
Iteration :  93   Loss :  1589.83527111
Iteration :  94   Loss :  357.482037486
Iteration :  95   Loss :  582.505397869
Iteration :  96   Loss :  228.074620744
Iteration :  97   Loss :  120.512947208
Iteration :  98   Loss :  63.67815234
Iteration :  99   Loss :  33.6470659742
[-0.07623477 -0.01243295 -0.01002579 ...,  0.01867944  0.01577554
  0.00476253]
CROSS VALIDATION 15
Iteration :  0   Loss :  1174.46304686
Iteration :  1   Loss :  327.419709025
Iteration :  2   Loss :  173.006159036
Iteration :  3   Loss :  91.4151782535
Iteration :  4   Loss :  48.3031058646
Iteration :  5   Loss :  25.5230048307
Iteration :  6   Loss :  13.486167482
Iteration :  7   Loss :  7.12599141669
Iteration :  8   Loss :  3.7653212999
Iteration :  9   Loss :  1.98956810359
Iteration :  10   Loss :  1.05128142134
Iteration :  11   Loss :  0.555551877017
Iteration :  12   Loss :  0.293810943066
Iteration :  13   Loss :  0.155262821176
Iteration :  14   Loss :  0.0828877984628
Iteration :  15   Loss :  0.0450402363885
Iteration :  16   Loss :  0.0244303452028
Iteration :  17   Loss :  0.0140492870422
Iteration :  18   Loss :  0.00872720445385
Iteration :  19   Loss :  0.00643473988311
Iteration :  20   Loss :  0.00453982026111
Iteration :  21   Loss :  0.0109415157596
Iteration :  22   Loss :  9653.60321341
Iteration :  23   Loss :  6111.80230959
Iteration :  24   Loss :  1592.63943013
Iteration :  25   Loss :  356.490248155
Iteration :  26   Loss :  700.774730081
Iteration :  27   Loss :  227.867939957
Iteration :  28   Loss :  120.403738604
Iteration :  29   Loss :  63.6204473196
Iteration :  30   Loss :  33.6165750671
Iteration :  31   Loss :  17.7627502926
Iteration :  32   Loss :  9.3857062276
Iteration :  33   Loss :  4.95933793695
Iteration :  34   Loss :  2.6204775833
Iteration :  35   Loss :  1.38464102505
Iteration :  36   Loss :  0.731634103897
Iteration :  37   Loss :  0.386590106576
Iteration :  38   Loss :  0.204275889619
Iteration :  39   Loss :  0.10805872697
Iteration :  40   Loss :  0.0573516294901
Iteration :  41   Loss :  0.0311475087417
Iteration :  42   Loss :  0.0168961045897
Iteration :  43   Loss :  0.0107096417614
Iteration :  44   Loss :  0.00780895295106
Iteration :  45   Loss :  0.117808935853
Iteration :  46   Loss :  10316.1419987
Iteration :  47   Loss :  2597.4222534
Iteration :  48   Loss :  555.804220336
Iteration :  49   Loss :  294.985199296
Iteration :  50   Loss :  155.868003348
Iteration :  51   Loss :  82.359503208
Iteration :  52   Loss :  43.5181539698
Iteration :  53   Loss :  22.9946715457
Iteration :  54   Loss :  12.1502148245
Iteration :  55   Loss :  6.42008388711
Iteration :  56   Loss :  3.39232496819
Iteration :  57   Loss :  1.79247950838
Iteration :  58   Loss :  0.947135382861
Iteration :  59   Loss :  0.500527434157
Iteration :  60   Loss :  0.264540704628
Iteration :  61   Loss :  0.139852525332
Iteration :  62   Loss :  0.07408912804
Iteration :  63   Loss :  0.0393673455082
Iteration :  64   Loss :  0.0219762091065
Iteration :  65   Loss :  0.0125847732691
Iteration :  66   Loss :  0.00849142267996
Iteration :  67   Loss :  0.00646445661573
Iteration :  68   Loss :  11251.5849126
Iteration :  69   Loss :  4061.66176116
Iteration :  70   Loss :  3521.58718771
Iteration :  71   Loss :  390.33437198
Iteration :  72   Loss :  206.249802851
Iteration :  73   Loss :  108.980874424
Iteration :  74   Loss :  57.5846901474
Iteration :  75   Loss :  30.4273255001
Iteration :  76   Loss :  16.0775743469
Iteration :  77   Loss :  8.49527168865
Iteration :  78   Loss :  4.48883892246
Iteration :  79   Loss :  2.37187256446
Iteration :  80   Loss :  1.25337719377
Iteration :  81   Loss :  0.66284201171
Iteration :  82   Loss :  0.350041247286
Iteration :  83   Loss :  0.18555005244
Iteration :  84   Loss :  0.0980987993188
Iteration :  85   Loss :  0.0525549559928
Iteration :  86   Loss :  0.0283200339428
Iteration :  87   Loss :  0.0162449429391
Iteration :  88   Loss :  0.00989806912372
Iteration :  89   Loss :  0.00722699389828
Iteration :  90   Loss :  0.0179720431983
Iteration :  91   Loss :  5020.79256534
Iteration :  92   Loss :  2262.68991317
Iteration :  93   Loss :  438.08348626
Iteration :  94   Loss :  231.480082615
Iteration :  95   Loss :  122.31236814
Iteration :  96   Loss :  64.6289530871
Iteration :  97   Loss :  34.1494620754
Iteration :  98   Loss :  18.044323857
Iteration :  99   Loss :  9.53448762197
[-0.01252497 -0.00223873 -0.00778058 ...,  0.01374412  0.01065941
  0.00239757]
CROSS VALIDATION 16
Iteration :  0   Loss :  2822.86218467
Iteration :  1   Loss :  332.021734113
Iteration :  2   Loss :  175.437835145
Iteration :  3   Loss :  92.7000579725
Iteration :  4   Loss :  48.9820268302
Iteration :  5   Loss :  25.881741663
Iteration :  6   Loss :  13.6757218733
Iteration :  7   Loss :  7.22617611284
Iteration :  8   Loss :  3.81850058517
Iteration :  9   Loss :  2.01756389314
Iteration :  10   Loss :  1.06627256789
Iteration :  11   Loss :  0.563347077676
Iteration :  12   Loss :  0.297946701068
Iteration :  13   Loss :  0.157625443079
Iteration :  14   Loss :  0.0839221880582
Iteration :  15   Loss :  0.0450532356655
Iteration :  16   Loss :  0.0250191403057
Iteration :  17   Loss :  0.0139851621187
Iteration :  18   Loss :  0.00910962827578
Iteration :  19   Loss :  0.00593142690452
Iteration :  20   Loss :  0.00545195270355
Iteration :  21   Loss :  0.00763204603341
Iteration :  22   Loss :  9430.15141979
Iteration :  23   Loss :  6184.51525142
Iteration :  24   Loss :  1775.48101697
Iteration :  25   Loss :  198.583215289
Iteration :  26   Loss :  104.929906114
Iteration :  27   Loss :  55.4441894255
Iteration :  28   Loss :  29.2963412919
Iteration :  29   Loss :  15.4800175805
Iteration :  30   Loss :  8.17953494605
Iteration :  31   Loss :  4.32207957503
Iteration :  32   Loss :  2.28373736332
Iteration :  33   Loss :  1.2068231804
Iteration :  34   Loss :  0.637657615601
Iteration :  35   Loss :  0.33724857096
Iteration :  36   Loss :  0.178166678374
Iteration :  37   Loss :  0.0946745537807
Iteration :  38   Loss :  0.0504014778981
Iteration :  39   Loss :  0.0275776289392
Iteration :  40   Loss :  0.0156967728614
Iteration :  41   Loss :  0.0102360819809
Iteration :  42   Loss :  0.00658982620682
Iteration :  43   Loss :  0.00567449433159
Iteration :  44   Loss :  0.0049964652466
Iteration :  45   Loss :  0.0194420080833
Iteration :  46   Loss :  4228.40593753
Iteration :  47   Loss :  652.724088737
Iteration :  48   Loss :  632.927425356
Iteration :  49   Loss :  250.346130799
Iteration :  50   Loss :  132.281048835
Iteration :  51   Loss :  69.8963304324
Iteration :  52   Loss :  36.9327054098
Iteration :  53   Loss :  19.5149691042
Iteration :  54   Loss :  10.3115657224
Iteration :  55   Loss :  5.44855526489
Iteration :  56   Loss :  2.87897658188
Iteration :  57   Loss :  1.52124196808
Iteration :  58   Loss :  0.804049312022
Iteration :  59   Loss :  0.425212735283
Iteration :  60   Loss :  0.224787350212
Iteration :  61   Loss :  0.119216996721
Iteration :  62   Loss :  0.0631884415019
Iteration :  63   Loss :  0.0343327075976
Iteration :  64   Loss :  0.0186603439569
Iteration :  65   Loss :  0.0115050108385
Iteration :  66   Loss :  0.00722871608884
Iteration :  67   Loss :  0.00736574734077
Iteration :  68   Loss :  0.00487955092783
Iteration :  69   Loss :  0.00718302619477
Iteration :  70   Loss :  0.00754616798369
Iteration :  71   Loss :  0.00502008376786
Iteration :  72   Loss :  0.00608140396932
Iteration :  73   Loss :  0.00455665709292
Iteration :  74   Loss :  0.00563197868028
Iteration :  75   Loss :  0.0079672599569
Iteration :  76   Loss :  0.0151444981558
Iteration :  77   Loss :  10129.8607794
Iteration :  78   Loss :  764.877310074
Iteration :  79   Loss :  523.060138436
Iteration :  80   Loss :  211.987322945
Iteration :  81   Loss :  112.012537718
Iteration :  82   Loss :  59.1865986693
Iteration :  83   Loss :  31.2737621466
Iteration :  84   Loss :  16.5248252272
Iteration :  85   Loss :  8.73159575458
Iteration :  86   Loss :  4.61371078806
Iteration :  87   Loss :  2.43785074929
Iteration :  88   Loss :  1.28814286416
Iteration :  89   Loss :  0.680670404066
Iteration :  90   Loss :  0.359785140723
Iteration :  91   Loss :  0.190219501197
Iteration :  92   Loss :  0.100927438694
Iteration :  93   Loss :  0.0536518350498
Iteration :  94   Loss :  0.0293317019252
Iteration :  95   Loss :  0.0165459915266
Iteration :  96   Loss :  0.0103233855598
Iteration :  97   Loss :  0.00722135344643
Iteration :  98   Loss :  0.00471083255482
Iteration :  99   Loss :  0.00570222099072
[ -3.60089933e-04  -2.51035448e-04   2.57750570e-05 ...,   1.99968312e-04
   1.60477961e-04   2.49318979e-05]
CROSS VALIDATION 17
Iteration :  0   Loss :  1175.20205303
Iteration :  1   Loss :  327.515279909
Iteration :  2   Loss :  173.056657985
Iteration :  3   Loss :  91.4418615262
Iteration :  4   Loss :  48.317205109
Iteration :  5   Loss :  25.5304547675
Iteration :  6   Loss :  13.4901039736
Iteration :  7   Loss :  7.12807142983
Iteration :  8   Loss :  3.76642036346
Iteration :  9   Loss :  1.99014882653
Iteration :  10   Loss :  1.05158754372
Iteration :  11   Loss :  0.555715955512
Iteration :  12   Loss :  0.293897259103
Iteration :  13   Loss :  0.155314416841
Iteration :  14   Loss :  0.0830159140602
Iteration :  15   Loss :  0.0451460040251
Iteration :  16   Loss :  0.0244746010056
Iteration :  17   Loss :  0.0141736613554
Iteration :  18   Loss :  0.00872193783915
Iteration :  19   Loss :  0.00650822368591
Iteration :  20   Loss :  0.00458821548741
Iteration :  21   Loss :  0.0116168138491
Iteration :  22   Loss :  9621.85881319
Iteration :  23   Loss :  6917.69074936
Iteration :  24   Loss :  1594.19491633
Iteration :  25   Loss :  354.900251094
Iteration :  26   Loss :  692.446289657
Iteration :  27   Loss :  228.147086346
Iteration :  28   Loss :  120.551237497
Iteration :  29   Loss :  63.6983846464
Iteration :  30   Loss :  33.6577565757
Iteration :  31   Loss :  17.784510298
Iteration :  32   Loss :  9.39720405392
Iteration :  33   Loss :  4.96541330356
Iteration :  34   Loss :  2.62368776221
Iteration :  35   Loss :  1.3863372599
Iteration :  36   Loss :  0.73253038189
Iteration :  37   Loss :  0.387063697295
Iteration :  38   Loss :  0.20452643137
Iteration :  39   Loss :  0.108195516673
Iteration :  40   Loss :  0.0574176630783
Iteration :  41   Loss :  0.0312129520107
Iteration :  42   Loss :  0.0170190915492
Iteration :  43   Loss :  0.0108085943807
Iteration :  44   Loss :  0.00774964338689
Iteration :  45   Loss :  0.245059734388
Iteration :  46   Loss :  5508.60756878
Iteration :  47   Loss :  788.012313483
Iteration :  48   Loss :  346.262131535
Iteration :  49   Loss :  601.459010848
Iteration :  50   Loss :  193.589476798
Iteration :  51   Loss :  102.291251526
Iteration :  52   Loss :  54.0499427545
Iteration :  53   Loss :  28.5595910518
Iteration :  54   Loss :  15.0906772418
Iteration :  55   Loss :  7.9738025381
Iteration :  56   Loss :  4.21329844233
Iteration :  57   Loss :  2.22627581751
Iteration :  58   Loss :  1.17634771984
Iteration :  59   Loss :  0.621573466748
Iteration :  60   Loss :  0.328436397663
Iteration :  61   Loss :  0.173607602415
Iteration :  62   Loss :  0.0920032842212
Iteration :  63   Loss :  0.048728031407
Iteration :  64   Loss :  0.0263284923914
Iteration :  65   Loss :  0.0152606106707
Iteration :  66   Loss :  0.00982386198518
Iteration :  67   Loss :  0.00778041478989
Iteration :  68   Loss :  0.331691764513
Iteration :  69   Loss :  2184.97413287
Iteration :  70   Loss :  449.790609181
Iteration :  71   Loss :  390.736781777
Iteration :  72   Loss :  656.851865036
Iteration :  73   Loss :  149.871244477
Iteration :  74   Loss :  79.1908600558
Iteration :  75   Loss :  41.8438663017
Iteration :  76   Loss :  22.1099902924
Iteration :  77   Loss :  11.6827558361
Iteration :  78   Loss :  6.17308760357
Iteration :  79   Loss :  3.26193749171
Iteration :  80   Loss :  1.72357887823
Iteration :  81   Loss :  0.910878736177
Iteration :  82   Loss :  0.481244023094
Iteration :  83   Loss :  0.25451774886
Iteration :  84   Loss :  0.134866578044
Iteration :  85   Loss :  0.0716787966695
Iteration :  86   Loss :  0.0383112107922
Iteration :  87   Loss :  0.021062842016
Iteration :  88   Loss :  0.0127842216968
Iteration :  89   Loss :  0.00800625069214
Iteration :  90   Loss :  0.00636963219888
Iteration :  91   Loss :  0.00491852508304
Iteration :  92   Loss :  0.00988517599279
Iteration :  93   Loss :  10.596237274
Iteration :  94   Loss :  3685.17908212
Iteration :  95   Loss :  1770.7861795
Iteration :  96   Loss :  1353.75811808
Iteration :  97   Loss :  485.75842279
Iteration :  98   Loss :  366.620952957
Iteration :  99   Loss :  373.378617705
[-0.11291164 -0.02567967  0.0320685  ...,  0.06470009  0.00052789
 -0.00339968]
CROSS VALIDATION 18
Iteration :  0   Loss :  3624.85791083
Iteration :  1   Loss :  1780.76163774
Iteration :  2   Loss :  189.874779428
Iteration :  3   Loss :  100.328432838
Iteration :  4   Loss :  53.0128038381
Iteration :  5   Loss :  28.0115744988
Iteration :  6   Loss :  14.8011093377
Iteration :  7   Loss :  7.82079699356
Iteration :  8   Loss :  4.13245144125
Iteration :  9   Loss :  2.18355686613
Iteration :  10   Loss :  1.1537767286
Iteration :  11   Loss :  0.609691238845
Iteration :  12   Loss :  0.322217033981
Iteration :  13   Loss :  0.170315292586
Iteration :  14   Loss :  0.0903603497834
Iteration :  15   Loss :  0.0485222933042
Iteration :  16   Loss :  0.0263921612936
Iteration :  17   Loss :  0.0149749253838
Iteration :  18   Loss :  0.00901335717448
Iteration :  19   Loss :  0.00638059620507
Iteration :  20   Loss :  0.00478557303998
Iteration :  21   Loss :  0.0050377777632
Iteration :  22   Loss :  0.0207895104784
Iteration :  23   Loss :  5421.25476781
Iteration :  24   Loss :  1966.93870165
Iteration :  25   Loss :  477.82430917
Iteration :  26   Loss :  722.92557425
Iteration :  27   Loss :  142.798539131
Iteration :  28   Loss :  75.4536947224
Iteration :  29   Loss :  39.8691757139
Iteration :  30   Loss :  21.0665795519
Iteration :  31   Loss :  11.131462169
Iteration :  32   Loss :  5.88178534947
Iteration :  33   Loss :  3.10794629851
Iteration :  34   Loss :  1.64221261705
Iteration :  35   Loss :  0.86779871641
Iteration :  36   Loss :  0.458544931129
Iteration :  37   Loss :  0.242360220717
Iteration :  38   Loss :  0.128127024965
Iteration :  39   Loss :  0.0681864444231
Iteration :  40   Loss :  0.0361004624581
Iteration :  41   Loss :  0.0202429714523
Iteration :  42   Loss :  0.0117671542
Iteration :  43   Loss :  0.00777850782403
Iteration :  44   Loss :  0.00535256622867
Iteration :  45   Loss :  0.00959634576161
Iteration :  46   Loss :  0.00636276941329
Iteration :  47   Loss :  0.00368481165941
Iteration :  48   Loss :  0.00566598286881
Iteration :  49   Loss :  0.00586299004495
Iteration :  50   Loss :  0.00468529404418
Iteration :  51   Loss :  0.00523227447146
Iteration :  52   Loss :  0.216722305548
Iteration :  53   Loss :  694.105162137
Iteration :  54   Loss :  1791.34488025
Iteration :  55   Loss :  251.195657656
Iteration :  56   Loss :  132.729933317
Iteration :  57   Loss :  70.1335441765
Iteration :  58   Loss :  37.0580533785
Iteration :  59   Loss :  19.5812427439
Iteration :  60   Loss :  10.3465959683
Iteration :  61   Loss :  5.46728871618
Iteration :  62   Loss :  2.88897142668
Iteration :  63   Loss :  1.52690929195
Iteration :  64   Loss :  0.806737234731
Iteration :  65   Loss :  0.426746080934
Iteration :  66   Loss :  0.225400247314
Iteration :  67   Loss :  0.119829165077
Iteration :  68   Loss :  0.0632191005439
Iteration :  69   Loss :  0.0346299039275
Iteration :  70   Loss :  0.0183846344299
Iteration :  71   Loss :  0.0122253480037
Iteration :  72   Loss :  0.00750045204191
Iteration :  73   Loss :  0.00590265316383
Iteration :  74   Loss :  0.00450585383469
Iteration :  75   Loss :  0.0489806546079
Iteration :  76   Loss :  5440.32961059
Iteration :  77   Loss :  1974.45625959
Iteration :  78   Loss :  488.821357412
Iteration :  79   Loss :  728.045564548
Iteration :  80   Loss :  144.299595961
Iteration :  81   Loss :  76.2468420789
Iteration :  82   Loss :  40.2882689192
Iteration :  83   Loss :  21.2880241107
Iteration :  84   Loss :  11.2484350662
Iteration :  85   Loss :  5.9436008313
Iteration :  86   Loss :  3.14059930039
Iteration :  87   Loss :  1.65946836066
Iteration :  88   Loss :  0.876909500173
Iteration :  89   Loss :  0.46337790211
Iteration :  90   Loss :  0.244880682581
Iteration :  91   Loss :  0.129470361818
Iteration :  92   Loss :  0.0688307379526
Iteration :  93   Loss :  0.0365159799186
Iteration :  94   Loss :  0.0203949190694
Iteration :  95   Loss :  0.0119198219453
Iteration :  96   Loss :  0.00779364791283
Iteration :  97   Loss :  0.00537866206179
Iteration :  98   Loss :  0.00982053513645
Iteration :  99   Loss :  0.00650942713453
[ -4.29018757e-04  -3.47307689e-04  -3.72384633e-05 ...,   2.78930929e-04
   1.51143348e-04   5.20120316e-05]
CROSS VALIDATION 19
Iteration :  0   Loss :  3399.80879477
Iteration :  1   Loss :  363.60297359
Iteration :  2   Loss :  192.125129126
Iteration :  3   Loss :  101.517501018
Iteration :  4   Loss :  53.6410986936
Iteration :  5   Loss :  28.3435608658
Iteration :  6   Loss :  14.9765284907
Iteration :  7   Loss :  7.91348817984
Iteration :  8   Loss :  4.18147131543
Iteration :  9   Loss :  2.20982161371
Iteration :  10   Loss :  1.16758376971
Iteration :  11   Loss :  0.617483081337
Iteration :  12   Loss :  0.326126862591
Iteration :  13   Loss :  0.173149365682
Iteration :  14   Loss :  0.0916419146681
Iteration :  15   Loss :  0.0494190678954
Iteration :  16   Loss :  0.0266659525132
Iteration :  17   Loss :  0.0153945636205
Iteration :  18   Loss :  0.0091194121085
Iteration :  19   Loss :  0.00626527712933
Iteration :  20   Loss :  0.00567999284513
Iteration :  21   Loss :  0.00659961446815
Iteration :  22   Loss :  0.0212178546965
Iteration :  23   Loss :  1.45918037501
Iteration :  24   Loss :  2588.72761502
Iteration :  25   Loss :  1744.74717729
Iteration :  26   Loss :  319.02259458
Iteration :  27   Loss :  168.569185704
Iteration :  28   Loss :  89.0707142746
Iteration :  29   Loss :  47.0643083921
Iteration :  30   Loss :  24.8684333843
Iteration :  31   Loss :  13.1402967581
Iteration :  32   Loss :  6.94323587752
Iteration :  33   Loss :  3.66875462086
Iteration :  34   Loss :  1.9385428795
Iteration :  35   Loss :  1.02431191872
Iteration :  36   Loss :  0.541251535618
Iteration :  37   Loss :  0.286291833998
Iteration :  38   Loss :  0.152296019079
Iteration :  39   Loss :  0.0804104677574
Iteration :  40   Loss :  0.0438173444864
Iteration :  41   Loss :  0.023825538862
Iteration :  42   Loss :  0.0140165993534
Iteration :  43   Loss :  0.00914532738309
Iteration :  44   Loss :  0.00655158448007
Iteration :  45   Loss :  0.00936801818526
Iteration :  46   Loss :  6728.73485173
Iteration :  47   Loss :  1165.86761725
Iteration :  48   Loss :  2281.92912357
Iteration :  49   Loss :  240.164460158
Iteration :  50   Loss :  126.901129173
Iteration :  51   Loss :  67.0536205683
Iteration :  52   Loss :  35.4306384869
Iteration :  53   Loss :  18.7212880222
Iteration :  54   Loss :  9.89219049324
Iteration :  55   Loss :  5.22696048683
Iteration :  56   Loss :  2.76188736602
Iteration :  57   Loss :  1.45936129295
Iteration :  58   Loss :  0.771136874909
Iteration :  59   Loss :  0.407662768633
Iteration :  60   Loss :  0.215324107736
Iteration :  61   Loss :  0.11399124279
Iteration :  62   Loss :  0.0605909453629
Iteration :  63   Loss :  0.0327156269884
Iteration :  64   Loss :  0.0182576458134
Iteration :  65   Loss :  0.0111202542863
Iteration :  66   Loss :  0.00653494875734
Iteration :  67   Loss :  0.00532917153083
Iteration :  68   Loss :  0.00424938018731
Iteration :  69   Loss :  0.00613146111697
Iteration :  70   Loss :  0.00981154982731
Iteration :  71   Loss :  0.00446994070368
Iteration :  72   Loss :  0.00591509648637
Iteration :  73   Loss :  0.102881158304
Iteration :  74   Loss :  6419.33543856
Iteration :  75   Loss :  792.668872807
Iteration :  76   Loss :  282.779130292
Iteration :  77   Loss :  149.418406525
Iteration :  78   Loss :  78.951583822
Iteration :  79   Loss :  41.7174345048
Iteration :  80   Loss :  22.0431846634
Iteration :  81   Loss :  11.6474561745
Iteration :  82   Loss :  6.15442992508
Iteration :  83   Loss :  3.25195537423
Iteration :  84   Loss :  1.71830923172
Iteration :  85   Loss :  0.907941955469
Iteration :  86   Loss :  0.479753936068
Iteration :  87   Loss :  0.253626955785
Iteration :  88   Loss :  0.134714930486
Iteration :  89   Loss :  0.0714271205328
Iteration :  90   Loss :  0.0393920974642
Iteration :  91   Loss :  0.021705611238
Iteration :  92   Loss :  0.0132975435916
Iteration :  93   Loss :  0.00823000161406
Iteration :  94   Loss :  0.00721525720499
Iteration :  95   Loss :  0.0257582994845
Iteration :  96   Loss :  8149.90199193
Iteration :  97   Loss :  622.264582655
Iteration :  98   Loss :  634.049043598
Iteration :  99   Loss :  241.580907444
[-0.20140901 -0.08974001 -0.11986032 ...,  0.05021399  0.03405398
 -0.00581234]
Accuracy (Logistic Loss):	0.75
Hinge Loss
CROSS VALIDATION 0
Iteration :  0   Loss :  1614.87321296
Iteration :  1   Loss :  379.581090888
Iteration :  2   Loss :  200.567848443
Iteration :  3   Loss :  105.978571628
Iteration :  4   Loss :  55.9982954966
Iteration :  5   Loss :  29.5890862687
Iteration :  6   Loss :  15.6346549203
Iteration :  7   Loss :  8.26123633078
Iteration :  8   Loss :  4.36517633813
Iteration :  9   Loss :  2.30652697732
Iteration :  10   Loss :  1.21875184071
Iteration :  11   Loss :  0.6439794825
Iteration :  12   Loss :  0.340274008235
Iteration :  13   Loss :  0.179798275918
Iteration :  14   Loss :  0.0950040827123
Iteration :  15   Loss :  0.0501994565071
Iteration :  16   Loss :  0.0265250225218
Iteration :  17   Loss :  0.0140156262385
Iteration :  18   Loss :  0.108096021209
Iteration :  19   Loss :  1307.09120234
Iteration :  20   Loss :  2290.83719734
Iteration :  21   Loss :  960.705695426
Iteration :  22   Loss :  187.303124868
Iteration :  23   Loss :  98.9695895379
Iteration :  24   Loss :  52.2948010621
Iteration :  25   Loss :  27.6321871283
Iteration :  26   Loss :  14.6006438496
Iteration :  27   Loss :  7.71487250842
Iteration :  28   Loss :  4.07648172465
Iteration :  29   Loss :  2.15398287312
Iteration :  30   Loss :  1.13814866129
Iteration :  31   Loss :  0.601389357065
Iteration :  32   Loss :  0.317769700122
Iteration :  33   Loss :  0.167907165514
Iteration :  34   Loss :  0.0887209076897
Iteration :  35   Loss :  0.046879473173
Iteration :  36   Loss :  0.0247707678179
Iteration :  37   Loss :  0.0130886909933
Iteration :  38   Loss :  0.00691596777205
Iteration :  39   Loss :  0.00365434635508
Iteration :  40   Loss :  0.196420784792
Iteration :  41   Loss :  3770.44565271
Iteration :  42   Loss :  1695.45983617
Iteration :  43   Loss :  237.297843959
Iteration :  44   Loss :  125.386430319
Iteration :  45   Loss :  66.2532648669
Iteration :  46   Loss :  35.0077364381
Iteration :  47   Loss :  18.4978296991
Iteration :  48   Loss :  9.77411676369
Iteration :  49   Loss :  5.16457119911
Iteration :  50   Loss :  2.72892132512
Iteration :  51   Loss :  1.44194189829
Iteration :  52   Loss :  0.761911462565
Iteration :  53   Loss :  0.402588396575
Iteration :  54   Loss :  0.212724739055
Iteration :  55   Loss :  0.112402182951
Iteration :  56   Loss :  0.0593924843362
Iteration :  57   Loss :  0.0313825506143
Iteration :  58   Loss :  0.016582308251
Iteration :  59   Loss :  0.00876196936036
Iteration :  60   Loss :  0.0195507791994
Iteration :  61   Loss :  6413.46322787
Iteration :  62   Loss :  1086.17059572
Iteration :  63   Loss :  452.277828024
Iteration :  64   Loss :  1076.30445101
Iteration :  65   Loss :  196.215005744
Iteration :  66   Loss :  103.678561654
Iteration :  67   Loss :  54.7829871927
Iteration :  68   Loss :  28.9469263256
Iteration :  69   Loss :  15.2953423433
Iteration :  70   Loss :  8.08194606802
Iteration :  71   Loss :  4.27044068582
Iteration :  72   Loss :  2.25646935746
Iteration :  73   Loss :  1.19230176362
Iteration :  74   Loss :  0.630003456878
Iteration :  75   Loss :  0.332889179391
Iteration :  76   Loss :  0.175896186831
Iteration :  77   Loss :  0.0929422476225
Iteration :  78   Loss :  0.0491099980547
Iteration :  79   Loss :  0.025949360712
Iteration :  80   Loss :  0.0137114507846
Iteration :  81   Loss :  0.00724502945199
Iteration :  82   Loss :  1870.81473104
Iteration :  83   Loss :  1470.53903117
Iteration :  84   Loss :  293.16622465
Iteration :  85   Loss :  154.906870563
Iteration :  86   Loss :  81.851647734
Iteration :  87   Loss :  43.2498068835
Iteration :  88   Loss :  22.8528789248
Iteration :  89   Loss :  12.0752926495
Iteration :  90   Loss :  6.38049556253
Iteration :  91   Loss :  3.37140678948
Iteration :  92   Loss :  1.78142647836
Iteration :  93   Loss :  0.941292610464
Iteration :  94   Loss :  0.497372071919
Iteration :  95   Loss :  0.262807733934
Iteration :  96   Loss :  0.138865668008
Iteration :  97   Loss :  0.0733755946323
Iteration :  98   Loss :  0.226520567762
Iteration :  99   Loss :  1255.63067616
[-0.03780288 -0.03487231 -0.11328657 ...,  0.04899272 -0.0415524
  0.02375153]
CROSS VALIDATION 1
Iteration :  0   Loss :  1663.96700285
Iteration :  1   Loss :  1549.61258768
Iteration :  2   Loss :  440.295528241
Iteration :  3   Loss :  179.716373783
Iteration :  4   Loss :  94.9608062284
Iteration :  5   Loss :  50.1765895322
Iteration :  6   Loss :  26.5129397809
Iteration :  7   Loss :  14.0092418074
Iteration :  8   Loss :  7.40238003178
Iteration :  9   Loss :  3.9113630051
Iteration :  10   Loss :  2.0667353597
Iteration :  11   Loss :  1.09204771878
Iteration :  12   Loss :  0.577029959109
Iteration :  13   Loss :  0.304898373929
Iteration :  14   Loss :  0.161106051699
Iteration :  15   Loss :  0.0851272493183
Iteration :  16   Loss :  0.0449806105982
Iteration :  17   Loss :  0.0237674228404
Iteration :  18   Loss :  0.0125585309083
Iteration :  19   Loss :  1285.82884349
Iteration :  20   Loss :  740.998637249
Iteration :  21   Loss :  247.755914758
Iteration :  22   Loss :  130.912397785
Iteration :  23   Loss :  69.1731453138
Iteration :  24   Loss :  36.5505797277
Iteration :  25   Loss :  19.3130567126
Iteration :  26   Loss :  10.2048767041
Iteration :  27   Loss :  5.3921815741
Iteration :  28   Loss :  2.8491889683
Iteration :  29   Loss :  7352.23553937
Iteration :  30   Loss :  2411.19141295
Iteration :  31   Loss :  372.789270164
Iteration :  32   Loss :  196.979100472
Iteration :  33   Loss :  104.082303672
Iteration :  34   Loss :  54.9963214967
Iteration :  35   Loss :  29.0596505982
Iteration :  36   Loss :  15.3549050174
Iteration :  37   Loss :  8.11341854567
Iteration :  38   Loss :  4.28707051086
Iteration :  39   Loss :  2.26525643434
Iteration :  40   Loss :  1.19694479023
Iteration :  41   Loss :  0.632456797887
Iteration :  42   Loss :  0.334185506682
Iteration :  43   Loss :  0.17658115661
Iteration :  44   Loss :  0.0933041806008
Iteration :  45   Loss :  0.0493012407707
Iteration :  46   Loss :  0.0260504119523
Iteration :  47   Loss :  4535.0957285
Iteration :  48   Loss :  1687.02008945
Iteration :  49   Loss :  825.234246365
Iteration :  50   Loss :  616.728491618
Iteration :  51   Loss :  215.091076797
Iteration :  52   Loss :  113.652538359
Iteration :  53   Loss :  60.0531629104
Iteration :  54   Loss :  31.7316482994
Iteration :  55   Loss :  16.7667688927
Iteration :  56   Loss :  8.85943700275
Iteration :  57   Loss :  4.68126116057
Iteration :  58   Loss :  2.4735438659
Iteration :  59   Loss :  1.30700233263
Iteration :  60   Loss :  0.690610391452
Iteration :  61   Loss :  0.364913436552
Iteration :  62   Loss :  0.192817568088
Iteration :  63   Loss :  0.101883380658
Iteration :  64   Loss :  0.0538344267964
Iteration :  65   Loss :  0.0284457140093
Iteration :  66   Loss :  0.0150305054525
Iteration :  67   Loss :  0.00794200820851
Iteration :  68   Loss :  0.00419649855311
Iteration :  69   Loss :  2363.32393303
Iteration :  70   Loss :  581.192520054
Iteration :  71   Loss :  280.033150926
Iteration :  72   Loss :  147.967451283
Iteration :  73   Loss :  78.1849097753
Iteration :  74   Loss :  41.3123295939
Iteration :  75   Loss :  21.8291302168
Iteration :  76   Loss :  11.5343513839
Iteration :  77   Loss :  6.09466618802
Iteration :  78   Loss :  3.22037665641
Iteration :  79   Loss :  1.70162327012
Iteration :  80   Loss :  0.8991251839
Iteration :  81   Loss :  0.475091114771
Iteration :  82   Loss :  0.251034640533
Iteration :  83   Loss :  0.132644852299
Iteration :  84   Loss :  0.0700885614994
Iteration :  85   Loss :  0.0818876103544
Iteration :  86   Loss :  4131.78507792
Iteration :  87   Loss :  545.028641555
Iteration :  88   Loss :  287.989113791
Iteration :  89   Loss :  152.171323374
Iteration :  90   Loss :  80.4062047783
Iteration :  91   Loss :  42.486045488
Iteration :  92   Loss :  22.4493130373
Iteration :  93   Loss :  11.8620514114
Iteration :  94   Loss :  6.26782046532
Iteration :  95   Loss :  3.31187009927
Iteration :  96   Loss :  1.74996773042
Iteration :  97   Loss :  0.92467004011
Iteration :  98   Loss :  0.488588828361
Iteration :  99   Loss :  0.258166732828
[-0.00471991 -0.00187484 -0.00191879 ...,  0.00232249  0.00052483
  0.00047904]
CROSS VALIDATION 2
Iteration :  0   Loss :  2325.10428029
Iteration :  1   Loss :  339.429830117
Iteration :  2   Loss :  179.352218428
Iteration :  3   Loss :  94.7683892247
Iteration :  4   Loss :  50.0749178068
Iteration :  5   Loss :  26.4592171912
Iteration :  6   Loss :  13.9808551873
Iteration :  7   Loss :  7.3873807511
Iteration :  8   Loss :  3.90343749583
Iteration :  9   Loss :  2.06254757907
Iteration :  10   Loss :  1.08983492639
Iteration :  11   Loss :  0.57586073593
Iteration :  12   Loss :  7574.77201153
Iteration :  13   Loss :  1241.98124963
Iteration :  14   Loss :  244.509085556
Iteration :  15   Loss :  129.196797185
Iteration :  16   Loss :  68.2666346118
Iteration :  17   Loss :  36.0715861597
Iteration :  18   Loss :  19.0599600445
Iteration :  19   Loss :  10.0711422916
Iteration :  20   Loss :  5.32151729706
Iteration :  21   Loss :  2.81185048557
Iteration :  22   Loss :  1.48576105494
Iteration :  23   Loss :  3379.29101047
Iteration :  24   Loss :  1721.27165422
Iteration :  25   Loss :  931.605586247
Iteration :  26   Loss :  341.18966665
Iteration :  27   Loss :  285.096924634
Iteration :  28   Loss :  150.605916291
Iteration :  29   Loss :  79.5790552227
Iteration :  30   Loss :  42.0489857643
Iteration :  31   Loss :  22.2183739033
Iteration :  32   Loss :  11.7400248766
Iteration :  33   Loss :  6.20334254441
Iteration :  34   Loss :  3.27780044148
Iteration :  35   Loss :  1.73196557456
Iteration :  36   Loss :  0.915157833747
Iteration :  37   Loss :  0.483562648685
Iteration :  38   Loss :  0.255510936563
Iteration :  39   Loss :  0.135010094102
Iteration :  40   Loss :  0.0713383378208
Iteration :  41   Loss :  0.262242493967
Iteration :  42   Loss :  1254.7929904
Iteration :  43   Loss :  1187.38978769
Iteration :  44   Loss :  249.727274307
Iteration :  45   Loss :  131.954049629
Iteration :  46   Loss :  69.7235464644
Iteration :  47   Loss :  36.8414076357
Iteration :  48   Loss :  19.4667280338
Iteration :  49   Loss :  10.2860754967
Iteration :  50   Loss :  5.43508641714
Iteration :  51   Loss :  2.87185957088
Iteration :  52   Loss :  1.51746941296
Iteration :  53   Loss :  0.801819644188
Iteration :  54   Loss :  0.423675585362
Iteration :  55   Loss :  0.22386705406
Iteration :  56   Loss :  0.118289700009
Iteration :  57   Loss :  0.0625034049201
Iteration :  58   Loss :  0.0330263381031
Iteration :  59   Loss :  0.0174508734347
Iteration :  60   Loss :  0.00922091279644
Iteration :  61   Loss :  0.18809601388
Iteration :  62   Loss :  3389.78058305
Iteration :  63   Loss :  536.091813848
Iteration :  64   Loss :  1864.53136842
Iteration :  65   Loss :  291.292520154
Iteration :  66   Loss :  153.916818929
Iteration :  67   Loss :  81.3285117537
Iteration :  68   Loss :  42.9733856905
Iteration :  69   Loss :  22.7068200054
Iteration :  70   Loss :  11.9981161939
Iteration :  71   Loss :  6.33971609268
Iteration :  72   Loss :  3.34985921842
Iteration :  73   Loss :  1.77004090076
Iteration :  74   Loss :  0.935276555246
Iteration :  75   Loss :  0.494193232719
Iteration :  76   Loss :  0.26112805875
Iteration :  77   Loss :  0.137978140031
Iteration :  78   Loss :  0.0729066313958
Iteration :  79   Loss :  0.0385233262334
Iteration :  80   Loss :  0.0203554414142
Iteration :  81   Loss :  0.0107556650912
Iteration :  82   Loss :  0.00568321409499
Iteration :  83   Loss :  0.00300296840553
Iteration :  84   Loss :  0.00158674635407
Iteration :  85   Loss :  6423.16778554
Iteration :  86   Loss :  1104.07078388
Iteration :  87   Loss :  460.627353458
Iteration :  88   Loss :  703.38365779
Iteration :  89   Loss :  292.225295651
Iteration :  90   Loss :  154.40969062
Iteration :  91   Loss :  81.5889415198
Iteration :  92   Loss :  43.1109948578
Iteration :  93   Loss :  22.7795316744
Iteration :  94   Loss :  12.0365365034
Iteration :  95   Loss :  6.36001710083
Iteration :  96   Loss :  3.36058612138
Iteration :  97   Loss :  1.77570891715
Iteration :  98   Loss :  0.938271493298
Iteration :  99   Loss :  0.495775735894
[-0.00718214 -0.00307314 -0.00128695 ...,  0.00554799  0.00159942
  0.00014468]
CROSS VALIDATION 3
Iteration :  0   Loss :  2398.23718562
Iteration :  1   Loss :  423.834637869
Iteration :  2   Loss :  296.556252337
Iteration :  3   Loss :  156.698136186
Iteration :  4   Loss :  82.7981392761
Iteration :  5   Loss :  43.7499260326
Iteration :  6   Loss :  23.117138194
Iteration :  7   Loss :  12.214925298
Iteration :  8   Loss :  6.45427642402
Iteration :  9   Loss :  3.41039205245
Iteration :  10   Loss :  1.80202600374
Iteration :  11   Loss :  0.952177247721
Iteration :  12   Loss :  0.503123433955
Iteration :  13   Loss :  0.265846711209
Iteration :  14   Loss :  0.140471441183
Iteration :  15   Loss :  0.0742240733329
Iteration :  16   Loss :  0.0392194528348
Iteration :  17   Loss :  0.0207232695754
Iteration :  18   Loss :  0.230315800401
Iteration :  19   Loss :  1310.84789368
Iteration :  20   Loss :  2270.17566189
Iteration :  21   Loss :  1328.49761934
Iteration :  22   Loss :  176.735040473
Iteration :  23   Loss :  93.3854916987
Iteration :  24   Loss :  49.344204955
Iteration :  25   Loss :  26.0731139104
Iteration :  26   Loss :  13.7768410618
Iteration :  27   Loss :  7.27958119212
Iteration :  28   Loss :  3.84647700405
Iteration :  29   Loss :  2.03245007538
Iteration :  30   Loss :  1.07393162745
Iteration :  31   Loss :  0.567457550083
Iteration :  32   Loss :  0.29984038361
Iteration :  33   Loss :  0.158433446925
Iteration :  34   Loss :  0.0837150646702
Iteration :  35   Loss :  0.0442344226471
Iteration :  36   Loss :  0.0233731426313
Iteration :  37   Loss :  0.0503658819819
Iteration :  38   Loss :  6133.30583622
Iteration :  39   Loss :  480.315477067
Iteration :  40   Loss :  253.795155033
Iteration :  41   Loss :  134.103487798
Iteration :  42   Loss :  70.8592937373
Iteration :  43   Loss :  37.4415281168
Iteration :  44   Loss :  19.7838272693
Iteration :  45   Loss :  10.4536283936
Iteration :  46   Loss :  5.52362013193
Iteration :  47   Loss :  2.9186401327
Iteration :  48   Loss :  1.54218791675
Iteration :  49   Loss :  0.814880719255
Iteration :  50   Loss :  9339.58838065
Iteration :  51   Loss :  1096.16645843
Iteration :  52   Loss :  1575.16570309
Iteration :  53   Loss :  304.932764809
Iteration :  54   Loss :  1315.31330668
Iteration :  55   Loss :  197.463024735
Iteration :  56   Loss :  104.338005683
Iteration :  57   Loss :  55.1314325531
Iteration :  58   Loss :  29.1310422837
Iteration :  59   Loss :  15.3926278574
Iteration :  60   Loss :  8.13335101607
Iteration :  61   Loss :  4.29760268118
Iteration :  62   Loss :  2.27082155545
Iteration :  63   Loss :  1.19988535917
Iteration :  64   Loss :  0.634010572825
Iteration :  65   Loss :  0.335006509899
Iteration :  66   Loss :  0.177014968653
Iteration :  67   Loss :  0.0935334036844
Iteration :  68   Loss :  0.0908953358184
Iteration :  69   Loss :  1963.76409412
Iteration :  70   Loss :  950.98995546
Iteration :  71   Loss :  207.248637135
Iteration :  72   Loss :  109.508651091
Iteration :  73   Loss :  57.8635634455
Iteration :  74   Loss :  30.5746800937
Iteration :  75   Loss :  16.1554354272
Iteration :  76   Loss :  8.53641290908
Iteration :  77   Loss :  4.51057761226
Iteration :  78   Loss :  2.38335593802
Iteration :  79   Loss :  1.25934769681
Iteration :  80   Loss :  0.665430033411
Iteration :  81   Loss :  0.351608321106
Iteration :  82   Loss :  0.185787243232
Iteration :  83   Loss :  0.098168608863
Iteration :  84   Loss :  0.051871568782
Iteration :  85   Loss :  0.0274085543135
Iteration :  86   Loss :  0.0144824779199
Iteration :  87   Loss :  0.0076524345028
Iteration :  88   Loss :  0.00404348994305
Iteration :  89   Loss :  0.00213655025908
Iteration :  90   Loss :  13760.8935918
Iteration :  91   Loss :  4795.64172243
Iteration :  92   Loss :  506.974707875
Iteration :  93   Loss :  256.73290361
Iteration :  94   Loss :  135.655772476
Iteration :  95   Loss :  71.6795095106
Iteration :  96   Loss :  37.8749240811
Iteration :  97   Loss :  20.0128304999
Iteration :  98   Loss :  10.7036048728
Iteration :  99   Loss :  1558.53078118
[-0.11955355 -0.09759172 -0.08881207 ...,  0.07217428  0.01390825
  0.0256782 ]
CROSS VALIDATION 4
Iteration :  0   Loss :  2398.23718562
Iteration :  1   Loss :  423.834637869
Iteration :  2   Loss :  296.556252337
Iteration :  3   Loss :  156.698136186
Iteration :  4   Loss :  82.7981392761
Iteration :  5   Loss :  43.7499260326
Iteration :  6   Loss :  23.117138194
Iteration :  7   Loss :  12.214925298
Iteration :  8   Loss :  6.45427642402
Iteration :  9   Loss :  3.41039205245
Iteration :  10   Loss :  1.80202600374
Iteration :  11   Loss :  0.952177247721
Iteration :  12   Loss :  0.503123433955
Iteration :  13   Loss :  0.265846711209
Iteration :  14   Loss :  0.140471441183
Iteration :  15   Loss :  0.0742240733329
Iteration :  16   Loss :  0.0392194528348
Iteration :  17   Loss :  0.0207232695754
Iteration :  18   Loss :  0.230315800401
Iteration :  19   Loss :  1310.84789368
Iteration :  20   Loss :  2270.17566189
Iteration :  21   Loss :  1328.49761934
Iteration :  22   Loss :  176.735040473
Iteration :  23   Loss :  93.3854916987
Iteration :  24   Loss :  49.344204955
Iteration :  25   Loss :  26.0731139104
Iteration :  26   Loss :  13.7768410618
Iteration :  27   Loss :  7.27958119212
Iteration :  28   Loss :  3.84647700405
Iteration :  29   Loss :  2.03245007538
Iteration :  30   Loss :  1.07393162745
Iteration :  31   Loss :  0.567457550083
Iteration :  32   Loss :  0.29984038361
Iteration :  33   Loss :  0.158433446925
Iteration :  34   Loss :  0.0837150646702
Iteration :  35   Loss :  0.0442344226471
Iteration :  36   Loss :  0.0233731426313
Iteration :  37   Loss :  0.0503658819819
Iteration :  38   Loss :  6133.30583622
Iteration :  39   Loss :  480.315477067
Iteration :  40   Loss :  253.795155033
Iteration :  41   Loss :  134.103487798
Iteration :  42   Loss :  70.8592937373
Iteration :  43   Loss :  37.4415281168
Iteration :  44   Loss :  19.7838272693
Iteration :  45   Loss :  10.4536283936
Iteration :  46   Loss :  5.52362013193
Iteration :  47   Loss :  2.9186401327
Iteration :  48   Loss :  1.54218791675
Iteration :  49   Loss :  0.814880719255
Iteration :  50   Loss :  9220.10538262
Iteration :  51   Loss :  1096.16645843
Iteration :  52   Loss :  1575.16570309
Iteration :  53   Loss :  304.932764809
Iteration :  54   Loss :  1397.53140077
Iteration :  55   Loss :  207.691213967
Iteration :  56   Loss :  109.742505425
Iteration :  57   Loss :  57.9871303502
Iteration :  58   Loss :  30.6399719347
Iteration :  59   Loss :  16.1899351544
Iteration :  60   Loss :  8.55464231043
Iteration :  61   Loss :  4.52020989346
Iteration :  62   Loss :  2.38844556435
Iteration :  63   Loss :  1.26203701782
Iteration :  64   Loss :  0.666851050795
Iteration :  65   Loss :  0.35235917621
Iteration :  66   Loss :  0.18618398953
Iteration :  67   Loss :  0.0983782466804
Iteration :  68   Loss :  0.0519823398583
Iteration :  69   Loss :  0.0274670849331
Iteration :  70   Loss :  0.0145134050675
Iteration :  71   Loss :  0.137216985604
Iteration :  72   Loss :  3687.29912667
Iteration :  73   Loss :  1373.84189701
Iteration :  74   Loss :  277.770414462
Iteration :  75   Loss :  146.771838027
Iteration :  76   Loss :  77.5531565501
Iteration :  77   Loss :  40.9785158498
Iteration :  78   Loss :  21.6527454968
Iteration :  79   Loss :  11.4411509989
Iteration :  80   Loss :  6.04541979211
Iteration :  81   Loss :  3.19435522408
Iteration :  82   Loss :  1.68787373723
Iteration :  83   Loss :  0.891860032141
Iteration :  84   Loss :  0.471252262173
Iteration :  85   Loss :  0.249006219137
Iteration :  86   Loss :  0.131573049396
Iteration :  87   Loss :  0.180553335404
Iteration :  88   Loss :  1467.41572985
Iteration :  89   Loss :  384.012035485
Iteration :  90   Loss :  202.909126884
Iteration :  91   Loss :  107.215685885
Iteration :  92   Loss :  56.6519775446
Iteration :  93   Loss :  29.9344870409
Iteration :  94   Loss :  15.8171621405
Iteration :  95   Loss :  8.35767179968
Iteration :  96   Loss :  4.41613212854
Iteration :  97   Loss :  2.3334516411
Iteration :  98   Loss :  1.23297863444
Iteration :  99   Loss :  0.651496815363
[ -6.03246307e-03   1.26224019e-05   7.49740300e-04 ...,   1.97966403e-03
   2.49980866e-03  -3.62401124e-04]
CROSS VALIDATION 5
Iteration :  0   Loss :  2278.75999765
Iteration :  1   Loss :  957.70640665
Iteration :  2   Loss :  239.709027653
Iteration :  3   Loss :  126.660481997
Iteration :  4   Loss :  66.9264643754
Iteration :  5   Loss :  35.3634500926
Iteration :  6   Loss :  18.6857861702
Iteration :  7   Loss :  9.87343157655
Iteration :  8   Loss :  5.21704841363
Iteration :  9   Loss :  2.75664989817
Iteration :  10   Loss :  1.45659347175
Iteration :  11   Loss :  0.769653245905
Iteration :  12   Loss :  0.406679097785
Iteration :  13   Loss :  0.214886235399
Iteration :  14   Loss :  0.113544301675
Iteration :  15   Loss :  0.0599959714451
Iteration :  16   Loss :  0.105507859298
Iteration :  17   Loss :  5984.24604598
Iteration :  18   Loss :  383.882694888
Iteration :  19   Loss :  718.69464136
Iteration :  20   Loss :  1350.86799471
Iteration :  21   Loss :  2489.43823905
Iteration :  22   Loss :  955.809766823
Iteration :  23   Loss :  149.624212494
Iteration :  24   Loss :  79.0603301781
Iteration :  25   Loss :  41.7748952772
Iteration :  26   Loss :  22.0735465117
Iteration :  27   Loss :  11.6634991512
Iteration :  28   Loss :  6.16290691561
Iteration :  29   Loss :  3.25643455349
Iteration :  30   Loss :  1.72067599696
Iteration :  31   Loss :  0.909192504217
Iteration :  32   Loss :  0.480410612565
Iteration :  33   Loss :  0.253845423928
Iteration :  34   Loss :  0.134130049511
Iteration :  35   Loss :  0.0708733287507
Iteration :  36   Loss :  0.0374489441144
Iteration :  37   Loss :  10330.5342148
Iteration :  38   Loss :  4394.62356777
Iteration :  39   Loss :  346.708350488
Iteration :  40   Loss :  183.198134902
Iteration :  41   Loss :  96.8005431202
Iteration :  42   Loss :  51.148692935
Iteration :  43   Loss :  27.0265920482
Iteration :  44   Loss :  14.2806518765
Iteration :  45   Loss :  7.54579111023
Iteration :  46   Loss :  3.98714036108
Iteration :  47   Loss :  2.10677555563
Iteration :  48   Loss :  1.11320466295
Iteration :  49   Loss :  0.588209132345
Iteration :  50   Loss :  0.328390981791
Iteration :  51   Loss :  1208.37159653
Iteration :  52   Loss :  509.103007756
Iteration :  53   Loss :  285.32942112
Iteration :  54   Loss :  150.765961386
Iteration :  55   Loss :  79.6636218704
Iteration :  56   Loss :  42.0936701572
Iteration :  57   Loss :  22.2419848069
Iteration :  58   Loss :  11.7525007038
Iteration :  59   Loss :  6.20993467952
Iteration :  60   Loss :  3.28128367704
Iteration :  61   Loss :  1.73380609054
Iteration :  62   Loss :  0.916130348813
Iteration :  63   Loss :  0.484076518472
Iteration :  64   Loss :  0.255782461568
Iteration :  65   Loss :  0.135153565912
Iteration :  66   Loss :  0.0714141472667
Iteration :  67   Loss :  0.0377347086289
Iteration :  68   Loss :  0.0199387416892
Iteration :  69   Loss :  0.0105354840304
Iteration :  70   Loss :  0.00556687204659
Iteration :  71   Loss :  7166.03855573
Iteration :  72   Loss :  2012.15464508
Iteration :  73   Loss :  1316.50844391
Iteration :  74   Loss :  222.372238583
Iteration :  75   Loss :  117.499850537
Iteration :  76   Loss :  62.0860542856
Iteration :  77   Loss :  32.8058131064
Iteration :  78   Loss :  17.3343496532
Iteration :  79   Loss :  9.15934248981
Iteration :  80   Loss :  4.83972900768
Iteration :  81   Loss :  2.55727710737
Iteration :  82   Loss :  1.35124635977
Iteration :  83   Loss :  0.713988609024
Iteration :  84   Loss :  0.377266314268
Iteration :  85   Loss :  0.199344737552
Iteration :  86   Loss :  0.105332288855
Iteration :  87   Loss :  0.0556568044468
Iteration :  88   Loss :  0.029408644917
Iteration :  89   Loss :  0.0155393110411
Iteration :  90   Loss :  0.0082108573283
Iteration :  91   Loss :  6232.65019013
Iteration :  92   Loss :  373.369265033
Iteration :  93   Loss :  766.154385751
Iteration :  94   Loss :  1404.48799502
Iteration :  95   Loss :  2510.69067908
Iteration :  96   Loss :  956.574641048
Iteration :  97   Loss :  149.504264979
Iteration :  98   Loss :  78.9969507956
Iteration :  99   Loss :  41.741406079
[-0.05753272 -0.0004542  -0.0128458  ...,  0.04410291  0.01930674
  0.00628284]
CROSS VALIDATION 6
Iteration :  0   Loss :  2389.96475389
Iteration :  1   Loss :  423.708734197
Iteration :  2   Loss :  296.627212655
Iteration :  3   Loss :  156.735631095
Iteration :  4   Loss :  82.8179513092
Iteration :  5   Loss :  43.7603945647
Iteration :  6   Loss :  23.1226696892
Iteration :  7   Loss :  12.2178480993
Iteration :  8   Loss :  6.45582081064
Iteration :  9   Loss :  3.41120809494
Iteration :  10   Loss :  1.80245719457
Iteration :  11   Loss :  0.952405085777
Iteration :  12   Loss :  0.503243821904
Iteration :  13   Loss :  0.265910323313
Iteration :  14   Loss :  0.14050505335
Iteration :  15   Loss :  0.0742418337537
Iteration :  16   Loss :  0.0392288373102
Iteration :  17   Loss :  0.020728228263
Iteration :  18   Loss :  0.227312906344
Iteration :  19   Loss :  1310.66842001
Iteration :  20   Loss :  2270.04804887
Iteration :  21   Loss :  1334.37076833
Iteration :  22   Loss :  176.887444569
Iteration :  23   Loss :  93.4660208989
Iteration :  24   Loss :  49.3867559904
Iteration :  25   Loss :  26.0955975636
Iteration :  26   Loss :  13.7887212583
Iteration :  27   Loss :  7.28585860031
Iteration :  28   Loss :  3.84979394023
Iteration :  29   Loss :  2.03420271999
Iteration :  30   Loss :  1.07485771194
Iteration :  31   Loss :  0.567946886294
Iteration :  32   Loss :  0.300098945254
Iteration :  33   Loss :  0.158570068991
Iteration :  34   Loss :  0.0837872547618
Iteration :  35   Loss :  0.044272567359
Iteration :  36   Loss :  0.0233932980157
Iteration :  37   Loss :  0.0446014826873
Iteration :  38   Loss :  6132.97566443
Iteration :  39   Loss :  480.609311289
Iteration :  40   Loss :  253.950414868
Iteration :  41   Loss :  134.18552595
Iteration :  42   Loss :  70.9026420915
Iteration :  43   Loss :  37.4644330674
Iteration :  44   Loss :  19.7959300762
Iteration :  45   Loss :  10.4600234275
Iteration :  46   Loss :  5.52699922067
Iteration :  47   Loss :  2.92042561826
Iteration :  48   Loss :  1.54313135416
Iteration :  49   Loss :  0.815379224619
Iteration :  50   Loss :  9574.35931231
Iteration :  51   Loss :  1096.20318251
Iteration :  52   Loss :  1575.15070397
Iteration :  53   Loss :  304.98313045
Iteration :  54   Loss :  1432.39011638
Iteration :  55   Loss :  207.988465219
Iteration :  56   Loss :  109.899570794
Iteration :  57   Loss :  58.0701225327
Iteration :  58   Loss :  30.6838243917
Iteration :  59   Loss :  16.2131064692
Iteration :  60   Loss :  8.56688586225
Iteration :  61   Loss :  4.52667929593
Iteration :  62   Loss :  2.3918639489
Iteration :  63   Loss :  1.263843267
Iteration :  64   Loss :  0.667805459536
Iteration :  65   Loss :  0.352863478748
Iteration :  66   Loss :  0.186450459271
Iteration :  67   Loss :  0.0985190473261
Iteration :  68   Loss :  0.0520567378808
Iteration :  69   Loss :  0.0275063962994
Iteration :  70   Loss :  0.0145341769035
Iteration :  71   Loss :  0.134883229679
Iteration :  72   Loss :  3687.01166995
Iteration :  73   Loss :  1853.14018549
Iteration :  74   Loss :  313.38753928
Iteration :  75   Loss :  165.591664051
Iteration :  76   Loss :  87.4974137965
Iteration :  77   Loss :  46.4659118324
Iteration :  78   Loss :  313.323771126
Iteration :  79   Loss :  1428.35752597
Iteration :  80   Loss :  653.696032802
Iteration :  81   Loss :  1319.76350157
Iteration :  82   Loss :  219.976036451
Iteration :  83   Loss :  538.177109959
Iteration :  84   Loss :  118.991772011
Iteration :  85   Loss :  62.8743745872
Iteration :  86   Loss :  33.2223557387
Iteration :  87   Loss :  17.5544477075
Iteration :  88   Loss :  9.27564067814
Iteration :  89   Loss :  4.90118011252
Iteration :  90   Loss :  2.58974741787
Iteration :  91   Loss :  1.36840343232
Iteration :  92   Loss :  0.723054279602
Iteration :  93   Loss :  0.382056547728
Iteration :  94   Loss :  0.201875861577
Iteration :  95   Loss :  0.106669716119
Iteration :  96   Loss :  0.0563634911475
Iteration :  97   Loss :  0.0297820529568
Iteration :  98   Loss :  0.0157366170949
Iteration :  99   Loss :  0.0083151123917
[ -1.05473106e-03  -2.02632340e-04  -8.20791426e-04 ...,   2.38602372e-05
   1.79072839e-04   2.21230492e-04]
CROSS VALIDATION 7
Iteration :  0   Loss :  2389.96475389
Iteration :  1   Loss :  423.708734197
Iteration :  2   Loss :  296.627212655
Iteration :  3   Loss :  156.735631095
Iteration :  4   Loss :  82.8179513092
Iteration :  5   Loss :  43.7603945647
Iteration :  6   Loss :  23.1226696892
Iteration :  7   Loss :  12.2178480993
Iteration :  8   Loss :  6.45582081064
Iteration :  9   Loss :  3.41120809494
Iteration :  10   Loss :  1.80245719457
Iteration :  11   Loss :  0.952405085777
Iteration :  12   Loss :  0.503243821904
Iteration :  13   Loss :  0.265910323313
Iteration :  14   Loss :  0.14050505335
Iteration :  15   Loss :  0.0742418337537
Iteration :  16   Loss :  0.0392288373102
Iteration :  17   Loss :  0.020728228263
Iteration :  18   Loss :  0.227312906344
Iteration :  19   Loss :  1310.66842001
Iteration :  20   Loss :  2270.04804887
Iteration :  21   Loss :  1334.37076833
Iteration :  22   Loss :  176.887444569
Iteration :  23   Loss :  93.4660208989
Iteration :  24   Loss :  49.3867559904
Iteration :  25   Loss :  26.0955975636
Iteration :  26   Loss :  13.7887212583
Iteration :  27   Loss :  7.28585860031
Iteration :  28   Loss :  3.84979394023
Iteration :  29   Loss :  2.03420271999
Iteration :  30   Loss :  1.07485771194
Iteration :  31   Loss :  0.567946886294
Iteration :  32   Loss :  0.300098945254
Iteration :  33   Loss :  0.158570068991
Iteration :  34   Loss :  0.0837872547618
Iteration :  35   Loss :  0.044272567359
Iteration :  36   Loss :  0.0233932980157
Iteration :  37   Loss :  0.0446014826873
Iteration :  38   Loss :  6132.97566443
Iteration :  39   Loss :  480.609311289
Iteration :  40   Loss :  253.950414868
Iteration :  41   Loss :  134.18552595
Iteration :  42   Loss :  70.9026420915
Iteration :  43   Loss :  37.4644330674
Iteration :  44   Loss :  19.7959300762
Iteration :  45   Loss :  10.4600234275
Iteration :  46   Loss :  5.52699922067
Iteration :  47   Loss :  2.92042561826
Iteration :  48   Loss :  1.54313135416
Iteration :  49   Loss :  0.815379224619
Iteration :  50   Loss :  9158.32336544
Iteration :  51   Loss :  1096.20318251
Iteration :  52   Loss :  1575.15070397
Iteration :  53   Loss :  304.98313045
Iteration :  54   Loss :  1432.39011638
Iteration :  55   Loss :  207.988465219
Iteration :  56   Loss :  109.899570794
Iteration :  57   Loss :  58.0701225327
Iteration :  58   Loss :  30.6838243917
Iteration :  59   Loss :  16.2131064692
Iteration :  60   Loss :  8.56688586225
Iteration :  61   Loss :  4.52667929593
Iteration :  62   Loss :  2.3918639489
Iteration :  63   Loss :  1.263843267
Iteration :  64   Loss :  0.667805459536
Iteration :  65   Loss :  0.352863478748
Iteration :  66   Loss :  0.186450459271
Iteration :  67   Loss :  0.0985190473261
Iteration :  68   Loss :  0.0520567378808
Iteration :  69   Loss :  0.0275063962994
Iteration :  70   Loss :  0.0145341769035
Iteration :  71   Loss :  0.134883229679
Iteration :  72   Loss :  3687.01166995
Iteration :  73   Loss :  1292.24245102
Iteration :  74   Loss :  750.559097054
Iteration :  75   Loss :  440.126325748
Iteration :  76   Loss :  176.854421251
Iteration :  77   Loss :  93.4485716213
Iteration :  78   Loss :  49.3775359208
Iteration :  79   Loss :  26.0907257469
Iteration :  80   Loss :  13.7861470263
Iteration :  81   Loss :  7.2844983951
Iteration :  82   Loss :  3.84907521783
Iteration :  83   Loss :  2.03382295238
Iteration :  84   Loss :  1.07465704553
Iteration :  85   Loss :  0.567840855645
Iteration :  86   Loss :  0.300042919442
Iteration :  87   Loss :  0.158540465365
Iteration :  88   Loss :  0.0837716124241
Iteration :  89   Loss :  0.225826449562
Iteration :  90   Loss :  2952.0881763
Iteration :  91   Loss :  590.278925988
Iteration :  92   Loss :  249.259377318
Iteration :  93   Loss :  131.706816312
Iteration :  94   Loss :  69.5929102031
Iteration :  95   Loss :  36.7723803988
Iteration :  96   Loss :  19.4302545511
Iteration :  97   Loss :  10.2668031774
Iteration :  98   Loss :  5.42490306582
Iteration :  99   Loss :  2.86647876315
[-0.0152364  -0.00221758 -0.00875751 ...,  0.01126168  0.00065616
  0.00090526]
CROSS VALIDATION 8
Iteration :  0   Loss :  2078.57878322
Iteration :  1   Loss :  1751.07844086
Iteration :  2   Loss :  266.492027539
Iteration :  3   Loss :  140.812421572
Iteration :  4   Loss :  74.404244855
Iteration :  5   Loss :  39.3146541381
Iteration :  6   Loss :  20.7735732418
Iteration :  7   Loss :  10.9766028646
Iteration :  8   Loss :  5.79995598468
Iteration :  9   Loss :  3.0646539589
Iteration :  10   Loss :  1.61934054545
Iteration :  11   Loss :  0.855647599139
Iteration :  12   Loss :  0.452117879696
Iteration :  13   Loss :  0.238895752581
Iteration :  14   Loss :  0.126230753448
Iteration :  15   Loss :  0.0666993989802
Iteration :  16   Loss :  0.0352434704127
Iteration :  17   Loss :  0.0186223897924
Iteration :  18   Loss :  0.00983993339815
Iteration :  19   Loss :  6547.74645818
Iteration :  20   Loss :  753.082862747
Iteration :  21   Loss :  281.575420548
Iteration :  22   Loss :  148.782375175
Iteration :  23   Loss :  78.6155095486
Iteration :  24   Loss :  41.5398553377
Iteration :  25   Loss :  21.9493531415
Iteration :  26   Loss :  11.5978762904
Iteration :  27   Loss :  6.12823228007
Iteration :  28   Loss :  3.23811273185
Iteration :  29   Loss :  1.71099488155
Iteration :  30   Loss :  0.904077074251
Iteration :  31   Loss :  4501.63473797
Iteration :  32   Loss :  582.021152066
Iteration :  33   Loss :  274.055569642
Iteration :  34   Loss :  144.808941427
Iteration :  35   Loss :  76.5159764659
Iteration :  36   Loss :  40.4304775441
Iteration :  37   Loss :  21.3631660987
Iteration :  38   Loss :  11.288139381
Iteration :  39   Loss :  5.96456958191
Iteration :  40   Loss :  3.1516345694
Iteration :  41   Loss :  1.66530045842
Iteration :  42   Loss :  0.879932478133
Iteration :  43   Loss :  0.597807584051
Iteration :  44   Loss :  3971.38249436
Iteration :  45   Loss :  559.299736754
Iteration :  46   Loss :  270.317054064
Iteration :  47   Loss :  142.833537372
Iteration :  48   Loss :  75.4721875351
Iteration :  49   Loss :  39.8789471725
Iteration :  50   Loss :  21.0717415187
Iteration :  51   Loss :  11.1341527827
Iteration :  52   Loss :  5.88320419922
Iteration :  53   Loss :  3.10864170138
Iteration :  54   Loss :  1.6425833441
Iteration :  55   Loss :  0.867928922497
Iteration :  56   Loss :  0.458607240365
Iteration :  57   Loss :  0.24232468289
Iteration :  58   Loss :  0.12804257493
Iteration :  59   Loss :  0.166995803762
Iteration :  60   Loss :  4375.62051783
Iteration :  61   Loss :  1744.25401017
Iteration :  62   Loss :  379.602842089
Iteration :  63   Loss :  251.596385193
Iteration :  64   Loss :  132.941674034
Iteration :  65   Loss :  70.2453999146
Iteration :  66   Loss :  37.1171511493
Iteration :  67   Loss :  19.6124288724
Iteration :  68   Loss :  10.363062745
Iteration :  69   Loss :  5.47576591127
Iteration :  70   Loss :  2.8933543155
Iteration :  71   Loss :  1.52882707747
Iteration :  72   Loss :  0.807820950332
Iteration :  73   Loss :  0.42684663126
Iteration :  74   Loss :  0.225542611321
Iteration :  75   Loss :  0.119175052106
Iteration :  76   Loss :  0.069968059899
Iteration :  77   Loss :  4471.19142538
Iteration :  78   Loss :  445.751706906
Iteration :  79   Loss :  235.531913839
Iteration :  80   Loss :  124.45332587
Iteration :  81   Loss :  65.7602193591
Iteration :  82   Loss :  34.7472148288
Iteration :  83   Loss :  18.3601720025
Iteration :  84   Loss :  9.70137945221
Iteration :  85   Loss :  5.12613734027
Iteration :  86   Loss :  2.70861315762
Iteration :  87   Loss :  1.43121121239
Iteration :  88   Loss :  0.756241447288
Iteration :  89   Loss :  0.537157379306
Iteration :  90   Loss :  4097.26529007
Iteration :  91   Loss :  568.215176625
Iteration :  92   Loss :  270.547740451
Iteration :  93   Loss :  142.955430357
Iteration :  94   Loss :  75.5365948894
Iteration :  95   Loss :  39.9129795434
Iteration :  96   Loss :  21.0897239724
Iteration :  97   Loss :  11.1436545786
Iteration :  98   Loss :  5.88822487809
Iteration :  99   Loss :  3.11129459106
[-0.00318819 -0.00247073 -0.00676575 ...,  0.00820539  0.00386622
  0.00111001]
CROSS VALIDATION 9
Iteration :  0   Loss :  2162.08288899
Iteration :  1   Loss :  422.318465865
Iteration :  2   Loss :  296.734841682
Iteration :  3   Loss :  156.792501479
Iteration :  4   Loss :  82.8480012006
Iteration :  5   Loss :  43.7762727057
Iteration :  6   Loss :  23.1310595818
Iteration :  7   Loss :  12.2222812566
Iteration :  8   Loss :  6.45816325827
Iteration :  9   Loss :  3.412445827
Iteration :  10   Loss :  1.80311120306
Iteration :  11   Loss :  0.952750659037
Iteration :  12   Loss :  0.503426420265
Iteration :  13   Loss :  0.26600680694
Iteration :  14   Loss :  0.140556034587
Iteration :  15   Loss :  0.0742687718632
Iteration :  16   Loss :  0.0392430712085
Iteration :  17   Loss :  0.0207357493499
Iteration :  18   Loss :  0.0109566169991
Iteration :  19   Loss :  0.00578939560078
Iteration :  20   Loss :  0.00305907392994
Iteration :  21   Loss :  0.0231435115964
Iteration :  22   Loss :  3443.81486898
Iteration :  23   Loss :  352.282349925
Iteration :  24   Loss :  186.143395088
Iteration :  25   Loss :  98.3567968765
Iteration :  26   Loss :  51.9710059401
Iteration :  27   Loss :  27.4610961743
Iteration :  28   Loss :  14.5102406516
Iteration :  29   Loss :  7.66710412544
Iteration :  30   Loss :  4.05124126345
Iteration :  31   Loss :  2.1406459996
Iteration :  32   Loss :  1.1311015557
Iteration :  33   Loss :  0.597665718457
Iteration :  34   Loss :  0.315802156948
Iteration :  35   Loss :  0.166867530215
Iteration :  36   Loss :  0.0881715720665
Iteration :  37   Loss :  0.0465892082818
Iteration :  38   Loss :  0.0246173939906
Iteration :  39   Loss :  4432.13073946
Iteration :  40   Loss :  2529.15098434
Iteration :  41   Loss :  1452.91887911
Iteration :  42   Loss :  275.870006732
Iteration :  43   Loss :  145.767676601
Iteration :  44   Loss :  77.0225650598
Iteration :  45   Loss :  40.6981552203
Iteration :  46   Loss :  21.5046050082
Iteration :  47   Loss :  11.3628746575
Iteration :  48   Loss :  6.00405915072
Iteration :  49   Loss :  3.17250056627
Iteration :  50   Loss :  1.67632589725
Iteration :  51   Loss :  0.885758238679
Iteration :  52   Loss :  0.468028119518
Iteration :  53   Loss :  0.247302605941
Iteration :  54   Loss :  0.317592755466
Iteration :  55   Loss :  3916.56585941
Iteration :  56   Loss :  924.430544535
Iteration :  57   Loss :  269.635376417
Iteration :  58   Loss :  142.473343932
Iteration :  59   Loss :  75.2818639783
Iteration :  60   Loss :  39.778381609
Iteration :  61   Loss :  21.0186034168
Iteration :  62   Loss :  11.1060750016
Iteration :  63   Loss :  5.86836810682
Iteration :  64   Loss :  3.10080241959
Iteration :  65   Loss :  1.63844112543
Iteration :  66   Loss :  0.865740204706
Iteration :  67   Loss :  0.457450738029
Iteration :  68   Loss :  0.241713595587
Iteration :  69   Loss :  0.127719680906
Iteration :  70   Loss :  0.0674861372661
Iteration :  71   Loss :  0.0620622019995
Iteration :  72   Loss :  1167.20951949
Iteration :  73   Loss :  1157.53471843
Iteration :  74   Loss :  250.086373624
Iteration :  75   Loss :  132.14379506
Iteration :  76   Loss :  69.8238065501
Iteration :  77   Loss :  36.8943843254
Iteration :  78   Loss :  19.4947205259
Iteration :  79   Loss :  10.300866523
Iteration :  80   Loss :  5.44290188634
Iteration :  81   Loss :  2.87598920715
Iteration :  82   Loss :  1.51965148231
Iteration :  83   Loss :  0.802972633534
Iteration :  84   Loss :  0.424284816428
Iteration :  85   Loss :  0.224188967261
Iteration :  86   Loss :  0.11845979657
Iteration :  87   Loss :  0.0625932826881
Iteration :  88   Loss :  0.0330738288527
Iteration :  89   Loss :  0.0174759671965
Iteration :  90   Loss :  0.00923417215506
Iteration :  91   Loss :  12764.0700956
Iteration :  92   Loss :  6674.23725365
Iteration :  93   Loss :  1695.44911133
Iteration :  94   Loss :  346.18180121
Iteration :  95   Loss :  267.431123207
Iteration :  96   Loss :  141.308632795
Iteration :  97   Loss :  74.6664392051
Iteration :  98   Loss :  39.4531956986
Iteration :  99   Loss :  20.8467775804
[-0.04628441  0.00230924 -0.01988681 ...,  0.01624547  0.00941025
  0.00503565]
CROSS VALIDATION 10
Iteration :  0   Loss :  1443.95560813
Iteration :  1   Loss :  342.442456115
Iteration :  2   Loss :  180.944067782
Iteration :  3   Loss :  95.6095106814
Iteration :  4   Loss :  50.51936018
Iteration :  5   Loss :  26.6940572628
Iteration :  6   Loss :  14.1049429488
Iteration :  7   Loss :  7.45294780895
Iteration :  8   Loss :  3.93808264554
Iteration :  9   Loss :  2.08085382062
Iteration :  10   Loss :  1.09950780938
Iteration :  11   Loss :  0.580971815953
Iteration :  12   Loss :  0.306981222008
Iteration :  13   Loss :  0.16220661326
Iteration :  14   Loss :  0.0857087779288
Iteration :  15   Loss :  0.0452878860263
Iteration :  16   Loss :  0.02392978491
Iteration :  17   Loss :  0.158060626643
Iteration :  18   Loss :  1842.39935159
Iteration :  19   Loss :  1472.65201422
Iteration :  20   Loss :  293.929763426
Iteration :  21   Loss :  155.31031882
Iteration :  22   Loss :  82.064826817
Iteration :  23   Loss :  43.3624491385
Iteration :  24   Loss :  22.9123982614
Iteration :  25   Loss :  12.1067422279
Iteration :  26   Loss :  6.39711328778
Iteration :  27   Loss :  3.38018747292
Iteration :  28   Loss :  1.78606612672
Iteration :  29   Loss :  0.943744166433
Iteration :  30   Loss :  0.498667456009
Iteration :  31   Loss :  0.263492205332
Iteration :  32   Loss :  0.139227337646
Iteration :  33   Loss :  0.0735666982009
Iteration :  34   Loss :  0.0388721006642
Iteration :  35   Loss :  0.0205397312507
Iteration :  36   Loss :  0.0108530424814
Iteration :  37   Loss :  0.00573466758966
Iteration :  38   Loss :  0.00303015605257
Iteration :  39   Loss :  0.057733762799
Iteration :  40   Loss :  2879.49603552
Iteration :  41   Loss :  1783.02871662
Iteration :  42   Loss :  366.688015617
Iteration :  43   Loss :  193.755242576
Iteration :  44   Loss :  102.378840941
Iteration :  45   Loss :  54.0962243549
Iteration :  46   Loss :  28.5840459081
Iteration :  47   Loss :  15.1035990075
Iteration :  48   Loss :  7.98063030376
Iteration :  49   Loss :  4.21690618334
Iteration :  50   Loss :  2.22818212125
Iteration :  51   Loss :  1.17735499667
Iteration :  52   Loss :  0.622105695475
Iteration :  53   Loss :  0.328716060523
Iteration :  54   Loss :  0.173691141605
Iteration :  55   Loss :  0.0917771179902
Iteration :  56   Loss :  0.0484943521516
Iteration :  57   Loss :  0.20235921233
Iteration :  58   Loss :  1695.68782809
Iteration :  59   Loss :  1890.39427208
Iteration :  60   Loss :  227.238920581
Iteration :  61   Loss :  120.071369406
Iteration :  62   Loss :  63.4448258869
Iteration :  63   Loss :  33.5237779976
Iteration :  64   Loss :  17.7137170056
Iteration :  65   Loss :  9.35979740042
Iteration :  66   Loss :  4.94564790379
Iteration :  67   Loss :  2.61324387076
Iteration :  68   Loss :  1.3808187847
Iteration :  69   Loss :  0.72961446022
Iteration :  70   Loss :  0.385522898775
Iteration :  71   Loss :  0.203707455901
Iteration :  72   Loss :  4686.93604186
Iteration :  73   Loss :  1332.46015779
Iteration :  74   Loss :  553.166469213
Iteration :  75   Loss :  309.422920237
Iteration :  76   Loss :  189.037249606
Iteration :  77   Loss :  99.8858882574
Iteration :  78   Loss :  52.7789665464
Iteration :  79   Loss :  27.8880165988
Iteration :  80   Loss :  14.7358222547
Iteration :  81   Loss :  7.78629978051
Iteration :  82   Loss :  4.11422336834
Iteration :  83   Loss :  2.17392527924
Iteration :  84   Loss :  1.14868608158
Iteration :  85   Loss :  0.606957252217
Iteration :  86   Loss :  0.320711734848
Iteration :  87   Loss :  0.169461714962
Iteration :  88   Loss :  0.0895423201506
Iteration :  89   Loss :  0.0473135014582
Iteration :  90   Loss :  0.0250001051622
Iteration :  91   Loss :  0.0132098711543
Iteration :  92   Loss :  0.00697999847525
Iteration :  93   Loss :  0.00368817970633
Iteration :  94   Loss :  0.00194880695094
Iteration :  95   Loss :  0.00102973521749
Iteration :  96   Loss :  0.0218435141316
Iteration :  97   Loss :  5784.81581057
Iteration :  98   Loss :  1459.42251122
Iteration :  99   Loss :  479.51545123
[-0.25521193 -0.10938736 -0.09967236 ...,  0.07720335 -0.03730265
  0.00601417]
CROSS VALIDATION 11
Iteration :  0   Loss :  2389.5071552
Iteration :  1   Loss :  422.818270999
Iteration :  2   Loss :  296.837867766
Iteration :  3   Loss :  156.846939702
Iteration :  4   Loss :  82.876765956
Iteration :  5   Loss :  43.7914717903
Iteration :  6   Loss :  23.1390906659
Iteration :  7   Loss :  12.2265248223
Iteration :  8   Loss :  6.46040552706
Iteration :  9   Loss :  3.41363062528
Iteration :  10   Loss :  1.80373724173
Iteration :  11   Loss :  0.953081453255
Iteration :  12   Loss :  0.503601209489
Iteration :  13   Loss :  0.266099164277
Iteration :  14   Loss :  0.140604835522
Iteration :  15   Loss :  0.0742945579176
Iteration :  16   Loss :  0.0392566963694
Iteration :  17   Loss :  0.0207429487844
Iteration :  18   Loss :  0.228719320993
Iteration :  19   Loss :  1222.55632374
Iteration :  20   Loss :  1910.26888758
Iteration :  21   Loss :  1335.05025381
Iteration :  22   Loss :  176.907109546
Iteration :  23   Loss :  93.4764117281
Iteration :  24   Loss :  49.3922464279
Iteration :  25   Loss :  26.0984986703
Iteration :  26   Loss :  13.7902541817
Iteration :  27   Loss :  7.28666858572
Iteration :  28   Loss :  3.85022193054
Iteration :  29   Loss :  2.03442886692
Iteration :  30   Loss :  1.0749772063
Iteration :  31   Loss :  0.568010026235
Iteration :  32   Loss :  0.300132307934
Iteration :  33   Loss :  0.158587697585
Iteration :  34   Loss :  0.0837965695807
Iteration :  35   Loss :  0.0442774892406
Iteration :  36   Loss :  0.0233958987016
Iteration :  37   Loss :  0.043537463694
Iteration :  38   Loss :  6132.90308512
Iteration :  39   Loss :  480.969884784
Iteration :  40   Loss :  254.140939243
Iteration :  41   Loss :  134.286197624
Iteration :  42   Loss :  70.9558362617
Iteration :  43   Loss :  37.4925404746
Iteration :  44   Loss :  19.8107818228
Iteration :  45   Loss :  10.4678709808
Iteration :  46   Loss :  5.53114580997
Iteration :  47   Loss :  2.92261664546
Iteration :  48   Loss :  1.54428907677
Iteration :  49   Loss :  0.815990956706
Iteration :  50   Loss :  9449.74860713
Iteration :  51   Loss :  1096.19377707
Iteration :  52   Loss :  1572.46360325
Iteration :  53   Loss :  305.981582796
Iteration :  54   Loss :  1106.68517629
Iteration :  55   Loss :  208.010450273
Iteration :  56   Loss :  109.911187534
Iteration :  57   Loss :  58.0762607326
Iteration :  58   Loss :  30.6870677713
Iteration :  59   Loss :  16.214820247
Iteration :  60   Loss :  8.56779140981
Iteration :  61   Loss :  4.52715778059
Iteration :  62   Loss :  2.39211677666
Iteration :  63   Loss :  1.26397685932
Iteration :  64   Loss :  0.667876048735
Iteration :  65   Loss :  0.35290077756
Iteration :  66   Loss :  0.186470167688
Iteration :  67   Loss :  0.0985294611083
Iteration :  68   Loss :  0.0520622404465
Iteration :  69   Loss :  0.0275093038146
Iteration :  70   Loss :  0.0145357132131
Iteration :  71   Loss :  0.135066278576
Iteration :  72   Loss :  3687.01749365
Iteration :  73   Loss :  1572.67422248
Iteration :  74   Loss :  313.894431828
Iteration :  75   Loss :  165.859502334
Iteration :  76   Loss :  87.6389375697
Iteration :  77   Loss :  46.3077681427
Iteration :  78   Loss :  309.037205675
Iteration :  79   Loss :  1252.23550069
Iteration :  80   Loss :  184.188685281
Iteration :  81   Loss :  97.3239426336
Iteration :  82   Loss :  51.425253377
Iteration :  83   Loss :  27.1727245457
Iteration :  84   Loss :  14.3578672102
Iteration :  85   Loss :  7.586591123
Iteration :  86   Loss :  4.00869878686
Iteration :  87   Loss :  2.11816686879
Iteration :  88   Loss :  1.1192237488
Iteration :  89   Loss :  0.591389572905
Iteration :  90   Loss :  0.312485887935
Iteration :  91   Loss :  0.165115238131
Iteration :  92   Loss :  0.087245673855
Iteration :  93   Loss :  0.0460999704968
Iteration :  94   Loss :  0.0243588843538
Iteration :  95   Loss :  0.212953119958
Iteration :  96   Loss :  3375.4108208
Iteration :  97   Loss :  825.48901845
Iteration :  98   Loss :  286.541768558
Iteration :  99   Loss :  151.406556829
[-0.08338581 -0.01702653 -0.01186858 ...,  0.00241714  0.03031263
  0.01379742]
CROSS VALIDATION 12
Iteration :  0   Loss :  1635.35607829
Iteration :  1   Loss :  1014.6610832
Iteration :  2   Loss :  455.217270994
Iteration :  3   Loss :  303.888013323
Iteration :  4   Loss :  160.572184608
Iteration :  5   Loss :  84.8451578854
Iteration :  6   Loss :  44.8315555659
Iteration :  7   Loss :  23.6886632608
Iteration :  8   Loss :  12.5169149275
Iteration :  9   Loss :  6.61384551658
Iteration :  10   Loss :  3.49470718389
Iteration :  11   Loss :  1.84657749724
Iteration :  12   Loss :  0.975717928254
Iteration :  13   Loss :  0.515562155901
Iteration :  14   Loss :  0.272419240131
Iteration :  15   Loss :  0.143944316983
Iteration :  16   Loss :  0.0760591152879
Iteration :  17   Loss :  0.0401890754677
Iteration :  18   Loss :  0.0212356110222
Iteration :  19   Loss :  0.0112207402195
Iteration :  20   Loss :  0.00592895636215
Iteration :  21   Loss :  0.00313281680678
Iteration :  22   Loss :  0.00165535729146
Iteration :  23   Loss :  0.231953144347
Iteration :  24   Loss :  2734.7264867
Iteration :  25   Loss :  1018.93930397
Iteration :  26   Loss :  254.258173621
Iteration :  27   Loss :  134.348143404
Iteration :  28   Loss :  70.9885679551
Iteration :  29   Loss :  37.5098356599
Iteration :  30   Loss :  19.8199204712
Iteration :  31   Loss :  10.4726997752
Iteration :  32   Loss :  5.53369730931
Iteration :  33   Loss :  2.92396483889
Iteration :  34   Loss :  1.54500145223
Iteration :  35   Loss :  0.816367370645
Iteration :  36   Loss :  0.431362496711
Iteration :  37   Loss :  0.227928761314
Iteration :  38   Loss :  0.120435876161
Iteration :  39   Loss :  0.0636374285674
Iteration :  40   Loss :  0.0336255478332
Iteration :  41   Loss :  0.017767491436
Iteration :  42   Loss :  0.0093882114128
Iteration :  43   Loss :  0.00496066165833
Iteration :  44   Loss :  5256.41356111
Iteration :  45   Loss :  2752.07599727
Iteration :  46   Loss :  332.298181272
Iteration :  47   Loss :  175.583907785
Iteration :  48   Loss :  92.7772416779
Iteration :  49   Loss :  49.0228101308
Iteration :  50   Loss :  25.9032912561
Iteration :  51   Loss :  13.6871080239
Iteration :  52   Loss :  7.23216691683
Iteration :  53   Loss :  3.82142365073
Iteration :  54   Loss :  2.0192120683
Iteration :  55   Loss :  1.06693676216
Iteration :  56   Loss :  0.56376151486
Iteration :  57   Loss :  0.297887425862
Iteration :  58   Loss :  0.157401518457
Iteration :  59   Loss :  0.0831698012798
Iteration :  60   Loss :  0.0439463094937
Iteration :  61   Loss :  0.0232209057663
Iteration :  62   Loss :  0.159645284957
Iteration :  63   Loss :  2130.64694688
Iteration :  64   Loss :  2331.97000251
Iteration :  65   Loss :  288.105451802
Iteration :  66   Loss :  152.232795521
Iteration :  67   Loss :  80.4386862072
Iteration :  68   Loss :  42.5032084355
Iteration :  69   Loss :  22.4583818122
Iteration :  70   Loss :  11.8668432852
Iteration :  71   Loss :  6.27035245605
Iteration :  72   Loss :  3.31320798449
Iteration :  73   Loss :  1.75067465911
Iteration :  74   Loss :  0.925043576012
Iteration :  75   Loss :  0.488786201976
Iteration :  76   Loss :  0.25827102359
Iteration :  77   Loss :  0.136468503727
Iteration :  78   Loss :  0.0721089507089
Iteration :  79   Loss :  0.050858298935
Iteration :  80   Loss :  1132.03352657
Iteration :  81   Loss :  3255.84163931
Iteration :  82   Loss :  335.395537387
Iteration :  83   Loss :  177.220527909
Iteration :  84   Loss :  93.6420196787
Iteration :  85   Loss :  49.4797524474
Iteration :  86   Loss :  26.1447362055
Iteration :  87   Loss :  13.8146857542
Iteration :  88   Loss :  7.29957804842
Iteration :  89   Loss :  3.85704319541
Iteration :  90   Loss :  2.03803317296
Iteration :  91   Loss :  1.07688169503
Iteration :  92   Loss :  0.569016344029
Iteration :  93   Loss :  0.300664038833
Iteration :  94   Loss :  0.158868660269
Iteration :  95   Loss :  0.0839450281908
Iteration :  96   Loss :  0.044355933675
Iteration :  97   Loss :  0.0234373481621
Iteration :  98   Loss :  0.0123841218831
Iteration :  99   Loss :  0.00654367865151
[ -9.72462993e-04  -1.91706300e-05  -6.02178967e-04 ...,   1.88267990e-04
   5.76856044e-05   1.91454426e-05]
CROSS VALIDATION 13
Iteration :  0   Loss :  2418.057994
Iteration :  1   Loss :  420.744856843
Iteration :  2   Loss :  296.916300688
Iteration :  3   Loss :  156.888383079
Iteration :  4   Loss :  82.8986643308
Iteration :  5   Loss :  43.80304273
Iteration :  6   Loss :  23.1452046651
Iteration :  7   Loss :  12.2297554143
Iteration :  8   Loss :  6.46211254808
Iteration :  9   Loss :  3.4145326026
Iteration :  10   Loss :  1.80421383989
Iteration :  11   Loss :  0.953333284207
Iteration :  12   Loss :  0.503734275107
Iteration :  13   Loss :  0.266169475168
Iteration :  14   Loss :  0.140641987278
Iteration :  15   Loss :  0.0743141886314
Iteration :  16   Loss :  0.0392670690938
Iteration :  17   Loss :  0.0207484296554
Iteration :  18   Loss :  0.23312204483
Iteration :  19   Loss :  2757.70459335
Iteration :  20   Loss :  3510.67142064
Iteration :  21   Loss :  355.272356344
Iteration :  22   Loss :  187.723292424
Iteration :  23   Loss :  99.1916029748
Iteration :  24   Loss :  52.4121113245
Iteration :  25   Loss :  27.6941730056
Iteration :  26   Loss :  14.63339673
Iteration :  27   Loss :  7.73217888878
Iteration :  28   Loss :  4.08562628837
Iteration :  29   Loss :  2.15881479313
Iteration :  30   Loss :  1.14070181218
Iteration :  31   Loss :  0.602738423164
Iteration :  32   Loss :  0.318482536697
Iteration :  33   Loss :  0.168283823102
Iteration :  34   Loss :  0.0889199307808
Iteration :  35   Loss :  0.0469846355062
Iteration :  36   Loss :  0.0248263348191
Iteration :  37   Loss :  0.0131180521868
Iteration :  38   Loss :  0.00693148201013
Iteration :  39   Loss :  4474.87643008
Iteration :  40   Loss :  996.682841551
Iteration :  41   Loss :  814.297756485
Iteration :  42   Loss :  509.955965858
Iteration :  43   Loss :  1517.76640633
Iteration :  44   Loss :  329.985279649
Iteration :  45   Loss :  157.845119419
Iteration :  46   Loss :  83.4041967556
Iteration :  47   Loss :  44.0701623342
Iteration :  48   Loss :  23.2863486936
Iteration :  49   Loss :  12.3043348778
Iteration :  50   Loss :  6.50151978644
Iteration :  51   Loss :  3.43535509666
Iteration :  52   Loss :  1.81521629216
Iteration :  53   Loss :  0.959146898825
Iteration :  54   Loss :  0.506806146187
Iteration :  55   Loss :  0.267792629187
Iteration :  56   Loss :  0.141499649889
Iteration :  57   Loss :  0.0747673712282
Iteration :  58   Loss :  0.0395065274351
Iteration :  59   Loss :  0.0208749576766
Iteration :  60   Loss :  0.0110301736521
Iteration :  61   Loss :  0.00582826239362
Iteration :  62   Loss :  10946.569017
Iteration :  63   Loss :  1467.8226159
Iteration :  64   Loss :  379.808193507
Iteration :  65   Loss :  200.687847792
Iteration :  66   Loss :  106.041978398
Iteration :  67   Loss :  56.0317991665
Iteration :  68   Loss :  29.6067893607
Iteration :  69   Loss :  15.6440091036
Iteration :  70   Loss :  8.2661790122
Iteration :  71   Loss :  4.36778801453
Iteration :  72   Loss :  2.30790696787
Iteration :  73   Loss :  1.21948101753
Iteration :  74   Loss :  0.644364774156
Iteration :  75   Loss :  0.340477593504
Iteration :  76   Loss :  0.179905848873
Iteration :  77   Loss :  0.0950609234679
Iteration :  78   Loss :  0.050229490743
Iteration :  79   Loss :  0.0265408923905
Iteration :  80   Loss :  0.0140240117601
Iteration :  81   Loss :  0.103957386047
Iteration :  82   Loss :  2805.96119914
Iteration :  83   Loss :  3473.50920802
Iteration :  84   Loss :  355.111095255
Iteration :  85   Loss :  187.638083254
Iteration :  86   Loss :  99.1465790779
Iteration :  87   Loss :  52.3883210295
Iteration :  88   Loss :  27.6816023893
Iteration :  89   Loss :  14.626754509
Iteration :  90   Loss :  7.72866918822
Iteration :  91   Loss :  4.08377178849
Iteration :  92   Loss :  2.15783488907
Iteration :  93   Loss :  1.1401840381
Iteration :  94   Loss :  0.602464835161
Iteration :  95   Loss :  0.318337974814
Iteration :  96   Loss :  0.16820743767
Iteration :  97   Loss :  0.0888795692817
Iteration :  98   Loss :  0.0469633087878
Iteration :  99   Loss :  0.0248150659384
[ -2.13922037e-03  -8.11278031e-04  -3.83767083e-04 ...,   9.72153866e-04
   3.46121961e-04  -4.06629013e-05]
CROSS VALIDATION 14
Iteration :  0   Loss :  2418.057994
Iteration :  1   Loss :  419.073202222
Iteration :  2   Loss :  221.435188758
Iteration :  3   Loss :  117.004720322
Iteration :  4   Loss :  61.8244311322
Iteration :  5   Loss :  32.667573362
Iteration :  6   Loss :  17.2613047919
Iteration :  7   Loss :  9.12074612396
Iteration :  8   Loss :  4.81933497267
Iteration :  9   Loss :  2.54650104971
Iteration :  10   Loss :  1.34555237039
Iteration :  11   Loss :  0.710979947038
Iteration :  12   Loss :  0.375676559467
Iteration :  13   Loss :  0.198504722842
Iteration :  14   Loss :  0.104888431279
Iteration :  15   Loss :  0.0554222733776
Iteration :  16   Loss :  0.02928472043
Iteration :  17   Loss :  0.0154738302563
Iteration :  18   Loss :  0.0829474342562
Iteration :  19   Loss :  1309.60169163
Iteration :  20   Loss :  2268.5322463
Iteration :  21   Loss :  1339.30631068
Iteration :  22   Loss :  176.999264655
Iteration :  23   Loss :  93.5251058078
Iteration :  24   Loss :  49.4179760204
Iteration :  25   Loss :  26.1120939972
Iteration :  26   Loss :  13.7974378521
Iteration :  27   Loss :  7.29046438419
Iteration :  28   Loss :  3.85222760245
Iteration :  29   Loss :  2.03548864916
Iteration :  30   Loss :  1.07553718742
Iteration :  31   Loss :  0.568305916128
Iteration :  32   Loss :  0.300288653971
Iteration :  33   Loss :  0.158670309677
Iteration :  34   Loss :  0.083840221201
Iteration :  35   Loss :  0.0443005544348
Iteration :  36   Loss :  0.0234080861801
Iteration :  37   Loss :  0.0362131949572
Iteration :  38   Loss :  5276.47288751
Iteration :  39   Loss :  849.125217333
Iteration :  40   Loss :  934.39212316
Iteration :  41   Loss :  1006.85065428
Iteration :  42   Loss :  137.085272852
Iteration :  43   Loss :  72.4348469648
Iteration :  44   Loss :  38.2740388202
Iteration :  45   Loss :  20.2237197839
Iteration :  46   Loss :  10.6860643534
Iteration :  47   Loss :  5.64643757855
Iteration :  48   Loss :  2.98353596553
Iteration :  49   Loss :  1.57647839612
Iteration :  50   Loss :  0.832999555607
Iteration :  51   Loss :  0.440150820557
Iteration :  52   Loss :  0.232572446807
Iteration :  53   Loss :  0.122889565321
Iteration :  54   Loss :  0.0649339398201
Iteration :  55   Loss :  0.0343106148154
Iteration :  56   Loss :  0.018129475776
Iteration :  57   Loss :  0.00957948126788
Iteration :  58   Loss :  0.00506172723885
Iteration :  59   Loss :  3757.55419575
Iteration :  60   Loss :  1691.33774879
Iteration :  61   Loss :  238.501374583
Iteration :  62   Loss :  126.022367023
Iteration :  63   Loss :  66.5892891303
Iteration :  64   Loss :  35.1852891802
Iteration :  65   Loss :  18.5916472583
Iteration :  66   Loss :  9.82368926992
Iteration :  67   Loss :  5.19076494573
Iteration :  68   Loss :  2.74276190762
Iteration :  69   Loss :  1.44925515999
Iteration :  70   Loss :  0.765775736099
Iteration :  71   Loss :  0.404630250206
Iteration :  72   Loss :  0.213803639452
Iteration :  73   Loss :  0.112972265963
Iteration :  74   Loss :  0.0596937119941
Iteration :  75   Loss :  0.0315417170867
Iteration :  76   Loss :  0.016666410641
Iteration :  77   Loss :  0.00880640844282
Iteration :  78   Loss :  0.00465324126066
Iteration :  79   Loss :  6009.48756008
Iteration :  80   Loss :  1083.65936988
Iteration :  81   Loss :  454.331466076
Iteration :  82   Loss :  1072.55379313
Iteration :  83   Loss :  197.025775472
Iteration :  84   Loss :  104.106966398
Iteration :  85   Loss :  55.0093530991
Iteration :  86   Loss :  29.0665364008
Iteration :  87   Loss :  15.3585434247
Iteration :  88   Loss :  8.11534105323
Iteration :  89   Loss :  4.28808634967
Iteration :  90   Loss :  2.26579319607
Iteration :  91   Loss :  1.19722841117
Iteration :  92   Loss :  0.6326066611
Iteration :  93   Loss :  0.334264693299
Iteration :  94   Loss :  0.176622998234
Iteration :  95   Loss :  0.0933262894063
Iteration :  96   Loss :  0.0493129229004
Iteration :  97   Loss :  0.0260565847035
Iteration :  98   Loss :  0.0137681071508
Iteration :  99   Loss :  0.00727496625797
[ -8.40485110e-04   4.63848216e-05  -1.69513999e-04 ...,   1.72376674e-04
   2.71512138e-04   6.76289138e-06]
CROSS VALIDATION 15
Iteration :  0   Loss :  2418.057994
Iteration :  1   Loss :  420.744856843
Iteration :  2   Loss :  297.266924335
Iteration :  3   Loss :  157.073650027
Iteration :  4   Loss :  82.9965580198
Iteration :  5   Loss :  43.8547690331
Iteration :  6   Loss :  23.1725364622
Iteration :  7   Loss :  12.2441973343
Iteration :  8   Loss :  6.46974355204
Iteration :  9   Loss :  3.4185647688
Iteration :  10   Loss :  1.80634440677
Iteration :  11   Loss :  0.954459060029
Iteration :  12   Loss :  0.504329126747
Iteration :  13   Loss :  0.266483790387
Iteration :  14   Loss :  0.140808069121
Iteration :  15   Loss :  0.0744019450522
Iteration :  16   Loss :  0.0393134389392
Iteration :  17   Loss :  0.0207729311397
Iteration :  18   Loss :  0.235073716206
Iteration :  19   Loss :  1312.13179154
Iteration :  20   Loss :  2269.77379851
Iteration :  21   Loss :  1335.71097188
Iteration :  22   Loss :  177.010685605
Iteration :  23   Loss :  93.5311405536
Iteration :  24   Loss :  49.4211647356
Iteration :  25   Loss :  26.1137788908
Iteration :  26   Loss :  13.7983281375
Iteration :  27   Loss :  7.29093480439
Iteration :  28   Loss :  3.85247616902
Iteration :  29   Loss :  2.0356199899
Iteration :  30   Loss :  1.0756065869
Iteration :  31   Loss :  0.568342586301
Iteration :  32   Loss :  0.300308030219
Iteration :  33   Loss :  0.158680547944
Iteration :  34   Loss :  0.0838456310257
Iteration :  35   Loss :  0.0443034129463
Iteration :  36   Loss :  0.0234095965964
Iteration :  37   Loss :  0.0402622746984
Iteration :  38   Loss :  6132.72808773
Iteration :  39   Loss :  481.663804498
Iteration :  40   Loss :  254.507601301
Iteration :  41   Loss :  134.479939151
Iteration :  42   Loss :  71.0582078553
Iteration :  43   Loss :  37.5466328695
Iteration :  44   Loss :  19.8393638453
Iteration :  45   Loss :  10.4829735107
Iteration :  46   Loss :  5.53912587534
Iteration :  47   Loss :  2.92683325314
Iteration :  48   Loss :  1.54651710116
Iteration :  49   Loss :  8531.28559547
Iteration :  50   Loss :  1029.17506002
Iteration :  51   Loss :  1556.73098828
Iteration :  52   Loss :  254.239407013
Iteration :  53   Loss :  1390.58424339
Iteration :  54   Loss :  208.983755017
Iteration :  55   Loss :  110.425474581
Iteration :  56   Loss :  58.3480062145
Iteration :  57   Loss :  30.8306560793
Iteration :  58   Loss :  16.2906912498
Iteration :  59   Loss :  8.60788108804
Iteration :  60   Loss :  4.54834087084
Iteration :  61   Loss :  2.40330976529
Iteration :  62   Loss :  1.26989115195
Iteration :  63   Loss :  0.671001117332
Iteration :  64   Loss :  0.354552040755
Iteration :  65   Loss :  0.18734268298
Iteration :  66   Loss :  0.0989904917523
Iteration :  67   Loss :  0.052305845638
Iteration :  68   Loss :  0.0276380230008
Iteration :  69   Loss :  0.0146037274816
Iteration :  70   Loss :  0.128781801305
Iteration :  71   Loss :  3685.87276203
Iteration :  72   Loss :  1341.59121722
Iteration :  73   Loss :  278.588204918
Iteration :  74   Loss :  147.203952471
Iteration :  75   Loss :  77.7814826351
Iteration :  76   Loss :  41.0991616688
Iteration :  77   Loss :  21.7164938576
Iteration :  78   Loss :  11.4748351625
Iteration :  79   Loss :  6.0632182557
Iteration :  80   Loss :  3.20375980095
Iteration :  81   Loss :  1.69284304627
Iteration :  82   Loss :  0.894485778384
Iteration :  83   Loss :  0.472639687119
Iteration :  84   Loss :  0.249739324245
Iteration :  85   Loss :  0.131960416727
Iteration :  86   Loss :  0.214713351711
Iteration :  87   Loss :  1467.67514817
Iteration :  88   Loss :  385.001880954
Iteration :  89   Loss :  203.432153928
Iteration :  90   Loss :  107.492049517
Iteration :  91   Loss :  56.7980060491
Iteration :  92   Loss :  30.0116474256
Iteration :  93   Loss :  15.8579331186
Iteration :  94   Loss :  8.37921488373
Iteration :  95   Loss :  4.42751533524
Iteration :  96   Loss :  2.33946644355
Iteration :  97   Loss :  1.23615681169
Iteration :  98   Loss :  0.653176140777
Iteration :  99   Loss :  0.345133454629
[ -4.38321484e-03   1.25993814e-05   5.50023175e-04 ...,   1.44818508e-03
   1.82640698e-03  -2.66163045e-04]
CROSS VALIDATION 16
Iteration :  0   Loss :  2418.057994
Iteration :  1   Loss :  314.508150755
Iteration :  2   Loss :  166.183786888
Iteration :  3   Loss :  87.810287136
Iteration :  4   Loss :  46.3983079896
Iteration :  5   Loss :  24.5165236843
Iteration :  6   Loss :  12.9543502685
Iteration :  7   Loss :  6.84498312406
Iteration :  8   Loss :  3.6168385907
Iteration :  9   Loss :  1.9111108317
Iteration :  10   Loss :  1.00981686615
Iteration :  11   Loss :  0.533579783154
Iteration :  12   Loss :  0.281939621464
Iteration :  13   Loss :  0.148974816253
Iteration :  14   Loss :  0.0787171939943
Iteration :  15   Loss :  10475.8617403
Iteration :  16   Loss :  1007.98782619
Iteration :  17   Loss :  1577.30432106
Iteration :  18   Loss :  265.891503397
Iteration :  19   Loss :  140.495109045
Iteration :  20   Loss :  74.2365792564
Iteration :  21   Loss :  39.2260608725
Iteration :  22   Loss :  20.7267612138
Iteration :  23   Loss :  10.9518677343
Iteration :  24   Loss :  5.7868861243
Iteration :  25   Loss :  3.05774794108
Iteration :  26   Loss :  1.61569145657
Iteration :  27   Loss :  0.853719447497
Iteration :  28   Loss :  0.451099058591
Iteration :  29   Loss :  0.238357415025
Iteration :  30   Loss :  0.125946299854
Iteration :  31   Loss :  0.0665490957995
Iteration :  32   Loss :  0.0351640513207
Iteration :  33   Loss :  0.0185804253301
Iteration :  34   Loss :  0.00981775968579
Iteration :  35   Loss :  0.00518763179722
Iteration :  36   Loss :  0.00274110637506
Iteration :  37   Loss :  0.00144838038879
Iteration :  38   Loss :  4532.82940794
Iteration :  39   Loss :  2123.26041056
Iteration :  40   Loss :  755.639779157
Iteration :  41   Loss :  1472.04497554
Iteration :  42   Loss :  140.441843178
Iteration :  43   Loss :  74.2084339655
Iteration :  44   Loss :  39.2111891084
Iteration :  45   Loss :  20.7189030834
Iteration :  46   Loss :  10.9477155561
Iteration :  47   Loss :  5.78469214398
Iteration :  48   Loss :  3.05658865807
Iteration :  49   Loss :  1.61507889998
Iteration :  50   Loss :  0.853395777112
Iteration :  51   Loss :  0.450928033547
Iteration :  52   Loss :  0.238267046653
Iteration :  53   Loss :  0.125898549873
Iteration :  54   Loss :  0.0665238650618
Iteration :  55   Loss :  0.035150719585
Iteration :  56   Loss :  0.0185733809393
Iteration :  57   Loss :  0.00981403748166
Iteration :  58   Loss :  6131.18106788
Iteration :  59   Loss :  700.290076052
Iteration :  60   Loss :  282.244744341
Iteration :  61   Loss :  149.13604093
Iteration :  62   Loss :  78.8023839246
Iteration :  63   Loss :  41.6385983796
Iteration :  64   Loss :  22.0015282364
Iteration :  65   Loss :  11.6254452257
Iteration :  66   Loss :  6.14279950212
Iteration :  67   Loss :  3.24580994453
Iteration :  68   Loss :  1.71506203195
Iteration :  69   Loss :  0.90622612652
Iteration :  70   Loss :  0.478843200471
Iteration :  71   Loss :  0.253017214939
Iteration :  72   Loss :  0.133692429991
Iteration :  73   Loss :  0.0706420938244
Iteration :  74   Loss :  0.037326761285
Iteration :  75   Loss :  0.019723185322
Iteration :  76   Loss :  0.0104215856359
Iteration :  77   Loss :  0.00550668897515
Iteration :  78   Loss :  0.00530966250491
Iteration :  79   Loss :  1351.18402828
Iteration :  80   Loss :  1469.67670395
Iteration :  81   Loss :  219.956443817
Iteration :  82   Loss :  116.223362403
Iteration :  83   Loss :  61.4115673714
Iteration :  84   Loss :  32.4494191963
Iteration :  85   Loss :  17.14603374
Iteration :  86   Loss :  9.05983775042
Iteration :  87   Loss :  4.78715143738
Iteration :  88   Loss :  2.52949550706
Iteration :  89   Loss :  1.33656676709
Iteration :  90   Loss :  0.706232020541
Iteration :  91   Loss :  0.37316779013
Iteration :  92   Loss :  0.19717910763
Iteration :  93   Loss :  0.104187985979
Iteration :  94   Loss :  0.0550521632481
Iteration :  95   Loss :  0.0290891569677
Iteration :  96   Loss :  0.0153704959654
Iteration :  97   Loss :  0.00812165668758
Iteration :  98   Loss :  0.00429142348427
Iteration :  99   Loss :  0.0302542607147
[ -4.75000125e-04  -6.13256866e-05  -3.94736338e-04 ...,   2.21306821e-04
  -1.53685998e-04   5.99204892e-05]
CROSS VALIDATION 17
Iteration :  0   Loss :  2359.95699504
Iteration :  1   Loss :  422.603235922
Iteration :  2   Loss :  297.455729979
Iteration :  3   Loss :  157.173413537
Iteration :  4   Loss :  83.0492723224
Iteration :  5   Loss :  43.8826228818
Iteration :  6   Loss :  23.1872542303
Iteration :  7   Loss :  12.2519740944
Iteration :  8   Loss :  6.47385273473
Iteration :  9   Loss :  3.42073603062
Iteration :  10   Loss :  1.8074916855
Iteration :  11   Loss :  0.955065273647
Iteration :  12   Loss :  0.504649445551
Iteration :  13   Loss :  0.26665304448
Iteration :  14   Loss :  0.140897501736
Iteration :  15   Loss :  0.0744492005859
Iteration :  16   Loss :  0.0393384084147
Iteration :  17   Loss :  0.0207861248263
Iteration :  18   Loss :  0.241415202187
Iteration :  19   Loss :  4301.74806566
Iteration :  20   Loss :  1729.77118647
Iteration :  21   Loss :  380.981417364
Iteration :  22   Loss :  201.307770624
Iteration :  23   Loss :  106.36954105
Iteration :  24   Loss :  56.2048808552
Iteration :  25   Loss :  29.6982444483
Iteration :  26   Loss :  15.6923332972
Iteration :  27   Loss :  8.29171315966
Iteration :  28   Loss :  4.3812800697
Iteration :  29   Loss :  2.31503607029
Iteration :  30   Loss :  1.22324798267
Iteration :  31   Loss :  0.646355210746
Iteration :  32   Loss :  0.341529325515
Iteration :  33   Loss :  0.180461576309
Iteration :  34   Loss :  0.0953545657461
Iteration :  35   Loss :  0.0503846491568
Iteration :  36   Loss :  0.0266228769518
Iteration :  37   Loss :  0.0140673317975
Iteration :  38   Loss :  0.0884523822341
Iteration :  39   Loss :  5113.97316438
Iteration :  40   Loss :  1885.56484344
Iteration :  41   Loss :  1884.29451923
Iteration :  42   Loss :  314.345586688
Iteration :  43   Loss :  310.444841389
Iteration :  44   Loss :  131.854982167
Iteration :  45   Loss :  69.6711999478
Iteration :  46   Loss :  36.8137481225
Iteration :  47   Loss :  19.4521129511
Iteration :  48   Loss :  10.2783529947
Iteration :  49   Loss :  5.4310059041
Iteration :  50   Loss :  2.86970345788
Iteration :  51   Loss :  1.51633013876
Iteration :  52   Loss :  0.801217660104
Iteration :  53   Loss :  0.423357501413
Iteration :  54   Loss :  0.223698980848
Iteration :  55   Loss :  0.118200891363
Iteration :  56   Loss :  0.107807445283
Iteration :  57   Loss :  5949.82213168
Iteration :  58   Loss :  318.731024809
Iteration :  59   Loss :  168.415122388
Iteration :  60   Loss :  88.9893083548
Iteration :  61   Loss :  47.0212941046
Iteration :  62   Loss :  24.845704952
Iteration :  63   Loss :  13.1282872222
Iteration :  64   Loss :  6.93689012732
Iteration :  65   Loss :  3.74378070047
Iteration :  66   Loss :  7923.22769544
Iteration :  67   Loss :  470.642912367
Iteration :  68   Loss :  651.004137047
Iteration :  69   Loss :  301.18704643
Iteration :  70   Loss :  133.354606798
Iteration :  71   Loss :  70.4635905411
Iteration :  72   Loss :  37.2324414669
Iteration :  73   Loss :  19.6733474258
Iteration :  74   Loss :  10.3952516592
Iteration :  75   Loss :  5.49277429611
Iteration :  76   Loss :  2.90234142058
Iteration :  77   Loss :  1.5335757975
Iteration :  78   Loss :  0.810330138969
Iteration :  79   Loss :  0.428172468027
Iteration :  80   Loss :  0.226243173689
Iteration :  81   Loss :  0.119545224093
Iteration :  82   Loss :  0.0631668145848
Iteration :  83   Loss :  0.0333768788763
Iteration :  84   Loss :  6664.21373645
Iteration :  85   Loss :  4607.55897512
Iteration :  86   Loss :  481.04294375
Iteration :  87   Loss :  521.407181698
Iteration :  88   Loss :  579.131862516
Iteration :  89   Loss :  230.446480076
Iteration :  90   Loss :  121.766220183
Iteration :  91   Loss :  64.3403725354
Iteration :  92   Loss :  33.9969782406
Iteration :  93   Loss :  17.9637525234
Iteration :  94   Loss :  9.49191432366
Iteration :  95   Loss :  5.01545751147
Iteration :  96   Loss :  2.65013075251
Iteration :  97   Loss :  1.40030954092
Iteration :  98   Loss :  0.739913232033
Iteration :  99   Loss :  0.390964693832
[ -8.31340021e-03  -2.17812466e-03  -5.21519481e-04 ...,   2.39712615e-03
   1.70565543e-03  -1.94059139e-05]
CROSS VALIDATION 18
Iteration :  0   Loss :  1258.4744524
Iteration :  1   Loss :  2608.17567497
Iteration :  2   Loss :  260.551933391
Iteration :  3   Loss :  137.673719641
Iteration :  4   Loss :  72.7457779074
Iteration :  5   Loss :  38.4383324366
Iteration :  6   Loss :  20.310531319
Iteration :  7   Loss :  10.7319349282
Iteration :  8   Loss :  5.67067525188
Iteration :  9   Loss :  2.99634297333
Iteration :  10   Loss :  1.58324552457
Iteration :  11   Loss :  0.836575256362
Iteration :  12   Loss :  0.44204019446
Iteration :  13   Loss :  0.233570778041
Iteration :  14   Loss :  0.123417076181
Iteration :  15   Loss :  0.0652126726671
Iteration :  16   Loss :  0.0856580477155
Iteration :  17   Loss :  2019.57782037
Iteration :  18   Loss :  507.934528528
Iteration :  19   Loss :  308.662959848
Iteration :  20   Loss :  163.09523113
Iteration :  21   Loss :  86.1783170561
Iteration :  22   Loss :  45.5359870376
Iteration :  23   Loss :  24.0608796542
Iteration :  24   Loss :  12.7135913241
Iteration :  25   Loss :  6.71776787377
Iteration :  26   Loss :  3.54961899085
Iteration :  27   Loss :  1.87559249098
Iteration :  28   Loss :  0.991049236916
Iteration :  29   Loss :  0.523663106307
Iteration :  30   Loss :  0.276699722569
Iteration :  31   Loss :  0.146206092328
Iteration :  32   Loss :  0.0772542206963
Iteration :  33   Loss :  0.0408205603497
Iteration :  34   Loss :  0.0215692829757
Iteration :  35   Loss :  0.0113970500184
Iteration :  36   Loss :  0.00602211715934
Iteration :  37   Loss :  0.00318204228482
Iteration :  38   Loss :  1373.06338762
Iteration :  39   Loss :  1211.08610958
Iteration :  40   Loss :  249.585758297
Iteration :  41   Loss :  131.879273614
Iteration :  42   Loss :  69.6840353693
Iteration :  43   Loss :  36.8205302645
Iteration :  44   Loss :  19.4556965849
Iteration :  45   Loss :  10.2802465604
Iteration :  46   Loss :  5.43200645023
Iteration :  47   Loss :  2.87023213908
Iteration :  48   Loss :  1.51660949001
Iteration :  49   Loss :  0.801365267241
Iteration :  50   Loss :  0.423435495935
Iteration :  51   Loss :  0.223740192578
Iteration :  52   Loss :  0.118222667337
Iteration :  53   Loss :  0.0624679853509
Iteration :  54   Loss :  0.033007622664
Iteration :  55   Loss :  0.0174409843348
Iteration :  56   Loss :  0.00921568746897
Iteration :  57   Loss :  11061.2357751
Iteration :  58   Loss :  6006.13250033
Iteration :  59   Loss :  1809.95727827
Iteration :  60   Loss :  655.074603717
Iteration :  61   Loss :  462.801042304
Iteration :  62   Loss :  177.246598089
Iteration :  63   Loss :  93.6557949697
Iteration :  64   Loss :  49.4870312095
Iteration :  65   Loss :  26.1485822498
Iteration :  66   Loss :  13.8167179756
Iteration :  67   Loss :  7.30065185922
Iteration :  68   Loss :  3.85761058911
Iteration :  69   Loss :  2.0383329796
Iteration :  70   Loss :  1.07704011065
Iteration :  71   Loss :  0.569100049674
Iteration :  72   Loss :  0.300708268278
Iteration :  73   Loss :  0.158892030782
Iteration :  74   Loss :  0.0839573769973
Iteration :  75   Loss :  0.0443624586934
Iteration :  76   Loss :  0.0234407959337
Iteration :  77   Loss :  0.0123859436602
Iteration :  78   Loss :  0.00654464126506
Iteration :  79   Loss :  0.00345814016788
Iteration :  80   Loss :  0.00182725575572
Iteration :  81   Loss :  5254.69470948
Iteration :  82   Loss :  4851.90615902
Iteration :  83   Loss :  367.594590248
Iteration :  84   Loss :  194.234269924
Iteration :  85   Loss :  102.631955457
Iteration :  86   Loss :  54.2299681985
Iteration :  87   Loss :  28.6547151685
Iteration :  88   Loss :  15.1409401234
Iteration :  89   Loss :  8.00036107395
Iteration :  90   Loss :  4.22733177677
Iteration :  91   Loss :  2.23753505694
Iteration :  92   Loss :  5145.91696943
Iteration :  93   Loss :  2988.91395182
Iteration :  94   Loss :  1280.67793679
Iteration :  95   Loss :  408.001112922
Iteration :  96   Loss :  207.188103258
Iteration :  97   Loss :  109.476665437
Iteration :  98   Loss :  57.8466624619
Iteration :  99   Loss :  30.5657497386
[-0.02315695 -0.00657573 -0.01610858 ...,  0.03850332  0.00665006
  0.00484909]
CROSS VALIDATION 19
Iteration :  0   Loss :  2211.63236729
Iteration :  1   Loss :  1047.38507314
Iteration :  2   Loss :  282.357997028
Iteration :  3   Loss :  225.781752626
Iteration :  4   Loss :  119.301412607
Iteration :  5   Loss :  63.0379863943
Iteration :  6   Loss :  33.3088070107
Iteration :  7   Loss :  17.6001279219
Iteration :  8   Loss :  9.29977776647
Iteration :  9   Loss :  4.91393397194
Iteration :  10   Loss :  2.59648646311
Iteration :  11   Loss :  1.37196429411
Iteration :  12   Loss :  0.724935812707
Iteration :  13   Loss :  0.383050735942
Iteration :  14   Loss :  0.202401183296
Iteration :  15   Loss :  0.10694729224
Iteration :  16   Loss :  0.0565101603224
Iteration :  17   Loss :  0.0298595518668
Iteration :  18   Loss :  0.0157775669473
Iteration :  19   Loss :  0.00833674999164
Iteration :  20   Loss :  0.0044050771995
Iteration :  21   Loss :  0.15382330412
Iteration :  22   Loss :  6450.75491639
Iteration :  23   Loss :  1096.69877118
Iteration :  24   Loss :  460.145202502
Iteration :  25   Loss :  1087.847866
Iteration :  26   Loss :  197.196882535
Iteration :  27   Loss :  104.197378108
Iteration :  28   Loss :  55.0571259796
Iteration :  29   Loss :  29.0917792385
Iteration :  30   Loss :  15.3718815539
Iteration :  31   Loss :  8.12238882233
Iteration :  32   Loss :  4.29181033888
Iteration :  33   Loss :  2.2677609245
Iteration :  34   Loss :  1.19826814436
Iteration :  35   Loss :  0.633156048446
Iteration :  36   Loss :  0.33455498552
Iteration :  37   Loss :  0.176776386502
Iteration :  38   Loss :  0.0934073386357
Iteration :  39   Loss :  0.0493557487154
Iteration :  40   Loss :  0.0260792135483
Iteration :  41   Loss :  0.0137800640654
Iteration :  42   Loss :  0.00728128420346
Iteration :  43   Loss :  3751.80849761
Iteration :  44   Loss :  446.277433102
Iteration :  45   Loss :  235.809703683
Iteration :  46   Loss :  124.600107975
Iteration :  47   Loss :  65.8377779404
Iteration :  48   Loss :  34.788196211
Iteration :  49   Loss :  18.3818262626
Iteration :  50   Loss :  9.7128214035
Iteration :  51   Loss :  5.13218318293
Iteration :  52   Loss :  2.71180773628
Iteration :  53   Loss :  1.43289920419
Iteration :  54   Loss :  0.78565212129
Iteration :  55   Loss :  4378.9671084
Iteration :  56   Loss :  405.116228193
Iteration :  57   Loss :  214.060426635
Iteration :  58   Loss :  113.107950416
Iteration :  59   Loss :  59.7654066585
Iteration :  60   Loss :  31.5796000185
Iteration :  61   Loss :  16.6864277027
Iteration :  62   Loss :  8.81698531058
Iteration :  63   Loss :  4.65882999957
Iteration :  64   Loss :  2.46169140589
Iteration :  65   Loss :  1.30073958019
Iteration :  66   Loss :  0.687301199261
Iteration :  67   Loss :  0.363164883809
Iteration :  68   Loss :  0.191893645717
Iteration :  69   Loss :  0.101395186892
Iteration :  70   Loss :  0.250510513469
Iteration :  71   Loss :  1689.11509582
Iteration :  72   Loss :  644.122892664
Iteration :  73   Loss :  304.44182731
Iteration :  74   Loss :  160.864815834
Iteration :  75   Loss :  84.9997820664
Iteration :  76   Loss :  44.9132578425
Iteration :  77   Loss :  23.7318341411
Iteration :  78   Loss :  12.5397261022
Iteration :  79   Loss :  6.62589877303
Iteration :  80   Loss :  3.50107603569
Iteration :  81   Loss :  1.84994275155
Iteration :  82   Loss :  0.977496103807
Iteration :  83   Loss :  0.516501730747
Iteration :  84   Loss :  0.272915704549
Iteration :  85   Loss :  0.144206645119
Iteration :  86   Loss :  0.07619772754
Iteration :  87   Loss :  0.0402623171592
Iteration :  88   Loss :  6261.07381984
Iteration :  89   Loss :  1013.47997672
Iteration :  90   Loss :  468.926907487
Iteration :  91   Loss :  1051.40323138
Iteration :  92   Loss :  197.754253356
Iteration :  93   Loss :  104.491888738
Iteration :  94   Loss :  55.2127432241
Iteration :  95   Loss :  29.1740062425
Iteration :  96   Loss :  15.4153296963
Iteration :  97   Loss :  8.14534650029
Iteration :  98   Loss :  4.30394100656
Iteration :  99   Loss :  2.27417067982
[-0.0148703   0.00080781 -0.0030206  ...,  0.00304015  0.00479005
  0.00011881]
Accuracy (Hinge Loss):	0.9
lmda : 0.2  eta : 0.01
Logistic Loss
CROSS VALIDATION 0
Iteration :  0   Loss :  1283.86225402
Iteration :  1   Loss :  368.440960902
Iteration :  2   Loss :  252.351032376
Iteration :  3   Loss :  165.004184748
Iteration :  4   Loss :  107.890903904
Iteration :  5   Loss :  70.5463753124
Iteration :  6   Loss :  46.1279949431
Iteration :  7   Loss :  30.1616051576
Iteration :  8   Loss :  19.7216988687
Iteration :  9   Loss :  12.8953815367
Iteration :  10   Loss :  8.43187324197
Iteration :  11   Loss :  5.51332941692
Iteration :  12   Loss :  3.60498792939
Iteration :  13   Loss :  2.35718510328
Iteration :  14   Loss :  1.541287161
Iteration :  15   Loss :  1.00779786464
Iteration :  16   Loss :  0.658966482204
Iteration :  17   Loss :  0.430877746537
Iteration :  18   Loss :  0.28175098319
Iteration :  19   Loss :  0.184348275367
Iteration :  20   Loss :  0.120865568887
Iteration :  21   Loss :  0.0793240965508
Iteration :  22   Loss :  0.0522322240429
Iteration :  23   Loss :  0.0346232611742
Iteration :  24   Loss :  0.023239596866
Iteration :  25   Loss :  0.0157653375427
Iteration :  26   Loss :  0.0110947002512
Iteration :  27   Loss :  0.00800833666398
Iteration :  28   Loss :  0.00605716140961
Iteration :  29   Loss :  0.00479504032747
Iteration :  30   Loss :  0.00435857036019
Iteration :  31   Loss :  0.00352240600435
Iteration :  32   Loss :  0.00347580384057
[ -3.83022606e-04  -1.51492643e-04   1.32018716e-05 ...,   1.84424734e-04
   1.13148042e-04   3.19346417e-05]
CROSS VALIDATION 1
Iteration :  0   Loss :  1291.53788389
Iteration :  1   Loss :  299.999556555
Iteration :  2   Loss :  196.160014833
Iteration :  3   Loss :  128.262694323
Iteration :  4   Loss :  83.866830704
Iteration :  5   Loss :  54.8378102422
Iteration :  6   Loss :  35.8566719389
Iteration :  7   Loss :  23.4455199205
Iteration :  8   Loss :  15.3302799618
Iteration :  9   Loss :  10.0240262608
Iteration :  10   Loss :  6.55438678261
Iteration :  11   Loss :  4.28573859869
Iteration :  12   Loss :  2.80231077506
Iteration :  13   Loss :  1.83237337802
Iteration :  14   Loss :  1.19813814177
Iteration :  15   Loss :  0.783459239544
Iteration :  16   Loss :  0.51234439207
Iteration :  17   Loss :  0.33521833267
Iteration :  18   Loss :  0.219208859267
Iteration :  19   Loss :  0.143497344972
Iteration :  20   Loss :  0.0939277414869
Iteration :  21   Loss :  0.0616549206898
Iteration :  22   Loss :  0.0405853533493
Iteration :  23   Loss :  0.027042781968
Iteration :  24   Loss :  0.0183164542591
Iteration :  25   Loss :  0.0125172575679
Iteration :  26   Loss :  0.00884750889014
Iteration :  27   Loss :  0.00664562866461
Iteration :  28   Loss :  0.00511550886362
Iteration :  29   Loss :  0.00420740008343
Iteration :  30   Loss :  0.00415721166348
[ -3.33233175e-04  -1.27518424e-04  -7.18724158e-05 ...,   1.85745460e-04
   6.10348910e-05   6.43290337e-05]
CROSS VALIDATION 2
Iteration :  0   Loss :  5301.07655705
Iteration :  1   Loss :  1058.15415562
Iteration :  2   Loss :  1481.86233753
Iteration :  3   Loss :  362.583532123
Iteration :  4   Loss :  205.597761085
Iteration :  5   Loss :  134.433731594
Iteration :  6   Loss :  87.9018725439
Iteration :  7   Loss :  57.4761936987
Iteration :  8   Loss :  37.5818255799
Iteration :  9   Loss :  24.5735412008
Iteration :  10   Loss :  16.0678444388
Iteration :  11   Loss :  10.506244208
Iteration :  12   Loss :  6.86969355339
Iteration :  13   Loss :  4.4918706041
Iteration :  14   Loss :  2.93708902256
Iteration :  15   Loss :  1.92046759284
Iteration :  16   Loss :  1.25573169449
Iteration :  17   Loss :  0.821082373075
Iteration :  18   Loss :  0.536879228527
Iteration :  19   Loss :  0.351047977035
Iteration :  20   Loss :  0.229538930183
Iteration :  21   Loss :  0.150088214578
Iteration :  22   Loss :  0.0981403932826
Iteration :  23   Loss :  0.064192293657
Iteration :  24   Loss :  0.0420064322039
Iteration :  25   Loss :  0.0276475275509
Iteration :  26   Loss :  0.0184658580318
Iteration :  27   Loss :  0.0124443109808
Iteration :  28   Loss :  0.00874423838641
Iteration :  29   Loss :  0.00630545147571
Iteration :  30   Loss :  0.0046903554322
Iteration :  31   Loss :  0.00374965266775
Iteration :  32   Loss :  0.0036916034666
[ -5.60953469e-04  -7.37901738e-05  -7.51617567e-05 ...,   4.28615262e-04
   9.10946430e-05   4.34314132e-05]
CROSS VALIDATION 3
Iteration :  0   Loss :  1598.40195129
Iteration :  1   Loss :  994.42392267
Iteration :  2   Loss :  230.892804647
Iteration :  3   Loss :  150.973343109
Iteration :  4   Loss :  98.7165899971
Iteration :  5   Loss :  64.5475879381
Iteration :  6   Loss :  42.2055817441
Iteration :  7   Loss :  27.5968659289
Iteration :  8   Loss :  18.0446987727
Iteration :  9   Loss :  11.7988453701
Iteration :  10   Loss :  7.71488367983
Iteration :  11   Loss :  5.04451311346
Iteration :  12   Loss :  3.29844409947
Iteration :  13   Loss :  2.15674599959
Iteration :  14   Loss :  1.41022650876
Iteration :  15   Loss :  0.922101539275
Iteration :  16   Loss :  0.602932398227
Iteration :  17   Loss :  0.394238088663
Iteration :  18   Loss :  0.257781372673
Iteration :  19   Loss :  0.16857431357
Iteration :  20   Loss :  0.110277354333
Iteration :  21   Loss :  0.0722693859254
Iteration :  22   Loss :  0.0474500030592
Iteration :  23   Loss :  0.0313083697394
Iteration :  24   Loss :  0.0207856718146
Iteration :  25   Loss :  0.0141888353477
Iteration :  26   Loss :  0.00987871384701
Iteration :  27   Loss :  0.00731512773895
Iteration :  28   Loss :  0.00545540809785
Iteration :  29   Loss :  0.0044267826017
Iteration :  30   Loss :  0.00368740537285
Iteration :  31   Loss :  0.0037897802292
Iteration :  32   Loss :  0.00356515971908
Iteration :  33   Loss :  0.00325418971417
Iteration :  34   Loss :  0.00382595769357
Iteration :  35   Loss :  0.00347774612213
Iteration :  36   Loss :  0.00396311510028
Iteration :  37   Loss :  0.00393958143332
[ -3.66830846e-04  -2.11633590e-04  -5.47265812e-05 ...,   1.83086952e-04
   9.16742832e-05   3.10867079e-05]
CROSS VALIDATION 4
Iteration :  0   Loss :  1598.40195129
Iteration :  1   Loss :  994.42392267
Iteration :  2   Loss :  230.892804647
Iteration :  3   Loss :  150.973343109
Iteration :  4   Loss :  98.7165899971
Iteration :  5   Loss :  64.5475879381
Iteration :  6   Loss :  42.2055817441
Iteration :  7   Loss :  27.5968659289
Iteration :  8   Loss :  18.0446987727
Iteration :  9   Loss :  11.7988453701
Iteration :  10   Loss :  7.71488367983
Iteration :  11   Loss :  5.04451311346
Iteration :  12   Loss :  3.29844409947
Iteration :  13   Loss :  2.15674599959
Iteration :  14   Loss :  1.41022650876
Iteration :  15   Loss :  0.922101539275
Iteration :  16   Loss :  0.602932398227
Iteration :  17   Loss :  0.394238088663
Iteration :  18   Loss :  0.257781372673
Iteration :  19   Loss :  0.16857431357
Iteration :  20   Loss :  0.110277354337
Iteration :  21   Loss :  0.0722693865439
Iteration :  22   Loss :  0.047450038197
Iteration :  23   Loss :  0.0313092838026
Iteration :  24   Loss :  0.0207958686022
Iteration :  25   Loss :  0.0142472444661
Iteration :  26   Loss :  0.00992817879395
Iteration :  27   Loss :  0.00729406494925
Iteration :  28   Loss :  0.00543680240372
Iteration :  29   Loss :  0.0044188933061
Iteration :  30   Loss :  0.0036594457426
Iteration :  31   Loss :  0.00378589117754
Iteration :  32   Loss :  0.0034799678148
Iteration :  33   Loss :  0.00326752256262
Iteration :  34   Loss :  0.00370939975883
Iteration :  35   Loss :  0.00344827012488
Iteration :  36   Loss :  0.00382417618218
Iteration :  37   Loss :  0.00406565427276
Iteration :  38   Loss :  0.00407760789381
[ -3.63741417e-04  -2.17952806e-04  -7.01761817e-05 ...,   1.91933988e-04
   8.41344210e-05   3.54481836e-05]
CROSS VALIDATION 5
Iteration :  0   Loss :  1598.40195129
Iteration :  1   Loss :  994.42392267
Iteration :  2   Loss :  230.892804647
Iteration :  3   Loss :  150.973343109
Iteration :  4   Loss :  98.7165899971
Iteration :  5   Loss :  64.5475879381
Iteration :  6   Loss :  42.2055817441
Iteration :  7   Loss :  27.5968659289
Iteration :  8   Loss :  18.0446987727
Iteration :  9   Loss :  11.7988453701
Iteration :  10   Loss :  7.71488367983
Iteration :  11   Loss :  5.04451311346
Iteration :  12   Loss :  3.29844409947
Iteration :  13   Loss :  2.15674599959
Iteration :  14   Loss :  1.41022650876
Iteration :  15   Loss :  0.922101539275
Iteration :  16   Loss :  0.602932398197
Iteration :  17   Loss :  0.394238085549
Iteration :  18   Loss :  0.257781240359
Iteration :  19   Loss :  0.168571584971
Iteration :  20   Loss :  0.11024771837
Iteration :  21   Loss :  0.0721350886998
Iteration :  22   Loss :  0.0473384502477
Iteration :  23   Loss :  0.0311653989035
Iteration :  24   Loss :  0.0206775914338
Iteration :  25   Loss :  0.014108000842
Iteration :  26   Loss :  0.00989995135402
Iteration :  27   Loss :  0.00725396874493
Iteration :  28   Loss :  0.00543758004357
Iteration :  29   Loss :  0.00437513070156
Iteration :  30   Loss :  0.00367406620595
Iteration :  31   Loss :  0.00373382928764
[ -4.20895449e-04  -9.99944740e-05  -6.50343122e-05 ...,   2.63706682e-04
   5.23714377e-05   4.74109828e-05]
CROSS VALIDATION 6
Iteration :  0   Loss :  1598.40195129
Iteration :  1   Loss :  994.42392267
Iteration :  2   Loss :  230.892804647
Iteration :  3   Loss :  150.973343109
Iteration :  4   Loss :  98.7165899971
Iteration :  5   Loss :  64.5475879381
Iteration :  6   Loss :  42.2055817441
Iteration :  7   Loss :  27.5968659289
Iteration :  8   Loss :  18.0446987727
Iteration :  9   Loss :  11.7988453701
Iteration :  10   Loss :  7.71488367983
Iteration :  11   Loss :  5.04451311346
Iteration :  12   Loss :  3.29844409947
Iteration :  13   Loss :  2.15674599959
Iteration :  14   Loss :  1.41022650876
Iteration :  15   Loss :  0.922101539275
Iteration :  16   Loss :  0.602932398227
Iteration :  17   Loss :  0.394238088663
Iteration :  18   Loss :  0.257781372691
Iteration :  19   Loss :  0.168574313613
Iteration :  20   Loss :  0.11027728646
Iteration :  21   Loss :  0.072266584149
Iteration :  22   Loss :  0.0474451846811
Iteration :  23   Loss :  0.0313055788484
Iteration :  24   Loss :  0.0207915290259
Iteration :  25   Loss :  0.014252497425
Iteration :  26   Loss :  0.00995926888531
Iteration :  27   Loss :  0.00735883331725
Iteration :  28   Loss :  0.0054609021495
Iteration :  29   Loss :  0.00444150201485
Iteration :  30   Loss :  0.00369494545294
Iteration :  31   Loss :  0.00379672489128
Iteration :  32   Loss :  0.003490891894
Iteration :  33   Loss :  0.00332929492939
Iteration :  34   Loss :  0.0037083656278
Iteration :  35   Loss :  0.00353041080422
Iteration :  36   Loss :  0.00378944233512
Iteration :  37   Loss :  0.00423652021848
Iteration :  38   Loss :  0.00374573419575
Iteration :  39   Loss :  0.00407925769952
Iteration :  40   Loss :  0.00454175630637
Iteration :  41   Loss :  0.00361149177024
Iteration :  42   Loss :  0.00375527727021
Iteration :  43   Loss :  0.00457377199356
Iteration :  44   Loss :  0.00361098213735
Iteration :  45   Loss :  0.00400026615046
Iteration :  46   Loss :  0.00417924044584
Iteration :  47   Loss :  0.00394215756523
Iteration :  48   Loss :  0.00392702831924
[ -3.68446824e-04  -2.21665867e-04  -5.70273742e-05 ...,   1.72579227e-04
   9.31090929e-05   3.02134053e-05]
CROSS VALIDATION 7
Iteration :  0   Loss :  1598.40195129
Iteration :  1   Loss :  994.42392267
Iteration :  2   Loss :  230.892804647
Iteration :  3   Loss :  150.973343109
Iteration :  4   Loss :  98.7165899971
Iteration :  5   Loss :  64.5475879381
Iteration :  6   Loss :  42.2055817441
Iteration :  7   Loss :  27.5968659289
Iteration :  8   Loss :  18.0446987727
Iteration :  9   Loss :  11.7988453701
Iteration :  10   Loss :  7.71488367983
Iteration :  11   Loss :  5.04451311346
Iteration :  12   Loss :  3.29844409947
Iteration :  13   Loss :  2.15674599959
Iteration :  14   Loss :  1.41022650876
Iteration :  15   Loss :  0.922101539275
Iteration :  16   Loss :  0.602932398227
Iteration :  17   Loss :  0.394238088663
Iteration :  18   Loss :  0.257781372691
Iteration :  19   Loss :  0.168574313612
Iteration :  20   Loss :  0.110277286258
Iteration :  21   Loss :  0.0722665679088
Iteration :  22   Loss :  0.0474446169596
Iteration :  23   Loss :  0.0312951509845
Iteration :  24   Loss :  0.0207031111006
Iteration :  25   Loss :  0.0141372184097
Iteration :  26   Loss :  0.00989377468642
Iteration :  27   Loss :  0.00729876178564
Iteration :  28   Loss :  0.00543264376662
Iteration :  29   Loss :  0.00441711115016
Iteration :  30   Loss :  0.00365816365487
Iteration :  31   Loss :  0.00376847430379
Iteration :  32   Loss :  0.00352541211876
Iteration :  33   Loss :  0.00322146875662
Iteration :  34   Loss :  0.00375243988407
Iteration :  35   Loss :  0.00346237265257
Iteration :  36   Loss :  0.00387254239396
Iteration :  37   Loss :  0.00398081156239
Iteration :  38   Loss :  0.00425716030178
Iteration :  39   Loss :  0.0036957681742
Iteration :  40   Loss :  0.00404916404685
Iteration :  41   Loss :  0.00391594512489
Iteration :  42   Loss :  0.00417697052348
Iteration :  43   Loss :  0.00349126067369
Iteration :  44   Loss :  0.00417707607863
Iteration :  45   Loss :  0.00443920106427
Iteration :  46   Loss :  0.00359290167755
Iteration :  47   Loss :  0.00373414384956
Iteration :  48   Loss :  0.00442122722429
Iteration :  49   Loss :  0.00370380675697
Iteration :  50   Loss :  0.00404312019514
Iteration :  51   Loss :  0.00390837985424
Iteration :  52   Loss :  0.00376385661765
Iteration :  53   Loss :  0.00399581260275
Iteration :  54   Loss :  0.00410219600858
Iteration :  55   Loss :  0.00370847395967
Iteration :  56   Loss :  0.0039742432815
Iteration :  57   Loss :  0.00394180710227
[ -3.79627203e-04  -2.23882959e-04  -4.69496791e-05 ...,   1.71574643e-04
   8.75024683e-05   3.03535407e-05]
CROSS VALIDATION 8
Iteration :  0   Loss :  1598.40195129
Iteration :  1   Loss :  994.42392267
Iteration :  2   Loss :  230.892804647
Iteration :  3   Loss :  150.973343109
Iteration :  4   Loss :  98.7165899971
Iteration :  5   Loss :  64.5475879381
Iteration :  6   Loss :  42.2055817441
Iteration :  7   Loss :  27.5968659289
Iteration :  8   Loss :  18.0446987727
Iteration :  9   Loss :  11.7988453701
Iteration :  10   Loss :  7.71488367983
Iteration :  11   Loss :  5.04451311346
Iteration :  12   Loss :  3.29844409947
Iteration :  13   Loss :  2.15674599959
Iteration :  14   Loss :  1.41022650876
Iteration :  15   Loss :  0.922101539275
Iteration :  16   Loss :  0.602932398227
Iteration :  17   Loss :  0.394238088663
Iteration :  18   Loss :  0.257781372685
Iteration :  19   Loss :  0.168574312773
Iteration :  20   Loss :  0.110277244155
Iteration :  21   Loss :  0.0722656234852
Iteration :  22   Loss :  0.047433724774
Iteration :  23   Loss :  0.0312447099797
Iteration :  24   Loss :  0.0206842394795
Iteration :  25   Loss :  0.0141656971744
Iteration :  26   Loss :  0.00988743359638
Iteration :  27   Loss :  0.00734400272906
Iteration :  28   Loss :  0.00543597845157
Iteration :  29   Loss :  0.00444045735147
Iteration :  30   Loss :  0.00366172560283
Iteration :  31   Loss :  0.00377348811018
Iteration :  32   Loss :  0.0035042288377
Iteration :  33   Loss :  0.00326431335426
Iteration :  34   Loss :  0.00375900435257
Iteration :  35   Loss :  0.00352712208582
Iteration :  36   Loss :  0.00375936284835
Iteration :  37   Loss :  0.00415641040101
Iteration :  38   Loss :  0.00384042643567
Iteration :  39   Loss :  0.00385667570614
[ -3.51499524e-04  -2.09536269e-04  -3.74132195e-05 ...,   1.74532757e-04
   7.57411338e-05   2.95014039e-05]
CROSS VALIDATION 9
Iteration :  0   Loss :  2608.00963136
Iteration :  1   Loss :  1766.56362995
Iteration :  2   Loss :  278.382382097
Iteration :  3   Loss :  182.025243066
Iteration :  4   Loss :  119.020423863
Iteration :  5   Loss :  77.8236087362
Iteration :  6   Loss :  50.8863426979
Iteration :  7   Loss :  33.2729349771
Iteration :  8   Loss :  21.7560968876
Iteration :  9   Loss :  14.2256086549
Iteration :  10   Loss :  9.30166576511
Iteration :  11   Loss :  6.08205863839
Iteration :  12   Loss :  3.97686158825
Iteration :  13   Loss :  2.60034131321
Iteration :  14   Loss :  1.70028022527
Iteration :  15   Loss :  1.11177227482
Iteration :  16   Loss :  0.727020138974
Iteration :  17   Loss :  0.475398743297
Iteration :  18   Loss :  0.31095379525
Iteration :  19   Loss :  0.20337958897
Iteration :  20   Loss :  0.13309133216
Iteration :  21   Loss :  0.0873180244071
Iteration :  22   Loss :  0.0572236259464
Iteration :  23   Loss :  0.0377453042348
Iteration :  24   Loss :  0.0250409522327
Iteration :  25   Loss :  0.0169199798662
Iteration :  26   Loss :  0.0115551070371
Iteration :  27   Loss :  0.00824085792074
Iteration :  28   Loss :  0.00600707190074
Iteration :  29   Loss :  0.00469206447207
Iteration :  30   Loss :  0.00381225230946
Iteration :  31   Loss :  0.00361563363137
Iteration :  32   Loss :  0.003340306835
Iteration :  33   Loss :  0.00302351710655
Iteration :  34   Loss :  0.00325963295095
Iteration :  35   Loss :  0.00339286081656
Iteration :  36   Loss :  0.00353912720793
Iteration :  37   Loss :  0.00359407969138
[ -3.69621772e-04  -1.75796309e-04  -2.39177172e-05 ...,   1.75952184e-04
   9.81263096e-05   2.95212035e-05]
CROSS VALIDATION 10
Iteration :  0   Loss :  1596.20822899
Iteration :  1   Loss :  992.849822407
Iteration :  2   Loss :  231.048603747
Iteration :  3   Loss :  151.075215106
Iteration :  4   Loss :  98.7832008034
Iteration :  5   Loss :  64.5911425916
Iteration :  6   Loss :  42.2340607245
Iteration :  7   Loss :  27.6154874138
Iteration :  8   Loss :  18.0568747598
Iteration :  9   Loss :  11.8068068547
Iteration :  10   Loss :  7.72008943727
Iteration :  11   Loss :  5.04791699001
Iteration :  12   Loss :  3.30066978434
Iteration :  13   Loss :  2.158201303
Iteration :  14   Loss :  1.41117808463
Iteration :  15   Loss :  0.922723743977
Iteration :  16   Loss :  0.603339237292
Iteration :  17   Loss :  0.394504088095
Iteration :  18   Loss :  0.257954940768
Iteration :  19   Loss :  0.168685433711
Iteration :  20   Loss :  0.110351363741
Iteration :  21   Loss :  0.0722994260163
Iteration :  22   Loss :  0.0473969983429
Iteration :  23   Loss :  0.0312094915016
Iteration :  24   Loss :  0.0207124601437
Iteration :  25   Loss :  0.0141422016276
Iteration :  26   Loss :  0.00986003351429
Iteration :  27   Loss :  0.00724382834129
Iteration :  28   Loss :  0.00547752759777
Iteration :  29   Loss :  0.00436206978618
Iteration :  30   Loss :  0.00370176108068
Iteration :  31   Loss :  0.00373003208294
[ -4.31569896e-04  -9.68934596e-05  -7.23160988e-05 ...,   2.84965387e-04
   7.16174303e-05   4.66586571e-05]
CROSS VALIDATION 11
Iteration :  0   Loss :  1596.20822899
Iteration :  1   Loss :  992.849822407
Iteration :  2   Loss :  231.048603747
Iteration :  3   Loss :  151.075215106
Iteration :  4   Loss :  98.7832008034
Iteration :  5   Loss :  64.5911425916
Iteration :  6   Loss :  42.2340607245
Iteration :  7   Loss :  27.6154874138
Iteration :  8   Loss :  18.0568747598
Iteration :  9   Loss :  11.8068068547
Iteration :  10   Loss :  7.72008943727
Iteration :  11   Loss :  5.04791699001
Iteration :  12   Loss :  3.30066978434
Iteration :  13   Loss :  2.158201303
Iteration :  14   Loss :  1.41117808463
Iteration :  15   Loss :  0.922723743977
Iteration :  16   Loss :  0.603339237292
Iteration :  17   Loss :  0.394504088098
Iteration :  18   Loss :  0.257954941219
Iteration :  19   Loss :  0.168685461456
Iteration :  20   Loss :  0.110352133843
Iteration :  21   Loss :  0.0723106811657
Iteration :  22   Loss :  0.0474788269067
Iteration :  23   Loss :  0.0313138329135
Iteration :  24   Loss :  0.0208011012509
Iteration :  25   Loss :  0.0142399574942
Iteration :  26   Loss :  0.0098896630006
Iteration :  27   Loss :  0.00721886783946
Iteration :  28   Loss :  0.00549569186404
Iteration :  29   Loss :  0.00434181386251
Iteration :  30   Loss :  0.00369419571859
Iteration :  31   Loss :  0.00373633015342
[ -4.51389961e-04  -9.75255213e-05  -5.91215158e-05 ...,   2.88388056e-04
   8.20497396e-05   4.00485814e-05]
CROSS VALIDATION 12
Iteration :  0   Loss :  2073.29375552
Iteration :  1   Loss :  731.49399163
Iteration :  2   Loss :  209.963481206
Iteration :  3   Loss :  137.288334892
Iteration :  4   Loss :  89.7684053876
Iteration :  5   Loss :  58.6966592037
Iteration :  6   Loss :  38.3798485314
Iteration :  7   Loss :  25.0953426187
Iteration :  8   Loss :  16.4090335228
Iteration :  9   Loss :  10.7293367237
Iteration :  10   Loss :  7.01556653962
Iteration :  11   Loss :  4.58725223557
Iteration :  12   Loss :  2.99945598889
Iteration :  13   Loss :  1.96124733659
Iteration :  14   Loss :  1.28239625103
Iteration :  15   Loss :  0.838517464898
Iteration :  16   Loss :  0.548279475036
Iteration :  17   Loss :  0.358502392259
Iteration :  18   Loss :  0.234416786573
Iteration :  19   Loss :  0.15331850294
Iteration :  20   Loss :  0.100389273146
Iteration :  21   Loss :  0.0658270833941
Iteration :  22   Loss :  0.043326794905
Iteration :  23   Loss :  0.028584959998
Iteration :  24   Loss :  0.0192636753648
Iteration :  25   Loss :  0.0133078319166
Iteration :  26   Loss :  0.00943884646414
Iteration :  27   Loss :  0.00686153400536
Iteration :  28   Loss :  0.00530735337843
Iteration :  29   Loss :  0.00427066727036
Iteration :  30   Loss :  0.00368181756731
Iteration :  31   Loss :  0.00369064998519
[ -5.99825906e-04  -2.86044971e-04  -8.23499983e-05 ...,   2.13592623e-04
   7.74699859e-05   1.65486690e-05]
CROSS VALIDATION 13
Iteration :  0   Loss :  3108.12568635
Iteration :  1   Loss :  390.519876716
Iteration :  2   Loss :  255.348326807
Iteration :  3   Loss :  166.964018712
Iteration :  4   Loss :  109.172375997
Iteration :  5   Loss :  71.3842884998
Iteration :  6   Loss :  46.6758792972
Iteration :  7   Loss :  30.5198490306
Iteration :  8   Loss :  19.9559429597
Iteration :  9   Loss :  13.0485461777
Iteration :  10   Loss :  8.53202265082
Iteration :  11   Loss :  5.57881387881
Iteration :  12   Loss :  3.64780610479
Iteration :  13   Loss :  2.38518261782
Iteration :  14   Loss :  1.55959585998
Iteration :  15   Loss :  1.01979218052
Iteration :  16   Loss :  0.666898700472
Iteration :  17   Loss :  0.436061134842
Iteration :  18   Loss :  0.285188291596
Iteration :  19   Loss :  0.186488813581
Iteration :  20   Loss :  0.12205560994
Iteration :  21   Loss :  0.0799298964876
Iteration :  22   Loss :  0.0524394629563
Iteration :  23   Loss :  0.0344806444277
Iteration :  24   Loss :  0.022873748759
Iteration :  25   Loss :  0.0155916433033
Iteration :  26   Loss :  0.0106890707574
Iteration :  27   Loss :  0.00762997518966
Iteration :  28   Loss :  0.0057251459905
Iteration :  29   Loss :  0.00454942300058
Iteration :  30   Loss :  0.00415626973943
Iteration :  31   Loss :  0.00344659367273
Iteration :  32   Loss :  0.00329826893889
Iteration :  33   Loss :  0.00387308397267
Iteration :  34   Loss :  0.00303246072128
Iteration :  35   Loss :  0.0036930644553
Iteration :  36   Loss :  0.00440089947119
Iteration :  37   Loss :  0.00347759748888
Iteration :  38   Loss :  0.00401264431692
Iteration :  39   Loss :  0.00437068964648
Iteration :  40   Loss :  0.00374525653276
Iteration :  41   Loss :  0.00382702971668
[ -3.82013944e-04  -2.24517817e-04  -5.47660225e-05 ...,   1.72642913e-04
   9.12822690e-05   2.69118649e-05]
CROSS VALIDATION 14
Iteration :  0   Loss :  1598.65350771
Iteration :  1   Loss :  993.907689218
Iteration :  2   Loss :  230.696402751
Iteration :  3   Loss :  150.844922257
Iteration :  4   Loss :  98.632619752
Iteration :  5   Loss :  64.4926825085
Iteration :  6   Loss :  42.1696808581
Iteration :  7   Loss :  27.5733914997
Iteration :  8   Loss :  18.0293496021
Iteration :  9   Loss :  11.7888090435
Iteration :  10   Loss :  7.70832125021
Iteration :  11   Loss :  5.04022215281
Iteration :  12   Loss :  3.29563837897
Iteration :  13   Loss :  2.15491142963
Iteration :  14   Loss :  1.40902694276
Iteration :  15   Loss :  0.921317181998
Iteration :  16   Loss :  0.602419544048
Iteration :  17   Loss :  0.393903096697
Iteration :  18   Loss :  0.257567058225
Iteration :  19   Loss :  0.16844653282
Iteration :  20   Loss :  0.110179394254
Iteration :  21   Loss :  0.0722307237131
Iteration :  22   Loss :  0.0473906311501
Iteration :  23   Loss :  0.0313108025715
Iteration :  24   Loss :  0.0207514710484
Iteration :  25   Loss :  0.0142165112894
Iteration :  26   Loss :  0.00995315973018
Iteration :  27   Loss :  0.0072946511101
Iteration :  28   Loss :  0.00543806660874
Iteration :  29   Loss :  0.00442149227941
Iteration :  30   Loss :  0.00364606534668
Iteration :  31   Loss :  0.00376479771446
Iteration :  32   Loss :  0.00343412745582
Iteration :  33   Loss :  0.00324043183656
Iteration :  34   Loss :  0.00374113380401
Iteration :  35   Loss :  0.00350347656564
Iteration :  36   Loss :  0.00374223327758
Iteration :  37   Loss :  0.00417212709652
Iteration :  38   Loss :  0.00373587287338
Iteration :  39   Loss :  0.00400275935185
Iteration :  40   Loss :  0.00434301120333
Iteration :  41   Loss :  0.0034616051244
Iteration :  42   Loss :  0.00387051490852
Iteration :  43   Loss :  0.00435127210569
Iteration :  44   Loss :  0.00362631847687
Iteration :  45   Loss :  0.00388289658571
Iteration :  46   Loss :  0.0044376673885
Iteration :  47   Loss :  0.00364528524491
Iteration :  48   Loss :  0.00377892159534
Iteration :  49   Loss :  0.00397144628843
Iteration :  50   Loss :  0.00414906450033
Iteration :  51   Loss :  0.00372915984247
Iteration :  52   Loss :  0.00392256826792
Iteration :  53   Loss :  0.00398756020278
[ -3.24626377e-04  -1.94152633e-04  -5.06532450e-05 ...,   1.63292502e-04
   5.39368715e-05   3.18249589e-05]
CROSS VALIDATION 15
Iteration :  0   Loss :  1598.65350771
Iteration :  1   Loss :  993.920378713
Iteration :  2   Loss :  230.712495189
Iteration :  3   Loss :  150.855444582
Iteration :  4   Loss :  98.6394999602
Iteration :  5   Loss :  64.4971812543
Iteration :  6   Loss :  42.1726224426
Iteration :  7   Loss :  27.5753149068
Iteration :  8   Loss :  18.0306072558
Iteration :  9   Loss :  11.7896313827
Iteration :  10   Loss :  7.70885895118
Iteration :  11   Loss :  5.0405737381
Iteration :  12   Loss :  3.29586826923
Iteration :  13   Loss :  2.15506174744
Iteration :  14   Loss :  1.40912523071
Iteration :  15   Loss :  0.921381449269
Iteration :  16   Loss :  0.60246156392
Iteration :  17   Loss :  0.393930535305
Iteration :  18   Loss :  0.25758524192
Iteration :  19   Loss :  0.168473590141
Iteration :  20   Loss :  0.110273007569
Iteration :  21   Loss :  0.0723050552259
Iteration :  22   Loss :  0.0474821100721
Iteration :  23   Loss :  0.0313811502172
Iteration :  24   Loss :  0.0208423651216
Iteration :  25   Loss :  0.0142451997827
Iteration :  26   Loss :  0.00994458258227
Iteration :  27   Loss :  0.00731165865254
Iteration :  28   Loss :  0.00544734898077
Iteration :  29   Loss :  0.00442426705106
Iteration :  30   Loss :  0.00366105078715
Iteration :  31   Loss :  0.00376845376614
Iteration :  32   Loss :  0.00349808311577
Iteration :  33   Loss :  0.00325208394231
Iteration :  34   Loss :  0.00370694365712
Iteration :  35   Loss :  0.00348061683468
Iteration :  36   Loss :  0.00372534070514
Iteration :  37   Loss :  0.00417822264195
Iteration :  38   Loss :  0.00379796399295
Iteration :  39   Loss :  0.00388229531321
[ -3.65548710e-04  -1.91067784e-04  -6.43629964e-05 ...,   1.79634767e-04
   8.21474535e-05   2.92628690e-05]
CROSS VALIDATION 16
Iteration :  0   Loss :  2392.69976314
Iteration :  1   Loss :  290.290476973
Iteration :  2   Loss :  189.811561467
Iteration :  3   Loss :  124.111645832
Iteration :  4   Loss :  81.1525942468
Iteration :  5   Loss :  53.0630587392
Iteration :  6   Loss :  34.6962192508
Iteration :  7   Loss :  22.6867364774
Iteration :  8   Loss :  14.8341238071
Iteration :  9   Loss :  9.69955417636
Iteration :  10   Loss :  6.34222557147
Iteration :  11   Loss :  4.14698481616
Iteration :  12   Loss :  2.71164170273
Iteration :  13   Loss :  1.77312471094
Iteration :  14   Loss :  1.1593939464
Iteration :  15   Loss :  0.758152117329
Iteration :  16   Loss :  0.49574283904
Iteration :  17   Loss :  0.324207828982
Iteration :  18   Loss :  0.212025604263
Iteration :  19   Loss :  0.138804034535
Iteration :  20   Loss :  0.090896039293
Iteration :  21   Loss :  0.0597460278837
Iteration :  22   Loss :  0.0393984958311
Iteration :  23   Loss :  0.0262367598049
Iteration :  24   Loss :  0.0177902795239
Iteration :  25   Loss :  0.012231774879
Iteration :  26   Loss :  0.00876600193548
Iteration :  27   Loss :  0.0064863599406
Iteration :  28   Loss :  0.00503451367147
Iteration :  29   Loss :  0.00417460494566
Iteration :  30   Loss :  0.00407944936517
[ -4.21544778e-04  -7.38520290e-05  -1.35555661e-04 ...,   2.89503745e-04
   3.86033622e-05   6.33014325e-05]
CROSS VALIDATION 17
Iteration :  0   Loss :  1603.37826362
Iteration :  1   Loss :  995.105123408
Iteration :  2   Loss :  230.819111054
Iteration :  3   Loss :  150.925157251
Iteration :  4   Loss :  98.6850828213
Iteration :  5   Loss :  64.5269864141
Iteration :  6   Loss :  42.1921110734
Iteration :  7   Loss :  27.5880578926
Iteration :  8   Loss :  18.0389394824
Iteration :  9   Loss :  11.7950795564
Iteration :  10   Loss :  7.7124213359
Iteration :  11   Loss :  5.04290306633
Iteration :  12   Loss :  3.29739134169
Iteration :  13   Loss :  2.1560576353
Iteration :  14   Loss :  1.4097764096
Iteration :  15   Loss :  0.921807234045
Iteration :  16   Loss :  0.602739966817
Iteration :  17   Loss :  0.394112436594
Iteration :  18   Loss :  0.25770182257
Iteration :  19   Loss :  0.168532148819
Iteration :  20   Loss :  0.110239788041
Iteration :  21   Loss :  0.0722688876559
Iteration :  22   Loss :  0.0474602243894
Iteration :  23   Loss :  0.0314171787641
Iteration :  24   Loss :  0.0208322496276
Iteration :  25   Loss :  0.0143283473828
Iteration :  26   Loss :  0.0100037183624
Iteration :  27   Loss :  0.0073677914557
Iteration :  28   Loss :  0.00547155934867
Iteration :  29   Loss :  0.0044550270147
Iteration :  30   Loss :  0.0036939840617
Iteration :  31   Loss :  0.00380554719558
Iteration :  32   Loss :  0.00348006886291
Iteration :  33   Loss :  0.00329623437074
Iteration :  34   Loss :  0.00373495325414
Iteration :  35   Loss :  0.00350013992293
Iteration :  36   Loss :  0.00378874032299
Iteration :  37   Loss :  0.00425446636861
Iteration :  38   Loss :  0.0037219239235
Iteration :  39   Loss :  0.00404299948045
Iteration :  40   Loss :  0.00446010102972
Iteration :  41   Loss :  0.00373723868757
Iteration :  42   Loss :  0.00372988542124
[ -3.59186890e-04  -2.03824583e-04  -5.01323870e-05 ...,   1.84534839e-04
   8.70848364e-05   3.15876844e-05]
CROSS VALIDATION 18
Iteration :  0   Loss :  1804.44499688
Iteration :  1   Loss :  1476.36722769
Iteration :  2   Loss :  277.905774307
Iteration :  3   Loss :  209.96445055
Iteration :  4   Loss :  137.288968715
Iteration :  5   Loss :  89.7688198237
Iteration :  6   Loss :  58.6969301901
Iteration :  7   Loss :  38.3800257206
Iteration :  8   Loss :  25.0954584771
Iteration :  9   Loss :  16.4091092788
Iteration :  10   Loss :  10.7293862581
Iteration :  11   Loss :  7.01559892856
Iteration :  12   Loss :  4.58727341365
Iteration :  13   Loss :  2.99946983655
Iteration :  14   Loss :  1.96125639113
Iteration :  15   Loss :  1.28240217152
Iteration :  16   Loss :  0.838521338338
Iteration :  17   Loss :  0.548282105169
Iteration :  18   Loss :  0.358506107619
Iteration :  19   Loss :  0.234439709479
Iteration :  20   Loss :  0.153391378621
Iteration :  21   Loss :  0.100302417133
Iteration :  22   Loss :  0.065699523379
Iteration :  23   Loss :  0.0432124918322
Iteration :  24   Loss :  0.0284970668891
Iteration :  25   Loss :  0.0189005475268
Iteration :  26   Loss :  0.0128046315074
Iteration :  27   Loss :  0.00910380880031
Iteration :  28   Loss :  0.00677460818576
Iteration :  29   Loss :  0.00519157825393
Iteration :  30   Loss :  0.00431976302853
Iteration :  31   Loss :  0.00411801104161
Iteration :  32   Loss :  0.00405008773294
[ -4.79252218e-04  -2.94244994e-04  -1.35766703e-04 ...,   3.21937354e-04
   1.69086801e-04   3.26076788e-05]
CROSS VALIDATION 19
Iteration :  0   Loss :  3314.99561729
Iteration :  1   Loss :  291.058908379
Iteration :  2   Loss :  190.314014169
Iteration :  3   Loss :  124.440187734
Iteration :  4   Loss :  81.367456189
Iteration :  5   Loss :  53.2036293435
Iteration :  6   Loss :  34.7881269355
Iteration :  7   Loss :  22.7469083708
Iteration :  8   Loss :  14.8734888704
Iteration :  9   Loss :  9.7253434441
Iteration :  10   Loss :  6.35912609359
Iteration :  11   Loss :  4.1580659678
Iteration :  12   Loss :  2.71886941286
Iteration :  13   Loss :  1.77781645778
Iteration :  14   Loss :  1.16250251071
Iteration :  15   Loss :  0.760158595776
Iteration :  16   Loss :  0.497093342534
Iteration :  17   Loss :  0.325112433774
Iteration :  18   Loss :  0.212717754554
Iteration :  19   Loss :  0.139345404607
Iteration :  20   Loss :  0.0913823334025
Iteration :  21   Loss :  0.0601301727461
Iteration :  22   Loss :  0.0395170640134
Iteration :  23   Loss :  0.026591972569
Iteration :  24   Loss :  0.0178440126596
Iteration :  25   Loss :  0.0125043867679
Iteration :  26   Loss :  0.00888524403014
Iteration :  27   Loss :  0.00664873573461
Iteration :  28   Loss :  0.00529285493586
Iteration :  29   Loss :  0.00425364615554
Iteration :  30   Loss :  0.00416004206743
[ -2.86532523e-04  -1.83429622e-04  -5.34897474e-05 ...,   1.37718400e-04
   1.42449011e-04   4.74327141e-05]
Accuracy (Logistic Loss):	0.8
Hinge Loss
CROSS VALIDATION 0
Iteration :  0   Loss :  1390.4478102
Iteration :  1   Loss :  331.369683487
Iteration :  2   Loss :  216.671927034
Iteration :  3   Loss :  141.67477082
Iteration :  4   Loss :  92.6365540825
Iteration :  5   Loss :  60.5720489443
Iteration :  6   Loss :  39.6061052751
Iteration :  7   Loss :  25.897152274
Iteration :  8   Loss :  16.933310944
Iteration :  9   Loss :  11.0721447861
Iteration :  10   Loss :  7.23971765301
Iteration :  11   Loss :  4.73381740463
Iteration :  12   Loss :  3.09529021634
Iteration :  13   Loss :  2.02391024081
Iteration :  14   Loss :  1.323369499
Iteration :  15   Loss :  0.865308547569
Iteration :  16   Loss :  0.565797294755
Iteration :  17   Loss :  0.369956565957
Iteration :  18   Loss :  0.241902642454
Iteration :  19   Loss :  0.158172320243
Iteration :  20   Loss :  0.103423768492
Iteration :  21   Loss :  0.0676254598325
Iteration :  22   Loss :  0.0442181027072
Iteration :  23   Loss :  0.0289127883472
Iteration :  24   Loss :  0.0189051379148
Iteration :  25   Loss :  0.0123614587181
Iteration :  26   Loss :  0.00808275836587
Iteration :  27   Loss :  0.0902665391112
Iteration :  28   Loss :  1231.72106398
Iteration :  29   Loss :  1898.62141279
Iteration :  30   Loss :  306.761834612
Iteration :  31   Loss :  200.581649916
Iteration :  32   Loss :  131.153858607
Iteration :  33   Loss :  85.757269595
Iteration :  34   Loss :  56.0739071384
Iteration :  35   Loss :  36.6649157163
Iteration :  36   Loss :  23.9740034731
Iteration :  37   Loss :  15.6758260942
Iteration :  38   Loss :  10.2499160815
Iteration :  39   Loss :  6.70208887528
Iteration :  40   Loss :  4.38227932163
Iteration :  41   Loss :  2.86543082465
Iteration :  42   Loss :  1.87361261304
Iteration :  43   Loss :  1.22509473743
Iteration :  44   Loss :  0.801049856957
Iteration :  45   Loss :  0.523780613636
Iteration :  46   Loss :  0.342483216042
Iteration :  47   Loss :  0.223938706811
Iteration :  48   Loss :  0.146426283273
Iteration :  49   Loss :  0.0957434145191
Iteration :  50   Loss :  0.0626035245783
Iteration :  51   Loss :  0.0409344215402
Iteration :  52   Loss :  6310.72002179
Iteration :  53   Loss :  353.159780909
Iteration :  54   Loss :  1201.98255639
Iteration :  55   Loss :  182.509469924
Iteration :  56   Loss :  119.337044156
Iteration :  57   Loss :  78.0306365138
Iteration :  58   Loss :  51.021711471
Iteration :  59   Loss :  33.3614482431
Iteration :  60   Loss :  21.8139728518
Iteration :  61   Loss :  14.2634518774
Iteration :  62   Loss :  9.32641022534
Iteration :  63   Loss :  6.09823824126
Iteration :  64   Loss :  3.98744090691
Iteration :  65   Loss :  2.60725874541
Iteration :  66   Loss :  1.70480223386
Iteration :  67   Loss :  1.11471508599
Iteration :  68   Loss :  0.728876170064
Iteration :  69   Loss :  0.476588572241
Iteration :  70   Loss :  0.311625865297
Iteration :  71   Loss :  0.203762082388
Iteration :  72   Loss :  0.133233440618
Iteration :  73   Loss :  0.135852989663
Iteration :  74   Loss :  4074.52973429
Iteration :  75   Loss :  515.341194499
Iteration :  76   Loss :  254.486708475
Iteration :  77   Loss :  166.400634329
Iteration :  78   Loss :  108.803997156
Iteration :  79   Loss :  71.143417481
Iteration :  80   Loss :  46.5183815223
Iteration :  81   Loss :  30.4168663255
Iteration :  82   Loss :  19.888605897
Iteration :  83   Loss :  13.0045166485
Iteration :  84   Loss :  8.50323316462
Iteration :  85   Loss :  5.55998936417
Iteration :  86   Loss :  3.63549736097
Iteration :  87   Loss :  2.37713423461
Iteration :  88   Loss :  1.55433125327
Iteration :  89   Loss :  1.01632697461
Iteration :  90   Loss :  0.664543363678
Iteration :  91   Loss :  0.434523429212
Iteration :  92   Loss :  0.284120827705
Iteration :  93   Loss :  0.185777427197
Iteration :  94   Loss :  0.121473855806
Iteration :  95   Loss :  0.0794278285952
Iteration :  96   Loss :  0.0519352902195
Iteration :  97   Loss :  0.19213023217
Iteration :  98   Loss :  1203.30377752
Iteration :  99   Loss :  2097.92778303
[-0.12218713 -0.03318454 -0.11826002 ...,  0.10913529 -0.07270819
  0.02100178]
CROSS VALIDATION 1
Iteration :  0   Loss :  1366.19597189
Iteration :  1   Loss :  1498.41943886
Iteration :  2   Loss :  412.348743414
Iteration :  3   Loss :  197.740874492
Iteration :  4   Loss :  129.296367364
Iteration :  5   Loss :  84.5427161003
Iteration :  6   Loss :  55.2797498594
Iteration :  7   Loss :  36.1456419367
Iteration :  8   Loss :  23.6344671301
Iteration :  9   Loss :  15.4538142525
Iteration :  10   Loss :  10.104749713
Iteration :  11   Loss :  6.60716927834
Iteration :  12   Loss :  4.32021446474
Iteration :  13   Loss :  2.8248486205
Iteration :  14   Loss :  1.84707722126
Iteration :  15   Loss :  1.20774410229
Iteration :  16   Loss :  0.789704837371
Iteration :  17   Loss :  0.51636247197
Iteration :  18   Loss :  0.337632732941
Iteration :  19   Loss :  0.220767132667
Iteration :  20   Loss :  0.144352493437
Iteration :  21   Loss :  0.0943874303647
Iteration :  22   Loss :  0.0617168903615
Iteration :  23   Loss :  0.0403546800795
Iteration :  24   Loss :  0.0263866211467
Iteration :  25   Loss :  0.0172533588215
Iteration :  26   Loss :  0.0112814137501
Iteration :  27   Loss :  0.00737655186553
Iteration :  28   Loss :  0.00482328887408
Iteration :  29   Loss :  0.00315379271873
Iteration :  30   Loss :  12764.6717592
Iteration :  31   Loss :  4877.70370774
Iteration :  32   Loss :  1898.85699975
Iteration :  33   Loss :  334.653588141
Iteration :  34   Loss :  217.545395735
Iteration :  35   Loss :  142.245903775
Iteration :  36   Loss :  93.0099994641
Iteration :  37   Loss :  60.8162328106
Iteration :  38   Loss :  39.7657692139
Iteration :  39   Loss :  26.0015513637
Iteration :  40   Loss :  17.0015741348
Iteration :  41   Loss :  11.1167798805
Iteration :  42   Loss :  7.26890309874
Iteration :  43   Loss :  4.7529008244
Iteration :  44   Loss :  3.1077682478
Iteration :  45   Loss :  2.03206922233
Iteration :  46   Loss :  1.32870439334
Iteration :  47   Loss :  0.868796862565
Iteration :  48   Loss :  0.568078191198
Iteration :  49   Loss :  0.371447970428
Iteration :  50   Loss :  0.24287782364
Iteration :  51   Loss :  0.158809959704
Iteration :  52   Loss :  0.10384070033
Iteration :  53   Loss :  0.0678980780868
Iteration :  54   Loss :  0.0443963589734
Iteration :  55   Loss :  0.0290293443589
Iteration :  56   Loss :  0.0189813501241
Iteration :  57   Loss :  0.0124112914187
Iteration :  58   Loss :  0.00811534235836
Iteration :  59   Loss :  0.00530636010157
Iteration :  60   Loss :  0.00346965738279
Iteration :  61   Loss :  0.0374433867011
Iteration :  62   Loss :  3096.31174277
Iteration :  63   Loss :  1739.36713976
Iteration :  64   Loss :  250.614336711
Iteration :  65   Loss :  163.868615578
Iteration :  66   Loss :  107.148391923
Iteration :  67   Loss :  70.0608707238
Iteration :  68   Loss :  45.8105391829
Iteration :  69   Loss :  29.9540310953
Iteration :  70   Loss :  19.5859728976
Iteration :  71   Loss :  12.8066347105
Iteration :  72   Loss :  8.3738445604
Iteration :  73   Loss :  5.47538633737
Iteration :  74   Loss :  3.5801781759
Iteration :  75   Loss :  2.34096280726
Iteration :  76   Loss :  1.53067992589
Iteration :  77   Loss :  1.00086213599
Iteration :  78   Loss :  0.654431405485
Iteration :  79   Loss :  0.427911546541
Iteration :  80   Loss :  0.279797531306
Iteration :  81   Loss :  0.182950563399
Iteration :  82   Loss :  0.119625460925
Iteration :  83   Loss :  0.0782192229179
Iteration :  84   Loss :  0.0511450220261
Iteration :  85   Loss :  0.03344207703
Iteration :  86   Loss :  1639.59554677
Iteration :  87   Loss :  418.191518865
Iteration :  88   Loss :  300.27134656
Iteration :  89   Loss :  196.337729534
Iteration :  90   Loss :  128.378896222
Iteration :  91   Loss :  83.9428113709
Iteration :  92   Loss :  54.8874915441
Iteration :  93   Loss :  35.8891569011
Iteration :  94   Loss :  23.4667598543
Iteration :  95   Loss :  15.3441558847
Iteration :  96   Loss :  10.0330476501
Iteration :  97   Loss :  6.56028561667
Iteration :  98   Loss :  4.28955875353
Iteration :  99   Loss :  2.80480384166
[-0.01350665 -0.01183155 -0.00790965 ...,  0.00809119  0.0056557
  0.00256613]
CROSS VALIDATION 2
Iteration :  0   Loss :  2176.62356176
Iteration :  1   Loss :  308.223351601
Iteration :  2   Loss :  201.537288642
Iteration :  3   Loss :  131.778719887
Iteration :  4   Loss :  86.165846192
Iteration :  5   Loss :  56.341062171
Iteration :  6   Loss :  36.8395997584
Iteration :  7   Loss :  24.0882237229
Iteration :  8   Loss :  15.7505110242
Iteration :  9   Loss :  10.2987501436
Iteration :  10   Loss :  6.73401989032
Iteration :  11   Loss :  4.40315798044
Iteration :  12   Loss :  2.8790827049
Iteration :  13   Loss :  1.88253913634
Iteration :  14   Loss :  1.23093150253
Iteration :  15   Loss :  0.804866329031
Iteration :  16   Loss :  0.526276081386
Iteration :  17   Loss :  0.402964132915
Iteration :  18   Loss :  7579.86223257
Iteration :  19   Loss :  1156.24339832
Iteration :  20   Loss :  228.900945095
Iteration :  21   Loss :  149.67093052
Iteration :  22   Loss :  97.8649844955
Iteration :  23   Loss :  63.9907506222
Iteration :  24   Loss :  41.8414838188
Iteration :  25   Loss :  27.3587940622
Iteration :  26   Loss :  17.8890312729
Iteration :  27   Loss :  11.6970594228
Iteration :  28   Loss :  7.64832913833
Iteration :  29   Loss :  5.00099524963
Iteration :  30   Loss :  3.2699891747
Iteration :  31   Loss :  2.13814024387
Iteration :  32   Loss :  1.39806080639
Iteration :  33   Loss :  0.914146779642
Iteration :  34   Loss :  0.597731036384
Iteration :  35   Loss :  0.390837007594
Iteration :  36   Loss :  0.255555688439
Iteration :  37   Loss :  0.167099605782
Iteration :  38   Loss :  0.109261032
Iteration :  39   Loss :  0.0714422577951
Iteration :  40   Loss :  0.120374430334
Iteration :  41   Loss :  2520.86579149
Iteration :  42   Loss :  509.028879694
Iteration :  43   Loss :  342.069738892
Iteration :  44   Loss :  181.594487185
Iteration :  45   Loss :  118.738766513
Iteration :  46   Loss :  77.6394420979
Iteration :  47   Loss :  50.7659220849
Iteration :  48   Loss :  33.19419583
Iteration :  49   Loss :  21.704611904
Iteration :  50   Loss :  14.1919442879
Iteration :  51   Loss :  9.2796537235
Iteration :  52   Loss :  6.06766567575
Iteration :  53   Loss :  3.96745049435
Iteration :  54   Loss :  2.59418766067
Iteration :  55   Loss :  1.69625547398
Iteration :  56   Loss :  1.10912663591
Iteration :  57   Loss :  0.725222063161
Iteration :  58   Loss :  0.474199269828
Iteration :  59   Loss :  0.310063577665
Iteration :  60   Loss :  0.202740552994
Iteration :  61   Loss :  0.252757125356
Iteration :  62   Loss :  3122.96423984
Iteration :  63   Loss :  260.135377614
Iteration :  64   Loss :  170.094116529
Iteration :  65   Loss :  111.219045803
Iteration :  66   Loss :  72.722539743
Iteration :  67   Loss :  47.5509185364
Iteration :  68   Loss :  31.0920089101
Iteration :  69   Loss :  20.3300598142
Iteration :  70   Loss :  13.2931690984
Iteration :  71   Loss :  8.69197367309
Iteration :  72   Loss :  5.68340068307
Iteration :  73   Loss :  3.71619203407
Iteration :  74   Loss :  2.42989787351
Iteration :  75   Loss :  1.58883169157
Iteration :  76   Loss :  1.03888569625
Iteration :  77   Loss :  0.679293782717
Iteration :  78   Loss :  0.444168251527
Iteration :  79   Loss :  0.290427265322
Iteration :  80   Loss :  0.189901002947
Iteration :  81   Loss :  0.124170128725
Iteration :  82   Loss :  0.0811908343209
Iteration :  83   Loss :  0.0530880626878
Iteration :  84   Loss :  0.0938281284367
Iteration :  85   Loss :  2768.51117779
Iteration :  86   Loss :  528.747055584
Iteration :  87   Loss :  258.32114629
Iteration :  88   Loss :  168.907849298
Iteration :  89   Loss :  110.443383997
Iteration :  90   Loss :  72.2153595557
Iteration :  91   Loss :  47.2192898026
Iteration :  92   Loss :  30.8751675985
Iteration :  93   Loss :  20.1882742883
Iteration :  94   Loss :  13.2004601251
Iteration :  95   Loss :  8.63135427158
Iteration :  96   Loss :  5.64376361547
Iteration :  97   Loss :  3.69027463654
Iteration :  98   Loss :  2.41295132484
Iteration :  99   Loss :  1.57775089106
[-0.01358105 -0.00496276 -0.00416837 ...,  0.00872555  0.00178525
  0.00098704]
CROSS VALIDATION 3
Iteration :  0   Loss :  2145.41706173
Iteration :  1   Loss :  358.657368242
Iteration :  2   Loss :  234.514462229
Iteration :  3   Loss :  153.34142796
Iteration :  4   Loss :  100.265004151
Iteration :  5   Loss :  65.5600459124
Iteration :  6   Loss :  42.8675952936
Iteration :  7   Loss :  28.0297351943
Iteration :  8   Loss :  18.3277379961
Iteration :  9   Loss :  11.9839155713
Iteration :  10   Loss :  7.83589510337
Iteration :  11   Loss :  5.12363857254
Iteration :  12   Loss :  3.35018168004
Iteration :  13   Loss :  2.1905755315
Iteration :  14   Loss :  1.4323465464
Iteration :  15   Loss :  0.936565117009
Iteration :  16   Loss :  0.675844979715
Iteration :  17   Loss :  1722.10279648
Iteration :  18   Loss :  1523.5269901
Iteration :  19   Loss :  569.59039919
Iteration :  20   Loss :  1196.24043385
Iteration :  21   Loss :  170.402703173
Iteration :  22   Loss :  111.420820637
Iteration :  23   Loss :  72.8544737859
Iteration :  24   Loss :  47.6371859433
Iteration :  25   Loss :  31.1484163795
Iteration :  26   Loss :  20.3669428355
Iteration :  27   Loss :  13.317285714
Iteration :  28   Loss :  8.70774274869
Iteration :  29   Loss :  5.69371157199
Iteration :  30   Loss :  3.72293399111
Iteration :  31   Loss :  2.43430622134
Iteration :  32   Loss :  1.59171416775
Iteration :  33   Loss :  1.04077045427
Iteration :  34   Loss :  0.680526165076
Iteration :  35   Loss :  0.444974066524
Iteration :  36   Loss :  0.290954161706
Iteration :  37   Loss :  0.190245523465
Iteration :  38   Loss :  0.124395399558
Iteration :  39   Loss :  0.0813381316382
Iteration :  40   Loss :  0.0531843756434
Iteration :  41   Loss :  0.0347755444538
Iteration :  42   Loss :  0.022738604664
Iteration :  43   Loss :  0.130391682053
Iteration :  44   Loss :  1248.93120992
Iteration :  45   Loss :  2123.58103555
Iteration :  46   Loss :  1034.29694601
Iteration :  47   Loss :  203.128234583
Iteration :  48   Loss :  132.818988022
Iteration :  49   Loss :  86.8460439068
Iteration :  50   Loss :  56.7858214745
Iteration :  51   Loss :  37.1304134935
Iteration :  52   Loss :  24.2783774259
Iteration :  53   Loss :  15.8748463854
Iteration :  54   Loss :  10.3800490181
Iteration :  55   Loss :  6.78717859697
Iteration :  56   Loss :  4.43791674077
Iteration :  57   Loss :  2.90181033497
Iteration :  58   Loss :  1.89739999914
Iteration :  59   Loss :  1.24064854045
Iteration :  60   Loss :  0.811219986099
Iteration :  61   Loss :  0.53043053241
Iteration :  62   Loss :  0.346831383021
Iteration :  63   Loss :  0.226781832678
Iteration :  64   Loss :  0.148285311394
Iteration :  65   Loss :  0.0969589729288
Iteration :  66   Loss :  0.0633983389387
Iteration :  67   Loss :  0.0414541249641
Iteration :  68   Loss :  0.0271055126256
Iteration :  69   Loss :  0.0177234187269
Iteration :  70   Loss :  0.0115887707311
Iteration :  71   Loss :  0.00757752266244
Iteration :  72   Loss :  0.00495469718333
Iteration :  73   Loss :  4829.6837409
Iteration :  74   Loss :  1719.02964496
Iteration :  75   Loss :  309.068929221
Iteration :  76   Loss :  251.347087672
Iteration :  77   Loss :  164.34773775
Iteration :  78   Loss :  107.461674428
Iteration :  79   Loss :  70.265716029
Iteration :  80   Loss :  45.9444809079
Iteration :  81   Loss :  30.0416112607
Iteration :  82   Loss :  19.6432387374
Iteration :  83   Loss :  12.8440789925
Iteration :  84   Loss :  8.39832816625
Iteration :  85   Loss :  5.49139537599
Iteration :  86   Loss :  3.59064596888
Iteration :  87   Loss :  2.3478073588
Iteration :  88   Loss :  1.53515535695
Iteration :  89   Loss :  1.00378847572
Iteration :  90   Loss :  0.656344844464
Iteration :  91   Loss :  0.429162682453
Iteration :  92   Loss :  0.280615608645
Iteration :  93   Loss :  0.183485477733
Iteration :  94   Loss :  0.119975224121
Iteration :  95   Loss :  0.0784479217683
Iteration :  96   Loss :  0.0512945608134
Iteration :  97   Loss :  0.0335398556103
Iteration :  98   Loss :  0.0219306276635
Iteration :  99   Loss :  0.0143397286889
[ -1.44413445e-03   1.02103217e-05  -9.00977092e-04 ...,   8.24568892e-04
   3.62155784e-05  -6.74689026e-05]
CROSS VALIDATION 4
Iteration :  0   Loss :  2145.41706173
Iteration :  1   Loss :  358.657368242
Iteration :  2   Loss :  234.514462229
Iteration :  3   Loss :  153.34142796
Iteration :  4   Loss :  100.265004151
Iteration :  5   Loss :  65.5600459124
Iteration :  6   Loss :  42.8675952936
Iteration :  7   Loss :  28.0297351943
Iteration :  8   Loss :  18.3277379961
Iteration :  9   Loss :  11.9839155713
Iteration :  10   Loss :  7.83589510337
Iteration :  11   Loss :  5.12363857254
Iteration :  12   Loss :  3.35018168004
Iteration :  13   Loss :  2.1905755315
Iteration :  14   Loss :  1.4323465464
Iteration :  15   Loss :  0.936565117009
Iteration :  16   Loss :  0.675844979715
Iteration :  17   Loss :  1722.10279648
Iteration :  18   Loss :  1523.5269901
Iteration :  19   Loss :  569.59039919
Iteration :  20   Loss :  1196.24043385
Iteration :  21   Loss :  170.402703173
Iteration :  22   Loss :  111.420820637
Iteration :  23   Loss :  72.8544737859
Iteration :  24   Loss :  47.6371859433
Iteration :  25   Loss :  31.1484163795
Iteration :  26   Loss :  20.3669428355
Iteration :  27   Loss :  13.317285714
Iteration :  28   Loss :  8.70774274869
Iteration :  29   Loss :  5.69371157199
Iteration :  30   Loss :  3.72293399111
Iteration :  31   Loss :  2.43430622134
Iteration :  32   Loss :  1.59171416775
Iteration :  33   Loss :  1.04077045427
Iteration :  34   Loss :  0.680526165076
Iteration :  35   Loss :  0.444974066524
Iteration :  36   Loss :  0.290954161706
Iteration :  37   Loss :  0.190245523465
Iteration :  38   Loss :  0.124395399558
Iteration :  39   Loss :  0.0813381316382
Iteration :  40   Loss :  0.0531843756434
Iteration :  41   Loss :  0.0347755444538
Iteration :  42   Loss :  0.022738604664
Iteration :  43   Loss :  0.130391682053
Iteration :  44   Loss :  1248.93120992
Iteration :  45   Loss :  2123.58103555
Iteration :  46   Loss :  1034.29694601
Iteration :  47   Loss :  203.128234583
Iteration :  48   Loss :  132.818988022
Iteration :  49   Loss :  86.8460439068
Iteration :  50   Loss :  56.7858214745
Iteration :  51   Loss :  37.1304134935
Iteration :  52   Loss :  24.2783774259
Iteration :  53   Loss :  15.8748463854
Iteration :  54   Loss :  10.3800490181
Iteration :  55   Loss :  6.78717859697
Iteration :  56   Loss :  4.43791674077
Iteration :  57   Loss :  2.90181033497
Iteration :  58   Loss :  1.89739999914
Iteration :  59   Loss :  1.24064854045
Iteration :  60   Loss :  0.811219986099
Iteration :  61   Loss :  0.53043053241
Iteration :  62   Loss :  0.346831383021
Iteration :  63   Loss :  0.226781832678
Iteration :  64   Loss :  0.148285311394
Iteration :  65   Loss :  0.0969589729288
Iteration :  66   Loss :  0.0633983389387
Iteration :  67   Loss :  0.0414541249641
Iteration :  68   Loss :  0.0271055126256
Iteration :  69   Loss :  0.0177234187269
Iteration :  70   Loss :  0.0115887707311
Iteration :  71   Loss :  0.00757752266244
Iteration :  72   Loss :  0.00495469718333
Iteration :  73   Loss :  4829.6837409
Iteration :  74   Loss :  1719.02964496
Iteration :  75   Loss :  309.068929221
Iteration :  76   Loss :  251.347087672
Iteration :  77   Loss :  164.34773775
Iteration :  78   Loss :  107.461674428
Iteration :  79   Loss :  70.265716029
Iteration :  80   Loss :  45.9444809079
Iteration :  81   Loss :  30.0416112607
Iteration :  82   Loss :  19.6432387374
Iteration :  83   Loss :  12.8440789925
Iteration :  84   Loss :  8.39832816625
Iteration :  85   Loss :  5.49139537599
Iteration :  86   Loss :  3.59064596888
Iteration :  87   Loss :  2.3478073588
Iteration :  88   Loss :  1.53515535695
Iteration :  89   Loss :  1.00378847572
Iteration :  90   Loss :  0.656344844464
Iteration :  91   Loss :  0.429162682453
Iteration :  92   Loss :  0.280615608645
Iteration :  93   Loss :  0.183485477733
Iteration :  94   Loss :  0.119975224121
Iteration :  95   Loss :  0.0784479217683
Iteration :  96   Loss :  0.0512945608134
Iteration :  97   Loss :  0.0335398556103
Iteration :  98   Loss :  0.0219306276635
Iteration :  99   Loss :  0.0143397286889
[ -1.44413445e-03   1.02103217e-05  -9.00977092e-04 ...,   8.24568892e-04
   3.62155784e-05  -6.74689026e-05]
CROSS VALIDATION 5
Iteration :  0   Loss :  2073.63983073
Iteration :  1   Loss :  792.328193425
Iteration :  2   Loss :  243.750267518
Iteration :  3   Loss :  159.380422561
Iteration :  4   Loss :  104.213707556
Iteration :  5   Loss :  68.1419754574
Iteration :  6   Loss :  44.5558355818
Iteration :  7   Loss :  29.1336209593
Iteration :  8   Loss :  19.0495332232
Iteration :  9   Loss :  12.4558741437
Iteration :  10   Loss :  8.14449356143
Iteration :  11   Loss :  5.32542113117
Iteration :  12   Loss :  3.48212077404
Iteration :  13   Loss :  2.27684624114
Iteration :  14   Loss :  1.48875617539
Iteration :  15   Loss :  0.973449550397
Iteration :  16   Loss :  0.636507201671
Iteration :  17   Loss :  0.416191488931
Iteration :  18   Loss :  0.272134164396
Iteration :  19   Loss :  0.177939735437
Iteration :  20   Loss :  0.116349042457
Iteration :  21   Loss :  0.076076878767
Iteration :  22   Loss :  0.0497442124207
Iteration :  23   Loss :  0.0325261328996
Iteration :  24   Loss :  0.0212677871438
Iteration :  25   Loss :  0.0139063186943
Iteration :  26   Loss :  0.00909289237848
Iteration :  27   Loss :  0.00594554846788
Iteration :  28   Loss :  0.0512524596794
Iteration :  29   Loss :  6229.61141269
Iteration :  30   Loss :  339.069958642
Iteration :  31   Loss :  657.761691813
Iteration :  32   Loss :  925.572760641
Iteration :  33   Loss :  192.840995899
Iteration :  34   Loss :  126.092495104
Iteration :  35   Loss :  82.44780757
Iteration :  36   Loss :  53.9099568733
Iteration :  37   Loss :  35.2499785712
Iteration :  38   Loss :  23.0488218009
Iteration :  39   Loss :  15.0708796982
Iteration :  40   Loss :  9.8543611834
Iteration :  41   Loss :  6.44344831074
Iteration :  42   Loss :  4.21316261506
Iteration :  43   Loss :  2.75485087563
Iteration :  44   Loss :  1.80130795803
Iteration :  45   Loss :  1.17781706021
Iteration :  46   Loss :  0.770136511717
Iteration :  47   Loss :  0.503567376222
Iteration :  48   Loss :  0.329266433336
Iteration :  49   Loss :  0.215296679732
Iteration :  50   Loss :  0.140775541053
Iteration :  51   Loss :  0.0920485767987
Iteration :  52   Loss :  0.0601875896003
Iteration :  53   Loss :  0.0393547197348
Iteration :  54   Loss :  0.0257327793933
Iteration :  55   Loss :  0.0168258328294
Iteration :  56   Loss :  0.0110018683204
Iteration :  57   Loss :  9935.57374047
Iteration :  58   Loss :  2350.98863769
Iteration :  59   Loss :  1245.7494997
Iteration :  60   Loss :  236.897479373
Iteration :  61   Loss :  154.899605857
Iteration :  62   Loss :  101.283846321
Iteration :  63   Loss :  66.2262338816
Iteration :  64   Loss :  43.3031940772
Iteration :  65   Loss :  28.3145591615
Iteration :  66   Loss :  18.5139751835
Iteration :  67   Loss :  12.1056900494
Iteration :  68   Loss :  7.91551949919
Iteration :  69   Loss :  5.17570239171
Iteration :  70   Loss :  3.38422452883
Iteration :  71   Loss :  2.21283505015
Iteration :  72   Loss :  1.44690132627
Iteration :  73   Loss :  0.946082017198
Iteration :  74   Loss :  0.618612456159
Iteration :  75   Loss :  0.404490693152
Iteration :  76   Loss :  0.264483392175
Iteration :  77   Loss :  0.305127040042
Iteration :  78   Loss :  2023.12518063
Iteration :  79   Loss :  2868.56087535
Iteration :  80   Loss :  590.614800392
Iteration :  81   Loss :  190.996529142
Iteration :  82   Loss :  124.8864579
Iteration :  83   Loss :  81.6592188181
Iteration :  84   Loss :  53.3943241736
Iteration :  85   Loss :  34.9128230128
Iteration :  86   Loss :  22.8283666773
Iteration :  87   Loss :  14.9267312174
Iteration :  88   Loss :  9.76010715028
Iteration :  89   Loss :  6.38181864453
Iteration :  90   Loss :  4.17286496803
Iteration :  91   Loss :  2.72850154656
Iteration :  92   Loss :  1.78407898329
Iteration :  93   Loss :  1.16655159042
Iteration :  94   Loss :  0.762770385092
Iteration :  95   Loss :  0.498750904076
Iteration :  96   Loss :  0.326117097855
Iteration :  97   Loss :  0.213237431039
Iteration :  98   Loss :  0.139429064883
Iteration :  99   Loss :  0.0911681595454
[-0.00347086 -0.00010822 -0.00155308 ...,  0.0017648   0.00141178
  0.00040089]
CROSS VALIDATION 6
Iteration :  0   Loss :  2140.56135892
Iteration :  1   Loss :  358.755244867
Iteration :  2   Loss :  234.578460591
Iteration :  3   Loss :  153.383274421
Iteration :  4   Loss :  100.292366199
Iteration :  5   Loss :  65.5779370715
Iteration :  6   Loss :  42.8792937443
Iteration :  7   Loss :  28.0373844332
Iteration :  8   Loss :  18.3327395862
Iteration :  9   Loss :  11.9871859494
Iteration :  10   Loss :  7.83803349794
Iteration :  11   Loss :  5.12503679964
Iteration :  12   Loss :  3.35109593555
Iteration :  13   Loss :  2.19117333363
Iteration :  14   Loss :  1.43273742989
Iteration :  15   Loss :  0.936820703094
Iteration :  16   Loss :  0.666848002286
Iteration :  17   Loss :  1671.63665346
Iteration :  18   Loss :  1522.96926369
Iteration :  19   Loss :  569.523757622
Iteration :  20   Loss :  1196.47173978
Iteration :  21   Loss :  170.450267674
Iteration :  22   Loss :  111.451921527
Iteration :  23   Loss :  72.8748096531
Iteration :  24   Loss :  47.6504829096
Iteration :  25   Loss :  31.1571108361
Iteration :  26   Loss :  20.3726278597
Iteration :  27   Loss :  13.3210029676
Iteration :  28   Loss :  8.71017334068
Iteration :  29   Loss :  5.69530085753
Iteration :  30   Loss :  3.72397317357
Iteration :  31   Loss :  2.43498570916
Iteration :  32   Loss :  1.59215846287
Iteration :  33   Loss :  1.04106096449
Iteration :  34   Loss :  0.680716120324
Iteration :  35   Loss :  0.445098272122
Iteration :  36   Loss :  0.291035375733
Iteration :  37   Loss :  0.190298626693
Iteration :  38   Loss :  0.124430122042
Iteration :  39   Loss :  0.0813608355486
Iteration :  40   Loss :  0.0531992209966
Iteration :  41   Loss :  0.0347852513505
Iteration :  42   Loss :  0.0227449516901
Iteration :  43   Loss :  0.126934404202
Iteration :  44   Loss :  1248.70128158
Iteration :  45   Loss :  2123.39917264
Iteration :  46   Loss :  1038.54909242
Iteration :  47   Loss :  203.230644101
Iteration :  48   Loss :  132.885950297
Iteration :  49   Loss :  86.8898283742
Iteration :  50   Loss :  56.8144507227
Iteration :  51   Loss :  37.1491332337
Iteration :  52   Loss :  24.2906176591
Iteration :  53   Loss :  15.8828498783
Iteration :  54   Loss :  10.3852822434
Iteration :  55   Loss :  6.7906004339
Iteration :  56   Loss :  4.44015416935
Iteration :  57   Loss :  2.90327331721
Iteration :  58   Loss :  1.89835659595
Iteration :  59   Loss :  1.24127402819
Iteration :  60   Loss :  0.811628972317
Iteration :  61   Loss :  0.530697955281
Iteration :  62   Loss :  0.347006242193
Iteration :  63   Loss :  0.226896167439
Iteration :  64   Loss :  0.1483600712
Iteration :  65   Loss :  0.0970078559498
Iteration :  66   Loss :  0.0634303019664
Iteration :  67   Loss :  0.041475024555
Iteration :  68   Loss :  0.0271191781926
Iteration :  69   Loss :  0.0177323541995
Iteration :  70   Loss :  0.0115946133479
Iteration :  71   Loss :  0.00758134296074
Iteration :  72   Loss :  0.00495719515293
Iteration :  73   Loss :  4829.52562362
Iteration :  74   Loss :  1717.32618679
Iteration :  75   Loss :  309.020474007
Iteration :  76   Loss :  251.408218623
Iteration :  77   Loss :  164.387709304
Iteration :  78   Loss :  107.487810535
Iteration :  79   Loss :  70.2828055849
Iteration :  80   Loss :  45.9556552162
Iteration :  81   Loss :  30.0489177798
Iteration :  82   Loss :  19.6480162341
Iteration :  83   Loss :  12.8472028431
Iteration :  84   Loss :  8.40037075127
Iteration :  85   Loss :  5.49273095632
Iteration :  86   Loss :  3.5915192617
Iteration :  87   Loss :  2.34837837675
Iteration :  88   Loss :  1.53552872713
Iteration :  89   Loss :  1.00403261042
Iteration :  90   Loss :  0.656504476252
Iteration :  91   Loss :  0.42926706052
Iteration :  92   Loss :  0.28068385809
Iteration :  93   Loss :  0.18353010384
Iteration :  94   Loss :  0.12000440369
Iteration :  95   Loss :  0.0784670013453
Iteration :  96   Loss :  0.0513070363322
Iteration :  97   Loss :  0.0335480129489
Iteration :  98   Loss :  0.0219359614836
Iteration :  99   Loss :  0.0143432163014
[ -1.44484118e-03   1.01929292e-05  -9.01045780e-04 ...,   8.25262773e-04
   3.70809770e-05  -6.76329646e-05]
CROSS VALIDATION 7
Iteration :  0   Loss :  2140.56135892
Iteration :  1   Loss :  358.755244867
Iteration :  2   Loss :  234.578460591
Iteration :  3   Loss :  153.383274421
Iteration :  4   Loss :  100.292366199
Iteration :  5   Loss :  65.5779370715
Iteration :  6   Loss :  42.8792937443
Iteration :  7   Loss :  28.0373844332
Iteration :  8   Loss :  18.3327395862
Iteration :  9   Loss :  11.9871859494
Iteration :  10   Loss :  7.83803349794
Iteration :  11   Loss :  5.12503679964
Iteration :  12   Loss :  3.35109593555
Iteration :  13   Loss :  2.19117333363
Iteration :  14   Loss :  1.43273742989
Iteration :  15   Loss :  0.936820703094
Iteration :  16   Loss :  0.666848002286
Iteration :  17   Loss :  1720.65661674
Iteration :  18   Loss :  1522.96926369
Iteration :  19   Loss :  569.523757622
Iteration :  20   Loss :  1196.47173978
Iteration :  21   Loss :  170.450267674
Iteration :  22   Loss :  111.451921527
Iteration :  23   Loss :  72.8748096531
Iteration :  24   Loss :  47.6504829096
Iteration :  25   Loss :  31.1571108361
Iteration :  26   Loss :  20.3726278597
Iteration :  27   Loss :  13.3210029676
Iteration :  28   Loss :  8.71017334068
Iteration :  29   Loss :  5.69530085753
Iteration :  30   Loss :  3.72397317357
Iteration :  31   Loss :  2.43498570916
Iteration :  32   Loss :  1.59215846287
Iteration :  33   Loss :  1.04106096449
Iteration :  34   Loss :  0.680716120324
Iteration :  35   Loss :  0.445098272122
Iteration :  36   Loss :  0.291035375733
Iteration :  37   Loss :  0.190298626693
Iteration :  38   Loss :  0.124430122042
Iteration :  39   Loss :  0.0813608355486
Iteration :  40   Loss :  0.0531992209966
Iteration :  41   Loss :  0.0347852513505
Iteration :  42   Loss :  0.0227449516901
Iteration :  43   Loss :  0.126934404202
Iteration :  44   Loss :  1248.70128158
Iteration :  45   Loss :  2123.39917264
Iteration :  46   Loss :  1038.54909242
Iteration :  47   Loss :  203.230644101
Iteration :  48   Loss :  132.885950297
Iteration :  49   Loss :  86.8898283742
Iteration :  50   Loss :  56.8144507227
Iteration :  51   Loss :  37.1491332337
Iteration :  52   Loss :  24.2906176591
Iteration :  53   Loss :  15.8828498783
Iteration :  54   Loss :  10.3852822434
Iteration :  55   Loss :  6.7906004339
Iteration :  56   Loss :  4.44015416935
Iteration :  57   Loss :  2.90327331721
Iteration :  58   Loss :  1.89835659595
Iteration :  59   Loss :  1.24127402819
Iteration :  60   Loss :  0.811628972317
Iteration :  61   Loss :  0.530697955281
Iteration :  62   Loss :  0.347006242193
Iteration :  63   Loss :  0.226896167439
Iteration :  64   Loss :  0.1483600712
Iteration :  65   Loss :  0.0970078559498
Iteration :  66   Loss :  0.0634303019664
Iteration :  67   Loss :  0.041475024555
Iteration :  68   Loss :  0.0271191781926
Iteration :  69   Loss :  0.0177323541995
Iteration :  70   Loss :  0.0115946133479
Iteration :  71   Loss :  0.00758134296074
Iteration :  72   Loss :  0.00495719515293
Iteration :  73   Loss :  4829.52562362
Iteration :  74   Loss :  1717.32618679
Iteration :  75   Loss :  309.020474007
Iteration :  76   Loss :  251.408218623
Iteration :  77   Loss :  164.387709304
Iteration :  78   Loss :  107.487810535
Iteration :  79   Loss :  70.2828055849
Iteration :  80   Loss :  45.9556552162
Iteration :  81   Loss :  30.0489177798
Iteration :  82   Loss :  19.6480162341
Iteration :  83   Loss :  12.8472028431
Iteration :  84   Loss :  8.40037075127
Iteration :  85   Loss :  5.49273095632
Iteration :  86   Loss :  3.5915192617
Iteration :  87   Loss :  2.34837837675
Iteration :  88   Loss :  1.53552872713
Iteration :  89   Loss :  1.00403261042
Iteration :  90   Loss :  0.656504476252
Iteration :  91   Loss :  0.42926706052
Iteration :  92   Loss :  0.28068385809
Iteration :  93   Loss :  0.18353010384
Iteration :  94   Loss :  0.12000440369
Iteration :  95   Loss :  0.0784670013453
Iteration :  96   Loss :  0.0513070363322
Iteration :  97   Loss :  0.0335480129489
Iteration :  98   Loss :  0.0219359614836
Iteration :  99   Loss :  0.0143432163014
[ -1.44484118e-03   1.01929292e-05  -9.01045780e-04 ...,   8.25262773e-04
   3.70809770e-05  -6.76329646e-05]
CROSS VALIDATION 8
Iteration :  0   Loss :  1859.41057431
Iteration :  1   Loss :  361.646766814
Iteration :  2   Loss :  395.974611314
Iteration :  3   Loss :  211.863077116
Iteration :  4   Loss :  138.530419268
Iteration :  5   Loss :  90.5805642196
Iteration :  6   Loss :  59.2277036172
Iteration :  7   Loss :  38.7270813114
Iteration :  8   Loss :  25.3223869119
Iteration :  9   Loss :  16.5574904486
Iteration :  10   Loss :  10.8264079097
Iteration :  11   Loss :  7.0790382511
Iteration :  12   Loss :  4.62875433646
Iteration :  13   Loss :  3.02659287143
Iteration :  14   Loss :  1.97899126709
Iteration :  15   Loss :  1.29399843375
Iteration :  16   Loss :  0.846103757195
Iteration :  17   Loss :  0.553239902978
Iteration :  18   Loss :  0.361745693296
Iteration :  19   Loss :  0.236533818175
Iteration :  20   Loss :  0.154661819553
Iteration :  21   Loss :  0.101128365542
Iteration :  22   Loss :  0.0661245700244
Iteration :  23   Loss :  0.0432367193664
Iteration :  24   Loss :  0.0282710934965
Iteration :  25   Loss :  0.0184855543898
Iteration :  26   Loss :  9704.62818171
Iteration :  27   Loss :  1903.39516581
Iteration :  28   Loss :  617.511375698
Iteration :  29   Loss :  258.325301753
Iteration :  30   Loss :  168.910566421
Iteration :  31   Loss :  110.445160636
Iteration :  32   Loss :  72.2165212425
Iteration :  33   Loss :  47.2200493921
Iteration :  34   Loss :  30.8756642695
Iteration :  35   Loss :  20.1885990454
Iteration :  36   Loss :  13.2006724733
Iteration :  37   Loss :  8.63149311919
Iteration :  38   Loss :  5.64385440344
Iteration :  39   Loss :  3.69033399985
Iteration :  40   Loss :  2.4129901406
Iteration :  41   Loss :  1.57777627143
Iteration :  42   Loss :  1.0316569143
Iteration :  43   Loss :  0.674567115819
Iteration :  44   Loss :  0.441077636796
Iteration :  45   Loss :  0.28840641223
Iteration :  46   Loss :  0.188579632419
Iteration :  47   Loss :  0.155633016185
Iteration :  48   Loss :  5417.73903404
Iteration :  49   Loss :  3169.22804502
Iteration :  50   Loss :  1295.17157759
Iteration :  51   Loss :  238.613962796
Iteration :  52   Loss :  156.021958896
Iteration :  53   Loss :  102.017716702
Iteration :  54   Loss :  66.7060880066
Iteration :  55   Loss :  43.6169551819
Iteration :  56   Loss :  28.5197174079
Iteration :  57   Loss :  18.6481215307
Iteration :  58   Loss :  12.1934040106
Iteration :  59   Loss :  7.9728728237
Iteration :  60   Loss :  5.2132038771
Iteration :  61   Loss :  3.40874553817
Iteration :  62   Loss :  2.22886854571
Iteration :  63   Loss :  1.45738511086
Iteration :  64   Loss :  0.952937025129
Iteration :  65   Loss :  0.623094724309
Iteration :  66   Loss :  0.40742150344
Iteration :  67   Loss :  0.266399754306
Iteration :  68   Loss :  0.17419019
Iteration :  69   Loss :  0.113897335871
Iteration :  70   Loss :  0.0744737870634
Iteration :  71   Loss :  0.0486960025635
Iteration :  72   Loss :  0.0318407423493
Iteration :  73   Loss :  0.020819632413
Iteration :  74   Loss :  0.013613284799
Iteration :  75   Loss :  0.0442734202713
Iteration :  76   Loss :  3403.07165436
Iteration :  77   Loss :  500.293430211
Iteration :  78   Loss :  425.992611461
Iteration :  79   Loss :  198.58362211
Iteration :  80   Loss :  129.847412796
Iteration :  81   Loss :  84.9030269
Iteration :  82   Loss :  55.515345447
Iteration :  83   Loss :  36.2996902776
Iteration :  84   Loss :  23.7351943618
Iteration :  85   Loss :  15.519676534
Iteration :  86   Loss :  10.1478149304
Iteration :  87   Loss :  6.63532823225
Iteration :  88   Loss :  4.33862669468
Iteration :  89   Loss :  2.83688778263
Iteration :  90   Loss :  1.85494924029
Iteration :  91   Loss :  1.21289136114
Iteration :  92   Loss :  0.793070463584
Iteration :  93   Loss :  0.518563146182
Iteration :  94   Loss :  0.339071682689
Iteration :  95   Loss :  0.221708015404
Iteration :  96   Loss :  0.144967706252
Iteration :  97   Loss :  0.0947896981422
Iteration :  98   Loss :  0.061979920261
Iteration :  99   Loss :  0.0405266668304
[-0.002546   -0.00049107 -0.00068562 ...,  0.00107721  0.00075592
  0.00010412]
CROSS VALIDATION 9
Iteration :  0   Loss :  1928.57216498
Iteration :  1   Loss :  358.883756502
Iteration :  2   Loss :  234.662490196
Iteration :  3   Loss :  153.438218663
Iteration :  4   Loss :  100.328292464
Iteration :  5   Loss :  65.6014280949
Iteration :  6   Loss :  42.8946537653
Iteration :  7   Loss :  28.0474278545
Iteration :  8   Loss :  18.3393066549
Iteration :  9   Loss :  11.9914799434
Iteration :  10   Loss :  7.84084120182
Iteration :  11   Loss :  5.12687266647
Iteration :  12   Loss :  3.35229634954
Iteration :  13   Loss :  2.19195824555
Iteration :  14   Loss :  1.43325065843
Iteration :  15   Loss :  0.937156286646
Iteration :  16   Loss :  0.613859573599
Iteration :  17   Loss :  1720.49175944
Iteration :  18   Loss :  1522.97513343
Iteration :  19   Loss :  569.456182818
Iteration :  20   Loss :  1196.47722447
Iteration :  21   Loss :  170.450400496
Iteration :  22   Loss :  111.452008375
Iteration :  23   Loss :  72.87486644
Iteration :  24   Loss :  47.6505200407
Iteration :  25   Loss :  31.1571351149
Iteration :  26   Loss :  20.3726437348
Iteration :  27   Loss :  13.3210133479
Iteration :  28   Loss :  8.71018012798
Iteration :  29   Loss :  5.69530529553
Iteration :  30   Loss :  3.72397607543
Iteration :  31   Loss :  2.4349876066
Iteration :  32   Loss :  1.59215970355
Iteration :  33   Loss :  1.04106177573
Iteration :  34   Loss :  0.680716650765
Iteration :  35   Loss :  0.44509861896
Iteration :  36   Loss :  0.291035602519
Iteration :  37   Loss :  0.190298774981
Iteration :  38   Loss :  0.124430219003
Iteration :  39   Loss :  0.0813608989481
Iteration :  40   Loss :  0.0531992624515
Iteration :  41   Loss :  0.0347852784566
Iteration :  42   Loss :  0.0227449694139
Iteration :  43   Loss :  0.014872200442
Iteration :  44   Loss :  0.00972445123857
Iteration :  45   Loss :  0.0275026209448
Iteration :  46   Loss :  5755.68020318
Iteration :  47   Loss :  1181.14730452
Iteration :  48   Loss :  660.8676874
Iteration :  49   Loss :  210.896944039
Iteration :  50   Loss :  137.898696071
Iteration :  51   Loss :  90.1675008372
Iteration :  52   Loss :  58.9576148206
Iteration :  53   Loss :  38.5504789758
Iteration :  54   Loss :  25.206912352
Iteration :  55   Loss :  16.4819853658
Iteration :  56   Loss :  10.7770375762
Iteration :  57   Loss :  7.0467565855
Iteration :  58   Loss :  4.60764639859
Iteration :  59   Loss :  3.012791073
Iteration :  60   Loss :  1.96996671714
Iteration :  61   Loss :  1.28809757218
Iteration :  62   Loss :  0.842245374519
Iteration :  63   Loss :  0.550717031239
Iteration :  64   Loss :  0.360096068999
Iteration :  65   Loss :  0.235455182159
Iteration :  66   Loss :  0.15395653432
Iteration :  67   Loss :  0.100667202321
Iteration :  68   Loss :  4546.05294129
Iteration :  69   Loss :  720.658346531
Iteration :  70   Loss :  281.331250859
Iteration :  71   Loss :  183.953413049
Iteration :  72   Loss :  120.281191901
Iteration :  73   Loss :  78.647984212
Iteration :  74   Loss :  51.4253751798
Iteration :  75   Loss :  33.625390897
Iteration :  76   Loss :  21.9865564232
Iteration :  77   Loss :  14.3762986973
Iteration :  78   Loss :  9.40019711396
Iteration :  79   Loss :  6.14648510313
Iteration :  80   Loss :  4.01898797067
Iteration :  81   Loss :  2.62788635088
Iteration :  82   Loss :  1.71828995845
Iteration :  83   Loss :  1.12353427321
Iteration :  84   Loss :  0.734642751576
Iteration :  85   Loss :  0.480359153531
Iteration :  86   Loss :  0.314091326547
Iteration :  87   Loss :  0.205374167822
Iteration :  88   Loss :  0.134287531185
Iteration :  89   Loss :  0.0878062768213
Iteration :  90   Loss :  0.0574136867451
Iteration :  91   Loss :  0.037540954303
Iteration :  92   Loss :  0.0245468167937
Iteration :  93   Loss :  0.0160503702127
Iteration :  94   Loss :  0.0104948183763
Iteration :  95   Loss :  0.00841576488638
Iteration :  96   Loss :  2385.65766927
Iteration :  97   Loss :  2230.34980004
Iteration :  98   Loss :  565.514835079
Iteration :  99   Loss :  218.71624984
[-0.11024534 -0.06742713 -0.00429827 ...,  0.07339935  0.0550788
  0.01356125]
CROSS VALIDATION 10
Iteration :  0   Loss :  1243.11438315
Iteration :  1   Loss :  310.345301152
Iteration :  2   Loss :  202.924762878
Iteration :  3   Loss :  132.68594445
Iteration :  4   Loss :  86.7590510148
Iteration :  5   Loss :  56.7289396338
Iteration :  6   Loss :  37.0932202961
Iteration :  7   Loss :  24.2540579962
Iteration :  8   Loss :  15.8589446962
Iteration :  9   Loss :  10.3696514174
Iteration :  10   Loss :  6.78037994192
Iteration :  11   Loss :  4.43347131995
Iteration :  12   Loss :  2.89890361796
Iteration :  13   Loss :  1.89549939083
Iteration :  14   Loss :  1.2394057941
Iteration :  15   Loss :  0.81040739442
Iteration :  16   Loss :  0.529899204972
Iteration :  17   Loss :  0.34648396518
Iteration :  18   Loss :  0.226554667379
Iteration :  19   Loss :  0.148136775347
Iteration :  20   Loss :  0.0968618500078
Iteration :  21   Loss :  0.0633348333994
Iteration :  22   Loss :  0.0414126007443
Iteration :  23   Loss :  0.0270783612802
Iteration :  24   Loss :  0.0177056653396
Iteration :  25   Loss :  0.0115771623649
Iteration :  26   Loss :  0.0697681795811
Iteration :  27   Loss :  2079.75847042
Iteration :  28   Loss :  378.24942288
Iteration :  29   Loss :  247.325073593
Iteration :  30   Loss :  161.717872725
Iteration :  31   Loss :  105.742090678
Iteration :  32   Loss :  69.1413357875
Iteration :  33   Loss :  45.209285005
Iteration :  34   Loss :  29.5608904194
Iteration :  35   Loss :  19.3289109149
Iteration :  36   Loss :  12.6385501876
Iteration :  37   Loss :  8.26393952289
Iteration :  38   Loss :  5.40352298519
Iteration :  39   Loss :  3.53318905234
Iteration :  40   Loss :  2.31023813793
Iteration :  41   Loss :  1.51059005756
Iteration :  42   Loss :  0.987726020335
Iteration :  43   Loss :  0.645842123985
Iteration :  44   Loss :  0.422295292952
Iteration :  45   Loss :  0.276125244586
Iteration :  46   Loss :  0.180549373792
Iteration :  47   Loss :  0.118055400641
Iteration :  48   Loss :  0.0771926112384
Iteration :  49   Loss :  0.0504737538262
Iteration :  50   Loss :  0.0330031564478
Iteration :  51   Loss :  0.0215796974258
Iteration :  52   Loss :  0.0141102667476
Iteration :  53   Loss :  0.00922624741946
Iteration :  54   Loss :  0.00603274501949
Iteration :  55   Loss :  0.00394461700577
Iteration :  56   Loss :  0.172583781822
Iteration :  57   Loss :  2790.77783625
Iteration :  58   Loss :  323.245006616
Iteration :  59   Loss :  211.359463396
Iteration :  60   Loss :  138.201122532
Iteration :  61   Loss :  90.3652477261
Iteration :  62   Loss :  59.0869151206
Iteration :  63   Loss :  38.6350242634
Iteration :  64   Loss :  25.2621937833
Iteration :  65   Loss :  16.5181321071
Iteration :  66   Loss :  10.8006727622
Iteration :  67   Loss :  7.06221086979
Iteration :  68   Loss :  4.61775145564
Iteration :  69   Loss :  3.01939844324
Iteration :  70   Loss :  1.974287063
Iteration :  71   Loss :  1.29092250672
Iteration :  72   Loss :  0.84409250792
Iteration :  73   Loss :  0.551924812075
Iteration :  74   Loss :  0.360885797855
Iteration :  75   Loss :  1981.36930811
Iteration :  76   Loss :  536.083477566
Iteration :  77   Loss :  202.177291079
Iteration :  78   Loss :  132.197196797
Iteration :  79   Loss :  86.4394747194
Iteration :  80   Loss :  56.5199790221
Iteration :  81   Loss :  36.9565877052
Iteration :  82   Loss :  24.1647183606
Iteration :  83   Loss :  15.8005283957
Iteration :  84   Loss :  10.3314548864
Iteration :  85   Loss :  6.7554044647
Iteration :  86   Loss :  4.41714066253
Iteration :  87   Loss :  2.88822552884
Iteration :  88   Loss :  1.88851733344
Iteration :  89   Loss :  1.23484045241
Iteration :  90   Loss :  0.807422265032
Iteration :  91   Loss :  0.527947325337
Iteration :  92   Loss :  0.345207694166
Iteration :  93   Loss :  0.225720155009
Iteration :  94   Loss :  0.147591114678
Iteration :  95   Loss :  0.0965050601303
Iteration :  96   Loss :  0.063101540029
Iteration :  97   Loss :  0.0412600577489
Iteration :  98   Loss :  0.0269786183452
Iteration :  99   Loss :  0.0606800355735
[ -1.00312891e-03  -9.09923581e-04  -1.34630945e-03 ...,   9.75538720e-04
   1.49845353e-04   5.86914268e-05]
CROSS VALIDATION 11
Iteration :  0   Loss :  2141.80876426
Iteration :  1   Loss :  359.011753277
Iteration :  2   Loss :  234.746183151
Iteration :  3   Loss :  153.492942782
Iteration :  4   Loss :  100.364074796
Iteration :  5   Loss :  65.6248250058
Iteration :  6   Loss :  42.9099522492
Iteration :  7   Loss :  28.0574310388
Iteration :  8   Loss :  18.3458474137
Iteration :  9   Loss :  11.9957567342
Iteration :  10   Loss :  7.84363765715
Iteration :  11   Loss :  5.12870117823
Iteration :  12   Loss :  3.35349195429
Iteration :  13   Loss :  2.19274001285
Iteration :  14   Loss :  1.4337618308
Iteration :  15   Loss :  0.937490525745
Iteration :  16   Loss :  0.624128509756
Iteration :  17   Loss :  1720.52042925
Iteration :  18   Loss :  1523.290939
Iteration :  19   Loss :  568.657954571
Iteration :  20   Loss :  1196.72652012
Iteration :  21   Loss :  170.501878794
Iteration :  22   Loss :  111.48566837
Iteration :  23   Loss :  72.8968756232
Iteration :  24   Loss :  47.6649111344
Iteration :  25   Loss :  31.166544986
Iteration :  26   Loss :  20.3787965454
Iteration :  27   Loss :  13.3250364718
Iteration :  28   Loss :  8.71281071874
Iteration :  29   Loss :  5.69702535382
Iteration :  30   Loss :  3.72510076596
Iteration :  31   Loss :  2.43572300538
Iteration :  32   Loss :  1.59264055705
Iteration :  33   Loss :  1.04137619029
Iteration :  34   Loss :  0.680922236287
Iteration :  35   Loss :  0.445233044689
Iteration :  36   Loss :  0.291123499159
Iteration :  37   Loss :  0.190356247753
Iteration :  38   Loss :  0.124467798592
Iteration :  39   Loss :  0.0813854710265
Iteration :  40   Loss :  0.0532153293395
Iteration :  41   Loss :  0.0347957840754
Iteration :  42   Loss :  0.0227518386985
Iteration :  43   Loss :  0.126002749768
Iteration :  44   Loss :  1159.33696302
Iteration :  45   Loss :  1780.18815215
Iteration :  46   Loss :  1039.142958
Iteration :  47   Loss :  203.249392185
Iteration :  48   Loss :  132.898209063
Iteration :  49   Loss :  86.8978439851
Iteration :  50   Loss :  56.8196918717
Iteration :  51   Loss :  37.1525602516
Iteration :  52   Loss :  24.2928584753
Iteration :  53   Loss :  15.8843150756
Iteration :  54   Loss :  10.3862402886
Iteration :  55   Loss :  6.79122686868
Iteration :  56   Loss :  4.44056377481
Iteration :  57   Loss :  2.90354114498
Iteration :  58   Loss :  1.89853171988
Iteration :  59   Loss :  1.24138853607
Iteration :  60   Loss :  0.811703845316
Iteration :  61   Loss :  0.530746912315
Iteration :  62   Loss :  0.347038253616
Iteration :  63   Loss :  0.226917098674
Iteration :  64   Loss :  0.148373757458
Iteration :  65   Loss :  0.0970168049516
Iteration :  66   Loss :  0.0634361534294
Iteration :  67   Loss :  0.0414788506376
Iteration :  68   Loss :  0.0271216799444
Iteration :  69   Loss :  0.0177339900141
Iteration :  70   Loss :  0.0115956829542
Iteration :  71   Loss :  0.00758204234174
Iteration :  72   Loss :  0.00495765245556
Iteration :  73   Loss :  4829.483652
Iteration :  74   Loss :  1715.91635845
Iteration :  75   Loss :  309.438957371
Iteration :  76   Loss :  251.517873464
Iteration :  77   Loss :  164.459409061
Iteration :  78   Loss :  107.534692689
Iteration :  79   Loss :  70.3134603102
Iteration :  80   Loss :  45.9756993504
Iteration :  81   Loss :  30.0620239913
Iteration :  82   Loss :  19.6565859622
Iteration :  83   Loss :  12.8528063115
Iteration :  84   Loss :  8.4040346781
Iteration :  85   Loss :  5.49512667967
Iteration :  86   Loss :  3.59308574777
Iteration :  87   Loss :  2.34940265137
Iteration :  88   Loss :  1.5361984672
Iteration :  89   Loss :  1.0044705318
Iteration :  90   Loss :  0.656790818896
Iteration :  91   Loss :  0.429454290721
Iteration :  92   Loss :  0.280806281867
Iteration :  93   Loss :  0.183610152791
Iteration :  94   Loss :  0.120056745112
Iteration :  95   Loss :  0.07850122571
Iteration :  96   Loss :  0.0513294145383
Iteration :  97   Loss :  0.0335626453346
Iteration :  98   Loss :  0.0219455291277
Iteration :  99   Loss :  0.0143494722747
[ -1.44544373e-03   9.59824389e-06  -9.01349665e-04 ...,   8.25374724e-04
   3.73793248e-05  -6.77999116e-05]
CROSS VALIDATION 12
Iteration :  0   Loss :  1514.63651093
Iteration :  1   Loss :  831.35000361
Iteration :  2   Loss :  516.793322722
Iteration :  3   Loss :  233.905552829
Iteration :  4   Loss :  152.943281782
Iteration :  5   Loss :  100.004669232
Iteration :  6   Loss :  65.3898213222
Iteration :  7   Loss :  42.7562909352
Iteration :  8   Loss :  27.9569568715
Iteration :  9   Loss :  18.2801506028
Iteration :  10   Loss :  11.9527997127
Iteration :  11   Loss :  7.81554944903
Iteration :  12   Loss :  5.11033520668
Iteration :  13   Loss :  3.34148303903
Iteration :  14   Loss :  2.18488777126
Iteration :  15   Loss :  1.42862750379
Iteration :  16   Loss :  0.93413335524
Iteration :  17   Loss :  0.610799612254
Iteration :  18   Loss :  0.399382127013
Iteration :  19   Loss :  0.261143065872
Iteration :  20   Loss :  0.170753011315
Iteration :  21   Loss :  0.11164987581
Iteration :  22   Loss :  0.0730042455613
Iteration :  23   Loss :  0.047735116867
Iteration :  24   Loss :  0.0312124502457
Iteration :  25   Loss :  0.0204088125112
Iteration :  26   Loss :  0.0133446629418
Iteration :  27   Loss :  0.00872564383319
Iteration :  28   Loss :  0.00570541651263
Iteration :  29   Loss :  0.0521425996783
Iteration :  30   Loss :  1223.83701242
Iteration :  31   Loss :  293.317163872
Iteration :  32   Loss :  191.790614215
Iteration :  33   Loss :  125.405684466
Iteration :  34   Loss :  81.9987242901
Iteration :  35   Loss :  53.6163158298
Iteration :  36   Loss :  35.0579761825
Iteration :  37   Loss :  22.923277644
Iteration :  38   Loss :  14.9887904313
Iteration :  39   Loss :  9.80068566465
Iteration :  40   Loss :  6.40835162368
Iteration :  41   Loss :  4.19021402563
Iteration :  42   Loss :  2.73984553464
Iteration :  43   Loss :  1.79149645049
Iteration :  44   Loss :  1.17140163251
Iteration :  45   Loss :  0.765941670874
Iteration :  46   Loss :  0.50082450536
Iteration :  47   Loss :  0.327472958722
Iteration :  48   Loss :  0.214123984642
Iteration :  49   Loss :  0.140008753632
Iteration :  50   Loss :  0.138916229448
Iteration :  51   Loss :  3535.55656534
Iteration :  52   Loss :  899.094585893
Iteration :  53   Loss :  242.429372623
Iteration :  54   Loss :  158.516732077
Iteration :  55   Loss :  103.648968261
Iteration :  56   Loss :  67.7727106831
Iteration :  57   Loss :  44.3143852796
Iteration :  58   Loss :  28.9757444097
Iteration :  59   Loss :  18.9463028495
Iteration :  60   Loss :  12.3883751385
Iteration :  61   Loss :  8.10035814333
Iteration :  62   Loss :  5.29656240762
Iteration :  63   Loss :  3.46325098735
Iteration :  64   Loss :  2.26450789745
Iteration :  65   Loss :  1.48068853119
Iteration :  66   Loss :  0.96817437858
Iteration :  67   Loss :  0.633057937299
Iteration :  68   Loss :  0.413936126429
Iteration :  69   Loss :  0.270659455743
Iteration :  70   Loss :  0.176975471108
Iteration :  71   Loss :  0.115718541175
Iteration :  72   Loss :  0.0756646143546
Iteration :  73   Loss :  0.0494746460448
Iteration :  74   Loss :  0.032349872158
Iteration :  75   Loss :  0.0224608499197
Iteration :  76   Loss :  5060.27400443
Iteration :  77   Loss :  2640.39778131
Iteration :  78   Loss :  303.19427592
Iteration :  79   Loss :  198.248938581
Iteration :  80   Loss :  129.628574053
Iteration :  81   Loss :  84.7599353181
Iteration :  82   Loss :  55.4217824859
Iteration :  83   Loss :  36.238512481
Iteration :  84   Loss :  23.6951921778
Iteration :  85   Loss :  15.4935203986
Iteration :  86   Loss :  10.1307122788
Iteration :  87   Loss :  6.62414536106
Iteration :  88   Loss :  4.33131457661
Iteration :  89   Loss :  2.83210662493
Iteration :  90   Loss :  1.85182299579
Iteration :  91   Loss :  1.21084721089
Iteration :  92   Loss :  0.791733859798
Iteration :  93   Loss :  0.517689184162
Iteration :  94   Loss :  0.338500227168
Iteration :  95   Loss :  0.221334359107
Iteration :  96   Loss :  0.144723384474
Iteration :  97   Loss :  0.094629944028
Iteration :  98   Loss :  0.0618754622086
Iteration :  99   Loss :  0.0404583650857
[ -3.04323101e-03  -1.21854029e-03  -7.68262224e-04 ...,   1.71222733e-03
  -3.85172019e-05  -4.00362924e-05]
CROSS VALIDATION 13
Iteration :  0   Loss :  2157.34923485
Iteration :  1   Loss :  359.104504069
Iteration :  2   Loss :  234.806829896
Iteration :  3   Loss :  153.532597729
Iteration :  4   Loss :  100.390003884
Iteration :  5   Loss :  65.6417791986
Iteration :  6   Loss :  42.921038048
Iteration :  7   Loss :  28.0646796843
Iteration :  8   Loss :  18.3505870688
Iteration :  9   Loss :  11.9988558415
Iteration :  10   Loss :  7.84566406324
Iteration :  11   Loss :  5.13002617969
Iteration :  12   Loss :  3.35435832992
Iteration :  13   Loss :  2.19330650788
Iteration :  14   Loss :  1.43413224359
Iteration :  15   Loss :  0.937732726697
Iteration :  16   Loss :  0.613153124927
Iteration :  17   Loss :  0.54191181115
Iteration :  18   Loss :  2214.37416269
Iteration :  19   Loss :  524.203381809
Iteration :  20   Loss :  368.000427293
Iteration :  21   Loss :  291.750520261
Iteration :  22   Loss :  190.766236588
Iteration :  23   Loss :  124.735877041
Iteration :  24   Loss :  81.560758861
Iteration :  25   Loss :  53.3299443896
Iteration :  26   Loss :  34.8707271526
Iteration :  27   Loss :  22.8008415547
Iteration :  28   Loss :  14.9087334293
Iteration :  29   Loss :  9.74833897829
Iteration :  30   Loss :  6.37412381718
Iteration :  31   Loss :  4.16783356911
Iteration :  32   Loss :  2.7252116774
Iteration :  33   Loss :  1.78192784417
Iteration :  34   Loss :  1.16514503007
Iteration :  35   Loss :  0.761850680734
Iteration :  36   Loss :  0.498149539118
Iteration :  37   Loss :  0.325723884744
Iteration :  38   Loss :  0.212980321693
Iteration :  39   Loss :  0.13926094939
Iteration :  40   Loss :  0.0910582342582
Iteration :  41   Loss :  0.0595400366183
Iteration :  42   Loss :  0.0389313057671
Iteration :  43   Loss :  0.0254559226835
Iteration :  44   Loss :  0.0166448051741
Iteration :  45   Loss :  0.010883500187
Iteration :  46   Loss :  0.00711636904611
Iteration :  47   Loss :  0.00465316373688
Iteration :  48   Loss :  0.0831925654599
Iteration :  49   Loss :  1266.57070136
Iteration :  50   Loss :  333.259985973
Iteration :  51   Loss :  217.90793474
Iteration :  52   Loss :  142.482956314
Iteration :  53   Loss :  93.1650004587
Iteration :  54   Loss :  60.9175829517
Iteration :  55   Loss :  39.8320387958
Iteration :  56   Loss :  26.0448829016
Iteration :  57   Loss :  17.0299072271
Iteration :  58   Loss :  11.1353059739
Iteration :  59   Loss :  7.28101671251
Iteration :  60   Loss :  4.76082152496
Iteration :  61   Loss :  3.11294733792
Iteration :  62   Loss :  2.03545566198
Iteration :  63   Loss :  1.33091867679
Iteration :  64   Loss :  0.870244710953
Iteration :  65   Loss :  0.569024892468
Iteration :  66   Loss :  0.372066987794
Iteration :  67   Loss :  0.24328257909
Iteration :  68   Loss :  0.159074616213
Iteration :  69   Loss :  0.104013750668
Iteration :  70   Loss :  0.0680112301102
Iteration :  71   Loss :  0.0444703454247
Iteration :  72   Loss :  0.0290777217084
Iteration :  73   Loss :  0.0190129825095
Iteration :  74   Loss :  0.0124319748133
Iteration :  75   Loss :  0.00812886656161
Iteration :  76   Loss :  0.08057448182
Iteration :  77   Loss :  2531.41298513
Iteration :  78   Loss :  2952.41388629
Iteration :  79   Loss :  310.394601474
Iteration :  80   Loss :  202.956998765
Iteration :  81   Loss :  132.707022455
Iteration :  82   Loss :  86.7728332409
Iteration :  83   Loss :  56.7379513861
Iteration :  84   Loss :  37.0991127897
Iteration :  85   Loss :  24.2579109073
Iteration :  86   Loss :  15.8614639903
Iteration :  87   Loss :  10.3712987024
Iteration :  88   Loss :  6.78145704833
Iteration :  89   Loss :  4.43417560501
Iteration :  90   Loss :  2.8993641272
Iteration :  91   Loss :  1.89580050294
Iteration :  92   Loss :  1.23960268158
Iteration :  93   Loss :  0.81053613278
Iteration :  94   Loss :  0.529983382825
Iteration :  95   Loss :  0.346539006358
Iteration :  96   Loss :  0.226590657026
Iteration :  97   Loss :  0.148160307814
Iteration :  98   Loss :  0.0968772371277
Iteration :  99   Loss :  0.0633448945398
[ -3.43759698e-03  -1.21459113e-03  -1.06068583e-03 ...,   1.81439241e-03
   1.11206946e-03   7.73433935e-05]
CROSS VALIDATION 14
Iteration :  0   Loss :  2157.34923485
Iteration :  1   Loss :  359.104504069
Iteration :  2   Loss :  234.806829896
Iteration :  3   Loss :  153.532597729
Iteration :  4   Loss :  100.390003884
Iteration :  5   Loss :  65.6417791986
Iteration :  6   Loss :  42.921038048
Iteration :  7   Loss :  28.0646796843
Iteration :  8   Loss :  18.3505870688
Iteration :  9   Loss :  11.9988558415
Iteration :  10   Loss :  7.84566406324
Iteration :  11   Loss :  5.13002617969
Iteration :  12   Loss :  3.35435832992
Iteration :  13   Loss :  2.19330650788
Iteration :  14   Loss :  1.43413224359
Iteration :  15   Loss :  0.937732726697
Iteration :  16   Loss :  0.613153124927
Iteration :  17   Loss :  0.400921012891
Iteration :  18   Loss :  0.262149293615
Iteration :  19   Loss :  0.171410951118
Iteration :  20   Loss :  0.112080081384
Iteration :  21   Loss :  0.073285543083
Iteration :  22   Loss :  0.0479190482256
Iteration :  23   Loss :  0.0313327170169
Iteration :  24   Loss :  0.0204874510662
Iteration :  25   Loss :  0.0133960821516
Iteration :  26   Loss :  0.0087592651927
Iteration :  27   Loss :  0.0685633971917
Iteration :  28   Loss :  1233.81973164
Iteration :  29   Loss :  1895.45610547
Iteration :  30   Loss :  307.333506752
Iteration :  31   Loss :  200.955447853
Iteration :  32   Loss :  131.398272998
Iteration :  33   Loss :  85.9170842665
Iteration :  34   Loss :  56.1784047873
Iteration :  35   Loss :  36.7332433518
Iteration :  36   Loss :  24.0186807058
Iteration :  37   Loss :  15.7050390928
Iteration :  38   Loss :  10.2690175172
Iteration :  39   Loss :  6.71457868678
Iteration :  40   Loss :  4.39044600573
Iteration :  41   Loss :  2.87077075546
Iteration :  42   Loss :  1.87710422122
Iteration :  43   Loss :  1.22737778718
Iteration :  44   Loss :  0.802542669406
Iteration :  45   Loss :  0.524756715453
Iteration :  46   Loss :  0.343121457475
Iteration :  47   Loss :  0.224356032258
Iteration :  48   Loss :  0.146699158896
Iteration :  49   Loss :  0.0959218390703
Iteration :  50   Loss :  0.062720190626
Iteration :  51   Loss :  0.0410107056985
Iteration :  52   Loss :  6075.10207656
Iteration :  53   Loss :  606.639769282
Iteration :  54   Loss :  244.860219149
Iteration :  55   Loss :  160.106184062
Iteration :  56   Loss :  104.688259547
Iteration :  57   Loss :  68.4522696678
Iteration :  58   Loss :  44.7587269382
Iteration :  59   Loss :  29.2662850603
Iteration :  60   Loss :  19.1362779914
Iteration :  61   Loss :  12.5125937443
Iteration :  62   Loss :  8.18158067524
Iteration :  63   Loss :  5.34967119633
Iteration :  64   Loss :  3.49797710795
Iteration :  65   Loss :  2.28721418545
Iteration :  66   Loss :  1.49553543911
Iteration :  67   Loss :  0.977882291856
Iteration :  68   Loss :  0.639405628056
Iteration :  69   Loss :  0.418086676275
Iteration :  70   Loss :  0.273373366152
Iteration :  71   Loss :  0.178750009417
Iteration :  72   Loss :  0.116878854427
Iteration :  73   Loss :  0.0764233056921
Iteration :  74   Loss :  0.0499707297915
Iteration :  75   Loss :  0.0326742452879
Iteration :  76   Loss :  0.0213646330479
Iteration :  77   Loss :  0.0139696431012
Iteration :  78   Loss :  0.00913429816167
Iteration :  79   Loss :  0.00597262237138
Iteration :  80   Loss :  0.00390530474917
Iteration :  81   Loss :  0.00255355256629
Iteration :  82   Loss :  0.0968229514392
Iteration :  83   Loss :  3821.31540464
Iteration :  84   Loss :  505.495814349
Iteration :  85   Loss :  262.119895635
Iteration :  86   Loss :  171.391728729
Iteration :  87   Loss :  112.067512485
Iteration :  88   Loss :  73.2773246862
Iteration :  89   Loss :  47.9136744816
Iteration :  90   Loss :  31.3292032993
Iteration :  91   Loss :  20.4851535598
Iteration :  92   Loss :  13.3945798864
Iteration :  93   Loss :  8.75828291006
Iteration :  94   Loss :  5.72675815018
Iteration :  95   Loss :  3.74454208061
Iteration :  96   Loss :  2.44843505274
Iteration :  97   Loss :  1.600952554
Iteration :  98   Loss :  1.04681113647
Iteration :  99   Loss :  0.684475971953
[-0.00956539 -0.00192004 -0.00650982 ...,  0.00743119 -0.00018776
  0.00028103]
CROSS VALIDATION 15
Iteration :  0   Loss :  2157.34923485
Iteration :  1   Loss :  359.104504069
Iteration :  2   Loss :  234.806829896
Iteration :  3   Loss :  153.532597729
Iteration :  4   Loss :  100.390003884
Iteration :  5   Loss :  65.6417791986
Iteration :  6   Loss :  42.921038048
Iteration :  7   Loss :  28.0646796843
Iteration :  8   Loss :  18.3505870688
Iteration :  9   Loss :  11.9988558415
Iteration :  10   Loss :  7.84566406324
Iteration :  11   Loss :  5.13002617969
Iteration :  12   Loss :  3.35435832992
Iteration :  13   Loss :  2.19330650788
Iteration :  14   Loss :  1.43413224359
Iteration :  15   Loss :  0.937732726697
Iteration :  16   Loss :  0.613153124927
Iteration :  17   Loss :  0.54191181115
Iteration :  18   Loss :  6737.85496695
Iteration :  19   Loss :  310.090872466
Iteration :  20   Loss :  202.758400183
Iteration :  21   Loss :  132.57716526
Iteration :  22   Loss :  86.6879238178
Iteration :  23   Loss :  56.6824318584
Iteration :  24   Loss :  37.0628103648
Iteration :  25   Loss :  24.2341739248
Iteration :  26   Loss :  15.8459431446
Iteration :  27   Loss :  10.3611501229
Iteration :  28   Loss :  6.77482122022
Iteration :  29   Loss :  4.42983665147
Iteration :  30   Loss :  2.8965270257
Iteration :  31   Loss :  1.89394541396
Iteration :  32   Loss :  1.23838969884
Iteration :  33   Loss :  0.809743002571
Iteration :  34   Loss :  0.529464780613
Iteration :  35   Loss :  0.346199909131
Iteration :  36   Loss :  0.226368932309
Iteration :  37   Loss :  0.148015329187
Iteration :  38   Loss :  0.0967824402886
Iteration :  39   Loss :  0.0632829099503
Iteration :  40   Loss :  0.041378649679
Iteration :  41   Loss :  0.0270561617758
Iteration :  42   Loss :  0.0176911498011
Iteration :  43   Loss :  0.123042368257
Iteration :  44   Loss :  4055.3606168
Iteration :  45   Loss :  555.091554123
Iteration :  46   Loss :  304.300499645
Iteration :  47   Loss :  198.972262525
Iteration :  48   Loss :  130.101532204
Iteration :  49   Loss :  85.0691873682
Iteration :  50   Loss :  55.6239924072
Iteration :  51   Loss :  36.3707310137
Iteration :  52   Loss :  23.7816456033
Iteration :  53   Loss :  15.5500494997
Iteration :  54   Loss :  10.167674831
Iteration :  55   Loss :  6.64831397947
Iteration :  56   Loss :  4.34711765512
Iteration :  57   Loss :  2.84243974725
Iteration :  58   Loss :  1.85857949054
Iteration :  59   Loss :  1.21526506445
Iteration :  60   Loss :  0.79462255147
Iteration :  61   Loss :  0.519578006291
Iteration :  62   Loss :  0.339735266917
Iteration :  63   Loss :  0.222141911685
Iteration :  64   Loss :  0.145251417007
Iteration :  65   Loss :  0.0949752074367
Iteration :  66   Loss :  0.0621012187935
Iteration :  67   Loss :  0.0406059800207
Iteration :  68   Loss :  0.0265509380569
Iteration :  69   Loss :  0.0173608003388
Iteration :  70   Loss :  0.011351666286
Iteration :  71   Loss :  0.00742248772839
Iteration :  72   Loss :  0.00485332485028
Iteration :  73   Loss :  6607.78897107
Iteration :  74   Loss :  670.888037585
Iteration :  75   Loss :  267.48807951
Iteration :  76   Loss :  174.901810679
Iteration :  77   Loss :  114.362641636
Iteration :  78   Loss :  74.7780354651
Iteration :  79   Loss :  48.8949407609
Iteration :  80   Loss :  31.9708216074
Iteration :  81   Loss :  20.9046870361
Iteration :  82   Loss :  13.6688992683
Iteration :  83   Loss :  8.93765148859
Iteration :  84   Loss :  5.84404146696
Iteration :  85   Loss :  3.82122985117
Iteration :  86   Loss :  2.49857870756
Iteration :  87   Loss :  1.63373986937
Iteration :  88   Loss :  1.0682497024
Iteration :  89   Loss :  0.698493957372
Iteration :  90   Loss :  0.465771679225
Iteration :  91   Loss :  2633.82039224
Iteration :  92   Loss :  857.898070989
Iteration :  93   Loss :  226.238243041
Iteration :  94   Loss :  147.929875698
Iteration :  95   Loss :  96.7265650139
Iteration :  96   Loss :  63.2463749139
Iteration :  97   Loss :  41.3547605994
Iteration :  98   Loss :  27.0405414787
Iteration :  99   Loss :  17.6809361936
[-0.02148384 -0.01784497 -0.031848   ...,  0.02474767  0.00043321
  0.00142668]
CROSS VALIDATION 16
Iteration :  0   Loss :  2157.34923485
Iteration :  1   Loss :  284.597838672
Iteration :  2   Loss :  186.089329252
Iteration :  3   Loss :  121.67779848
Iteration :  4   Loss :  79.5611801188
Iteration :  5   Loss :  52.0224844712
Iteration :  6   Loss :  34.0158213656
Iteration :  7   Loss :  22.2418462889
Iteration :  8   Loss :  14.5432244903
Iteration :  9   Loss :  9.50934449534
Iteration :  10   Loss :  6.21785304846
Iteration :  11   Loss :  4.0656531637
Iteration :  12   Loss :  2.65839921251
Iteration :  13   Loss :  1.73824133257
Iteration :  14   Loss :  1.13657983197
Iteration :  15   Loss :  0.743172820852
Iteration :  16   Loss :  0.485936690161
Iteration :  17   Loss :  0.317738297498
Iteration :  18   Loss :  0.207758804266
Iteration :  19   Loss :  0.135846767891
Iteration :  20   Loss :  0.0888258113128
Iteration :  21   Loss :  0.0580803274004
Iteration :  22   Loss :  10406.3077408
Iteration :  23   Loss :  3770.04436776
Iteration :  24   Loss :  479.047439744
Iteration :  25   Loss :  264.368799405
Iteration :  26   Loss :  172.862214226
Iteration :  27   Loss :  113.029015429
Iteration :  28   Loss :  73.9060203881
Iteration :  29   Loss :  48.3247582832
Iteration :  30   Loss :  31.5979977121
Iteration :  31   Loss :  20.6609095396
Iteration :  32   Loss :  13.5095010416
Iteration :  33   Loss :  8.83342613949
Iteration :  34   Loss :  5.77589187947
Iteration :  35   Loss :  3.77666903833
Iteration :  36   Loss :  2.46944183213
Iteration :  37   Loss :  1.61468820815
Iteration :  38   Loss :  1.05579243683
Iteration :  39   Loss :  0.690348554003
Iteration :  40   Loss :  0.451396609209
Iteration :  41   Loss :  0.295153654809
Iteration :  42   Loss :  0.192991436288
Iteration :  43   Loss :  0.126190863212
Iteration :  44   Loss :  0.0825121272971
Iteration :  45   Loss :  0.0539520134643
Iteration :  46   Loss :  0.0352774780168
Iteration :  47   Loss :  0.0230668027997
Iteration :  48   Loss :  0.0150826368922
Iteration :  49   Loss :  0.00986204883261
Iteration :  50   Loss :  0.00644847501612
Iteration :  51   Loss :  0.00421644941526
Iteration :  52   Loss :  0.00275700000806
Iteration :  53   Loss :  0.0611190547003
Iteration :  54   Loss :  3024.57994622
Iteration :  55   Loss :  1439.26385956
Iteration :  56   Loss :  271.741420509
Iteration :  57   Loss :  177.682932901
Iteration :  58   Loss :  116.181127578
Iteration :  59   Loss :  75.9670846545
Iteration :  60   Loss :  49.6724215988
Iteration :  61   Loss :  32.4791912012
Iteration :  62   Loss :  21.2370934843
Iteration :  63   Loss :  13.8862491023
Iteration :  64   Loss :  9.07976952086
Iteration :  65   Loss :  5.93696785537
Iteration :  66   Loss :  3.88199141341
Iteration :  67   Loss :  2.53830872946
Iteration :  68   Loss :  1.65971804672
Iteration :  69   Loss :  1.08523599302
Iteration :  70   Loss :  0.709600743858
Iteration :  71   Loss :  0.463984993976
Iteration :  72   Loss :  0.303384792784
Iteration :  73   Loss :  0.198373511401
Iteration :  74   Loss :  0.129710028194
Iteration :  75   Loss :  0.0848131955487
Iteration :  76   Loss :  0.0554566076294
Iteration :  77   Loss :  0.14816456172
Iteration :  78   Loss :  1754.10530891
Iteration :  79   Loss :  329.761811948
Iteration :  80   Loss :  215.620591797
Iteration :  81   Loss :  140.987336684
Iteration :  82   Loss :  92.1870631174
Iteration :  83   Loss :  60.2781413286
Iteration :  84   Loss :  39.4139285835
Iteration :  85   Loss :  25.771494146
Iteration :  86   Loss :  16.8511471551
Iteration :  87   Loss :  11.0184205399
Iteration :  88   Loss :  7.20458910459
Iteration :  89   Loss :  4.71084798209
Iteration :  90   Loss :  3.08027125325
Iteration :  91   Loss :  2.01408982622
Iteration :  92   Loss :  1.31694824727
Iteration :  93   Loss :  0.861109898579
Iteration :  94   Loss :  0.56305193387
Iteration :  95   Loss :  0.368161463197
Iteration :  96   Loss :  0.240728882773
Iteration :  97   Loss :  0.15740483672
Iteration :  98   Loss :  0.10292193582
Iteration :  99   Loss :  0.0953414388345
[-0.00149962 -0.00035044 -0.00201157 ...,  0.00081248  0.00083763
  0.00041195]
CROSS VALIDATION 17
Iteration :  0   Loss :  2157.34923485
Iteration :  1   Loss :  359.285366135
Iteration :  2   Loss :  234.925089756
Iteration :  3   Loss :  153.609924029
Iteration :  4   Loss :  100.440565053
Iteration :  5   Loss :  65.6748395129
Iteration :  6   Loss :  42.9426551191
Iteration :  7   Loss :  28.0788143885
Iteration :  8   Loss :  18.3598292951
Iteration :  9   Loss :  12.0048990346
Iteration :  10   Loss :  7.84961551189
Iteration :  11   Loss :  5.13260990426
Iteration :  12   Loss :  3.35604774392
Iteration :  13   Loss :  2.1944111611
Iteration :  14   Loss :  1.43485454063
Iteration :  15   Loss :  0.938205013388
Iteration :  16   Loss :  0.613461937932
Iteration :  17   Loss :  1426.8205917
Iteration :  18   Loss :  418.739081303
Iteration :  19   Loss :  431.323295034
Iteration :  20   Loss :  225.366447415
Iteration :  21   Loss :  147.359836713
Iteration :  22   Loss :  96.3538349433
Iteration :  23   Loss :  63.0026587664
Iteration :  24   Loss :  41.1954024868
Iteration :  25   Loss :  26.9363423589
Iteration :  26   Loss :  17.6551955016
Iteration :  27   Loss :  701.988750372
Iteration :  28   Loss :  1331.64692848
Iteration :  29   Loss :  151.967375384
Iteration :  30   Loss :  99.3665555767
Iteration :  31   Loss :  64.9725794254
Iteration :  32   Loss :  42.4834699431
Iteration :  33   Loss :  27.7785680416
Iteration :  34   Loss :  18.163507912
Iteration :  35   Loss :  11.8765308268
Iteration :  36   Loss :  7.76567968932
Iteration :  37   Loss :  5.07772698247
Iteration :  38   Loss :  3.32016157503
Iteration :  39   Loss :  2.17094635501
Iteration :  40   Loss :  1.41951166225
Iteration :  41   Loss :  0.928172801057
Iteration :  42   Loss :  0.606902198503
Iteration :  43   Loss :  0.396833734115
Iteration :  44   Loss :  0.259476754114
Iteration :  45   Loss :  0.169663463907
Iteration :  46   Loss :  0.110937455971
Iteration :  47   Loss :  0.0725384172523
Iteration :  48   Loss :  0.0474305267901
Iteration :  49   Loss :  0.0310132886379
Iteration :  50   Loss :  0.0202785871723
Iteration :  51   Loss :  0.0132595128013
Iteration :  52   Loss :  0.0086699669081
Iteration :  53   Loss :  0.00566901117064
Iteration :  54   Loss :  0.00370678319692
Iteration :  55   Loss :  0.00242374573897
Iteration :  56   Loss :  0.0922088513236
Iteration :  57   Loss :  1932.60564338
Iteration :  58   Loss :  3289.18058905
Iteration :  59   Loss :  299.234945482
Iteration :  60   Loss :  195.660060363
Iteration :  61   Loss :  127.935790252
Iteration :  62   Loss :  83.6530786969
Iteration :  63   Loss :  54.698044712
Iteration :  64   Loss :  35.76528374
Iteration :  65   Loss :  23.3857631975
Iteration :  66   Loss :  15.2911947884
Iteration :  67   Loss :  9.99841810085
Iteration :  68   Loss :  6.53764247352
Iteration :  69   Loss :  4.27475313399
Iteration :  70   Loss :  2.79512292552
Iteration :  71   Loss :  1.82764054996
Iteration :  72   Loss :  1.19503509108
Iteration :  73   Loss :  0.781394825658
Iteration :  74   Loss :  0.510928823869
Iteration :  75   Loss :  0.334079846049
Iteration :  76   Loss :  0.218444014748
Iteration :  77   Loss :  0.142833481707
Iteration :  78   Loss :  0.0933941976849
Iteration :  79   Loss :  0.0610674476109
Iteration :  80   Loss :  0.0399300304532
Iteration :  81   Loss :  0.0261089564796
Iteration :  82   Loss :  0.0170718028691
Iteration :  83   Loss :  0.0179058970528
Iteration :  84   Loss :  6478.19466169
Iteration :  85   Loss :  1175.50047477
Iteration :  86   Loss :  1034.40580205
Iteration :  87   Loss :  212.663634189
Iteration :  88   Loss :  139.053877666
Iteration :  89   Loss :  90.9228367494
Iteration :  90   Loss :  59.4515045631
Iteration :  91   Loss :  38.8734175173
Iteration :  92   Loss :  25.4180714277
Iteration :  93   Loss :  16.6200554613
Iteration :  94   Loss :  10.8673171496
Iteration :  95   Loss :  7.10578748094
Iteration :  96   Loss :  4.64624479339
Iteration :  97   Loss :  3.03802931596
Iteration :  98   Loss :  2.05169157502
Iteration :  99   Loss :  1908.90784091
[-0.12520929  0.0390445  -0.16762916 ...,  0.08155629 -0.10757387
  0.01760588]
CROSS VALIDATION 18
Iteration :  0   Loss :  1058.23713036
Iteration :  1   Loss :  2753.05099917
Iteration :  2   Loss :  269.748337839
Iteration :  3   Loss :  176.379720555
Iteration :  4   Loss :  115.328999141
Iteration :  5   Loss :  75.4099054079
Iteration :  6   Loss :  49.3081000961
Iteration :  7   Loss :  32.2409731445
Iteration :  8   Loss :  21.0813303956
Iteration :  9   Loss :  13.7844006524
Iteration :  10   Loss :  9.01317411097
Iteration :  11   Loss :  5.89342326904
Iteration :  12   Loss :  3.85351901566
Iteration :  13   Loss :  2.51969154872
Iteration :  14   Loss :  1.64754487389
Iteration :  15   Loss :  1.07727634871
Iteration :  16   Loss :  0.704396189681
Iteration :  17   Loss :  0.460581904197
Iteration :  18   Loss :  0.301159622356
Iteration :  19   Loss :  0.19691854437
Iteration :  20   Loss :  0.128758672273
Iteration :  21   Loss :  0.0841911346574
Iteration :  22   Loss :  0.055049862116
Iteration :  23   Loss :  0.0359953257706
Iteration :  24   Loss :  0.0496479103106
Iteration :  25   Loss :  1959.48460456
Iteration :  26   Loss :  377.434342476
Iteration :  27   Loss :  265.435952659
Iteration :  28   Loss :  173.55999125
Iteration :  29   Loss :  113.485269275
Iteration :  30   Loss :  74.2043500333
Iteration :  31   Loss :  48.519826397
Iteration :  32   Loss :  31.7255464476
Iteration :  33   Loss :  20.7443095357
Iteration :  34   Loss :  13.5640336038
Iteration :  35   Loss :  8.86908321955
Iteration :  36   Loss :  5.79920689178
Iteration :  37   Loss :  3.79191397139
Iteration :  38   Loss :  2.47941000119
Iteration :  39   Loss :  1.62120607176
Iteration :  40   Loss :  1.0600542572
Iteration :  41   Loss :  0.69313522061
Iteration :  42   Loss :  0.453218720447
Iteration :  43   Loss :  0.296345074461
Iteration :  44   Loss :  0.193770467095
Iteration :  45   Loss :  6550.8160311
Iteration :  46   Loss :  661.835219166
Iteration :  47   Loss :  695.115821599
Iteration :  48   Loss :  288.772214694
Iteration :  49   Loss :  188.818818829
Iteration :  50   Loss :  123.462523503
Iteration :  51   Loss :  80.7281541343
Iteration :  52   Loss :  52.7855310664
Iteration :  53   Loss :  34.5147528745
Iteration :  54   Loss :  22.5680814785
Iteration :  55   Loss :  14.7565391376
Iteration :  56   Loss :  9.64882404949
Iteration :  57   Loss :  6.3090542213
Iteration :  58   Loss :  4.12528666324
Iteration :  59   Loss :  2.69739163066
Iteration :  60   Loss :  1.76373721467
Iteration :  61   Loss :  1.15325076532
Iteration :  62   Loss :  0.804458872051
Iteration :  63   Loss :  3757.4890504
Iteration :  64   Loss :  1675.83428476
Iteration :  65   Loss :  229.509520605
Iteration :  66   Loss :  150.068858379
Iteration :  67   Loss :  98.1251766622
Iteration :  68   Loss :  64.1608818714
Iteration :  69   Loss :  41.9527271444
Iteration :  70   Loss :  27.4315324777
Iteration :  71   Loss :  17.9365925721
Iteration :  72   Loss :  11.7281582193
Iteration :  73   Loss :  7.66866363632
Iteration :  74   Loss :  5.01429132071
Iteration :  75   Loss :  3.27868304588
Iteration :  76   Loss :  2.14382488527
Iteration :  77   Loss :  1.40177780969
Iteration :  78   Loss :  0.916577207977
Iteration :  79   Loss :  0.599320214927
Iteration :  80   Loss :  0.391876120085
Iteration :  81   Loss :  0.256235130516
Iteration :  82   Loss :  0.167543870998
Iteration :  83   Loss :  0.109551522667
Iteration :  84   Loss :  0.0716322002546
Iteration :  85   Loss :  0.0468379807819
Iteration :  86   Loss :  0.0306258419527
Iteration :  87   Loss :  0.0200252483061
Iteration :  88   Loss :  0.0130938627039
Iteration :  89   Loss :  0.124906490728
Iteration :  90   Loss :  2003.37549711
Iteration :  91   Loss :  415.646526122
Iteration :  92   Loss :  264.544782135
Iteration :  93   Loss :  172.977283645
Iteration :  94   Loss :  113.104255604
Iteration :  95   Loss :  73.9552175067
Iteration :  96   Loss :  48.3569266892
Iteration :  97   Loss :  31.6190315931
Iteration :  98   Loss :  20.6746629147
Iteration :  99   Loss :  13.5184939291
[-0.02768287 -0.01622361 -0.01127658 ...,  0.01209715  0.01136312
  0.00589865]
CROSS VALIDATION 19
Iteration :  0   Loss :  2088.07414131
Iteration :  1   Loss :  869.96401839
Iteration :  2   Loss :  221.78419484
Iteration :  3   Loss :  145.017517523
Iteration :  4   Loss :  94.8222681228
Iteration :  5   Loss :  62.0012167187
Iteration :  6   Loss :  40.5405918958
Iteration :  7   Loss :  26.5081828752
Iteration :  8   Loss :  17.3328441072
Iteration :  9   Loss :  11.3333866097
Iteration :  10   Loss :  7.41053523877
Iteration :  11   Loss :  4.84550950359
Iteration :  12   Loss :  3.16832207025
Iteration :  13   Loss :  2.07166340989
Iteration :  14   Loss :  1.354593753
Iteration :  15   Loss :  0.885725078174
Iteration :  16   Loss :  0.579147004309
Iteration :  17   Loss :  0.378685509607
Iteration :  18   Loss :  0.24761021661
Iteration :  19   Loss :  0.161904318529
Iteration :  20   Loss :  0.105864001564
Iteration :  21   Loss :  0.189246040058
Iteration :  22   Loss :  2488.24184633
Iteration :  23   Loss :  647.407636042
Iteration :  24   Loss :  351.31345863
Iteration :  25   Loss :  204.452858796
Iteration :  26   Loss :  133.685116987
Iteration :  27   Loss :  87.412377646
Iteration :  28   Loss :  57.1561288042
Iteration :  29   Loss :  37.372545489
Iteration :  30   Loss :  24.4366997127
Iteration :  31   Loss :  15.9783682122
Iteration :  32   Loss :  10.447738595
Iteration :  33   Loss :  6.8314386238
Iteration :  34   Loss :  4.46685694194
Iteration :  35   Loss :  2.92073339725
Iteration :  36   Loss :  1.90977317803
Iteration :  37   Loss :  1.24873896226
Iteration :  38   Loss :  0.816510051454
Iteration :  39   Loss :  0.533889535185
Iteration :  40   Loss :  0.349093113149
Iteration :  41   Loss :  0.228260704915
Iteration :  42   Loss :  0.149252298157
Iteration :  43   Loss :  0.0975912543227
Iteration :  44   Loss :  0.0638117673085
Iteration :  45   Loss :  0.0417244524142
Iteration :  46   Loss :  0.0272822710089
Iteration :  47   Loss :  7958.74183088
Iteration :  48   Loss :  2200.10600302
Iteration :  49   Loss :  854.833357971
Iteration :  50   Loss :  254.850318416
Iteration :  51   Loss :  179.89492828
Iteration :  52   Loss :  117.62747987
Iteration :  53   Loss :  76.9128076759
Iteration :  54   Loss :  50.2907993193
Iteration :  55   Loss :  32.8835284083
Iteration :  56   Loss :  21.5014765169
Iteration :  57   Loss :  14.0591206231
Iteration :  58   Loss :  9.19280462158
Iteration :  59   Loss :  6.01087785474
Iteration :  60   Loss :  3.93031877342
Iteration :  61   Loss :  2.56990842836
Iteration :  62   Loss :  1.68038006861
Iteration :  63   Loss :  1.09874622139
Iteration :  64   Loss :  0.718434645574
Iteration :  65   Loss :  0.46976119682
Iteration :  66   Loss :  2978.34119783
Iteration :  67   Loss :  3323.44544137
Iteration :  68   Loss :  702.198714854
Iteration :  69   Loss :  269.43230184
Iteration :  70   Loss :  176.173074829
Iteration :  71   Loss :  115.193880179
Iteration :  72   Loss :  75.3215554854
Iteration :  73   Loss :  49.250330937
Iteration :  74   Loss :  32.2031997583
Iteration :  75   Loss :  21.0566315991
Iteration :  76   Loss :  13.7682509076
Iteration :  77   Loss :  9.00261431475
Iteration :  78   Loss :  5.88651855956
Iteration :  79   Loss :  3.84900424927
Iteration :  80   Loss :  2.51673948888
Iteration :  81   Loss :  1.64561461737
Iteration :  82   Loss :  1.07601421635
Iteration :  83   Loss :  0.703570922119
Iteration :  84   Loss :  0.460042288408
Iteration :  85   Loss :  0.300806785031
Iteration :  86   Loss :  0.196687835446
Iteration :  87   Loss :  0.128607819164
Iteration :  88   Loss :  0.0840924966842
Iteration :  89   Loss :  0.0549853659331
Iteration :  90   Loss :  0.035953153801
Iteration :  91   Loss :  0.0235086053589
Iteration :  92   Loss :  0.0153715173078
Iteration :  93   Loss :  0.0100509384005
Iteration :  94   Loss :  0.00657198380013
Iteration :  95   Loss :  0.00429720781765
Iteration :  96   Loss :  0.00280980531748
Iteration :  97   Loss :  0.0777711364751
Iteration :  98   Loss :  3746.97840506
Iteration :  99   Loss :  370.764919625
[-0.20184377 -0.15364679  0.01722729 ...,  0.1481294   0.07959275
  0.00688206]
Accuracy (Hinge Loss):	0.85
lmda : 0.1  eta : 0.01
Logistic Loss
CROSS VALIDATION 0
Iteration :  0   Loss :  1708.86859499
Iteration :  1   Loss :  184.599981888
Iteration :  2   Loss :  149.30317062
Iteration :  3   Loss :  120.755357253
Iteration :  4   Loss :  97.6660861556
Iteration :  5   Loss :  78.9916461971
Iteration :  6   Loss :  63.8878922306
Iteration :  7   Loss :  51.6720884899
Iteration :  8   Loss :  41.7920483289
Iteration :  9   Loss :  33.8011676798
Iteration :  10   Loss :  27.3382127785
Iteration :  11   Loss :  22.1110017028
Iteration :  12   Loss :  17.8832562733
Iteration :  13   Loss :  14.4638837905
Iteration :  14   Loss :  11.6983192522
Iteration :  15   Loss :  9.46155004899
Iteration :  16   Loss :  7.65246680548
Iteration :  17   Loss :  6.18929296656
Iteration :  18   Loss :  5.00588822795
Iteration :  19   Loss :  4.04875877544
Iteration :  20   Loss :  3.27463918731
Iteration :  21   Loss :  2.64853669395
Iteration :  22   Loss :  2.1421493891
Iteration :  23   Loss :  1.73258689244
Iteration :  24   Loss :  1.40133564259
Iteration :  25   Loss :  1.13342208691
Iteration :  26   Loss :  0.916736061229
Iteration :  27   Loss :  0.74148403166
Iteration :  28   Loss :  0.599747504492
Iteration :  29   Loss :  0.485123744704
Iteration :  30   Loss :  0.392425978476
Iteration :  31   Loss :  0.317449345502
Iteration :  32   Loss :  0.256802146286
Iteration :  33   Loss :  0.207748632647
Iteration :  34   Loss :  0.168077735461
Iteration :  35   Loss :  0.13600271381
Iteration :  36   Loss :  0.110080628527
Iteration :  37   Loss :  0.0891284169354
Iteration :  38   Loss :  0.0721764905499
Iteration :  39   Loss :  0.0584686451428
Iteration :  40   Loss :  0.0474046453565
Iteration :  41   Loss :  0.0384930843613
Iteration :  42   Loss :  0.031321629886
Iteration :  43   Loss :  0.0255314769072
Iteration :  44   Loss :  0.0208521224396
Iteration :  45   Loss :  0.0170868407604
Iteration :  46   Loss :  0.0140418639726
Iteration :  47   Loss :  0.011592345622
Iteration :  48   Loss :  0.00961718664622
Iteration :  49   Loss :  0.00804611661006
Iteration :  50   Loss :  0.00679953150816
Iteration :  51   Loss :  0.00578564934709
Iteration :  52   Loss :  0.00495233349225
Iteration :  53   Loss :  0.00427926189351
Iteration :  54   Loss :  0.00375404175677
Iteration :  55   Loss :  0.003378450339
Iteration :  56   Loss :  0.00313315157023
Iteration :  57   Loss :  0.00295125250139
Iteration :  58   Loss :  0.00278136933272
Iteration :  59   Loss :  0.00258502592308
Iteration :  60   Loss :  0.00243387531934
Iteration :  61   Loss :  0.00232475114175
Iteration :  62   Loss :  0.00224491807206
[ -4.06654045e-04  -1.73971185e-04   3.58173324e-05 ...,   2.16481634e-04
   1.35100281e-04   3.35596855e-05]
CROSS VALIDATION 1
Iteration :  0   Loss :  1093.88718713
Iteration :  1   Loss :  201.717594879
Iteration :  2   Loss :  163.147775945
Iteration :  3   Loss :  131.952776909
Iteration :  4   Loss :  106.722480482
Iteration :  5   Loss :  86.3163936911
Iteration :  6   Loss :  69.8120938172
Iteration :  7   Loss :  56.4635318359
Iteration :  8   Loss :  45.667308529
Iteration :  9   Loss :  36.9353988428
Iteration :  10   Loss :  29.873091549
Iteration :  11   Loss :  24.1611470375
Iteration :  12   Loss :  19.5413668087
Iteration :  13   Loss :  15.8049225715
Iteration :  14   Loss :  12.7829173673
Iteration :  15   Loss :  10.3387491746
Iteration :  16   Loss :  8.3619260293
Iteration :  17   Loss :  6.76307806374
Iteration :  18   Loss :  5.46993858401
Iteration :  19   Loss :  4.42405652927
Iteration :  20   Loss :  3.57815445306
Iteration :  21   Loss :  2.89399478715
Iteration :  22   Loss :  2.34065124785
Iteration :  23   Loss :  1.89311087428
Iteration :  24   Loss :  1.5311433592
Iteration :  25   Loss :  1.23838657309
Iteration :  26   Loss :  1.00160694781
Iteration :  27   Loss :  0.810101308034
Iteration :  28   Loss :  0.655213191274
Iteration :  29   Loss :  0.529942026656
Iteration :  30   Loss :  0.428627942448
Iteration :  31   Loss :  0.346697448923
Iteration :  32   Loss :  0.280450987655
Iteration :  33   Loss :  0.226876495415
Iteration :  34   Loss :  0.183536100428
Iteration :  35   Loss :  0.148483482172
Iteration :  36   Loss :  0.120134730994
Iteration :  37   Loss :  0.0972024040203
Iteration :  38   Loss :  0.0786558709257
Iteration :  39   Loss :  0.0636631131875
Iteration :  40   Loss :  0.0515489734828
Iteration :  41   Loss :  0.0417582741712
Iteration :  42   Loss :  0.0338482269141
Iteration :  43   Loss :  0.0274723417346
Iteration :  44   Loss :  0.0223348548161
Iteration :  45   Loss :  0.0182014308305
Iteration :  46   Loss :  0.0148945545558
Iteration :  47   Loss :  0.0122579062737
Iteration :  48   Loss :  0.0101214973786
Iteration :  49   Loss :  0.00838741127128
Iteration :  50   Loss :  0.00699020859513
Iteration :  51   Loss :  0.00587522485294
Iteration :  52   Loss :  0.00500619294859
Iteration :  53   Loss :  0.00433566854894
Iteration :  54   Loss :  0.00381957116865
Iteration :  55   Loss :  0.00341313502512
Iteration :  56   Loss :  0.0030765909315
Iteration :  57   Loss :  0.00280299983231
Iteration :  58   Loss :  0.00261147985213
Iteration :  59   Loss :  0.00251884550855
[ -3.42307827e-04  -1.09596419e-04  -1.10026327e-04 ...,   1.96668066e-04
   5.27003614e-05   7.52234901e-05]
CROSS VALIDATION 2
Iteration :  0   Loss :  5012.25047459
Iteration :  1   Loss :  1552.58458939
Iteration :  2   Loss :  204.669140392
Iteration :  3   Loss :  165.534965255
Iteration :  4   Loss :  133.88351888
Iteration :  5   Loss :  108.284051046
Iteration :  6   Loss :  87.5793810109
Iteration :  7   Loss :  70.8335891031
Iteration :  8   Loss :  57.2897100586
Iteration :  9   Loss :  46.3355156807
Iteration :  10   Loss :  37.4758400975
Iteration :  11   Loss :  30.3101966253
Iteration :  12   Loss :  24.5146744429
Iteration :  13   Loss :  19.82729675
Iteration :  14   Loss :  16.036178548
Iteration :  15   Loss :  12.9699487362
Iteration :  16   Loss :  10.49000357
Iteration :  17   Loss :  8.48424131335
Iteration :  18   Loss :  6.86199486802
Iteration :  19   Loss :  5.54993332105
Iteration :  20   Loss :  4.48874714431
Iteration :  21   Loss :  3.63046720744
Iteration :  22   Loss :  2.93629641425
Iteration :  23   Loss :  2.37485594544
Iteration :  24   Loss :  1.9207668321
Iteration :  25   Loss :  1.55350274239
Iteration :  26   Loss :  1.25646212515
Iteration :  27   Loss :  1.01621777503
Iteration :  28   Loss :  0.821909927227
Iteration :  29   Loss :  0.66475554505
Iteration :  30   Loss :  0.537651825902
Iteration :  31   Loss :  0.43485587725
Iteration :  32   Loss :  0.351722869387
Iteration :  33   Loss :  0.284487742708
Iteration :  34   Loss :  0.230107431223
Iteration :  35   Loss :  0.186133025475
Iteration :  36   Loss :  0.150569265836
Iteration :  37   Loss :  0.121797162275
Iteration :  38   Loss :  0.0985247834916
Iteration :  39   Loss :  0.0797031909088
Iteration :  40   Loss :  0.064483449615
Iteration :  41   Loss :  0.0521838236266
Iteration :  42   Loss :  0.0422623034478
Iteration :  43   Loss :  0.0342846912126
Iteration :  44   Loss :  0.0278686460619
Iteration :  45   Loss :  0.0226848435842
Iteration :  46   Loss :  0.0185042006253
Iteration :  47   Loss :  0.0151198827329
Iteration :  48   Loss :  0.0123679486205
Iteration :  49   Loss :  0.0101442809342
Iteration :  50   Loss :  0.0083734000756
Iteration :  51   Loss :  0.00698757724688
Iteration :  52   Loss :  0.0059102560207
Iteration :  53   Loss :  0.00506186114396
Iteration :  54   Loss :  0.00439802919652
Iteration :  55   Loss :  0.00387601637497
Iteration :  56   Loss :  0.00345170409537
Iteration :  57   Loss :  0.00310846168228
Iteration :  58   Loss :  0.00285075078067
Iteration :  59   Loss :  0.00266583406622
Iteration :  60   Loss :  0.00247121089883
Iteration :  61   Loss :  0.00229508448013
Iteration :  62   Loss :  0.00216494399922
Iteration :  63   Loss :  0.00207033567859
[ -4.29607696e-04  -2.40065967e-04  -6.62205125e-06 ...,   3.62121659e-04
   1.40693864e-04   3.36938309e-05]
CROSS VALIDATION 3
Iteration :  0   Loss :  1435.43600428
Iteration :  1   Loss :  828.37214988
Iteration :  2   Loss :  186.420820636
Iteration :  3   Loss :  150.775852224
Iteration :  4   Loss :  121.946451777
Iteration :  5   Loss :  98.6294348975
Iteration :  6   Loss :  79.7707951844
Iteration :  7   Loss :  64.518059654
Iteration :  8   Loss :  52.1817541357
Iteration :  9   Loss :  42.204236756
Iteration :  10   Loss :  34.134490679
Iteration :  11   Loss :  27.6077366509
Iteration :  12   Loss :  22.3289437699
Iteration :  13   Loss :  18.0594931119
Iteration :  14   Loss :  14.6063913644
Iteration :  15   Loss :  11.8135468902
Iteration :  16   Loss :  9.5547138677
Iteration :  17   Loss :  7.72778556197
Iteration :  18   Loss :  6.25017876189
Iteration :  19   Loss :  5.05510074553
Iteration :  20   Loss :  4.08853002785
Iteration :  21   Loss :  3.30677441066
Iteration :  22   Loss :  2.67449595051
Iteration :  23   Loss :  2.16311356657
Iteration :  24   Loss :  1.74951108114
Iteration :  25   Loss :  1.41499229182
Iteration :  26   Loss :  1.14443584127
Iteration :  27   Loss :  0.925611681917
Iteration :  28   Loss :  0.748628238743
Iteration :  29   Loss :  0.605485286
Iteration :  30   Loss :  0.489712352926
Iteration :  31   Loss :  0.396076403449
Iteration :  32   Loss :  0.32034580407
Iteration :  33   Loss :  0.259100144533
Iteration :  34   Loss :  0.209575622348
Iteration :  35   Loss :  0.169531148166
Iteration :  36   Loss :  0.137141491819
Iteration :  37   Loss :  0.110943473274
Iteration :  38   Loss :  0.0897542688801
Iteration :  39   Loss :  0.0726216337967
Iteration :  40   Loss :  0.0587790209954
Iteration :  41   Loss :  0.0475913517693
Iteration :  42   Loss :  0.0385421102981
Iteration :  43   Loss :  0.0312350543006
Iteration :  44   Loss :  0.0253476902153
Iteration :  45   Loss :  0.0206082845626
Iteration :  46   Loss :  0.0167790652734
Iteration :  47   Loss :  0.0136937106742
Iteration :  48   Loss :  0.0112281058915
Iteration :  49   Loss :  0.00925773805214
Iteration :  50   Loss :  0.00768843944994
Iteration :  51   Loss :  0.00645321672728
Iteration :  52   Loss :  0.00548635464455
Iteration :  53   Loss :  0.0047329287434
Iteration :  54   Loss :  0.00412366907388
Iteration :  55   Loss :  0.00363131454755
Iteration :  56   Loss :  0.00323079561378
Iteration :  57   Loss :  0.00289745318624
Iteration :  58   Loss :  0.00263176881402
Iteration :  59   Loss :  0.0024491058611
Iteration :  60   Loss :  0.00237389647256
[ -4.84498022e-04  -6.26025730e-05  -1.22303846e-04 ...,   3.40382051e-04
   6.58049239e-05   5.39026195e-05]
CROSS VALIDATION 4
Iteration :  0   Loss :  1435.43600428
Iteration :  1   Loss :  828.37214988
Iteration :  2   Loss :  186.420820636
Iteration :  3   Loss :  150.775852224
Iteration :  4   Loss :  121.946451777
Iteration :  5   Loss :  98.6294348975
Iteration :  6   Loss :  79.7707951844
Iteration :  7   Loss :  64.518059654
Iteration :  8   Loss :  52.1817541357
Iteration :  9   Loss :  42.204236756
Iteration :  10   Loss :  34.134490679
Iteration :  11   Loss :  27.6077366509
Iteration :  12   Loss :  22.3289437699
Iteration :  13   Loss :  18.0594931119
Iteration :  14   Loss :  14.6063913644
Iteration :  15   Loss :  11.8135468902
Iteration :  16   Loss :  9.5547138677
Iteration :  17   Loss :  7.72778556197
Iteration :  18   Loss :  6.25017876189
Iteration :  19   Loss :  5.05510074553
Iteration :  20   Loss :  4.08853002785
Iteration :  21   Loss :  3.30677441066
Iteration :  22   Loss :  2.67449595051
Iteration :  23   Loss :  2.16311356657
Iteration :  24   Loss :  1.74951108114
Iteration :  25   Loss :  1.41499229182
Iteration :  26   Loss :  1.14443584127
Iteration :  27   Loss :  0.925611681917
Iteration :  28   Loss :  0.748628238743
Iteration :  29   Loss :  0.605485286
Iteration :  30   Loss :  0.489712352926
Iteration :  31   Loss :  0.396076403449
Iteration :  32   Loss :  0.32034580407
Iteration :  33   Loss :  0.259100144533
Iteration :  34   Loss :  0.209575622348
Iteration :  35   Loss :  0.169531148166
Iteration :  36   Loss :  0.137141491819
Iteration :  37   Loss :  0.110943473274
Iteration :  38   Loss :  0.0897542688801
Iteration :  39   Loss :  0.0726216337968
Iteration :  40   Loss :  0.0587790209981
Iteration :  41   Loss :  0.0475913518089
Iteration :  42   Loss :  0.0385421107409
Iteration :  43   Loss :  0.0312350581797
Iteration :  44   Loss :  0.0253477175611
Iteration :  45   Loss :  0.0206084424492
Iteration :  46   Loss :  0.0167798115436
Iteration :  47   Loss :  0.0136965785708
Iteration :  48   Loss :  0.0112369742115
Iteration :  49   Loss :  0.00927807262821
Iteration :  50   Loss :  0.00771784616695
Iteration :  51   Loss :  0.00647766974138
Iteration :  52   Loss :  0.0054982244471
Iteration :  53   Loss :  0.004733323155
Iteration :  54   Loss :  0.00411705858258
Iteration :  55   Loss :  0.00362198467889
Iteration :  56   Loss :  0.00322104904741
Iteration :  57   Loss :  0.00288857494127
Iteration :  58   Loss :  0.00262376350279
Iteration :  59   Loss :  0.0024426517639
Iteration :  60   Loss :  0.00237056000873
[ -4.73761617e-04  -6.06376127e-05  -1.19901699e-04 ...,   3.38083649e-04
   7.27584950e-05   5.44915752e-05]
CROSS VALIDATION 5
Iteration :  0   Loss :  1435.43600428
Iteration :  1   Loss :  828.37214988
Iteration :  2   Loss :  186.420820636
Iteration :  3   Loss :  150.775852224
Iteration :  4   Loss :  121.946451777
Iteration :  5   Loss :  98.6294348975
Iteration :  6   Loss :  79.7707951844
Iteration :  7   Loss :  64.518059654
Iteration :  8   Loss :  52.1817541357
Iteration :  9   Loss :  42.204236756
Iteration :  10   Loss :  34.134490679
Iteration :  11   Loss :  27.6077366509
Iteration :  12   Loss :  22.3289437699
Iteration :  13   Loss :  18.0594931119
Iteration :  14   Loss :  14.6063913644
Iteration :  15   Loss :  11.8135468902
Iteration :  16   Loss :  9.5547138677
Iteration :  17   Loss :  7.72778556197
Iteration :  18   Loss :  6.25017876189
Iteration :  19   Loss :  5.05510074553
Iteration :  20   Loss :  4.08853002785
Iteration :  21   Loss :  3.30677441066
Iteration :  22   Loss :  2.67449595051
Iteration :  23   Loss :  2.16311356657
Iteration :  24   Loss :  1.74951108114
Iteration :  25   Loss :  1.41499229182
Iteration :  26   Loss :  1.14443584127
Iteration :  27   Loss :  0.925611681917
Iteration :  28   Loss :  0.748628238743
Iteration :  29   Loss :  0.605485285999
Iteration :  30   Loss :  0.489712352925
Iteration :  31   Loss :  0.396076403441
Iteration :  32   Loss :  0.320345803957
Iteration :  33   Loss :  0.259100143396
Iteration :  34   Loss :  0.209575613309
Iteration :  35   Loss :  0.16953108997
Iteration :  36   Loss :  0.137141182624
Iteration :  37   Loss :  0.110942097609
Iteration :  38   Loss :  0.0897491175555
Iteration :  39   Loss :  0.0726057504318
Iteration :  40   Loss :  0.05874195187
Iteration :  41   Loss :  0.0475337076482
Iteration :  42   Loss :  0.0384786820819
Iteration :  43   Loss :  0.0311696972013
Iteration :  44   Loss :  0.0252782492908
Iteration :  45   Loss :  0.0205360753236
Iteration :  46   Loss :  0.0167074443
Iteration :  47   Loss :  0.0136232477057
Iteration :  48   Loss :  0.0111612946116
Iteration :  49   Loss :  0.00920352847701
Iteration :  50   Loss :  0.00765271131167
Iteration :  51   Loss :  0.00642675624248
Iteration :  52   Loss :  0.00546268857612
Iteration :  53   Loss :  0.00470534105168
Iteration :  54   Loss :  0.00409264733608
Iteration :  55   Loss :  0.00359842729699
Iteration :  56   Loss :  0.00319772291854
Iteration :  57   Loss :  0.00286523452543
Iteration :  58   Loss :  0.00260055458766
Iteration :  59   Loss :  0.00242034663755
Iteration :  60   Loss :  0.00234870294992
[ -4.52712584e-04  -5.91324819e-05  -1.09583081e-04 ...,   3.08711010e-04
   3.62558120e-05   5.91987288e-05]
CROSS VALIDATION 6
Iteration :  0   Loss :  1435.43600428
Iteration :  1   Loss :  828.37214988
Iteration :  2   Loss :  186.420820636
Iteration :  3   Loss :  150.775852224
Iteration :  4   Loss :  121.946451777
Iteration :  5   Loss :  98.6294348975
Iteration :  6   Loss :  79.7707951844
Iteration :  7   Loss :  64.518059654
Iteration :  8   Loss :  52.1817541357
Iteration :  9   Loss :  42.204236756
Iteration :  10   Loss :  34.134490679
Iteration :  11   Loss :  27.6077366509
Iteration :  12   Loss :  22.3289437699
Iteration :  13   Loss :  18.0594931119
Iteration :  14   Loss :  14.6063913644
Iteration :  15   Loss :  11.8135468902
Iteration :  16   Loss :  9.5547138677
Iteration :  17   Loss :  7.72778556197
Iteration :  18   Loss :  6.25017876189
Iteration :  19   Loss :  5.05510074553
Iteration :  20   Loss :  4.08853002785
Iteration :  21   Loss :  3.30677441066
Iteration :  22   Loss :  2.67449595051
Iteration :  23   Loss :  2.16311356657
Iteration :  24   Loss :  1.74951108114
Iteration :  25   Loss :  1.41499229182
Iteration :  26   Loss :  1.14443584127
Iteration :  27   Loss :  0.925611681917
Iteration :  28   Loss :  0.748628238743
Iteration :  29   Loss :  0.605485286
Iteration :  30   Loss :  0.489712352926
Iteration :  31   Loss :  0.396076403449
Iteration :  32   Loss :  0.32034580407
Iteration :  33   Loss :  0.259100144534
Iteration :  34   Loss :  0.20957562235
Iteration :  35   Loss :  0.169531148178
Iteration :  36   Loss :  0.137141491875
Iteration :  37   Loss :  0.110943473307
Iteration :  38   Loss :  0.0897542659524
Iteration :  39   Loss :  0.0726215936304
Iteration :  40   Loss :  0.0587787461248
Iteration :  41   Loss :  0.0475905057345
Iteration :  42   Loss :  0.0385409602409
Iteration :  43   Loss :  0.0312339095012
Iteration :  44   Loss :  0.0253465706174
Iteration :  45   Loss :  0.0206072585553
Iteration :  46   Loss :  0.0167785950138
Iteration :  47   Loss :  0.0136954691296
Iteration :  48   Loss :  0.0112363982337
Iteration :  49   Loss :  0.00927920008375
Iteration :  50   Loss :  0.00772312291638
Iteration :  51   Loss :  0.0064901059158
Iteration :  52   Loss :  0.00551778132703
Iteration :  53   Loss :  0.00475419365667
Iteration :  54   Loss :  0.0041333272712
Iteration :  55   Loss :  0.00363413548428
Iteration :  56   Loss :  0.00323078992177
Iteration :  57   Loss :  0.00289591699089
Iteration :  58   Loss :  0.00263032702633
Iteration :  59   Loss :  0.00244851436006
Iteration :  60   Loss :  0.00237241451909
[ -4.86917486e-04  -6.82344219e-05  -1.18397063e-04 ...,   3.30450405e-04
   6.57280192e-05   5.17451021e-05]
CROSS VALIDATION 7
Iteration :  0   Loss :  1435.43600428
Iteration :  1   Loss :  828.37214988
Iteration :  2   Loss :  186.420820636
Iteration :  3   Loss :  150.775852224
Iteration :  4   Loss :  121.946451777
Iteration :  5   Loss :  98.6294348975
Iteration :  6   Loss :  79.7707951844
Iteration :  7   Loss :  64.518059654
Iteration :  8   Loss :  52.1817541357
Iteration :  9   Loss :  42.204236756
Iteration :  10   Loss :  34.134490679
Iteration :  11   Loss :  27.6077366509
Iteration :  12   Loss :  22.3289437699
Iteration :  13   Loss :  18.0594931119
Iteration :  14   Loss :  14.6063913644
Iteration :  15   Loss :  11.8135468902
Iteration :  16   Loss :  9.5547138677
Iteration :  17   Loss :  7.72778556197
Iteration :  18   Loss :  6.25017876189
Iteration :  19   Loss :  5.05510074553
Iteration :  20   Loss :  4.08853002785
Iteration :  21   Loss :  3.30677441066
Iteration :  22   Loss :  2.67449595051
Iteration :  23   Loss :  2.16311356657
Iteration :  24   Loss :  1.74951108114
Iteration :  25   Loss :  1.41499229182
Iteration :  26   Loss :  1.14443584127
Iteration :  27   Loss :  0.925611681917
Iteration :  28   Loss :  0.748628238743
Iteration :  29   Loss :  0.605485286
Iteration :  30   Loss :  0.489712352926
Iteration :  31   Loss :  0.396076403449
Iteration :  32   Loss :  0.32034580407
Iteration :  33   Loss :  0.259100144534
Iteration :  34   Loss :  0.20957562235
Iteration :  35   Loss :  0.169531148178
Iteration :  36   Loss :  0.137141491875
Iteration :  37   Loss :  0.110943473306
Iteration :  38   Loss :  0.0897542659519
Iteration :  39   Loss :  0.0726215936218
Iteration :  40   Loss :  0.0587787460078
Iteration :  41   Loss :  0.0475905045113
Iteration :  42   Loss :  0.0385409500761
Iteration :  43   Loss :  0.0312338406706
Iteration :  44   Loss :  0.025346182624
Iteration :  45   Loss :  0.0206054165426
Iteration :  46   Loss :  0.0167712833823
Iteration :  47   Loss :  0.0136725302381
Iteration :  48   Loss :  0.0111878469694
Iteration :  49   Loss :  0.00922140920462
Iteration :  50   Loss :  0.00767358826652
Iteration :  51   Loss :  0.00644675918772
Iteration :  52   Loss :  0.00548070111997
Iteration :  53   Loss :  0.00472465328236
Iteration :  54   Loss :  0.00411254821815
Iteration :  55   Loss :  0.00361963208997
Iteration :  56   Loss :  0.00321970418134
Iteration :  57   Loss :  0.00288715293569
Iteration :  58   Loss :  0.00262130386717
Iteration :  59   Loss :  0.00243748388557
Iteration :  60   Loss :  0.00236120992685
[ -4.95308096e-04  -6.73907692e-05  -1.24963998e-04 ...,   3.27186357e-04
   6.32693148e-05   5.50592200e-05]
CROSS VALIDATION 8
Iteration :  0   Loss :  1435.43600428
Iteration :  1   Loss :  828.37214988
Iteration :  2   Loss :  186.420820636
Iteration :  3   Loss :  150.775852224
Iteration :  4   Loss :  121.946451777
Iteration :  5   Loss :  98.6294348975
Iteration :  6   Loss :  79.7707951844
Iteration :  7   Loss :  64.518059654
Iteration :  8   Loss :  52.1817541357
Iteration :  9   Loss :  42.204236756
Iteration :  10   Loss :  34.134490679
Iteration :  11   Loss :  27.6077366509
Iteration :  12   Loss :  22.3289437699
Iteration :  13   Loss :  18.0594931119
Iteration :  14   Loss :  14.6063913644
Iteration :  15   Loss :  11.8135468902
Iteration :  16   Loss :  9.5547138677
Iteration :  17   Loss :  7.72778556197
Iteration :  18   Loss :  6.25017876189
Iteration :  19   Loss :  5.05510074553
Iteration :  20   Loss :  4.08853002785
Iteration :  21   Loss :  3.30677441066
Iteration :  22   Loss :  2.67449595051
Iteration :  23   Loss :  2.16311356657
Iteration :  24   Loss :  1.74951108114
Iteration :  25   Loss :  1.41499229182
Iteration :  26   Loss :  1.14443584127
Iteration :  27   Loss :  0.925611681917
Iteration :  28   Loss :  0.748628238743
Iteration :  29   Loss :  0.605485286
Iteration :  30   Loss :  0.489712352926
Iteration :  31   Loss :  0.396076403449
Iteration :  32   Loss :  0.32034580407
Iteration :  33   Loss :  0.259100144534
Iteration :  34   Loss :  0.20957562235
Iteration :  35   Loss :  0.169531148178
Iteration :  36   Loss :  0.137141491874
Iteration :  37   Loss :  0.110943473297
Iteration :  38   Loss :  0.0897542658359
Iteration :  39   Loss :  0.0726215925529
Iteration :  40   Loss :  0.0587787381705
Iteration :  41   Loss :  0.0475904578797
Iteration :  42   Loss :  0.0385407219332
Iteration :  43   Loss :  0.0312329181228
Iteration :  44   Loss :  0.0253431269055
Iteration :  45   Loss :  0.0205974134076
Iteration :  46   Loss :  0.0167563148922
Iteration :  47   Loss :  0.013657871956
Iteration :  48   Loss :  0.0111886330726
Iteration :  49   Loss :  0.00922757423085
Iteration :  50   Loss :  0.00767195354563
Iteration :  51   Loss :  0.006443084486
Iteration :  52   Loss :  0.00547936857372
Iteration :  53   Loss :  0.00472796316083
Iteration :  54   Loss :  0.00411795159917
Iteration :  55   Loss :  0.00362363391427
Iteration :  56   Loss :  0.00322164780738
Iteration :  57   Loss :  0.00288742217627
Iteration :  58   Loss :  0.0026211522255
Iteration :  59   Loss :  0.00243613926421
Iteration :  60   Loss :  0.00235589653809
[ -4.70782838e-04  -5.93143376e-05  -9.54618351e-05 ...,   3.34022226e-04
   5.68064715e-05   5.05125637e-05]
CROSS VALIDATION 9
Iteration :  0   Loss :  1859.95844877
Iteration :  1   Loss :  1006.69971367
Iteration :  2   Loss :  198.2384944
Iteration :  3   Loss :  160.333903878
Iteration :  4   Loss :  129.676936918
Iteration :  5   Loss :  104.881797059
Iteration :  6   Loss :  84.8276618486
Iteration :  7   Loss :  68.6080179445
Iteration :  8   Loss :  55.4896837151
Iteration :  9   Loss :  44.8796669989
Iteration :  10   Loss :  36.2983598947
Iteration :  11   Loss :  29.3578588958
Iteration :  12   Loss :  23.7444303667
Iteration :  13   Loss :  19.2043287435
Iteration :  14   Loss :  15.5323263937
Iteration :  15   Loss :  12.5624366476
Iteration :  16   Loss :  10.160410651
Iteration :  17   Loss :  8.21766887214
Iteration :  18   Loss :  6.64639294766
Iteration :  19   Loss :  5.37555600366
Iteration :  20   Loss :  4.34771201805
Iteration :  21   Loss :  3.51639912805
Iteration :  22   Loss :  2.84403961852
Iteration :  23   Loss :  2.30024190569
Iteration :  24   Loss :  1.86042759755
Iteration :  25   Loss :  1.50471799562
Iteration :  26   Loss :  1.2170276789
Iteration :  27   Loss :  0.984349179333
Iteration :  28   Loss :  0.796162434282
Iteration :  29   Loss :  0.643953525177
Iteration :  30   Loss :  0.520854325274
Iteration :  31   Loss :  0.421308079833
Iteration :  32   Loss :  0.340806229547
Iteration :  33   Loss :  0.275695089003
Iteration :  34   Loss :  0.223037693073
Iteration :  35   Loss :  0.180455132149
Iteration :  36   Loss :  0.146016398842
Iteration :  37   Loss :  0.118163996484
Iteration :  38   Loss :  0.0956391213515
Iteration :  39   Loss :  0.0774240401734
Iteration :  40   Loss :  0.0626964131988
Iteration :  41   Loss :  0.0507942567774
Iteration :  42   Loss :  0.0411751490961
Iteration :  43   Loss :  0.0334034269227
Iteration :  44   Loss :  0.0271356865752
Iteration :  45   Loss :  0.022084734858
Iteration :  46   Loss :  0.0180059234607
Iteration :  47   Loss :  0.0147118970055
Iteration :  48   Loss :  0.0120463934522
Iteration :  49   Loss :  0.00990292334597
Iteration :  50   Loss :  0.0081879215218
Iteration :  51   Loss :  0.00681118682255
Iteration :  52   Loss :  0.00571667652308
Iteration :  53   Loss :  0.00485670493737
Iteration :  54   Loss :  0.00417843988136
Iteration :  55   Loss :  0.00365971445873
Iteration :  56   Loss :  0.0032789836384
Iteration :  57   Loss :  0.00297766210197
Iteration :  58   Loss :  0.00274798138541
Iteration :  59   Loss :  0.00257586442117
Iteration :  60   Loss :  0.00237591262276
Iteration :  61   Loss :  0.00220909735193
Iteration :  62   Loss :  0.00208824475754
Iteration :  63   Loss :  0.00201127710148
[ -4.71165628e-04  -1.08549142e-04  -4.28422702e-05 ...,   3.21904018e-04
   1.77780374e-04   4.02466463e-05]
CROSS VALIDATION 10
Iteration :  0   Loss :  1434.20083289
Iteration :  1   Loss :  827.389220291
Iteration :  2   Loss :  186.434137385
Iteration :  3   Loss :  150.786622718
Iteration :  4   Loss :  121.955162877
Iteration :  5   Loss :  98.6364803737
Iteration :  6   Loss :  79.7764935162
Iteration :  7   Loss :  64.5226684248
Iteration :  8   Loss :  52.1854816772
Iteration :  9   Loss :  42.2072515654
Iteration :  10   Loss :  34.1369290357
Iteration :  11   Loss :  27.6097087767
Iteration :  12   Loss :  22.3305388115
Iteration :  13   Loss :  18.0607831702
Iteration :  14   Loss :  14.6074347546
Iteration :  15   Loss :  11.8143907768
Iteration :  16   Loss :  9.55539639731
Iteration :  17   Loss :  7.72833758713
Iteration :  18   Loss :  6.25062523597
Iteration :  19   Loss :  5.05546185063
Iteration :  20   Loss :  4.08882208713
Iteration :  21   Loss :  3.30701062616
Iteration :  22   Loss :  2.67468699994
Iteration :  23   Loss :  2.16326808601
Iteration :  24   Loss :  1.74963605538
Iteration :  25   Loss :  1.41509337011
Iteration :  26   Loss :  1.14451759268
Iteration :  27   Loss :  0.925677801888
Iteration :  28   Loss :  0.748681716126
Iteration :  29   Loss :  0.605528538228
Iteration :  30   Loss :  0.489747335525
Iteration :  31   Loss :  0.396104699292
Iteration :  32   Loss :  0.320368697277
Iteration :  33   Loss :  0.259118681948
Iteration :  34   Loss :  0.209590651623
Iteration :  35   Loss :  0.169543310319
Iteration :  36   Loss :  0.13715129792
Iteration :  37   Loss :  0.110951391749
Iteration :  38   Loss :  0.0897606366102
Iteration :  39   Loss :  0.0726266633775
Iteration :  40   Loss :  0.0587826518027
Iteration :  41   Loss :  0.0475930717445
Iteration :  42   Loss :  0.0385405712715
Iteration :  43   Loss :  0.0312253028039
Iteration :  44   Loss :  0.025319362069
Iteration :  45   Loss :  0.0205571932454
Iteration :  46   Loss :  0.0167199999434
Iteration :  47   Loss :  0.0136353849431
Iteration :  48   Loss :  0.0111743960517
Iteration :  49   Loss :  0.00921656438531
Iteration :  50   Loss :  0.00765976318544
Iteration :  51   Loss :  0.00642659268377
Iteration :  52   Loss :  0.00545802646406
Iteration :  53   Loss :  0.00470661194354
Iteration :  54   Loss :  0.0040987127861
Iteration :  55   Loss :  0.00360576151369
Iteration :  56   Loss :  0.00320437769521
Iteration :  57   Loss :  0.00287019761754
Iteration :  58   Loss :  0.0026042889931
Iteration :  59   Loss :  0.00242157246773
Iteration :  60   Loss :  0.00234572905291
[ -4.63669845e-04  -5.57983142e-05  -1.16606644e-04 ...,   3.30356407e-04
   5.65543800e-05   5.80410134e-05]
CROSS VALIDATION 11
Iteration :  0   Loss :  1434.20083289
Iteration :  1   Loss :  827.389220291
Iteration :  2   Loss :  186.434137385
Iteration :  3   Loss :  150.786622718
Iteration :  4   Loss :  121.955162877
Iteration :  5   Loss :  98.6364803737
Iteration :  6   Loss :  79.7764935162
Iteration :  7   Loss :  64.5226684248
Iteration :  8   Loss :  52.1854816772
Iteration :  9   Loss :  42.2072515654
Iteration :  10   Loss :  34.1369290357
Iteration :  11   Loss :  27.6097087767
Iteration :  12   Loss :  22.3305388115
Iteration :  13   Loss :  18.0607831702
Iteration :  14   Loss :  14.6074347546
Iteration :  15   Loss :  11.8143907768
Iteration :  16   Loss :  9.55539639731
Iteration :  17   Loss :  7.72833758713
Iteration :  18   Loss :  6.25062523597
Iteration :  19   Loss :  5.05546185063
Iteration :  20   Loss :  4.08882208713
Iteration :  21   Loss :  3.30701062616
Iteration :  22   Loss :  2.67468699994
Iteration :  23   Loss :  2.16326808601
Iteration :  24   Loss :  1.74963605538
Iteration :  25   Loss :  1.41509337011
Iteration :  26   Loss :  1.14451759268
Iteration :  27   Loss :  0.925677801888
Iteration :  28   Loss :  0.748681716126
Iteration :  29   Loss :  0.605528538228
Iteration :  30   Loss :  0.489747335525
Iteration :  31   Loss :  0.396104699292
Iteration :  32   Loss :  0.320368697277
Iteration :  33   Loss :  0.259118681948
Iteration :  34   Loss :  0.209590651623
Iteration :  35   Loss :  0.169543310321
Iteration :  36   Loss :  0.137151297954
Iteration :  37   Loss :  0.110951392141
Iteration :  38   Loss :  0.0897606400891
Iteration :  39   Loss :  0.072626688189
Iteration :  40   Loss :  0.058782798051
Iteration :  41   Loss :  0.0475937996713
Iteration :  42   Loss :  0.0385436448678
Iteration :  43   Loss :  0.031236055644
Iteration :  44   Loss :  0.0253481551637
Iteration :  45   Loss :  0.0206080007256
Iteration :  46   Loss :  0.0167785893656
Iteration :  47   Loss :  0.0136947751778
Iteration :  48   Loss :  0.0112344215391
Iteration :  49   Loss :  0.00927507166031
Iteration :  50   Loss :  0.00771479824299
Iteration :  51   Loss :  0.00647207708828
Iteration :  52   Loss :  0.00548641556742
Iteration :  53   Loss :  0.00471967344135
Iteration :  54   Loss :  0.0041073684229
Iteration :  55   Loss :  0.00361354433223
Iteration :  56   Loss :  0.00321233833835
Iteration :  57   Loss :  0.00287967634156
Iteration :  58   Loss :  0.00261436411577
Iteration :  59   Loss :  0.00243278453354
Iteration :  60   Loss :  0.00236050210384
[ -4.83389466e-04  -5.64506351e-05  -1.03675840e-04 ...,   3.33849737e-04
   6.68475290e-05   5.15055845e-05]
CROSS VALIDATION 12
Iteration :  0   Loss :  1310.24303339
Iteration :  1   Loss :  441.614233547
Iteration :  2   Loss :  169.427049064
Iteration :  3   Loss :  137.031408967
Iteration :  4   Loss :  110.83004247
Iteration :  5   Loss :  89.6385610176
Iteration :  6   Loss :  72.49903945
Iteration :  7   Loss :  58.6367146182
Iteration :  8   Loss :  47.4249635209
Iteration :  9   Loss :  38.3569778696
Iteration :  10   Loss :  31.0228546753
Iteration :  11   Loss :  25.0910672753
Iteration :  12   Loss :  20.2934792302
Iteration :  13   Loss :  16.4132236683
Iteration :  14   Loss :  13.274900185
Iteration :  15   Loss :  10.736646163
Iteration :  16   Loss :  8.68372411264
Iteration :  17   Loss :  7.02333515697
Iteration :  18   Loss :  5.68042421516
Iteration :  19   Loss :  4.59428726423
Iteration :  20   Loss :  3.7158273162
Iteration :  21   Loss :  3.00533507152
Iteration :  22   Loss :  2.43069392723
Iteration :  23   Loss :  1.9659282001
Iteration :  24   Loss :  1.59002893974
Iteration :  25   Loss :  1.28600425442
Iteration :  26   Loss :  1.04011122129
Iteration :  27   Loss :  0.841234660713
Iteration :  28   Loss :  0.680384693389
Iteration :  29   Loss :  0.550290367977
Iteration :  30   Loss :  0.445070990046
Iteration :  31   Loss :  0.359970295353
Iteration :  32   Loss :  0.291141452208
Iteration :  33   Loss :  0.23547318049
Iteration :  34   Loss :  0.190449157089
Iteration :  35   Loss :  0.154034466582
Iteration :  36   Loss :  0.124584304029
Iteration :  37   Loss :  0.100771456883
Iteration :  38   Loss :  0.0815286826704
Iteration :  39   Loss :  0.0659929419026
Iteration :  40   Loss :  0.0534402038926
Iteration :  41   Loss :  0.0432852414441
Iteration :  42   Loss :  0.0350828084759
Iteration :  43   Loss :  0.0284733382621
Iteration :  44   Loss :  0.0231573545749
Iteration :  45   Loss :  0.0188546102503
Iteration :  46   Loss :  0.0153762743516
Iteration :  47   Loss :  0.0125918118057
Iteration :  48   Loss :  0.0103727357802
Iteration :  49   Loss :  0.00861136414447
Iteration :  50   Loss :  0.0072197141315
Iteration :  51   Loss :  0.00611914201838
Iteration :  52   Loss :  0.0052428597428
Iteration :  53   Loss :  0.00452377824727
Iteration :  54   Loss :  0.00393880841355
Iteration :  55   Loss :  0.00348049771794
Iteration :  56   Loss :  0.00311289532949
Iteration :  57   Loss :  0.00280589173284
Iteration :  58   Loss :  0.00256021533694
Iteration :  59   Loss :  0.00238765084314
Iteration :  60   Loss :  0.0023148656631
[ -7.92121401e-04  -2.19550915e-04  -2.03490969e-04 ...,   2.63768435e-04
   5.90430229e-05   1.91127673e-05]
CROSS VALIDATION 13
Iteration :  0   Loss :  2008.64768025
Iteration :  1   Loss :  1202.99897152
Iteration :  2   Loss :  184.673730822
Iteration :  3   Loss :  149.362818236
Iteration :  4   Loss :  120.803599799
Iteration :  5   Loss :  97.7051042327
Iteration :  6   Loss :  79.0232030258
Iteration :  7   Loss :  63.9134123596
Iteration :  8   Loss :  51.6927196447
Iteration :  9   Loss :  41.8087090897
Iteration :  10   Loss :  33.8145906766
Iteration :  11   Loss :  27.3490037727
Iteration :  12   Loss :  22.1196824327
Iteration :  13   Loss :  17.89024401
Iteration :  14   Loss :  14.4695038779
Iteration :  15   Loss :  11.7028332512
Iteration :  16   Loss :  9.46516945299
Iteration :  17   Loss :  7.6553626674
Iteration :  18   Loss :  6.19160363272
Iteration :  19   Loss :  5.00772559188
Iteration :  20   Loss :  4.05021333586
Iteration :  21   Loss :  3.27578413892
Iteration :  22   Loss :  2.64943123608
Iteration :  23   Loss :  2.14284140134
Iteration :  24   Loss :  1.73311509608
Iteration :  25   Loss :  1.4017313341
Iteration :  26   Loss :  1.13371047165
Iteration :  27   Loss :  0.916937077929
Iteration :  28   Loss :  0.741612277479
Iteration :  29   Loss :  0.599810808596
Iteration :  30   Loss :  0.485122775536
Iteration :  31   Loss :  0.392363902227
Iteration :  32   Loss :  0.317341198893
Iteration :  33   Loss :  0.256663486322
Iteration :  34   Loss :  0.207588336119
Iteration :  35   Loss :  0.167898807832
Iteration :  36   Loss :  0.135804675785
Iteration :  37   Loss :  0.10986127763
Iteration :  38   Loss :  0.0888920978097
Iteration :  39   Loss :  0.0719281172497
Iteration :  40   Loss :  0.058205343435
Iteration :  41   Loss :  0.0471271180621
Iteration :  42   Loss :  0.038208948723
Iteration :  43   Loss :  0.0310395392799
Iteration :  44   Loss :  0.0252699884845
Iteration :  45   Loss :  0.020612701653
Iteration :  46   Loss :  0.0168517984735
Iteration :  47   Loss :  0.0138194655228
Iteration :  48   Loss :  0.011383639829
Iteration :  49   Loss :  0.009422719594
Iteration :  50   Loss :  0.00783164921201
Iteration :  51   Loss :  0.00654946436334
Iteration :  52   Loss :  0.00552348959052
Iteration :  53   Loss :  0.00471343709236
Iteration :  54   Loss :  0.00409669578237
Iteration :  55   Loss :  0.00362262925133
Iteration :  56   Loss :  0.00322065910162
Iteration :  57   Loss :  0.00287930999567
Iteration :  58   Loss :  0.00260990440875
Iteration :  59   Loss :  0.00243689072518
Iteration :  60   Loss :  0.0023707546099
[ -5.96120745e-04  -2.83452225e-04  -1.55790103e-04 ...,   2.80681256e-04
   6.92655642e-05   5.18920241e-05]
CROSS VALIDATION 14
Iteration :  0   Loss :  1435.53751046
Iteration :  1   Loss :  828.076736734
Iteration :  2   Loss :  186.501231329
Iteration :  3   Loss :  150.840887829
Iteration :  4   Loss :  121.999052118
Iteration :  5   Loss :  98.6719776845
Iteration :  6   Loss :  79.8052034922
Iteration :  7   Loss :  64.545888852
Iteration :  8   Loss :  52.2042621958
Iteration :  9   Loss :  42.2224411172
Iteration :  10   Loss :  34.1492142386
Iteration :  11   Loss :  27.6196449627
Iteration :  12   Loss :  22.338575129
Iteration :  13   Loss :  18.0672828875
Iteration :  14   Loss :  14.6126916803
Iteration :  15   Loss :  11.8186425415
Iteration :  16   Loss :  9.55883519474
Iteration :  17   Loss :  7.73111886238
Iteration :  18   Loss :  6.25287471188
Iteration :  19   Loss :  5.05728121097
Iteration :  20   Loss :  4.09029357301
Iteration :  21   Loss :  3.30820075362
Iteration :  22   Loss :  2.67564956669
Iteration :  23   Loss :  2.16404660324
Iteration :  24   Loss :  1.75026571466
Iteration :  25   Loss :  1.41560263411
Iteration :  26   Loss :  1.14492948181
Iteration :  27   Loss :  0.926010934659
Iteration :  28   Loss :  0.748951149163
Iteration :  29   Loss :  0.605746436563
Iteration :  30   Loss :  0.489923470019
Iteration :  31   Loss :  0.396246671833
Iteration :  32   Loss :  0.320481564944
Iteration :  33   Loss :  0.259203435667
Iteration :  34   Loss :  0.209642787279
Iteration :  35   Loss :  0.169560776322
Iteration :  36   Loss :  0.13714814094
Iteration :  37   Loss :  0.1109384428
Iteration :  38   Loss :  0.0897395270121
Iteration :  39   Loss :  0.0725991000814
Iteration :  40   Loss :  0.0587501671514
Iteration :  41   Loss :  0.0475569363544
Iteration :  42   Loss :  0.0385033878026
Iteration :  43   Loss :  0.0311934969562
Iteration :  44   Loss :  0.0253036567226
Iteration :  45   Loss :  0.0205608505713
Iteration :  46   Loss :  0.0167306109246
Iteration :  47   Loss :  0.0136473620098
Iteration :  48   Loss :  0.0111846723057
Iteration :  49   Loss :  0.00922239687605
Iteration :  50   Loss :  0.0076659302017
Iteration :  51   Loss :  0.00643894255915
Iteration :  52   Loss :  0.00547387039802
Iteration :  53   Loss :  0.00471507984148
Iteration :  54   Loss :  0.00410038641369
Iteration :  55   Loss :  0.00360597531935
Iteration :  56   Loss :  0.00320685060205
Iteration :  57   Loss :  0.00287574399918
Iteration :  58   Loss :  0.00261053625863
Iteration :  59   Loss :  0.00242695907182
Iteration :  60   Loss :  0.00234928193773
[ -4.43414898e-04  -4.34461191e-05  -9.78559243e-05 ...,   3.09298586e-04
   4.40461866e-05   4.80784219e-05]
CROSS VALIDATION 15
Iteration :  0   Loss :  1435.53751046
Iteration :  1   Loss :  828.076736734
Iteration :  2   Loss :  186.501231329
Iteration :  3   Loss :  150.840887829
Iteration :  4   Loss :  121.999052118
Iteration :  5   Loss :  98.6719776845
Iteration :  6   Loss :  79.8052034922
Iteration :  7   Loss :  64.545888852
Iteration :  8   Loss :  52.2042621958
Iteration :  9   Loss :  42.2224411172
Iteration :  10   Loss :  34.1492142386
Iteration :  11   Loss :  27.6196449627
Iteration :  12   Loss :  22.338575129
Iteration :  13   Loss :  18.0672828875
Iteration :  14   Loss :  14.6126916803
Iteration :  15   Loss :  11.8186425415
Iteration :  16   Loss :  9.55883519474
Iteration :  17   Loss :  7.73111886238
Iteration :  18   Loss :  6.25287471188
Iteration :  19   Loss :  5.05728121097
Iteration :  20   Loss :  4.09029357301
Iteration :  21   Loss :  3.30820075362
Iteration :  22   Loss :  2.67564956669
Iteration :  23   Loss :  2.16404660324
Iteration :  24   Loss :  1.75026571466
Iteration :  25   Loss :  1.41560263412
Iteration :  26   Loss :  1.14492948183
Iteration :  27   Loss :  0.926010934924
Iteration :  28   Loss :  0.748951151605
Iteration :  29   Loss :  0.605746454571
Iteration :  30   Loss :  0.489923578548
Iteration :  31   Loss :  0.396247216599
Iteration :  32   Loss :  0.320483867549
Iteration :  33   Loss :  0.259211550791
Iteration :  34   Loss :  0.209665205566
Iteration :  35   Loss :  0.169603112765
Iteration :  36   Loss :  0.137199556651
Iteration :  37   Loss :  0.110990270772
Iteration :  38   Loss :  0.0897919272621
Iteration :  39   Loss :  0.0726518309013
Iteration :  40   Loss :  0.0588029731643
Iteration :  41   Loss :  0.0476099452369
Iteration :  42   Loss :  0.0385565234942
Iteration :  43   Loss :  0.031246244396
Iteration :  44   Loss :  0.0253561430497
Iteration :  45   Loss :  0.02061426522
Iteration :  46   Loss :  0.0167832449579
Iteration :  47   Loss :  0.0136972529852
Iteration :  48   Loss :  0.0112325175599
Iteration :  49   Loss :  0.00926371259319
Iteration :  50   Loss :  0.00769122913123
Iteration :  51   Loss :  0.00645012243825
Iteration :  52   Loss :  0.00548316619883
Iteration :  53   Loss :  0.00472948187744
Iteration :  54   Loss :  0.00411831470146
Iteration :  55   Loss :  0.00362465071837
Iteration :  56   Loss :  0.00322379375427
Iteration :  57   Loss :  0.00289061243423
Iteration :  58   Loss :  0.00262439444183
Iteration :  59   Loss :  0.00244001872359
Iteration :  60   Loss :  0.00236267113485
[ -4.82363098e-04  -4.44104806e-05  -1.15260752e-04 ...,   3.34288154e-04
   6.53293869e-05   5.12315537e-05]
CROSS VALIDATION 16
Iteration :  0   Loss :  2057.06379668
Iteration :  1   Loss :  191.610266872
Iteration :  2   Loss :  154.973039943
Iteration :  3   Loss :  125.341107766
Iteration :  4   Loss :  101.375008852
Iteration :  5   Loss :  81.9913961421
Iteration :  6   Loss :  66.3140661339
Iteration :  7   Loss :  53.6343515799
Iteration :  8   Loss :  43.3790873807
Iteration :  9   Loss :  35.0847016242
Iteration :  10   Loss :  28.3762605989
Iteration :  11   Loss :  22.9505205488
Iteration :  12   Loss :  18.562220051
Iteration :  13   Loss :  15.0129933868
Iteration :  14   Loss :  12.1424037541
Iteration :  15   Loss :  9.82069099273
Iteration :  16   Loss :  7.94290599521
Iteration :  17   Loss :  6.42416665954
Iteration :  18   Loss :  5.19582093725
Iteration :  19   Loss :  4.2023435355
Iteration :  20   Loss :  3.39882600174
Iteration :  21   Loss :  2.74894676834
Iteration :  22   Loss :  2.22332950416
Iteration :  23   Loss :  1.79821581145
Iteration :  24   Loss :  1.45439237921
Iteration :  25   Loss :  1.17632148013
Iteration :  26   Loss :  0.951426926757
Iteration :  27   Loss :  0.769524771794
Iteration :  28   Loss :  0.622397867732
Iteration :  29   Loss :  0.503402696933
Iteration :  30   Loss :  0.407160235211
Iteration :  31   Loss :  0.329320010624
Iteration :  32   Loss :  0.266363405292
Iteration :  33   Loss :  0.215444669258
Iteration :  34   Loss :  0.174262528781
Iteration :  35   Loss :  0.140956916431
Iteration :  36   Loss :  0.114026413579
Iteration :  37   Loss :  0.0922596799594
Iteration :  38   Loss :  0.0746668747206
Iteration :  39   Loss :  0.0604359069633
Iteration :  40   Loss :  0.0489370242859
Iteration :  41   Loss :  0.0396609005688
Iteration :  42   Loss :  0.0321796916786
Iteration :  43   Loss :  0.0261351809714
Iteration :  44   Loss :  0.0212731869114
Iteration :  45   Loss :  0.0173712592148
Iteration :  46   Loss :  0.0142397725705
Iteration :  47   Loss :  0.0117298528675
Iteration :  48   Loss :  0.00970447704784
Iteration :  49   Loss :  0.00807268962094
Iteration :  50   Loss :  0.00677021618382
Iteration :  51   Loss :  0.00573011915457
Iteration :  52   Loss :  0.00489453243987
Iteration :  53   Loss :  0.0042395214637
Iteration :  54   Loss :  0.00373936033051
Iteration :  55   Loss :  0.00334543635063
Iteration :  56   Loss :  0.00302247472115
Iteration :  57   Loss :  0.00275805330479
Iteration :  58   Loss :  0.00256601016234
Iteration :  59   Loss :  0.00246972182171
[ -4.40561594e-04  -4.21836914e-05  -1.82346894e-04 ...,   3.07738234e-04
   2.59752925e-05   7.44525473e-05]
CROSS VALIDATION 17
Iteration :  0   Loss :  1438.03806812
Iteration :  1   Loss :  828.748720193
Iteration :  2   Loss :  186.532545028
Iteration :  3   Loss :  150.866214129
Iteration :  4   Loss :  122.019535852
Iteration :  5   Loss :  98.6885447847
Iteration :  6   Loss :  79.8186028466
Iteration :  7   Loss :  64.5567261559
Iteration :  8   Loss :  52.2130273312
Iteration :  9   Loss :  42.2295302972
Iteration :  10   Loss :  34.1549479177
Iteration :  11   Loss :  27.6242823221
Iteration :  12   Loss :  22.3423257928
Iteration :  13   Loss :  18.070316398
Iteration :  14   Loss :  14.6151451624
Iteration :  15   Loss :  11.8206269006
Iteration :  16   Loss :  9.56044013045
Iteration :  17   Loss :  7.73241692312
Iteration :  18   Loss :  6.25392457431
Iteration :  19   Loss :  5.05813033234
Iteration :  20   Loss :  4.09098033641
Iteration :  21   Loss :  3.30875620304
Iteration :  22   Loss :  2.67609881029
Iteration :  23   Loss :  2.16440994831
Iteration :  24   Loss :  1.7505595856
Iteration :  25   Loss :  1.41584031488
Iteration :  26   Loss :  1.14512171637
Iteration :  27   Loss :  0.926166412874
Iteration :  28   Loss :  0.749076901147
Iteration :  29   Loss :  0.605848160348
Iteration :  30   Loss :  0.490005839723
Iteration :  31   Loss :  0.39631375866
Iteration :  32   Loss :  0.320537721515
Iteration :  33   Loss :  0.259255206366
Iteration :  34   Loss :  0.209700682966
Iteration :  35   Loss :  0.169631844279
Iteration :  36   Loss :  0.137222671497
Iteration :  37   Loss :  0.111008985214
Iteration :  38   Loss :  0.0898071639469
Iteration :  39   Loss :  0.0726644235827
Iteration :  40   Loss :  0.058813438317
Iteration :  41   Loss :  0.0476182274365
Iteration :  42   Loss :  0.0385629552933
Iteration :  43   Loss :  0.0312512889293
Iteration :  44   Loss :  0.0253599744057
Iteration :  45   Loss :  0.0206174036673
Iteration :  46   Loss :  0.0167862461525
Iteration :  47   Loss :  0.0137008189608
Iteration :  48   Loss :  0.0112393872279
Iteration :  49   Loss :  0.00928029648427
Iteration :  50   Loss :  0.00772346863967
Iteration :  51   Loss :  0.00649053386972
Iteration :  52   Loss :  0.00552002216556
Iteration :  53   Loss :  0.00476018489113
Iteration :  54   Loss :  0.00414319805124
Iteration :  55   Loss :  0.00364445918825
Iteration :  56   Loss :  0.00323982750788
Iteration :  57   Loss :  0.00290424966822
Iteration :  58   Loss :  0.0026382550461
Iteration :  59   Loss :  0.00245622267564
Iteration :  60   Loss :  0.00238041854074
[ -4.78423656e-04  -5.83489513e-05  -1.08900248e-04 ...,   3.37699771e-04
   6.22259012e-05   5.19824586e-05]
CROSS VALIDATION 18
Iteration :  0   Loss :  1842.94013686
Iteration :  1   Loss :  1612.24983251
Iteration :  2   Loss :  166.762648211
Iteration :  3   Loss :  134.876460244
Iteration :  4   Loss :  109.087135058
Iteration :  5   Loss :  88.2289097267
Iteration :  6   Loss :  71.3589233729
Iteration :  7   Loss :  57.7145967315
Iteration :  8   Loss :  46.6791610422
Iteration :  9   Loss :  37.7537780602
Iteration :  10   Loss :  30.5349909305
Iteration :  11   Loss :  24.6964866661
Iteration :  12   Loss :  19.974345953
Iteration :  13   Loss :  16.1551136946
Iteration :  14   Loss :  13.0661503637
Iteration :  15   Loss :  10.5678235871
Iteration :  16   Loss :  8.5471915231
Iteration :  17   Loss :  6.91291483042
Iteration :  18   Loss :  5.59112324256
Iteration :  19   Loss :  4.52206767414
Iteration :  20   Loss :  3.65742381944
Iteration :  21   Loss :  2.9581085711
Iteration :  22   Loss :  2.39251338018
Iteration :  23   Loss :  1.93506944924
Iteration :  24   Loss :  1.56508773474
Iteration :  25   Loss :  1.26584514813
Iteration :  26   Loss :  1.023819839
Iteration :  27   Loss :  0.828071468793
Iteration :  28   Loss :  0.66975157373
Iteration :  29   Loss :  0.541703631143
Iteration :  30   Loss :  0.438139717334
Iteration :  31   Loss :  0.3543793636
Iteration :  32   Loss :  0.286639036422
Iteration :  33   Loss :  0.231861228842
Iteration :  34   Loss :  0.187568060466
Iteration :  35   Loss :  0.151742660332
Iteration :  36   Loss :  0.122766931912
Iteration :  37   Loss :  0.0993395422166
Iteration :  38   Loss :  0.0804051528641
Iteration :  39   Loss :  0.0651022934533
Iteration :  40   Loss :  0.0527331482158
Iteration :  41   Loss :  0.0427529750797
Iteration :  42   Loss :  0.0347170461942
Iteration :  43   Loss :  0.0282325794584
Iteration :  44   Loss :  0.0229982629624
Iteration :  45   Loss :  0.0187719983581
Iteration :  46   Loss :  0.0153540105091
Iteration :  47   Loss :  0.0126074151286
Iteration :  48   Loss :  0.0104116002353
Iteration :  49   Loss :  0.00865231390259
Iteration :  50   Loss :  0.00722931350729
Iteration :  51   Loss :  0.0060747947496
Iteration :  52   Loss :  0.00513784164367
Iteration :  53   Loss :  0.00439427137633
Iteration :  54   Loss :  0.00383165783972
Iteration :  55   Loss :  0.00342095444886
Iteration :  56   Loss :  0.00308372594501
Iteration :  57   Loss :  0.0027814110206
Iteration :  58   Loss :  0.00254105403728
Iteration :  59   Loss :  0.0023955434401
Iteration :  60   Loss :  0.0023443402324
[ -4.02093692e-04  -2.20869121e-04  -1.33398378e-04 ...,   2.72069236e-04
   1.18974371e-04   6.21416385e-05]
CROSS VALIDATION 19
Iteration :  0   Loss :  1815.08537467
Iteration :  1   Loss :  472.921367901
Iteration :  2   Loss :  198.924266367
Iteration :  3   Loss :  160.888551436
Iteration :  4   Loss :  130.125531972
Iteration :  5   Loss :  105.244617593
Iteration :  6   Loss :  85.1211085514
Iteration :  7   Loss :  68.8453555795
Iteration :  8   Loss :  55.6816407297
Iteration :  9   Loss :  45.034920486
Iteration :  10   Loss :  36.4239278261
Iteration :  11   Loss :  29.459417358
Iteration :  12   Loss :  23.8265701387
Iteration :  13   Loss :  19.2707628149
Iteration :  14   Loss :  15.5860578047
Iteration :  15   Loss :  12.605894236
Iteration :  16   Loss :  10.1955588437
Iteration :  17   Loss :  8.24609648383
Iteration :  18   Loss :  6.66938500019
Iteration :  19   Loss :  5.39415181086
Iteration :  20   Loss :  4.36275215148
Iteration :  21   Loss :  3.52856334093
Iteration :  22   Loss :  2.85387728174
Iteration :  23   Loss :  2.30819592914
Iteration :  24   Loss :  1.86685267841
Iteration :  25   Loss :  1.50989735269
Iteration :  26   Loss :  1.22119438884
Iteration :  27   Loss :  0.987693456562
Iteration :  28   Loss :  0.798839540248
Iteration :  29   Loss :  0.646095817309
Iteration :  30   Loss :  0.522557765561
Iteration :  31   Loss :  0.422641056408
Iteration :  32   Loss :  0.341829128114
Iteration :  33   Loss :  0.276469042266
Iteration :  34   Loss :  0.223606452523
Iteration :  35   Loss :  0.180852411681
Iteration :  36   Loss :  0.14627642914
Iteration :  37   Loss :  0.1183211633
Iteration :  38   Loss :  0.0957306815171
Iteration :  39   Loss :  0.0774766643628
Iteration :  40   Loss :  0.0627109255156
Iteration :  41   Loss :  0.0507794694865
Iteration :  42   Loss :  0.0411628887205
Iteration :  43   Loss :  0.0334040747422
Iteration :  44   Loss :  0.0271309967527
Iteration :  45   Loss :  0.022065927725
Iteration :  46   Loss :  0.0179912038223
Iteration :  47   Loss :  0.0147174142915
Iteration :  48   Loss :  0.0120774982383
Iteration :  49   Loss :  0.00994333952508
Iteration :  50   Loss :  0.00822861850802
Iteration :  51   Loss :  0.00686615674557
Iteration :  52   Loss :  0.00578676013941
Iteration :  53   Loss :  0.00493340475252
Iteration :  54   Loss :  0.00426606277828
Iteration :  55   Loss :  0.00374931692127
Iteration :  56   Loss :  0.00333502764092
Iteration :  57   Loss :  0.00300874031199
Iteration :  58   Loss :  0.00278340147163
Iteration :  59   Loss :  0.00265021515724
Iteration :  60   Loss :  0.0025041576576
Iteration :  61   Loss :  0.0023638788076
Iteration :  62   Loss :  0.00224911805833
Iteration :  63   Loss :  0.00216478950789
[ -4.30611516e-04  -1.98506202e-04  -1.28167489e-04 ...,   2.86547680e-04
   9.72896256e-05   3.36829017e-05]
Accuracy (Logistic Loss):	0.85
Hinge Loss
CROSS VALIDATION 0
Iteration :  0   Loss :  1144.6482135
Iteration :  1   Loss :  218.692742567
Iteration :  2   Loss :  176.877156335
Iteration :  3   Loss :  143.057003475
Iteration :  4   Loss :  115.703501047
Iteration :  5   Loss :  93.5801801329
Iteration :  6   Loss :  75.686993345
Iteration :  7   Loss :  61.215109369
Iteration :  8   Loss :  49.5103511112
Iteration :  9   Loss :  40.0436247263
Iteration :  10   Loss :  32.3870028233
Iteration :  11   Loss :  26.1943807297
Iteration :  12   Loss :  21.1858314137
Iteration :  13   Loss :  17.1349518556
Iteration :  14   Loss :  13.8586288808
Iteration :  15   Loss :  11.2087618381
Iteration :  16   Loss :  9.06556795938
Iteration :  17   Loss :  7.33216778206
Iteration :  18   Loss :  5.93020587626
Iteration :  19   Loss :  4.79630891983
Iteration :  20   Loss :  3.87922101433
Iteration :  21   Loss :  3.13748674856
Iteration :  22   Loss :  2.53757727674
Iteration :  23   Loss :  2.05237470354
Iteration :  24   Loss :  1.65994626541
Iteration :  25   Loss :  1.34255289704
Iteration :  26   Loss :  1.08584736682
Iteration :  27   Loss :  0.878225734438
Iteration :  28   Loss :  0.710302814372
Iteration :  29   Loss :  0.574487934389
Iteration :  30   Loss :  0.464641812028
Iteration :  31   Loss :  0.375799038694
Iteration :  32   Loss :  0.303943626742
Iteration :  33   Loss :  0.245827473529
Iteration :  34   Loss :  0.198823536422
Iteration :  35   Loss :  0.160807081762
Iteration :  36   Loss :  0.130059639871
Iteration :  37   Loss :  0.105191324524
Iteration :  38   Loss :  0.0850780054913
Iteration :  39   Loss :  0.0688104941268
Iteration :  40   Loss :  0.0556534450313
Iteration :  41   Loss :  0.0450121160029
Iteration :  42   Loss :  0.0364054837201
Iteration :  43   Loss :  0.0294444998944
Iteration :  44   Loss :  0.0238145049987
Iteration :  45   Loss :  0.0192610046144
Iteration :  46   Loss :  0.0155781654405
Iteration :  47   Loss :  0.0125995109472
Iteration :  48   Loss :  0.0101903960845
Iteration :  49   Loss :  0.0082419208804
Iteration :  50   Loss :  0.00666600780143
Iteration :  51   Loss :  0.00539142035619
Iteration :  52   Loss :  0.0043605429701
Iteration :  53   Loss :  0.00352677657053
Iteration :  54   Loss :  0.0643505733962
Iteration :  55   Loss :  1154.14695707
Iteration :  56   Loss :  1714.58092311
Iteration :  57   Loss :  219.377747247
Iteration :  58   Loss :  177.431183317
Iteration :  59   Loss :  143.505096611
Iteration :  60   Loss :  116.065915633
Iteration :  61   Loss :  93.8732985083
Iteration :  62   Loss :  75.9240654312
Iteration :  63   Loss :  61.4068516095
Iteration :  64   Loss :  49.6654308905
Iteration :  65   Loss :  40.1690521641
Iteration :  66   Loss :  32.4884476553
Iteration :  67   Loss :  26.2764285983
Iteration :  68   Loss :  21.2521911544
Iteration :  69   Loss :  17.1886231485
Iteration :  70   Loss :  13.902037846
Iteration :  71   Loss :  11.2438707047
Iteration :  72   Loss :  9.09396376443
Iteration :  73   Loss :  7.35513411003
Iteration :  74   Loss :  5.94878088124
Iteration :  75   Loss :  4.81133225358
Iteration :  76   Loss :  3.89137178129
Iteration :  77   Loss :  3.14731420366
Iteration :  78   Loss :  2.5455256535
Iteration :  79   Loss :  2.05880329491
Iteration :  80   Loss :  1.66514566502
Iteration :  81   Loss :  1.34675813498
Iteration :  82   Loss :  1.08924853377
Iteration :  83   Loss :  0.880976574414
Iteration :  84   Loss :  0.712527674451
Iteration :  85   Loss :  0.576287385616
Iteration :  86   Loss :  0.46609719556
Iteration :  87   Loss :  0.376976142687
Iteration :  88   Loss :  0.304895660195
Iteration :  89   Loss :  0.246597471509
Iteration :  90   Loss :  0.199446305387
Iteration :  91   Loss :  0.161310772934
Iteration :  92   Loss :  0.130467021758
Iteration :  93   Loss :  0.105520812137
Iteration :  94   Loss :  0.0853444927614
Iteration :  95   Loss :  0.0690260271619
Iteration :  96   Loss :  0.0558277666384
Iteration :  97   Loss :  0.045153106096
Iteration :  98   Loss :  0.0365195155186
Iteration :  99   Loss :  0.0295367280133
[-0.00278411 -0.00068849 -0.00212537 ...,  0.00122336 -0.00037017
  0.0002756 ]
CROSS VALIDATION 1
Iteration :  0   Loss :  1114.69505724
Iteration :  1   Loss :  185.855030882
Iteration :  2   Loss :  150.318245439
Iteration :  3   Loss :  121.576342619
Iteration :  4   Loss :  98.3300932063
Iteration :  5   Loss :  79.5286897241
Iteration :  6   Loss :  64.3222464558
Iteration :  7   Loss :  52.0233817943
Iteration :  8   Loss :  42.0761463171
Iteration :  9   Loss :  34.0308920304
Iteration :  10   Loss :  27.5239467905
Iteration :  11   Loss :  22.2611751186
Iteration :  12   Loss :  18.0046823019
Iteration :  13   Loss :  14.5620607657
Iteration :  14   Loss :  11.7776926129
Iteration :  15   Loss :  9.52571517972
Iteration :  16   Loss :  7.70433162653
Iteration :  17   Loss :  6.2312093834
Iteration :  18   Loss :  5.03975844525
Iteration :  19   Loss :  4.07612128299
Iteration :  20   Loss :  3.29673830486
Iteration :  21   Loss :  2.66637881852
Iteration :  22   Loss :  2.15654848715
Iteration :  23   Loss :  1.74420129095
Iteration :  24   Loss :  1.41069777076
Iteration :  25   Loss :  1.14096246273
Iteration :  26   Loss :  0.922802437448
Iteration :  27   Loss :  0.746356139117
Iteration :  28   Loss :  0.603647610574
Iteration :  29   Loss :  0.488225953609
Iteration :  30   Loss :  0.394873726992
Iteration :  31   Loss :  0.319371100852
Iteration :  32   Loss :  0.258305106385
Iteration :  33   Loss :  0.208915358361
Iteration :  34   Loss :  0.168969276566
Iteration :  35   Loss :  0.136661165781
Iteration :  36   Loss :  0.157523161093
Iteration :  37   Loss :  2037.43137638
Iteration :  38   Loss :  510.910940083
Iteration :  39   Loss :  180.806449096
Iteration :  40   Loss :  146.234987901
Iteration :  41   Loss :  118.273832561
Iteration :  42   Loss :  95.6590462341
Iteration :  43   Loss :  77.368365667
Iteration :  44   Loss :  62.5749915103
Iteration :  45   Loss :  50.6102142492
Iteration :  46   Loss :  40.9331863182
Iteration :  47   Loss :  33.1064739997
Iteration :  48   Loss :  26.7762839709
Iteration :  49   Loss :  21.656470674
Iteration :  50   Loss :  17.5156015884
Iteration :  51   Loss :  14.1664957149
Iteration :  52   Loss :  11.4577623742
Iteration :  53   Loss :  9.26695784655
Iteration :  54   Loss :  7.49505051031
Iteration :  55   Loss :  6.06194428445
Iteration :  56   Loss :  4.9028580204
Iteration :  57   Loss :  3.96539717956
Iteration :  58   Loss :  3.20718542658
Iteration :  59   Loss :  2.59394907868
Iteration :  60   Loss :  2.09796782156
Iteration :  61   Loss :  1.69682165949
Iteration :  62   Loss :  1.37237745714
Iteration :  63   Loss :  1.10996926185
Iteration :  64   Loss :  0.897735353952
Iteration :  65   Loss :  0.7260820578
Iteration :  66   Loss :  0.587250075802
Iteration :  67   Loss :  0.474963742493
Iteration :  68   Loss :  0.384147343659
Iteration :  69   Loss :  0.310695677244
Iteration :  70   Loss :  0.251288484618
Iteration :  71   Loss :  0.203240363889
Iteration :  72   Loss :  0.164379380839
Iteration :  73   Loss :  0.13294889031
Iteration :  74   Loss :  0.107528130014
Iteration :  75   Loss :  0.0869679973818
Iteration :  76   Loss :  0.070339106312
Iteration :  77   Loss :  0.0568897758454
Iteration :  78   Loss :  0.046012051694
Iteration :  79   Loss :  0.0372142246938
Iteration :  80   Loss :  0.0300986039217
Iteration :  81   Loss :  0.0243435397483
Iteration :  82   Loss :  0.0196888842093
Iteration :  83   Loss :  0.0159242314559
Iteration :  84   Loss :  0.0128794067132
Iteration :  85   Loss :  0.0104167738169
Iteration :  86   Loss :  0.00842501360262
Iteration :  87   Loss :  0.006814091911
Iteration :  88   Loss :  0.00551118974539
Iteration :  89   Loss :  0.00445741161205
Iteration :  90   Loss :  0.0636245036921
Iteration :  91   Loss :  3664.79896403
Iteration :  92   Loss :  490.722427804
Iteration :  93   Loss :  687.681989118
Iteration :  94   Loss :  161.998422133
Iteration :  95   Loss :  131.023187608
Iteration :  96   Loss :  105.970635175
Iteration :  97   Loss :  85.7083064798
Iteration :  98   Loss :  69.3202771456
Iteration :  99   Loss :  56.0657539614
[-0.03555757 -0.03378036 -0.01001421 ..., -0.01820774  0.02756007
  0.01393728]
CROSS VALIDATION 2
Iteration :  0   Loss :  1983.70360337
Iteration :  1   Loss :  210.805850538
Iteration :  2   Loss :  170.498293379
Iteration :  3   Loss :  137.897823855
Iteration :  4   Loss :  111.530792755
Iteration :  5   Loss :  90.2053229333
Iteration :  6   Loss :  72.9574325123
Iteration :  7   Loss :  59.0074597119
Iteration :  8   Loss :  47.7248195524
Iteration :  9   Loss :  38.5994993248
Iteration :  10   Loss :  31.2190043273
Iteration :  11   Loss :  25.2497117381
Iteration :  12   Loss :  20.421789759
Iteration :  13   Loss :  16.5170003241
Iteration :  14   Loss :  13.3588340163
Iteration :  15   Loss :  10.8045312571
Iteration :  16   Loss :  8.7386291006
Iteration :  17   Loss :  7.06774192615
Iteration :  18   Loss :  5.71634009861
Iteration :  19   Loss :  4.62333577887
Iteration :  20   Loss :  3.7393215511
Iteration :  21   Loss :  3.02433704392
Iteration :  22   Loss :  2.44606258923
Iteration :  23   Loss :  1.97835826614
Iteration :  24   Loss :  1.60008228997
Iteration :  25   Loss :  1.29413533358
Iteration :  26   Loss :  1.04668758108
Iteration :  27   Loss :  0.846553574387
Iteration :  28   Loss :  0.684686593463
Iteration :  29   Loss :  0.553769714583
Iteration :  30   Loss :  0.447885061161
Iteration :  31   Loss :  0.362246296121
Iteration :  32   Loss :  0.292982263603
Iteration :  33   Loss :  0.277445729628
Iteration :  34   Loss :  7284.0006653
Iteration :  35   Loss :  1719.74001681
Iteration :  36   Loss :  202.764925844
Iteration :  37   Loss :  221.264622787
Iteration :  38   Loss :  154.880129217
Iteration :  39   Loss :  125.265962222
Iteration :  40   Loss :  101.314231663
Iteration :  41   Loss :  81.9422399782
Iteration :  42   Loss :  66.2743089733
Iteration :  43   Loss :  53.6021962673
Iteration :  44   Loss :  43.3530803894
Iteration :  45   Loss :  35.063667352
Iteration :  46   Loss :  28.3592482271
Iteration :  47   Loss :  22.9367610618
Iteration :  48   Loss :  18.5510914745
Iteration :  49   Loss :  15.0039926721
Iteration :  50   Loss :  12.1351240392
Iteration :  51   Loss :  9.81480321037
Iteration :  52   Loss :  7.93814399813
Iteration :  53   Loss :  6.42031518966
Iteration :  54   Loss :  5.19270589501
Iteration :  55   Loss :  4.19982410763
Iteration :  56   Loss :  3.3967882818
Iteration :  57   Loss :  2.74729853815
Iteration :  58   Loss :  2.22199578884
Iteration :  59   Loss :  1.79713460952
Iteration :  60   Loss :  1.45350986756
Iteration :  61   Loss :  1.17558858635
Iteration :  62   Loss :  0.950807803374
Iteration :  63   Loss :  0.769006682656
Iteration :  64   Loss :  0.6988789655
Iteration :  65   Loss :  1916.2923575
Iteration :  66   Loss :  156.341651932
Iteration :  67   Loss :  126.448031544
Iteration :  68   Loss :  102.27028104
Iteration :  69   Loss :  82.7154860087
Iteration :  70   Loss :  66.8997049394
Iteration :  71   Loss :  54.1080121383
Iteration :  72   Loss :  43.7621807182
Iteration :  73   Loss :  35.3945448286
Iteration :  74   Loss :  28.6268596095
Iteration :  75   Loss :  23.153203271
Iteration :  76   Loss :  18.7261484151
Iteration :  77   Loss :  15.1455774979
Iteration :  78   Loss :  12.2496368533
Iteration :  79   Loss :  9.90742037125
Iteration :  80   Loss :  8.01305210825
Iteration :  81   Loss :  6.48090034374
Iteration :  82   Loss :  5.24170674271
Iteration :  83   Loss :  4.23945564956
Iteration :  84   Loss :  3.42884199495
Iteration :  85   Loss :  2.77322335653
Iteration :  86   Loss :  2.24296360011
Iteration :  87   Loss :  1.81409322822
Iteration :  88   Loss :  1.46722587942
Iteration :  89   Loss :  1.18668200054
Iteration :  90   Loss :  0.959780078966
Iteration :  91   Loss :  0.776263396226
Iteration :  92   Loss :  0.627836390362
Iteration :  93   Loss :  0.507789669048
Iteration :  94   Loss :  0.410696722824
Iteration :  95   Loss :  0.332168628903
Iteration :  96   Loss :  0.268655657314
Iteration :  97   Loss :  0.217286811356
Iteration :  98   Loss :  0.175740049033
Iteration :  99   Loss :  0.142137319064
[-0.00387721 -0.00151101 -0.00180794 ...,  0.00458601  0.00090692
  0.00083043]
CROSS VALIDATION 3
Iteration :  0   Loss :  1932.6325262
Iteration :  1   Loss :  239.642510213
Iteration :  2   Loss :  202.582505962
Iteration :  3   Loss :  163.847309962
Iteration :  4   Loss :  132.518555116
Iteration :  5   Loss :  107.180077928
Iteration :  6   Loss :  86.6864952956
Iteration :  7   Loss :  70.111429399
Iteration :  8   Loss :  56.705632355
Iteration :  9   Loss :  45.8631177305
Iteration :  10   Loss :  37.0937679488
Iteration :  11   Loss :  30.0011793511
Iteration :  12   Loss :  24.2647434389
Iteration :  13   Loss :  19.6251543069
Iteration :  14   Loss :  15.8726871578
Iteration :  15   Loss :  12.8377180465
Iteration :  16   Loss :  10.3830563156
Iteration :  17   Loss :  8.39774312409
Iteration :  18   Loss :  6.79203573923
Iteration :  19   Loss :  5.49335086836
Iteration :  20   Loss :  4.44298365342
Iteration :  21   Loss :  3.59345401698
Iteration :  22   Loss :  2.90636040541
Iteration :  23   Loss :  2.35064391145
Iteration :  24   Loss :  1.90118430879
Iteration :  25   Loss :  1.53766453455
Iteration :  26   Loss :  1.24365229077
Iteration :  27   Loss :  1.00585725013
Iteration :  28   Loss :  0.813530289092
Iteration :  29   Loss :  0.657977591933
Iteration :  30   Loss :  0.532167661475
Iteration :  31   Loss :  0.430413472119
Iteration :  32   Loss :  0.348115397444
Iteration :  33   Loss :  0.281553291864
Iteration :  34   Loss :  0.22771832772
Iteration :  35   Loss :  0.184176986304
Iteration :  36   Loss :  0.148961054754
Iteration :  37   Loss :  0.120478656312
Iteration :  38   Loss :  0.0974422922199
Iteration :  39   Loss :  0.0788106425132
Iteration :  40   Loss :  0.0637414949078
Iteration :  41   Loss :  0.0515536740155
Iteration :  42   Loss :  0.04169624996
Iteration :  43   Loss :  0.0337236345212
Iteration :  44   Loss :  0.0272754390722
Iteration :  45   Loss :  0.0220601838189
Iteration :  46   Loss :  0.0178421219485
Iteration :  47   Loss :  0.014430583092
Iteration :  48   Loss :  0.0116713543926
Iteration :  49   Loss :  0.00943970957293
Iteration :  50   Loss :  0.00763477089498
Iteration :  51   Loss :  0.0061749491516
Iteration :  52   Loss :  0.00499425556437
Iteration :  53   Loss :  0.0642034885548
Iteration :  54   Loss :  1125.03714128
Iteration :  55   Loss :  1703.87776215
Iteration :  56   Loss :  384.300089098
Iteration :  57   Loss :  782.279123521
Iteration :  58   Loss :  158.71751345
Iteration :  59   Loss :  128.369611676
Iteration :  60   Loss :  103.824441573
Iteration :  61   Loss :  83.9724801473
Iteration :  62   Loss :  67.9163529825
Iteration :  63   Loss :  54.930269945
Iteration :  64   Loss :  44.4272170652
Iteration :  65   Loss :  35.9324215617
Iteration :  66   Loss :  29.0618905386
Iteration :  67   Loss :  23.5050532353
Iteration :  68   Loss :  19.0107221985
Iteration :  69   Loss :  15.375738778
Iteration :  70   Loss :  12.4357896823
Iteration :  71   Loss :  10.0579794737
Iteration :  72   Loss :  8.13482325436
Iteration :  73   Loss :  6.57938799264
Iteration :  74   Loss :  5.32136286237
Iteration :  75   Loss :  4.30388096046
Iteration :  76   Loss :  3.48094873455
Iteration :  77   Loss :  2.81536692205
Iteration :  78   Loss :  2.27704902032
Iteration :  79   Loss :  1.84166127702
Iteration :  80   Loss :  1.48952272393
Iteration :  81   Loss :  1.204715532
Iteration :  82   Loss :  0.974365472727
Iteration :  83   Loss :  0.788059960402
Iteration :  84   Loss :  0.63737736873
Iteration :  85   Loss :  0.515506345434
Iteration :  86   Loss :  0.416937916562
Iteration :  87   Loss :  0.337216462624
Iteration :  88   Loss :  0.272738309824
Iteration :  89   Loss :  0.220588832071
Iteration :  90   Loss :  0.178410700228
Iteration :  91   Loss :  0.144297323019
Iteration :  92   Loss :  0.116706662795
Iteration :  93   Loss :  0.0943915303191
Iteration :  94   Loss :  0.076343207685
Iteration :  95   Loss :  0.0617458509247
Iteration :  96   Loss :  0.0499396111589
Iteration :  97   Loss :  0.0403908072422
Iteration :  98   Loss :  0.0326678016071
Iteration :  99   Loss :  0.0264214888165
[-0.00273671 -0.00058715 -0.00263428 ...,  0.00155795  0.00016134
  0.00025476]
CROSS VALIDATION 4
Iteration :  0   Loss :  1932.6325262
Iteration :  1   Loss :  239.642510213
Iteration :  2   Loss :  202.582505962
Iteration :  3   Loss :  163.847309962
Iteration :  4   Loss :  132.518555116
Iteration :  5   Loss :  107.180077928
Iteration :  6   Loss :  86.6864952956
Iteration :  7   Loss :  70.111429399
Iteration :  8   Loss :  56.705632355
Iteration :  9   Loss :  45.8631177305
Iteration :  10   Loss :  37.0937679488
Iteration :  11   Loss :  30.0011793511
Iteration :  12   Loss :  24.2647434389
Iteration :  13   Loss :  19.6251543069
Iteration :  14   Loss :  15.8726871578
Iteration :  15   Loss :  12.8377180465
Iteration :  16   Loss :  10.3830563156
Iteration :  17   Loss :  8.39774312409
Iteration :  18   Loss :  6.79203573923
Iteration :  19   Loss :  5.49335086836
Iteration :  20   Loss :  4.44298365342
Iteration :  21   Loss :  3.59345401698
Iteration :  22   Loss :  2.90636040541
Iteration :  23   Loss :  2.35064391145
Iteration :  24   Loss :  1.90118430879
Iteration :  25   Loss :  1.53766453455
Iteration :  26   Loss :  1.24365229077
Iteration :  27   Loss :  1.00585725013
Iteration :  28   Loss :  0.813530289092
Iteration :  29   Loss :  0.657977591933
Iteration :  30   Loss :  0.532167661475
Iteration :  31   Loss :  0.430413472119
Iteration :  32   Loss :  0.348115397444
Iteration :  33   Loss :  0.281553291864
Iteration :  34   Loss :  0.22771832772
Iteration :  35   Loss :  0.184176986304
Iteration :  36   Loss :  0.148961054754
Iteration :  37   Loss :  0.120478656312
Iteration :  38   Loss :  0.0974422922199
Iteration :  39   Loss :  0.0788106425132
Iteration :  40   Loss :  0.0637414949078
Iteration :  41   Loss :  0.0515536740155
Iteration :  42   Loss :  0.04169624996
Iteration :  43   Loss :  0.0337236345212
Iteration :  44   Loss :  0.0272754390722
Iteration :  45   Loss :  0.0220601838189
Iteration :  46   Loss :  0.0178421219485
Iteration :  47   Loss :  0.014430583092
Iteration :  48   Loss :  0.0116713543926
Iteration :  49   Loss :  0.00943970957293
Iteration :  50   Loss :  0.00763477089498
Iteration :  51   Loss :  0.0061749491516
Iteration :  52   Loss :  0.00499425556437
Iteration :  53   Loss :  0.0642034885548
Iteration :  54   Loss :  1125.03714128
Iteration :  55   Loss :  1703.87776215
Iteration :  56   Loss :  384.300089098
Iteration :  57   Loss :  782.279123521
Iteration :  58   Loss :  158.71751345
Iteration :  59   Loss :  128.369611676
Iteration :  60   Loss :  103.824441573
Iteration :  61   Loss :  83.9724801473
Iteration :  62   Loss :  67.9163529825
Iteration :  63   Loss :  54.930269945
Iteration :  64   Loss :  44.4272170652
Iteration :  65   Loss :  35.9324215617
Iteration :  66   Loss :  29.0618905386
Iteration :  67   Loss :  23.5050532353
Iteration :  68   Loss :  19.0107221985
Iteration :  69   Loss :  15.375738778
Iteration :  70   Loss :  12.4357896823
Iteration :  71   Loss :  10.0579794737
Iteration :  72   Loss :  8.13482325436
Iteration :  73   Loss :  6.57938799264
Iteration :  74   Loss :  5.32136286237
Iteration :  75   Loss :  4.30388096046
Iteration :  76   Loss :  3.48094873455
Iteration :  77   Loss :  2.81536692205
Iteration :  78   Loss :  2.27704902032
Iteration :  79   Loss :  1.84166127702
Iteration :  80   Loss :  1.48952272393
Iteration :  81   Loss :  1.204715532
Iteration :  82   Loss :  0.974365472727
Iteration :  83   Loss :  0.788059960402
Iteration :  84   Loss :  0.63737736873
Iteration :  85   Loss :  0.515506345434
Iteration :  86   Loss :  0.416937916562
Iteration :  87   Loss :  0.337216462624
Iteration :  88   Loss :  0.272738309824
Iteration :  89   Loss :  0.220588832071
Iteration :  90   Loss :  0.178410700228
Iteration :  91   Loss :  0.144297323019
Iteration :  92   Loss :  0.116706662795
Iteration :  93   Loss :  0.0943915303191
Iteration :  94   Loss :  0.076343207685
Iteration :  95   Loss :  0.0617458509247
Iteration :  96   Loss :  0.0499396111589
Iteration :  97   Loss :  0.0403908072422
Iteration :  98   Loss :  0.0326678016071
Iteration :  99   Loss :  0.0264214888165
[-0.00273671 -0.00058715 -0.00263428 ...,  0.00155795  0.00016134
  0.00025476]
CROSS VALIDATION 5
Iteration :  0   Loss :  3273.93022213
Iteration :  1   Loss :  242.411069478
Iteration :  2   Loss :  196.060372787
Iteration :  3   Loss :  158.572254395
Iteration :  4   Loss :  128.252127171
Iteration :  5   Loss :  103.72942093
Iteration :  6   Loss :  83.8956281173
Iteration :  7   Loss :  67.8541955997
Iteration :  8   Loss :  54.8799974897
Iteration :  9   Loss :  44.386557056
Iteration :  10   Loss :  35.8995360314
Iteration :  11   Loss :  29.0352929524
Iteration :  12   Loss :  23.4835412941
Iteration :  13   Loss :  18.9933234913
Iteration :  14   Loss :  15.3616668257
Iteration :  15   Loss :  12.424408386
Iteration :  16   Loss :  10.0487743611
Iteration :  17   Loss :  8.12737822381
Iteration :  18   Loss :  6.57336650415
Iteration :  19   Loss :  5.31649272471
Iteration :  20   Loss :  4.29994202728
Iteration :  21   Loss :  3.47776295301
Iteration :  22   Loss :  2.81279028429
Iteration :  23   Loss :  2.27496505377
Iteration :  24   Loss :  1.83997577949
Iteration :  25   Loss :  1.48815950535
Iteration :  26   Loss :  1.20361297037
Iteration :  27   Loss :  0.973473728618
Iteration :  28   Loss :  0.787338724021
Iteration :  29   Loss :  0.636794037803
Iteration :  30   Loss :  0.515034551472
Iteration :  31   Loss :  0.416556332915
Iteration :  32   Loss :  0.336907840446
Iteration :  33   Loss :  0.272488698371
Iteration :  34   Loss :  0.220386948079
Iteration :  35   Loss :  0.178247417871
Iteration :  36   Loss :  0.144165261394
Iteration :  37   Loss :  0.116599852279
Iteration :  38   Loss :  0.0943051427227
Iteration :  39   Loss :  0.076273338003
Iteration :  40   Loss :  0.0616893408159
Iteration :  41   Loss :  0.0498939061793
Iteration :  42   Loss :  0.0403538413752
Iteration :  43   Loss :  0.0465794192184
Iteration :  44   Loss :  405.197560196
Iteration :  45   Loss :  187.218441316
Iteration :  46   Loss :  2076.0218837
Iteration :  47   Loss :  215.567168779
Iteration :  48   Loss :  174.349214178
Iteration :  49   Loss :  141.012421588
Iteration :  50   Loss :  114.049857557
Iteration :  51   Loss :  92.2427248767
Iteration :  52   Loss :  74.6052689143
Iteration :  53   Loss :  60.3402182364
Iteration :  54   Loss :  48.8027452994
Iteration :  55   Loss :  39.4713181087
Iteration :  56   Loss :  31.924125245
Iteration :  57   Loss :  25.8200085908
Iteration :  58   Loss :  20.8830418536
Iteration :  59   Loss :  16.8900577831
Iteration :  60   Loss :  13.6605602726
Iteration :  61   Loss :  11.0485653369
Iteration :  62   Loss :  8.93600215281
Iteration :  63   Loss :  7.22737586645
Iteration :  64   Loss :  5.84545090989
Iteration :  65   Loss :  4.72775969748
Iteration :  66   Loss :  3.82377888408
Iteration :  67   Loss :  3.09264554247
Iteration :  68   Loss :  2.50131002376
Iteration :  69   Loss :  2.02304200369
Iteration :  70   Loss :  1.63622218351
Iteration :  71   Loss :  1.32336502599
Iteration :  72   Loss :  1.07032835129
Iteration :  73   Loss :  0.865674063528
Iteration :  74   Loss :  0.700151110974
Iteration :  75   Loss :  0.566277307882
Iteration :  76   Loss :  0.458001114897
Iteration :  77   Loss :  0.370428089431
Iteration :  78   Loss :  0.299599640647
Iteration :  79   Loss :  0.242314088043
Iteration :  80   Loss :  0.195981934883
Iteration :  81   Loss :  0.158508814368
Iteration :  82   Loss :  0.128200817322
Iteration :  83   Loss :  0.103687921884
Iteration :  84   Loss :  0.0838620639802
Iteration :  85   Loss :  0.067827049161
Iteration :  86   Loss :  0.0548580416405
Iteration :  87   Loss :  0.0443687993192
Iteration :  88   Loss :  0.0358851736984
Iteration :  89   Loss :  0.0290236767982
Iteration :  90   Loss :  0.02347414623
Iteration :  91   Loss :  0.0189857248294
Iteration :  92   Loss :  0.0153555210812
Iteration :  93   Loss :  0.012419437751
Iteration :  94   Loss :  0.0100447541463
Iteration :  95   Loss :  0.00812412670233
Iteration :  96   Loss :  0.00657073669638
Iteration :  97   Loss :  0.00531436575463
Iteration :  98   Loss :  0.0042982217488
Iteration :  99   Loss :  0.00347637160384
[ -1.00645501e-03  -3.44312264e-04  -4.64803977e-05 ...,   4.09587346e-04
   3.51734472e-04   5.60453219e-05]
CROSS VALIDATION 6
Iteration :  0   Loss :  1929.88357219
Iteration :  1   Loss :  229.381279864
Iteration :  2   Loss :  187.546101424
Iteration :  3   Loss :  151.685971433
Iteration :  4   Loss :  122.68254981
Iteration :  5   Loss :  99.2247858238
Iteration :  6   Loss :  80.2523108381
Iteration :  7   Loss :  64.9075061375
Iteration :  8   Loss :  52.49673572
Iteration :  9   Loss :  42.4589916522
Iteration :  10   Loss :  34.3405346522
Iteration :  11   Loss :  27.7743835714
Iteration :  12   Loss :  22.4637266304
Iteration :  13   Loss :  18.168504544
Iteration :  14   Loss :  14.6945590462
Iteration :  15   Loss :  11.8848562928
Iteration :  16   Loss :  9.61238841241
Iteration :  17   Loss :  7.77443232922
Iteration :  18   Loss :  6.28790634007
Iteration :  19   Loss :  5.08561454615
Iteration :  20   Loss :  4.11320937578
Iteration :  21   Loss :  3.32673489417
Iteration :  22   Loss :  2.69063984957
Iteration :  23   Loss :  2.1761706389
Iteration :  24   Loss :  1.76007154966
Iteration :  25   Loss :  1.42353352469
Iteration :  26   Loss :  1.15134393049
Iteration :  27   Loss :  0.931198895758
Iteration :  28   Loss :  0.753147135708
Iteration :  29   Loss :  0.609140120987
Iteration :  30   Loss :  0.492668257507
Iteration :  31   Loss :  0.398466631228
Iteration :  32   Loss :  0.322277016598
Iteration :  33   Loss :  0.260655390659
Iteration :  34   Loss :  0.210816251798
Iteration :  35   Loss :  0.170506705845
Iteration :  36   Loss :  0.137904627799
Iteration :  37   Loss :  0.111536295737
Iteration :  38   Loss :  0.0902097737063
Iteration :  39   Loss :  0.0729610322663
Iteration :  40   Loss :  0.0590103711677
Iteration :  41   Loss :  0.0477271743174
Iteration :  42   Loss :  0.0386014038422
Iteration :  43   Loss :  0.0312205446876
Iteration :  44   Loss :  10833.7047391
Iteration :  45   Loss :  737.859398638
Iteration :  46   Loss :  1426.10159918
Iteration :  47   Loss :  305.507919514
Iteration :  48   Loss :  292.252460957
Iteration :  49   Loss :  150.250253385
Iteration :  50   Loss :  121.521351122
Iteration :  51   Loss :  98.2856164693
Iteration :  52   Loss :  79.4927172512
Iteration :  53   Loss :  64.293152172
Iteration :  54   Loss :  51.9998505416
Iteration :  55   Loss :  42.057114405
Iteration :  56   Loss :  34.0154991533
Iteration :  57   Loss :  27.5114971395
Iteration :  58   Loss :  22.2511059281
Iteration :  59   Loss :  17.9965384113
Iteration :  60   Loss :  14.5554740442
Iteration :  61   Loss :  11.7723653187
Iteration :  62   Loss :  9.52140650153
Iteration :  63   Loss :  7.70084679782
Iteration :  64   Loss :  6.22839087839
Iteration :  65   Loss :  5.03747885816
Iteration :  66   Loss :  4.07427756895
Iteration :  67   Loss :  3.29524712186
Iteration :  68   Loss :  2.66517276018
Iteration :  69   Loss :  2.15557303562
Iteration :  70   Loss :  1.74341235259
Iteration :  71   Loss :  1.41005968295
Iteration :  72   Loss :  1.14044638178
Iteration :  73   Loss :  0.922385034799
Iteration :  74   Loss :  0.746018546782
Iteration :  75   Loss :  0.603374568262
Iteration :  76   Loss :  0.488005118902
Iteration :  77   Loss :  0.39469511743
Iteration :  78   Loss :  0.319226642691
Iteration :  79   Loss :  0.258188269637
Iteration :  80   Loss :  0.208820861619
Iteration :  81   Loss :  0.168892848263
Iteration :  82   Loss :  0.136599351106
Iteration :  83   Loss :  0.110480597103
Iteration :  84   Loss :  0.0893559320554
Iteration :  85   Loss :  0.0722704511276
Iteration :  86   Loss :  0.0584518339863
Iteration :  87   Loss :  0.0472754333625
Iteration :  88   Loss :  0.0382360389263
Iteration :  89   Loss :  0.0309250401062
Iteration :  90   Loss :  0.0250119555379
Iteration :  91   Loss :  0.0202294942118
Iteration :  92   Loss :  0.0163614730342
Iteration :  93   Loss :  0.0132330446351
Iteration :  94   Loss :  0.0107027936878
Iteration :  95   Loss :  0.00865634446814
Iteration :  96   Loss :  0.00700119069254
Iteration :  97   Loss :  0.00566251392763
Iteration :  98   Loss :  0.0118081701279
Iteration :  99   Loss :  12529.4745969
[-0.07058792 -0.0771763  -0.03350929 ...,  0.01504985  0.03433842
 -0.00981969]
CROSS VALIDATION 7
Iteration :  0   Loss :  1929.88357219
Iteration :  1   Loss :  239.128141773
Iteration :  2   Loss :  202.673140793
Iteration :  3   Loss :  163.920614778
Iteration :  4   Loss :  132.577843537
Iteration :  5   Loss :  107.228029987
Iteration :  6   Loss :  86.7252785845
Iteration :  7   Loss :  70.1427970511
Iteration :  8   Loss :  56.731002292
Iteration :  9   Loss :  45.8836367576
Iteration :  10   Loss :  37.1103635939
Iteration :  11   Loss :  30.0146017926
Iteration :  12   Loss :  24.2755994154
Iteration :  13   Loss :  19.6339345446
Iteration :  14   Loss :  15.8797885525
Iteration :  15   Loss :  12.8434616046
Iteration :  16   Loss :  10.3877016652
Iteration :  17   Loss :  8.40150025023
Iteration :  18   Loss :  6.7950744765
Iteration :  19   Loss :  5.49580857775
Iteration :  20   Loss :  4.44497143154
Iteration :  21   Loss :  3.59506171799
Iteration :  22   Loss :  2.9076607027
Iteration :  23   Loss :  2.35169558279
Iteration :  24   Loss :  1.90203489319
Iteration :  25   Loss :  1.53835248125
Iteration :  26   Loss :  1.24420869724
Iteration :  27   Loss :  1.00630726778
Iteration :  28   Loss :  0.813894260206
Iteration :  29   Loss :  0.658271969216
Iteration :  30   Loss :  0.532405751758
Iteration :  31   Loss :  0.430606037869
Iteration :  32   Loss :  0.348271143273
Iteration :  33   Loss :  0.281679257998
Iteration :  34   Loss :  0.227820208245
Iteration :  35   Loss :  0.184259386558
Iteration :  36   Loss :  0.149027699502
Iteration :  37   Loss :  0.120532558117
Iteration :  38   Loss :  0.0974858876215
Iteration :  39   Loss :  0.0788459021682
Iteration :  40   Loss :  0.0637700126694
Iteration :  41   Loss :  0.0515767389811
Iteration :  42   Loss :  0.0417149047424
Iteration :  43   Loss :  0.0337387223784
Iteration :  44   Loss :  0.0272876420252
Iteration :  45   Loss :  0.0220700534818
Iteration :  46   Loss :  0.0178501044626
Iteration :  47   Loss :  0.0144370392934
Iteration :  48   Loss :  0.011676576123
Iteration :  49   Loss :  0.00944393287187
Iteration :  50   Loss :  0.00763818666953
Iteration :  51   Loss :  0.00617771180611
Iteration :  52   Loss :  0.00499648997996
Iteration :  53   Loss :  0.0623022753337
Iteration :  54   Loss :  1124.93689508
Iteration :  55   Loss :  1703.85931293
Iteration :  56   Loss :  384.222958692
Iteration :  57   Loss :  783.351287373
Iteration :  58   Loss :  158.755892803
Iteration :  59   Loss :  128.400652628
Iteration :  60   Loss :  103.849547277
Iteration :  61   Loss :  83.9927854647
Iteration :  62   Loss :  67.9327757808
Iteration :  63   Loss :  54.9435525891
Iteration :  64   Loss :  44.4379599746
Iteration :  65   Loss :  35.9411103514
Iteration :  66   Loss :  29.0689179708
Iteration :  67   Loss :  23.5107369732
Iteration :  68   Loss :  19.0153191658
Iteration :  69   Loss :  15.3794567729
Iteration :  70   Loss :  12.4387967705
Iteration :  71   Loss :  10.0604115856
Iteration :  72   Loss :  8.13679032938
Iteration :  73   Loss :  6.58097894912
Iteration :  74   Loss :  5.32264961681
Iteration :  75   Loss :  4.30492167843
Iteration :  76   Loss :  3.48179045994
Iteration :  77   Loss :  2.81604770365
Iteration :  78   Loss :  2.2775996317
Iteration :  79   Loss :  1.84210660764
Iteration :  80   Loss :  1.48988290421
Iteration :  81   Loss :  1.20500684329
Iteration :  82   Loss :  0.974601083253
Iteration :  83   Loss :  0.788250520542
Iteration :  84   Loss :  0.637531492435
Iteration :  85   Loss :  0.515630999605
Iteration :  86   Loss :  0.417038735982
Iteration :  87   Loss :  0.337298004664
Iteration :  88   Loss :  0.272804260453
Iteration :  89   Loss :  0.220642172478
Iteration :  90   Loss :  0.178453841575
Iteration :  91   Loss :  0.144332215438
Iteration :  92   Loss :  0.116734883538
Iteration :  93   Loss :  0.0944143550585
Iteration :  94   Loss :  0.0763616681742
Iteration :  95   Loss :  0.0617607816388
Iteration :  96   Loss :  0.0499516870158
Iteration :  97   Loss :  0.0404005741105
Iteration :  98   Loss :  0.0326757009817
Iteration :  99   Loss :  0.0264278777753
[-0.00273762 -0.00058717 -0.00263437 ...,  0.00155885  0.00016246
  0.00025454]
CROSS VALIDATION 8
Iteration :  0   Loss :  1692.59968276
Iteration :  1   Loss :  224.973921312
Iteration :  2   Loss :  236.44545175
Iteration :  3   Loss :  180.824719574
Iteration :  4   Loss :  146.249764935
Iteration :  5   Loss :  118.285784123
Iteration :  6   Loss :  95.6687125733
Iteration :  7   Loss :  77.3761837344
Iteration :  8   Loss :  62.5813147084
Iteration :  9   Loss :  50.6153284075
Iteration :  10   Loss :  40.9373226135
Iteration :  11   Loss :  33.1098194063
Iteration :  12   Loss :  26.7789897124
Iteration :  13   Loss :  21.6586590587
Iteration :  14   Loss :  17.5173715386
Iteration :  15   Loss :  14.1679272382
Iteration :  16   Loss :  11.4589201802
Iteration :  17   Loss :  9.26789427192
Iteration :  18   Loss :  7.49580788459
Iteration :  19   Loss :  6.06255684345
Iteration :  20   Loss :  4.90335345382
Iteration :  21   Loss :  3.96579788264
Iteration :  22   Loss :  3.20750951242
Iteration :  23   Loss :  2.59421119703
Iteration :  24   Loss :  2.09817982105
Iteration :  25   Loss :  1.69699312319
Iteration :  26   Loss :  1.37251613578
Iteration :  27   Loss :  1.11008142415
Iteration :  28   Loss :  0.897826070041
Iteration :  29   Loss :  0.726155428336
Iteration :  30   Loss :  0.587309417377
Iteration :  31   Loss :  0.475011737542
Iteration :  32   Loss :  0.384186161718
Iteration :  33   Loss :  0.310727073018
Iteration :  34   Loss :  0.2513138773
Iteration :  35   Loss :  0.203260901312
Iteration :  36   Loss :  0.164395991363
Iteration :  37   Loss :  0.132962324785
Iteration :  38   Loss :  0.107538995723
Iteration :  39   Loss :  0.0869767854912
Iteration :  40   Loss :  0.0703462140733
Iteration :  41   Loss :  0.0568955245528
Iteration :  42   Loss :  0.0460167012083
Iteration :  43   Loss :  0.0372179851884
Iteration :  44   Loss :  0.0301016453833
Iteration :  45   Loss :  0.0243459996611
Iteration :  46   Loss :  0.0196908737695
Iteration :  47   Loss :  0.0159258405982
Iteration :  48   Loss :  0.0128807081762
Iteration :  49   Loss :  0.0104178264311
Iteration :  50   Loss :  0.0084258649496
Iteration :  51   Loss :  0.00681478047446
Iteration :  52   Loss :  0.00551174665069
Iteration :  53   Loss :  0.00445786203315
Iteration :  54   Loss :  0.00360548754615
Iteration :  55   Loss :  0.0029160930394
Iteration :  56   Loss :  0.00235851559756
Iteration :  57   Loss :  0.00190755087331
Iteration :  58   Loss :  0.0772736119017
Iteration :  59   Loss :  2682.08592352
Iteration :  60   Loss :  507.946296263
Iteration :  61   Loss :  197.620088505
Iteration :  62   Loss :  159.833741528
Iteration :  63   Loss :  129.272409117
Iteration :  64   Loss :  104.554617811
Iteration :  65   Loss :  84.5630415674
Iteration :  66   Loss :  68.3939949171
Iteration :  67   Loss :  55.3165833916
Iteration :  68   Loss :  44.7396646714
Iteration :  69   Loss :  36.1851269941
Iteration :  70   Loss :  29.26627692
Iteration :  71   Loss :  23.670359507
Iteration :  72   Loss :  19.1444207517
Iteration :  73   Loss :  15.4838732301
Iteration :  74   Loss :  12.5232480687
Iteration :  75   Loss :  10.1287152032
Iteration :  76   Loss :  8.19203381627
Iteration :  77   Loss :  6.62565949385
Iteration :  78   Loss :  5.35878692801
Iteration :  79   Loss :  4.33414928227
Iteration :  80   Loss :  3.50542954093
Iteration :  81   Loss :  2.83516682656
Iteration :  82   Loss :  2.29306304421
Iteration :  83   Loss :  1.85461330722
Iteration :  84   Loss :  1.49999823512
Iteration :  85   Loss :  1.21318805198
Iteration :  86   Loss :  0.98121798746
Iteration :  87   Loss :  0.793602226256
Iteration :  88   Loss :  0.641859914481
Iteration :  89   Loss :  0.519131796997
Iteration :  90   Loss :  0.419870156359
Iteration :  91   Loss :  0.339588037606
Iteration :  92   Loss :  0.274656423035
Iteration :  93   Loss :  0.222140188584
Iteration :  94   Loss :  0.179665426494
Iteration :  95   Loss :  0.145312136823
Iteration :  96   Loss :  0.117527437082
Iteration :  97   Loss :  0.0950553668057
Iteration :  98   Loss :  0.0768801139794
Iteration :  99   Loss :  0.0621800969699
[-0.0035958  -0.00117583 -0.0010317  ...,  0.00230808  0.00053407
  0.00035802]
CROSS VALIDATION 9
Iteration :  0   Loss :  1733.57144972
Iteration :  1   Loss :  237.351221877
Iteration :  2   Loss :  202.710745124
Iteration :  3   Loss :  163.951028897
Iteration :  4   Loss :  132.602442263
Iteration :  5   Loss :  107.247925264
Iteration :  6   Loss :  86.7413697454
Iteration :  7   Loss :  70.1558114692
Iteration :  8   Loss :  56.7415282621
Iteration :  9   Loss :  45.8921500884
Iteration :  10   Loss :  37.1172491162
Iteration :  11   Loss :  30.0201707546
Iteration :  12   Loss :  24.2801035529
Iteration :  13   Loss :  19.6375774594
Iteration :  14   Loss :  15.8827349164
Iteration :  15   Loss :  12.8458446031
Iteration :  16   Loss :  10.3896290177
Iteration :  17   Loss :  8.40305907938
Iteration :  18   Loss :  6.79633524659
Iteration :  19   Loss :  5.49682827975
Iteration :  20   Loss :  4.44579615937
Iteration :  21   Loss :  3.59572875207
Iteration :  22   Loss :  2.90820019519
Iteration :  23   Loss :  2.35213192053
Iteration :  24   Loss :  1.90238780009
Iteration :  25   Loss :  1.53863790987
Iteration :  26   Loss :  1.24443954991
Iteration :  27   Loss :  1.0064939798
Iteration :  28   Loss :  0.81404527158
Iteration :  29   Loss :  0.658394106157
Iteration :  30   Loss :  0.532504535259
Iteration :  31   Loss :  0.430685933272
Iteration :  32   Loss :  0.348335762114
Iteration :  33   Loss :  0.281731521263
Iteration :  34   Loss :  0.227862478408
Iteration :  35   Loss :  0.18429357437
Iteration :  36   Loss :  0.149055350364
Iteration :  37   Loss :  0.12055492194
Iteration :  38   Loss :  0.0975039753248
Iteration :  39   Loss :  0.0788605313757
Iteration :  40   Loss :  0.0637818446699
Iteration :  41   Loss :  0.0515863086201
Iteration :  42   Loss :  0.0417226445993
Iteration :  43   Loss :  0.0337449823204
Iteration :  44   Loss :  0.0272927050224
Iteration :  45   Loss :  0.0220741483984
Iteration :  46   Loss :  0.0178534164024
Iteration :  47   Loss :  0.0144397179671
Iteration :  48   Loss :  0.0116787426155
Iteration :  49   Loss :  0.00944568511581
Iteration :  50   Loss :  0.00763960387214
Iteration :  51   Loss :  0.00617885802964
Iteration :  52   Loss :  0.00499741703751
Iteration :  53   Loss :  0.00404187584938
Iteration :  54   Loss :  0.00326904083834
Iteration :  55   Loss :  0.00264397730187
Iteration :  56   Loss :  0.0021384302976
Iteration :  57   Loss :  0.00172954742631
Iteration :  58   Loss :  0.00139884582782
Iteration :  59   Loss :  0.00113137669441
Iteration :  60   Loss :  12763.9087824
Iteration :  61   Loss :  4768.35526122
Iteration :  62   Loss :  2119.96150945
Iteration :  63   Loss :  193.319996377
Iteration :  64   Loss :  156.355857175
Iteration :  65   Loss :  126.459520645
Iteration :  66   Loss :  102.279573344
Iteration :  67   Loss :  82.723001559
Iteration :  68   Loss :  66.9057834638
Iteration :  69   Loss :  54.1129284061
Iteration :  70   Loss :  43.7661569612
Iteration :  71   Loss :  35.397760786
Iteration :  72   Loss :  28.6294606533
Iteration :  73   Loss :  23.1553069769
Iteration :  74   Loss :  18.7278498778
Iteration :  75   Loss :  15.146953629
Iteration :  76   Loss :  12.2507498584
Iteration :  77   Loss :  9.908320562
Iteration :  78   Loss :  8.01378017622
Iteration :  79   Loss :  6.48148920001
Iteration :  80   Loss :  5.24218300553
Iteration :  81   Loss :  4.23984084759
Iteration :  82   Loss :  3.42915354042
Iteration :  83   Loss :  2.77347533232
Iteration :  84   Loss :  2.24316739636
Iteration :  85   Loss :  10275.8796681
Iteration :  86   Loss :  1348.13185945
Iteration :  87   Loss :  96.0412893236
Iteration :  88   Loss :  77.6775211969
Iteration :  89   Loss :  62.8250343346
Iteration :  90   Loss :  50.8124471317
Iteration :  91   Loss :  41.0967508551
Iteration :  92   Loss :  33.23876385
Iteration :  93   Loss :  26.883279074
Iteration :  94   Loss :  21.7430075629
Iteration :  95   Loss :  17.5855920172
Iteration :  96   Loss :  14.2231034828
Iteration :  97   Loss :  11.5035463399
Iteration :  98   Loss :  9.30398759687
Iteration :  99   Loss :  7.52499991265
[-0.06461236 -0.02308594 -0.01988343 ...,  0.01981249  0.00265945
  0.00655897]
CROSS VALIDATION 10
Iteration :  0   Loss :  995.31615605
Iteration :  1   Loss :  211.836780949
Iteration :  2   Loss :  171.332102665
Iteration :  3   Loss :  138.572202958
Iteration :  4   Loss :  112.076225843
Iteration :  5   Loss :  90.6464653894
Iteration :  6   Loss :  73.3142254367
Iteration :  7   Loss :  59.2960313268
Iteration :  8   Loss :  47.9582142505
Iteration :  9   Loss :  38.7882673197
Iteration :  10   Loss :  31.3716785575
Iteration :  11   Loss :  25.3731935847
Iteration :  12   Loss :  20.5216610105
Iteration :  13   Loss :  16.5977754918
Iteration :  14   Loss :  13.4241644054
Iteration :  15   Loss :  10.8573700176
Iteration :  16   Loss :  8.78136471951
Iteration :  17   Loss :  7.10230619499
Iteration :  18   Loss :  5.74429543682
Iteration :  19   Loss :  4.64594586034
Iteration :  20   Loss :  3.75760842641
Iteration :  21   Loss :  3.03912734042
Iteration :  22   Loss :  2.4580248773
Iteration :  23   Loss :  1.98803328083
Iteration :  24   Loss :  1.607907374
Iteration :  25   Loss :  1.30046420665
Iteration :  26   Loss :  1.05180632923
Iteration :  27   Loss :  0.850693581993
Iteration :  28   Loss :  0.688035002577
Iteration :  29   Loss :  0.55647788439
Iteration :  30   Loss :  0.450075409907
Iteration :  31   Loss :  0.364017834823
Iteration :  32   Loss :  0.294415071681
Iteration :  33   Loss :  0.238120845027
Iteration :  34   Loss :  0.192590469341
Iteration :  35   Loss :  0.155765820825
Iteration :  36   Loss :  0.125982303383
Iteration :  37   Loss :  0.10189360337
Iteration :  38   Loss :  0.082410831751
Iteration :  39   Loss :  0.0666533027126
Iteration :  40   Loss :  0.0539087237454
Iteration :  41   Loss :  0.0436009976638
Iteration :  42   Loss :  0.0352641811047
Iteration :  43   Loss :  0.0285214223439
Iteration :  44   Loss :  0.0230679263501
Iteration :  45   Loss :  0.0186571770396
Iteration :  46   Loss :  0.0150897939332
Iteration :  47   Loss :  0.0122045194974
Iteration :  48   Loss :  0.00987092977021
Iteration :  49   Loss :  0.00798353876607
Iteration :  50   Loss :  0.00645703015958
Iteration :  51   Loss :  0.00522240070518
Iteration :  52   Loss :  0.00422384106182
Iteration :  53   Loss :  0.0034162130259
Iteration :  54   Loss :  1938.0493008
Iteration :  55   Loss :  225.046683509
Iteration :  56   Loss :  182.016179204
Iteration :  57   Loss :  147.213409127
Iteration :  58   Loss :  119.065172786
Iteration :  59   Loss :  96.2990766583
Iteration :  60   Loss :  77.8860177851
Iteration :  61   Loss :  62.9936649128
Iteration :  62   Loss :  50.9488343606
Iteration :  63   Loss :  41.2070598893
Iteration :  64   Loss :  33.3279810231
Iteration :  65   Loss :  26.9554372979
Iteration :  66   Loss :  21.8013686283
Iteration :  67   Loss :  17.6327940376
Iteration :  68   Loss :  14.2612801459
Iteration :  69   Loss :  11.5344233571
Iteration :  70   Loss :  9.32896071188
Iteration :  71   Loss :  7.54519799292
Iteration :  72   Loss :  6.10250321666
Iteration :  73   Loss :  4.9356618003
Iteration :  74   Loss :  3.99192864667
Iteration :  75   Loss :  3.2286438911
Iteration :  76   Loss :  2.61130453427
Iteration :  77   Loss :  2.11200479232
Iteration :  78   Loss :  1.70817466299
Iteration :  79   Loss :  1.38155968674
Iteration :  80   Loss :  1.11739578475
Iteration :  81   Loss :  0.903741873587
Iteration :  82   Loss :  0.730940088752
Iteration :  83   Loss :  0.591179217163
Iteration :  84   Loss :  0.478141604468
Iteration :  85   Loss :  0.386717576136
Iteration :  86   Loss :  0.312774463244
Iteration :  87   Loss :  0.252969792155
Iteration :  88   Loss :  0.204600193632
Iteration :  89   Loss :  0.165479201598
Iteration :  90   Loss :  0.133838417626
Iteration :  91   Loss :  0.108247573469
Iteration :  92   Loss :  0.0875498782017
Iteration :  93   Loss :  0.070809727438
Iteration :  94   Loss :  0.0572704109113
Iteration :  95   Loss :  0.0463199066656
Iteration :  96   Loss :  0.0374632156356
Iteration :  97   Loss :  0.0302999860491
Iteration :  98   Loss :  0.0245064162005
Iteration :  99   Loss :  0.0198206175414
[-0.00132123 -0.00140615 -0.00066192 ...,  0.00102408 -0.00015583
  0.00024712]
CROSS VALIDATION 11
Iteration :  0   Loss :  1931.88356382
Iteration :  1   Loss :  237.804729967
Iteration :  2   Loss :  202.747802214
Iteration :  3   Loss :  163.981000412
Iteration :  4   Loss :  132.626683014
Iteration :  5   Loss :  107.267531013
Iteration :  6   Loss :  86.7572267378
Iteration :  7   Loss :  70.1686364934
Iteration :  8   Loss :  56.7519010517
Iteration :  9   Loss :  45.9005395279
Iteration :  10   Loss :  37.124034436
Iteration :  11   Loss :  30.0256586737
Iteration :  12   Loss :  24.2845421433
Iteration :  13   Loss :  19.6411673602
Iteration :  14   Loss :  15.885638403
Iteration :  15   Loss :  12.8481929227
Iteration :  16   Loss :  10.3915283221
Iteration :  17   Loss :  8.40459522349
Iteration :  18   Loss :  6.79757766916
Iteration :  19   Loss :  5.49783314242
Iteration :  20   Loss :  4.44660888525
Iteration :  21   Loss :  3.59638607906
Iteration :  22   Loss :  2.90873183665
Iteration :  23   Loss :  2.35256190841
Iteration :  24   Loss :  1.90273557128
Iteration :  25   Loss :  1.53891918477
Iteration :  26   Loss :  1.24466704307
Iteration :  27   Loss :  1.00667797467
Iteration :  28   Loss :  0.814194085344
Iteration :  29   Loss :  0.658514465685
Iteration :  30   Loss :  0.532601881201
Iteration :  31   Loss :  0.430764665987
Iteration :  32   Loss :  0.348399440582
Iteration :  33   Loss :  0.281783023962
Iteration :  34   Loss :  0.227904133429
Iteration :  35   Loss :  0.184327264658
Iteration :  36   Loss :  0.149082598833
Iteration :  37   Loss :  0.120576960311
Iteration :  38   Loss :  0.0975217998046
Iteration :  39   Loss :  0.07887494769
Iteration :  40   Loss :  0.0637935044838
Iteration :  41   Loss :  0.0515957389958
Iteration :  42   Loss :  0.041730271821
Iteration :  43   Loss :  0.0337511511638
Iteration :  44   Loss :  0.0272976943397
Iteration :  45   Loss :  0.0220781837232
Iteration :  46   Loss :  0.0178566801448
Iteration :  47   Loss :  0.0144423576591
Iteration :  48   Loss :  0.0116808775798
Iteration :  49   Loss :  0.00944741186001
Iteration :  50   Loss :  0.00764100045074
Iteration :  51   Loss :  0.00617998757261
Iteration :  52   Loss :  0.00499833060394
Iteration :  53   Loss :  0.0629326348521
Iteration :  54   Loss :  1027.87494132
Iteration :  55   Loss :  1703.04324685
Iteration :  56   Loss :  383.929787001
Iteration :  57   Loss :  783.336119612
Iteration :  58   Loss :  158.764107969
Iteration :  59   Loss :  128.407296996
Iteration :  60   Loss :  103.854921196
Iteration :  61   Loss :  83.9971318522
Iteration :  62   Loss :  67.9362911085
Iteration :  63   Loss :  54.9463957615
Iteration :  64   Loss :  44.4402595125
Iteration :  65   Loss :  35.9429702015
Iteration :  66   Loss :  29.0704222046
Iteration :  67   Loss :  23.5119535869
Iteration :  68   Loss :  19.0163031545
Iteration :  69   Loss :  15.380252616
Iteration :  70   Loss :  12.4394404429
Iteration :  71   Loss :  10.0609321833
Iteration :  72   Loss :  8.13721138511
Iteration :  73   Loss :  6.58131949604
Iteration :  74   Loss :  5.32292504878
Iteration :  75   Loss :  4.30514444587
Iteration :  76   Loss :  3.48197063268
Iteration :  77   Loss :  2.8161934261
Iteration :  78   Loss :  2.277717491
Iteration :  79   Loss :  1.84220193141
Iteration :  80   Loss :  1.48996000141
Iteration :  81   Loss :  1.20506919896
Iteration :  82   Loss :  0.974651516084
Iteration :  83   Loss :  0.788291310262
Iteration :  84   Loss :  0.637564482875
Iteration :  85   Loss :  0.515657682041
Iteration :  86   Loss :  0.417060316549
Iteration :  87   Loss :  0.337315458875
Iteration :  88   Loss :  0.272818377297
Iteration :  89   Loss :  0.220653590082
Iteration :  90   Loss :  0.178463076053
Iteration :  91   Loss :  0.144339684219
Iteration :  92   Loss :  0.116740924236
Iteration :  93   Loss :  0.0944192407326
Iteration :  94   Loss :  0.0763656196731
Iteration :  95   Loss :  0.0617639775835
Iteration :  96   Loss :  0.0499542718734
Iteration :  97   Loss :  0.0404026647252
Iteration :  98   Loss :  0.0326773918562
Iteration :  99   Loss :  0.0264292453428
[-0.0027377  -0.00058758 -0.00263479 ...,  0.00155885  0.00016246
  0.00025454]
CROSS VALIDATION 12
Iteration :  0   Loss :  1360.09475855
Iteration :  1   Loss :  563.760447008
Iteration :  2   Loss :  413.964849624
Iteration :  3   Loss :  158.268073659
Iteration :  4   Loss :  128.006107925
Iteration :  5   Loss :  103.530442289
Iteration :  6   Loss :  83.7346955883
Iteration :  7   Loss :  67.7240344989
Iteration :  8   Loss :  54.7747241044
Iteration :  9   Loss :  44.3014126803
Iteration :  10   Loss :  35.8306718575
Iteration :  11   Loss :  28.9795960915
Iteration :  12   Loss :  23.4384940636
Iteration :  13   Loss :  18.9568896073
Iteration :  14   Loss :  15.3321993558
Iteration :  15   Loss :  12.400575303
Iteration :  16   Loss :  10.0294983307
Iteration :  17   Loss :  8.11178790567
Iteration :  18   Loss :  6.56075716419
Iteration :  19   Loss :  5.30629437899
Iteration :  20   Loss :  4.29169367679
Iteration :  21   Loss :  3.47109174499
Iteration :  22   Loss :  2.80739465803
Iteration :  23   Loss :  2.2706011091
Iteration :  24   Loss :  1.83644625165
Iteration :  25   Loss :  1.48530484799
Iteration :  26   Loss :  1.20130414352
Iteration :  27   Loss :  0.97160636565
Iteration :  28   Loss :  0.785828413949
Iteration :  29   Loss :  0.635572509611
Iteration :  30   Loss :  0.514046588037
Iteration :  31   Loss :  0.415757275018
Iteration :  32   Loss :  0.336261568024
Iteration :  33   Loss :  0.271965997768
Iteration :  34   Loss :  0.219964191498
Iteration :  35   Loss :  0.177905495313
Iteration :  36   Loss :  0.143888716828
Iteration :  37   Loss :  0.116376184973
Iteration :  38   Loss :  0.0941242421745
Iteration :  39   Loss :  0.0761270269083
Iteration :  40   Loss :  0.0615710054288
Iteration :  41   Loss :  0.0497981973481
Iteration :  42   Loss :  0.0837311140323
Iteration :  43   Loss :  572.375666566
Iteration :  44   Loss :  216.4332012
Iteration :  45   Loss :  175.049655125
Iteration :  46   Loss :  141.578933314
Iteration :  47   Loss :  114.508048267
Iteration :  48   Loss :  92.6133063095
Iteration :  49   Loss :  74.904992578
Iteration :  50   Loss :  60.5826326334
Iteration :  51   Loss :  48.9988083634
Iteration :  52   Loss :  39.6298925397
Iteration :  53   Loss :  32.0523791326
Iteration :  54   Loss :  25.9237394356
Iteration :  55   Loss :  20.9669386333
Iteration :  56   Loss :  16.9579129101
Iteration :  57   Loss :  13.7154410234
Iteration :  58   Loss :  11.0929525033
Iteration :  59   Loss :  8.97190218162
Iteration :  60   Loss :  7.25641155796
Iteration :  61   Loss :  5.86893477354
Iteration :  62   Loss :  4.74675328169
Iteration :  63   Loss :  3.83914076177
Iteration :  64   Loss :  3.10507012141
Iteration :  65   Loss :  2.51135893606
Iteration :  66   Loss :  2.03116949348
Iteration :  67   Loss :  1.64279564024
Iteration :  68   Loss :  1.3286815917
Iteration :  69   Loss :  1.07462835235
Iteration :  70   Loss :  0.86915187422
Iteration :  71   Loss :  0.702963939869
Iteration :  72   Loss :  0.568552304164
Iteration :  73   Loss :  0.459841115933
Iteration :  74   Loss :  0.371916269364
Iteration :  75   Loss :  0.30080327014
Iteration :  76   Loss :  0.243287575135
Iteration :  77   Loss :  0.196769284415
Iteration :  78   Loss :  0.159145617147
Iteration :  79   Loss :  0.128715858944
Iteration :  80   Loss :  0.147877660994
Iteration :  81   Loss :  2184.03263714
Iteration :  82   Loss :  1017.29071289
Iteration :  83   Loss :  189.775276978
Iteration :  84   Loss :  153.488912987
Iteration :  85   Loss :  124.140756293
Iteration :  86   Loss :  100.404173
Iteration :  87   Loss :  81.2061909141
Iteration :  88   Loss :  65.6789976527
Iteration :  89   Loss :  53.1207126465
Iteration :  90   Loss :  42.9636598139
Iteration :  91   Loss :  34.7487067218
Iteration :  92   Loss :  28.1045102784
Iteration :  93   Loss :  22.7307307956
Iteration :  94   Loss :  18.3844556402
Iteration :  95   Loss :  14.8692187781
Iteration :  96   Loss :  12.026119859
Iteration :  97   Loss :  9.72664139396
Iteration :  98   Loss :  7.86683933936
Iteration :  99   Loss :  6.36264448175
[-0.05611046 -0.00164631 -0.02370826 ...,  0.02482139  0.00888047
  0.00569783]
CROSS VALIDATION 13
Iteration :  0   Loss :  1938.16754612
Iteration :  1   Loss :  238.628471338
Iteration :  2   Loss :  202.779096875
Iteration :  3   Loss :  164.006311314
Iteration :  4   Loss :  132.647154294
Iteration :  5   Loss :  107.284088041
Iteration :  6   Loss :  86.7706179458
Iteration :  7   Loss :  70.1794672086
Iteration :  8   Loss :  56.7606608582
Iteration :  9   Loss :  45.9076243979
Iteration :  10   Loss :  37.1297646292
Iteration :  11   Loss :  30.0302932138
Iteration :  12   Loss :  24.2882905268
Iteration :  13   Loss :  19.6441990265
Iteration :  14   Loss :  15.8880903934
Iteration :  15   Loss :  12.8501760754
Iteration :  16   Loss :  10.393132282
Iteration :  17   Loss :  8.40589249505
Iteration :  18   Loss :  6.79862689331
Iteration :  19   Loss :  5.49868174755
Iteration :  20   Loss :  4.44729523112
Iteration :  21   Loss :  3.59694119079
Iteration :  22   Loss :  2.90918080712
Iteration :  23   Loss :  2.35292503258
Iteration :  24   Loss :  1.90302926356
Iteration :  25   Loss :  1.53915672103
Iteration :  26   Loss :  1.24485916073
Iteration :  27   Loss :  1.00683335809
Iteration :  28   Loss :  0.81431975836
Iteration :  29   Loss :  0.658616109139
Iteration :  30   Loss :  0.532684089714
Iteration :  31   Loss :  0.430831155656
Iteration :  32   Loss :  0.348453216959
Iteration :  33   Loss :  0.281826517919
Iteration :  34   Loss :  0.227939311036
Iteration :  35   Loss :  0.184355716061
Iteration :  36   Loss :  0.149105610128
Iteration :  37   Loss :  0.120595571684
Iteration :  38   Loss :  0.097536852553
Iteration :  39   Loss :  0.0788871222474
Iteration :  40   Loss :  0.0638033511804
Iteration :  41   Loss :  0.051603702935
Iteration :  42   Loss :  0.0417367129991
Iteration :  43   Loss :  0.0337563607435
Iteration :  44   Loss :  0.0273019078112
Iteration :  45   Loss :  0.0220815915494
Iteration :  46   Loss :  0.0178594363707
Iteration :  47   Loss :  0.0144445868753
Iteration :  48   Loss :  0.0116826805543
Iteration :  49   Loss :  0.00944887009308
Iteration :  50   Loss :  0.00764217985942
Iteration :  51   Loss :  0.00618094147009
Iteration :  52   Loss :  0.00499910210953
Iteration :  53   Loss :  0.0648703963921
Iteration :  54   Loss :  2928.29755994
Iteration :  55   Loss :  520.325769016
Iteration :  56   Loss :  607.574462364
Iteration :  57   Loss :  196.191320066
Iteration :  58   Loss :  158.678163636
Iteration :  59   Loss :  128.337785822
Iteration :  60   Loss :  103.798701046
Iteration :  61   Loss :  83.9516613896
Iteration :  62   Loss :  67.8995149173
Iteration :  63   Loss :  54.916651436
Iteration :  64   Loss :  44.4162025106
Iteration :  65   Loss :  35.9235130672
Iteration :  66   Loss :  29.0546854108
Iteration :  67   Loss :  23.4992257784
Iteration :  68   Loss :  19.0060089922
Iteration :  69   Loss :  15.3719267698
Iteration :  70   Loss :  12.4327065568
Iteration :  71   Loss :  10.0554858635
Iteration :  72   Loss :  8.13280643988
Iteration :  73   Loss :  6.57775680723
Iteration :  74   Loss :  5.32004357105
Iteration :  75   Loss :  4.3028139269
Iteration :  76   Loss :  3.48008572528
Iteration :  77   Loss :  2.81466892621
Iteration :  78   Loss :  2.27648448618
Iteration :  79   Loss :  1.84120468577
Iteration :  80   Loss :  1.48915343613
Iteration :  81   Loss :  1.20441685462
Iteration :  82   Loss :  0.974123904554
Iteration :  83   Loss :  0.787864581757
Iteration :  84   Loss :  0.637219347853
Iteration :  85   Loss :  0.515378539256
Iteration :  86   Loss :  0.416834547822
Iteration :  87   Loss :  0.337132858712
Iteration :  88   Loss :  0.272670691566
Iteration :  89   Loss :  0.220534142899
Iteration :  90   Loss :  0.178366468009
Iteration :  91   Loss :  0.144261548311
Iteration :  92   Loss :  0.116677728462
Iteration :  93   Loss :  0.0943681284332
Iteration :  94   Loss :  0.0763242804038
Iteration :  95   Loss :  0.0617305426724
Iteration :  96   Loss :  0.0499272299518
Iteration :  97   Loss :  0.0403807934087
Iteration :  98   Loss :  0.0326597024889
Iteration :  99   Loss :  0.0264149383065
[-0.00263814 -0.00084995 -0.00191095 ...,  0.00111679  0.00027167
  0.00015896]
CROSS VALIDATION 14
Iteration :  0   Loss :  1938.16754612
Iteration :  1   Loss :  225.755831513
Iteration :  2   Loss :  189.262081522
Iteration :  3   Loss :  153.073844101
Iteration :  4   Loss :  123.805051489
Iteration :  5   Loss :  100.132657309
Iteration :  6   Loss :  80.9865909277
Iteration :  7   Loss :  65.5013867239
Iteration :  8   Loss :  52.977062173
Iteration :  9   Loss :  42.8474763185
Iteration :  10   Loss :  34.6547383256
Iteration :  11   Loss :  28.0285092986
Iteration :  12   Loss :  22.6692617362
Iteration :  13   Loss :  18.3347398961
Iteration :  14   Loss :  14.829009033
Iteration :  15   Loss :  11.9935984991
Iteration :  16   Loss :  9.7003383462
Iteration :  17   Loss :  7.84556561881
Iteration :  18   Loss :  6.34543844578
Iteration :  19   Loss :  5.13214610973
Iteration :  20   Loss :  4.15084377805
Iteration :  21   Loss :  3.35717333477
Iteration :  22   Loss :  2.71525824684
Iteration :  23   Loss :  2.19608182595
Iteration :  24   Loss :  1.77617557809
Iteration :  25   Loss :  1.43655834994
Iteration :  26   Loss :  1.16187831779
Iteration :  27   Loss :  0.939719034324
Iteration :  28   Loss :  0.760038164022
Iteration :  29   Loss :  0.614713536356
Iteration :  30   Loss :  0.497175996767
Iteration :  31   Loss :  0.402112459125
Iteration :  32   Loss :  0.325225736631
Iteration :  33   Loss :  0.26304029474
Iteration :  34   Loss :  0.212745145491
Iteration :  35   Loss :  0.172066781535
Iteration :  36   Loss :  0.139166406075
Iteration :  37   Loss :  0.112556813157
Iteration :  38   Loss :  0.0910351610382
Iteration :  39   Loss :  0.0736285997513
Iteration :  40   Loss :  0.0595502950675
Iteration :  41   Loss :  0.0481638609806
Iteration :  42   Loss :  0.0389545929526
Iteration :  43   Loss :  0.0315062015629
Iteration :  44   Loss :  0.025481994848
Iteration :  45   Loss :  0.0206096587091
Iteration :  46   Loss :  0.0166689474133
Iteration :  47   Loss :  0.0134817277563
Iteration :  48   Loss :  0.010903926852
Iteration :  49   Loss :  0.00881901956069
Iteration :  50   Loss :  0.00713276116643
Iteration :  51   Loss :  0.00576892720412
Iteration :  52   Loss :  0.00466586786097
Iteration :  53   Loss :  0.0037737212008
Iteration :  54   Loss :  0.00305215923933
Iteration :  55   Loss :  0.0196126486279
Iteration :  56   Loss :  1144.47011888
Iteration :  57   Loss :  1718.7893966
Iteration :  58   Loss :  219.555669574
Iteration :  59   Loss :  177.575085647
Iteration :  60   Loss :  143.621483807
Iteration :  61   Loss :  116.16004878
Iteration :  62   Loss :  93.9494327375
Iteration :  63   Loss :  75.9856422618
Iteration :  64   Loss :  61.4566545183
Iteration :  65   Loss :  49.7057111337
Iteration :  66   Loss :  40.2016305423
Iteration :  67   Loss :  32.5147968191
Iteration :  68   Loss :  26.2977396172
Iteration :  69   Loss :  21.269427357
Iteration :  70   Loss :  17.2025636682
Iteration :  71   Loss :  13.9133128407
Iteration :  72   Loss :  11.2529898413
Iteration :  73   Loss :  9.10133925813
Iteration :  74   Loss :  7.36109935761
Iteration :  75   Loss :  5.95360553164
Iteration :  76   Loss :  4.81523439699
Iteration :  77   Loss :  3.89452780751
Iteration :  78   Loss :  3.14986677553
Iteration :  79   Loss :  2.54759015572
Iteration :  80   Loss :  2.06047304983
Iteration :  81   Loss :  1.66649615109
Iteration :  82   Loss :  1.34785039864
Iteration :  83   Loss :  1.09013194896
Iteration :  84   Loss :  0.881691074422
Iteration :  85   Loss :  0.713105557046
Iteration :  86   Loss :  0.576754773006
Iteration :  87   Loss :  0.466475215203
Iteration :  88   Loss :  0.377281882323
Iteration :  89   Loss :  0.305142940268
Iteration :  90   Loss :  0.246797469897
Iteration :  91   Loss :  0.199608062681
Iteration :  92   Loss :  0.161441601099
Iteration :  93   Loss :  0.13057283466
Iteration :  94   Loss :  0.10560639287
Iteration :  95   Loss :  0.0854137098581
Iteration :  96   Loss :  0.0690820094642
Iteration :  97   Loss :  0.0558730447318
Iteration :  98   Loss :  0.0451897267004
Iteration :  99   Loss :  0.0365491340066
[-0.0030954  -0.00076441 -0.00236515 ...,  0.00136444 -0.00041108
  0.00030745]
CROSS VALIDATION 15
Iteration :  0   Loss :  1938.16754612
Iteration :  1   Loss :  238.628471338
Iteration :  2   Loss :  202.833971763
Iteration :  3   Loss :  164.050693738
Iteration :  4   Loss :  132.683050488
Iteration :  5   Loss :  107.313120631
Iteration :  6   Loss :  86.7940993005
Iteration :  7   Loss :  70.1984587635
Iteration :  8   Loss :  56.7760210946
Iteration :  9   Loss :  45.9200476494
Iteration :  10   Loss :  37.1398124679
Iteration :  11   Loss :  30.0384198352
Iteration :  12   Loss :  24.2948632813
Iteration :  13   Loss :  19.6495150242
Iteration :  14   Loss :  15.8923899351
Iteration :  15   Loss :  12.8536535145
Iteration :  16   Loss :  10.3959448104
Iteration :  17   Loss :  8.40816724828
Iteration :  18   Loss :  6.80046669776
Iteration :  19   Loss :  5.50016976848
Iteration :  20   Loss :  4.44849873202
Iteration :  21   Loss :  3.59791457387
Iteration :  22   Loss :  2.90996807253
Iteration :  23   Loss :  2.35356176732
Iteration :  24   Loss :  1.90354425015
Iteration :  25   Loss :  1.53957323857
Iteration :  26   Loss :  1.2451960372
Iteration :  27   Loss :  1.00710582141
Iteration :  28   Loss :  0.814540124787
Iteration :  29   Loss :  0.65879433996
Iteration :  30   Loss :  0.532828241551
Iteration :  31   Loss :  0.430947744651
Iteration :  32   Loss :  0.348547513321
Iteration :  33   Loss :  0.281902784155
Iteration :  34   Loss :  0.228000994633
Iteration :  35   Loss :  0.184405605321
Iteration :  36   Loss :  0.149145960212
Iteration :  37   Loss :  0.120628206549
Iteration :  38   Loss :  0.0975632474028
Iteration :  39   Loss :  0.0789084702167
Iteration :  40   Loss :  0.0638206172683
Iteration :  41   Loss :  0.0516176676258
Iteration :  42   Loss :  0.0417480075431
Iteration :  43   Loss :  0.0337654956915
Iteration :  44   Loss :  0.0273092960931
Iteration :  45   Loss :  0.0220875671399
Iteration :  46   Loss :  0.0178642693865
Iteration :  47   Loss :  0.0144484957845
Iteration :  48   Loss :  0.0116858420526
Iteration :  49   Loss :  0.00945142709077
Iteration :  50   Loss :  0.00764424794122
Iteration :  51   Loss :  0.00618261412014
Iteration :  52   Loss :  0.00500045493716
Iteration :  53   Loss :  0.0654246776475
Iteration :  54   Loss :  1125.30829345
Iteration :  55   Loss :  1704.70270953
Iteration :  56   Loss :  384.070445252
Iteration :  57   Loss :  783.815793836
Iteration :  58   Loss :  158.810127323
Iteration :  59   Loss :  128.444517127
Iteration :  60   Loss :  103.885024577
Iteration :  61   Loss :  84.0214792558
Iteration :  62   Loss :  67.9559831178
Iteration :  63   Loss :  54.9623225205
Iteration :  64   Loss :  44.4531409634
Iteration :  65   Loss :  35.9533886287
Iteration :  66   Loss :  29.0788485554
Iteration :  67   Loss :  23.5187687603
Iteration :  68   Loss :  19.021815219
Iteration :  69   Loss :  15.3847107351
Iteration :  70   Loss :  12.4430461383
Iteration :  71   Loss :  10.0638484445
Iteration :  72   Loss :  8.13957003672
Iteration :  73   Loss :  6.58322715692
Iteration :  74   Loss :  5.32446795151
Iteration :  75   Loss :  4.30639233478
Iteration :  76   Loss :  3.48297991648
Iteration :  77   Loss :  2.81700972775
Iteration :  78   Loss :  2.27837771005
Iteration :  79   Loss :  1.84273591195
Iteration :  80   Loss :  1.49039188113
Iteration :  81   Loss :  1.20541850025
Iteration :  82   Loss :  0.974934028514
Iteration :  83   Loss :  0.788519804334
Iteration :  84   Loss :  0.637749287277
Iteration :  85   Loss :  0.515807150546
Iteration :  86   Loss :  0.417181205628
Iteration :  87   Loss :  0.337413233114
Iteration :  88   Loss :  0.272897456416
Iteration :  89   Loss :  0.220717548719
Iteration :  90   Loss :  0.17851480535
Iteration :  91   Loss :  0.144381522511
Iteration :  92   Loss :  0.116774762754
Iteration :  93   Loss :  0.094446609088
Iteration :  94   Loss :  0.076387755007
Iteration :  95   Loss :  0.0617818804863
Iteration :  96   Loss :  0.0499687516156
Iteration :  97   Loss :  0.0404143758392
Iteration :  98   Loss :  0.0326868637231
Iteration :  99   Loss :  0.0264369061222
[-0.0027362  -0.00058675 -0.0026347  ...,  0.00155907  0.0001626
  0.00025486]
CROSS VALIDATION 16
Iteration :  0   Loss :  1938.16754612
Iteration :  1   Loss :  194.016130122
Iteration :  2   Loss :  156.918885265
Iteration :  3   Loss :  126.914893815
Iteration :  4   Loss :  102.64787597
Iteration :  5   Loss :  83.0208821398
Iteration :  6   Loss :  67.1467071883
Iteration :  7   Loss :  54.3077858248
Iteration :  8   Loss :  43.9237562747
Iteration :  9   Loss :  35.5252260055
Iteration :  10   Loss :  28.7325536287
Iteration :  11   Loss :  23.2386878524
Iteration :  12   Loss :  18.7952877451
Iteration :  13   Loss :  15.2014969031
Iteration :  14   Loss :  12.2948640759
Iteration :  15   Loss :  9.94399983172
Iteration :  16   Loss :  8.04263731932
Iteration :  17   Loss :  6.50482865495
Iteration :  18   Loss :  5.26105979299
Iteration :  19   Loss :  4.25510826089
Iteration :  20   Loss :  3.44150171721
Iteration :  21   Loss :  2.78346245111
Iteration :  22   Loss :  2.25124490799
Iteration :  23   Loss :  1.82079109195
Iteration :  24   Loss :  1.47264306462
Iteration :  25   Loss :  1.19106338194
Iteration :  26   Loss :  0.963323709512
Iteration :  27   Loss :  0.779129459757
Iteration :  28   Loss :  0.630154442445
Iteration :  29   Loss :  0.509664493314
Iteration :  30   Loss :  0.412213067541
Iteration :  31   Loss :  0.333395037874
Iteration :  32   Loss :  0.269647568289
Iteration :  33   Loss :  0.218089061996
Iteration :  34   Loss :  0.176388903723
Iteration :  35   Loss :  0.142662108187
Iteration :  36   Loss :  0.115384112507
Iteration :  37   Loss :  0.0933218609224
Iteration :  38   Loss :  0.0754780665794
Iteration :  39   Loss :  0.0610461308663
Iteration :  40   Loss :  0.0493736824834
Iteration :  41   Loss :  0.0399330880987
Iteration :  42   Loss :  0.0322976015741
Iteration :  43   Loss :  0.0261220736264
Iteration :  44   Loss :  0.0351709810274
Iteration :  45   Loss :  10481.5027231
Iteration :  46   Loss :  3261.10763
Iteration :  47   Loss :  226.321965477
Iteration :  48   Loss :  183.047618316
Iteration :  49   Loss :  148.047629846
Iteration :  50   Loss :  119.739884653
Iteration :  51   Loss :  96.8447788837
Iteration :  52   Loss :  78.3273779176
Iteration :  53   Loss :  63.3506338924
Iteration :  54   Loss :  51.2375483677
Iteration :  55   Loss :  41.4405697533
Iteration :  56   Loss :  33.5168421633
Iteration :  57   Loss :  27.1081868635
Iteration :  58   Loss :  21.9249114056
Iteration :  59   Loss :  17.7327145693
Iteration :  60   Loss :  14.3420951711
Iteration :  61   Loss :  11.5997859828
Iteration :  62   Loss :  9.38182554505
Iteration :  63   Loss :  7.58795469921
Iteration :  64   Loss :  6.13708454082
Iteration :  65   Loss :  4.96363093274
Iteration :  66   Loss :  4.01454988481
Iteration :  67   Loss :  3.24693978985
Iteration :  68   Loss :  2.62610212886
Iteration :  69   Loss :  2.12397298304
Iteration :  70   Loss :  1.7178544517
Iteration :  71   Loss :  1.38938863196
Iteration :  72   Loss :  1.12372778073
Iteration :  73   Loss :  0.908863147526
Iteration :  74   Loss :  0.735082139195
Iteration :  75   Loss :  0.59452927851
Iteration :  76   Loss :  0.480851110589
Iteration :  77   Loss :  0.388909005682
Iteration :  78   Loss :  0.314546876092
Iteration :  79   Loss :  0.254403307235
Iteration :  80   Loss :  0.205759610574
Iteration :  81   Loss :  0.166416929889
Iteration :  82   Loss :  0.134596845691
Iteration :  83   Loss :  0.108860984768
Iteration :  84   Loss :  0.0880460009584
Iteration :  85   Loss :  0.0712109880439
Iteration :  86   Loss :  0.057594947675
Iteration :  87   Loss :  0.0465823897239
Iteration :  88   Loss :  0.0376755100922
Iteration :  89   Loss :  0.0304716883166
Iteration :  90   Loss :  4901.28797052
Iteration :  91   Loss :  3936.47669324
Iteration :  92   Loss :  237.91861827
Iteration :  93   Loss :  182.525743419
Iteration :  94   Loss :  147.62554109
Iteration :  95   Loss :  119.398502227
Iteration :  96   Loss :  96.5686711722
Iteration :  97   Loss :  78.1040639371
Iteration :  98   Loss :  63.1700191112
Iteration :  99   Loss :  51.0914683995
[-0.08440007 -0.06243973 -0.00441298 ...,  0.02945868 -0.00233002
  0.0043345 ]
CROSS VALIDATION 17
Iteration :  0   Loss :  1938.16754612
Iteration :  1   Loss :  240.164328496
Iteration :  2   Loss :  202.875572517
Iteration :  3   Loss :  164.084340137
Iteration :  4   Loss :  132.710263459
Iteration :  5   Loss :  107.335130291
Iteration :  6   Loss :  86.8119005593
Iteration :  7   Loss :  70.2128562968
Iteration :  8   Loss :  56.7876657185
Iteration :  9   Loss :  45.9294657396
Iteration :  10   Loss :  37.1474297531
Iteration :  11   Loss :  30.0445806421
Iteration :  12   Loss :  24.2998460987
Iteration :  13   Loss :  19.6535450919
Iteration :  14   Loss :  15.8956494256
Iteration :  15   Loss :  12.8562897676
Iteration :  16   Loss :  10.3980769934
Iteration :  17   Loss :  8.40989174294
Iteration :  18   Loss :  6.80186145695
Iteration :  19   Loss :  5.50129783993
Iteration :  20   Loss :  4.4494111083
Iteration :  21   Loss :  3.59865249741
Iteration :  22   Loss :  2.91056489991
Iteration :  23   Loss :  2.35404447712
Iteration :  24   Loss :  1.90393466245
Iteration :  25   Loss :  1.5398890013
Iteration :  26   Loss :  1.24545142388
Iteration :  27   Loss :  1.00731237637
Iteration :  28   Loss :  0.814707184987
Iteration :  29   Loss :  0.658929457078
Iteration :  30   Loss :  0.532937523329
Iteration :  31   Loss :  0.431036130986
Iteration :  32   Loss :  0.348618999569
Iteration :  33   Loss :  0.28196060173
Iteration :  34   Loss :  0.228047757083
Iteration :  35   Loss :  0.184443426462
Iteration :  36   Loss :  0.149176549686
Iteration :  37   Loss :  0.120652947101
Iteration :  38   Loss :  0.0975832573877
Iteration :  39   Loss :  0.0789246541522
Iteration :  40   Loss :  0.0638337067218
Iteration :  41   Loss :  0.0516282542839
Iteration :  42   Loss :  0.041756569958
Iteration :  43   Loss :  0.0337724209126
Iteration :  44   Loss :  0.0273148971633
Iteration :  45   Loss :  0.0220920972463
Iteration :  46   Loss :  0.0178679333047
Iteration :  47   Loss :  0.0144514591359
Iteration :  48   Loss :  0.0116882387904
Iteration :  49   Loss :  0.0094533655555
Iteration :  50   Loss :  0.00764581575789
Iteration :  51   Loss :  0.00618388215927
Iteration :  52   Loss :  0.00500148051831
Iteration :  53   Loss :  0.0676438117126
Iteration :  54   Loss :  4120.90318242
Iteration :  55   Loss :  3379.50892944
Iteration :  56   Loss :  219.521219627
Iteration :  57   Loss :  177.547222771
Iteration :  58   Loss :  143.598948508
Iteration :  59   Loss :  116.141822389
Iteration :  60   Loss :  93.9346913601
Iteration :  61   Loss :  75.9737195391
Iteration :  62   Loss :  61.4470115038
Iteration :  63   Loss :  49.6979119313
Iteration :  64   Loss :  40.1953226021
Iteration :  65   Loss :  32.5096950013
Iteration :  66   Loss :  26.2936133027
Iteration :  67   Loss :  21.2660900228
Iteration :  68   Loss :  17.1998644557
Iteration :  69   Loss :  13.9111297365
Iteration :  70   Loss :  11.2512241619
Iteration :  71   Loss :  9.09991118909
Iteration :  72   Loss :  7.35994434538
Iteration :  73   Loss :  5.95267136585
Iteration :  74   Loss :  4.81447885025
Iteration :  75   Loss :  3.8939167266
Iteration :  76   Loss :  3.14937253758
Iteration :  77   Loss :  2.54719041954
Iteration :  78   Loss :  2.060149746
Iteration :  79   Loss :  1.66623466523
Iteration :  80   Loss :  1.34763891071
Iteration :  81   Loss :  1.089960899
Iteration :  82   Loss :  0.881552730415
Iteration :  83   Loss :  0.712993665384
Iteration :  84   Loss :  0.576664275814
Iteration :  85   Loss :  0.466402021709
Iteration :  86   Loss :  0.377222683938
Iteration :  87   Loss :  0.30509506103
Iteration :  88   Loss :  0.246758745505
Iteration :  89   Loss :  0.199576742665
Iteration :  90   Loss :  0.161416269689
Iteration :  91   Loss :  0.130552346794
Iteration :  92   Loss :  0.105589822427
Iteration :  93   Loss :  0.0854003078003
Iteration :  94   Loss :  0.0690711699738
Iteration :  95   Loss :  0.055864277828
Iteration :  96   Loss :  0.0451826360901
Iteration :  97   Loss :  0.0365433991707
Iteration :  98   Loss :  0.0295560449436
Iteration :  99   Loss :  0.0239047218521
[ -1.88140257e-03  -9.19547098e-04  -1.68123073e-03 ...,   8.99138205e-04
  -1.21679118e-04   8.25151353e-05]
CROSS VALIDATION 18
Iteration :  0   Loss :  313.532577291
Iteration :  1   Loss :  253.027520567
Iteration :  2   Loss :  300.291386763
Iteration :  3   Loss :  191.757222534
Iteration :  4   Loss :  155.091896651
Iteration :  5   Loss :  125.437238238
Iteration :  6   Loss :  101.452758504
Iteration :  7   Loss :  82.054279515
Iteration :  8   Loss :  66.3649257649
Iteration :  9   Loss :  53.6754864927
Iteration :  10   Loss :  43.4123570097
Iteration :  11   Loss :  35.1116098667
Iteration :  12   Loss :  28.3980237967
Iteration :  13   Loss :  22.9681224706
Iteration :  14   Loss :  18.5764563619
Iteration :  15   Loss :  15.0245076152
Iteration :  16   Loss :  12.1517163813
Iteration :  17   Loss :  9.82822298027
Iteration :  18   Loss :  7.9489978139
Iteration :  19   Loss :  6.42909367973
Iteration :  20   Loss :  5.19980587621
Iteration :  21   Loss :  4.20556652262
Iteration :  22   Loss :  3.40143270677
Iteration :  23   Loss :  2.75105491649
Iteration :  24   Loss :  2.22503392129
Iteration :  25   Loss :  1.79959182975
Iteration :  26   Loss :  1.45549725004
Iteration :  27   Loss :  1.17719596736
Iteration :  28   Loss :  0.952107841863
Iteration :  29   Loss :  0.770058144668
Iteration :  30   Loss :  0.622817626425
Iteration :  31   Loss :  0.503730527975
Iteration :  32   Loss :  0.407413717994
Iteration :  33   Loss :  0.329513357622
Iteration :  34   Loss :  0.266508092526
Iteration :  35   Loss :  0.215549876018
Iteration :  36   Loss :  0.174335227914
Iteration :  37   Loss :  0.141001109596
Iteration :  38   Loss :  0.114040708497
Iteration :  39   Loss :  0.0922353251811
Iteration :  40   Loss :  0.0745992840925
Iteration :  41   Loss :  0.0603353777545
Iteration :  42   Loss :  0.0487988303515
Iteration :  43   Loss :  0.0394681517263
Iteration :  44   Loss :  0.0319215642971
Iteration :  45   Loss :  0.0258179373141
Iteration :  46   Loss :  0.0208813666195
Iteration :  47   Loss :  0.0168887028655
Iteration :  48   Loss :  0.0136594644248
Iteration :  49   Loss :  0.0110476790229
Iteration :  50   Loss :  0.00893528530825
Iteration :  51   Loss :  0.0072267960876
Iteration :  52   Loss :  0.00584498198882
Iteration :  53   Loss :  0.00472738043741
Iteration :  54   Loss :  0.00382347214119
Iteration :  55   Loss :  10880.4937709
Iteration :  56   Loss :  5276.29014861
Iteration :  57   Loss :  1493.23710685
Iteration :  58   Loss :  496.276730885
Iteration :  59   Loss :  491.478486497
Iteration :  60   Loss :  156.771103462
Iteration :  61   Loss :  126.795368929
Iteration :  62   Loss :  102.551205081
Iteration :  63   Loss :  82.9426954025
Iteration :  64   Loss :  67.0834703036
Iteration :  65   Loss :  54.2566402759
Iteration :  66   Loss :  43.8823901135
Iteration :  67   Loss :  35.4917693444
Iteration :  68   Loss :  28.7054941159
Iteration :  69   Loss :  23.2168023083
Iteration :  70   Loss :  18.7775868705
Iteration :  71   Loss :  15.1871805598
Iteration :  72   Loss :  12.2832851179
Iteration :  73   Loss :  9.9346348517
Iteration :  74   Loss :  8.03506298917
Iteration :  75   Loss :  6.49870258984
Iteration :  76   Loss :  5.25610507449
Iteration :  77   Loss :  4.25110091933
Iteration :  78   Loss :  3.43826060746
Iteration :  79   Loss :  2.78084106427
Iteration :  80   Loss :  2.2491247487
Iteration :  81   Loss :  1.81907632199
Iteration :  82   Loss :  1.47125617071
Iteration :  83   Loss :  1.18994167187
Iteration :  84   Loss :  0.962416478273
Iteration :  85   Loss :  0.778395697494
Iteration :  86   Loss :  0.629560980673
Iteration :  87   Loss :  0.509184505595
Iteration :  88   Loss :  0.41182485684
Iteration :  89   Loss :  0.333081055783
Iteration :  90   Loss :  0.269393621775
Iteration :  91   Loss :  0.217883671836
Iteration :  92   Loss :  0.176222785603
Iteration :  93   Loss :  0.142527753016
Iteration :  94   Loss :  0.115275446988
Iteration :  95   Loss :  0.151145501893
Iteration :  96   Loss :  2182.43371046
Iteration :  97   Loss :  183.600250845
Iteration :  98   Loss :  148.49459516
Iteration :  99   Loss :  120.101387064
[-0.07510716 -0.05133206 -0.05751111 ...,  0.14406741  0.07305849
 -0.00705286]
CROSS VALIDATION 19
Iteration :  0   Loss :  2036.76070194
Iteration :  1   Loss :  728.202277142
Iteration :  2   Loss :  174.550396851
Iteration :  3   Loss :  141.175136723
Iteration :  4   Loss :  114.181460417
Iteration :  5   Loss :  92.3491643467
Iteration :  6   Loss :  74.6913564112
Iteration :  7   Loss :  60.4098452
Iteration :  8   Loss :  48.8590590992
Iteration :  9   Loss :  39.516864315
Iteration :  10   Loss :  31.9609626972
Iteration :  11   Loss :  25.8498024638
Iteration :  12   Loss :  20.9071389291
Iteration :  13   Loss :  16.9095473287
Iteration :  14   Loss :  13.6763232804
Iteration :  15   Loss :  11.0613143471
Iteration :  16   Loss :  8.94631346283
Iteration :  17   Loss :  7.23571558167
Iteration :  18   Loss :  5.85219601307
Iteration :  19   Loss :  4.73321508963
Iteration :  20   Loss :  3.82819116698
Iteration :  21   Loss :  3.09621416594
Iteration :  22   Loss :  2.50419630139
Iteration :  23   Loss :  2.0253764048
Iteration :  24   Loss :  1.63811023075
Iteration :  25   Loss :  1.32489206536
Iteration :  26   Loss :  1.07156341002
Iteration :  27   Loss :  0.866672970366
Iteration :  28   Loss :  0.700959019818
Iteration :  29   Loss :  0.566930738889
Iteration :  30   Loss :  0.458529605312
Iteration :  31   Loss :  0.370855528772
Iteration :  32   Loss :  0.299945350588
Iteration :  33   Loss :  0.242593695819
Iteration :  34   Loss :  0.196208079691
Iteration :  35   Loss :  0.158691718703
Iteration :  36   Loss :  0.128348749065
Iteration :  37   Loss :  0.10380756804
Iteration :  38   Loss :  0.0839588329521
Iteration :  39   Loss :  0.0679053152267
Iteration :  40   Loss :  0.0549213426855
Iteration :  41   Loss :  0.0444199967604
Iteration :  42   Loss :  0.0359265818298
Iteration :  43   Loss :  0.0290571674044
Iteration :  44   Loss :  0.0235012331975
Iteration :  45   Loss :  0.0190076325787
Iteration :  46   Loss :  0.0153732399152
Iteration :  47   Loss :  0.0124337686196
Iteration :  48   Loss :  0.0100563448524
Iteration :  49   Loss :  0.00813350118407
Iteration :  50   Loss :  0.00657831871146
Iteration :  51   Loss :  0.00532049803526
Iteration :  52   Loss :  0.00430318149436
Iteration :  53   Loss :  0.0516751203786
Iteration :  54   Loss :  3833.24991326
Iteration :  55   Loss :  808.926435913
Iteration :  56   Loss :  466.779057807
Iteration :  57   Loss :  193.321427937
Iteration :  58   Loss :  156.35701501
Iteration :  59   Loss :  126.460457094
Iteration :  60   Loss :  102.280330737
Iteration :  61   Loss :  82.7236141333
Iteration :  62   Loss :  66.9062789096
Iteration :  63   Loss :  54.1133291192
Iteration :  64   Loss :  43.7664810551
Iteration :  65   Loss :  35.3980229109
Iteration :  66   Loss :  28.6296726581
Iteration :  67   Loss :  23.1554784449
Iteration :  68   Loss :  18.7279885599
Iteration :  69   Loss :  15.1470657941
Iteration :  70   Loss :  12.2508405768
Iteration :  71   Loss :  9.90839393437
Iteration :  72   Loss :  8.01383951928
Iteration :  73   Loss :  6.48153719626
Iteration :  74   Loss :  5.24222182455
Iteration :  75   Loss :  4.23987224415
Iteration :  76   Loss :  3.42917893373
Iteration :  77   Loss :  2.77349587025
Iteration :  78   Loss :  2.2431840073
Iteration :  79   Loss :  1.81427149201
Iteration :  80   Loss :  1.46737005792
Iteration :  81   Loss :  1.1867986111
Iteration :  82   Loss :  0.959874392769
Iteration :  83   Loss :  0.776339676568
Iteration :  84   Loss :  0.627898085368
Iteration :  85   Loss :  0.507839567536
Iteration :  86   Loss :  0.410737080371
Iteration :  87   Loss :  0.332201269804
Iteration :  88   Loss :  0.268682057046
Iteration :  89   Loss :  0.217308163274
Iteration :  90   Loss :  0.175757318315
Iteration :  91   Loss :  0.142151286338
Iteration :  92   Loss :  0.114970963379
Iteration :  93   Loss :  0.0929877088049
Iteration :  94   Loss :  0.0752078066904
Iteration :  95   Loss :  0.0608275465636
Iteration :  96   Loss :  0.0491968930324
Iteration :  97   Loss :  0.0397901020307
Iteration :  98   Loss :  0.0321819554453
Iteration :  99   Loss :  0.0260285398485
[ -2.03911509e-03  -1.13484926e-03  -7.98899106e-04 ...,   1.38954491e-03
   4.64671943e-04   8.56497453e-05]
Accuracy (Hinge Loss):	0.75
lmda : 0.5  eta : 0.01
Logistic Loss
CROSS VALIDATION 0
Iteration :  0   Loss :  1736.86870299
Iteration :  1   Loss :  412.377477339
Iteration :  2   Loss :  142.110302829
Iteration :  3   Loss :  48.9729417341
Iteration :  4   Loss :  16.876672376
Iteration :  5   Loss :  5.81590691513
Iteration :  6   Loss :  2.00423278188
Iteration :  7   Loss :  0.690862313564
Iteration :  8   Loss :  0.238914960717
Iteration :  9   Loss :  0.0864155966281
Iteration :  10   Loss :  0.0344189711628
Iteration :  11   Loss :  0.0170631021748
Iteration :  12   Loss :  0.00860493685989
Iteration :  13   Loss :  0.0515452183124
Iteration :  14   Loss :  3823.65666408
Iteration :  15   Loss :  2803.3712057
Iteration :  16   Loss :  294.573063725
Iteration :  17   Loss :  101.514191472
Iteration :  18   Loss :  34.9828052377
Iteration :  19   Loss :  12.0558504243
Iteration :  20   Loss :  4.15476372154
Iteration :  21   Loss :  1.43296244149
Iteration :  22   Loss :  0.493678670341
Iteration :  23   Loss :  0.172166615481
Iteration :  24   Loss :  0.0604782272287
Iteration :  25   Loss :  0.034234969012
Iteration :  26   Loss :  8.79254630158
Iteration :  27   Loss :  3620.77038705
Iteration :  28   Loss :  1171.46448081
Iteration :  29   Loss :  294.814932033
Iteration :  30   Loss :  101.596817649
Iteration :  31   Loss :  35.0116290712
Iteration :  32   Loss :  12.0656879342
Iteration :  33   Loss :  4.15795576537
Iteration :  34   Loss :  1.43384566145
Iteration :  35   Loss :  0.495250663641
Iteration :  36   Loss :  0.173357979118
Iteration :  37   Loss :  0.06098578944
Iteration :  38   Loss :  0.024219389764
Iteration :  39   Loss :  0.0300866542132
Iteration :  40   Loss :  8436.35445208
Iteration :  41   Loss :  3447.94122596
Iteration :  42   Loss :  2450.75953662
Iteration :  43   Loss :  310.751917639
Iteration :  44   Loss :  167.824263382
Iteration :  45   Loss :  57.8342858229
Iteration :  46   Loss :  19.9303995103
Iteration :  47   Loss :  6.86826230542
Iteration :  48   Loss :  2.36701191643
Iteration :  49   Loss :  0.816072697054
Iteration :  50   Loss :  0.281248133027
Iteration :  51   Loss :  0.0979534300648
Iteration :  52   Loss :  0.0353434608619
Iteration :  53   Loss :  0.0152343861457
Iteration :  54   Loss :  9245.74245405
Iteration :  55   Loss :  3340.40137902
Iteration :  56   Loss :  951.423812226
Iteration :  57   Loss :  304.64663772
Iteration :  58   Loss :  104.984942974
Iteration :  59   Loss :  36.179090417
Iteration :  60   Loss :  12.4677560746
Iteration :  61   Loss :  4.29677321899
Iteration :  62   Loss :  1.48104708461
Iteration :  63   Loss :  0.510648839947
Iteration :  64   Loss :  0.176371588057
Iteration :  65   Loss :  0.0623594144282
Iteration :  66   Loss :  0.0288859714866
Iteration :  67   Loss :  0.0127578342339
Iteration :  68   Loss :  101.148003552
Iteration :  69   Loss :  3955.6145705
Iteration :  70   Loss :  2804.54491166
Iteration :  71   Loss :  295.293320504
Iteration :  72   Loss :  101.761847825
Iteration :  73   Loss :  35.0690764816
Iteration :  74   Loss :  12.0850149447
Iteration :  75   Loss :  4.16517652035
Iteration :  76   Loss :  1.43555432718
Iteration :  77   Loss :  0.49624395069
Iteration :  78   Loss :  0.172407778857
Iteration :  79   Loss :  0.0603724855722
Iteration :  80   Loss :  0.0322057796425
Iteration :  81   Loss :  0.0642982856907
Iteration :  82   Loss :  8820.31208219
Iteration :  83   Loss :  3394.26967288
Iteration :  84   Loss :  960.365030442
Iteration :  85   Loss :  304.79076388
Iteration :  86   Loss :  105.034610605
Iteration :  87   Loss :  36.1962064877
Iteration :  88   Loss :  12.4736545902
Iteration :  89   Loss :  4.29881900938
Iteration :  90   Loss :  1.48157816206
Iteration :  91   Loss :  0.510899399338
Iteration :  92   Loss :  0.176779801778
Iteration :  93   Loss :  0.0621554604406
Iteration :  94   Loss :  0.029061151799
Iteration :  95   Loss :  0.0129523881195
Iteration :  96   Loss :  246.998362355
Iteration :  97   Loss :  3943.16399005
Iteration :  98   Loss :  1177.94684356
Iteration :  99   Loss :  295.149794901
[-0.19151989 -0.04254353 -0.08891494 ...,  0.0985583   0.06812917
  0.00605949]
CROSS VALIDATION 1
Iteration :  0   Loss :  2976.17835882
Iteration :  1   Loss :  471.851919965
Iteration :  2   Loss :  162.605920356
Iteration :  3   Loss :  56.035981239
Iteration :  4   Loss :  19.3106818408
Iteration :  5   Loss :  6.65469623837
Iteration :  6   Loss :  2.29330629895
Iteration :  7   Loss :  0.791197977333
Iteration :  8   Loss :  0.274034189564
Iteration :  9   Loss :  0.0944048743118
Iteration :  10   Loss :  0.0356044382584
Iteration :  11   Loss :  0.0142425341726
Iteration :  12   Loss :  0.0306293041113
Iteration :  13   Loss :  9296.5339875
Iteration :  14   Loss :  6512.36928541
Iteration :  15   Loss :  383.419241026
Iteration :  16   Loss :  132.130941787
Iteration :  17   Loss :  45.5339323371
Iteration :  18   Loss :  15.6915478391
Iteration :  19   Loss :  5.40749857649
Iteration :  20   Loss :  1.86351190742
Iteration :  21   Loss :  0.64242592235
Iteration :  22   Loss :  0.221487082769
Iteration :  23   Loss :  0.0785284826777
Iteration :  24   Loss :  0.0379277578465
Iteration :  25   Loss :  0.0201901330217
Iteration :  26   Loss :  0.157201541705
Iteration :  27   Loss :  3392.48532381
Iteration :  28   Loss :  1856.35740467
Iteration :  29   Loss :  305.045937901
Iteration :  30   Loss :  1236.45561074
Iteration :  31   Loss :  2005.50003889
Iteration :  32   Loss :  312.474300116
Iteration :  33   Loss :  107.682450803
Iteration :  34   Loss :  37.1086844794
Iteration :  35   Loss :  12.7881047796
Iteration :  36   Loss :  4.40693692895
Iteration :  37   Loss :  1.51880738956
Iteration :  38   Loss :  0.523886064215
Iteration :  39   Loss :  0.181686400269
Iteration :  40   Loss :  0.0644630757245
Iteration :  41   Loss :  10418.5037498
Iteration :  42   Loss :  7019.17872567
Iteration :  43   Loss :  1737.34987025
Iteration :  44   Loss :  264.126437984
Iteration :  45   Loss :  91.0211884734
Iteration :  46   Loss :  31.3670104907
Iteration :  47   Loss :  10.8094539703
Iteration :  48   Loss :  3.72506953985
Iteration :  49   Loss :  1.28371888917
Iteration :  50   Loss :  0.442582461214
Iteration :  51   Loss :  0.153071383694
Iteration :  52   Loss :  0.0541441850272
Iteration :  53   Loss :  0.0298312828172
Iteration :  54   Loss :  0.0129448278482
Iteration :  55   Loss :  1.15391707812
Iteration :  56   Loss :  2420.06493715
Iteration :  57   Loss :  717.96625179
Iteration :  58   Loss :  205.685089179
Iteration :  59   Loss :  70.881587664
Iteration :  60   Loss :  24.426658684
Iteration :  61   Loss :  8.41772418099
Iteration :  62   Loss :  2.90085030883
Iteration :  63   Loss :  0.999668368209
Iteration :  64   Loss :  0.344517801623
Iteration :  65   Loss :  0.119382871507
Iteration :  66   Loss :  0.0434912523595
Iteration :  67   Loss :  0.0207984774644
Iteration :  68   Loss :  0.0102009678994
Iteration :  69   Loss :  3931.31621015
Iteration :  70   Loss :  2359.63177079
Iteration :  71   Loss :  511.356544943
Iteration :  72   Loss :  145.941911159
Iteration :  73   Loss :  50.2933611238
Iteration :  74   Loss :  17.3318746982
Iteration :  75   Loss :  5.97306654031
Iteration :  76   Loss :  2.05974177042
Iteration :  77   Loss :  0.709533627692
Iteration :  78   Loss :  0.245413255784
Iteration :  79   Loss :  0.0851313968016
Iteration :  80   Loss :  0.0343321831478
Iteration :  81   Loss :  17.2120119026
Iteration :  82   Loss :  7863.35500694
Iteration :  83   Loss :  582.357511534
Iteration :  84   Loss :  200.687493538
Iteration :  85   Loss :  69.1593553188
Iteration :  86   Loss :  23.8331566808
Iteration :  87   Loss :  8.21332708009
Iteration :  88   Loss :  2.8308894199
Iteration :  89   Loss :  0.976066117291
Iteration :  90   Loss :  0.337234139583
Iteration :  91   Loss :  0.118558119645
Iteration :  92   Loss :  0.0445227809928
Iteration :  93   Loss :  0.0212463255152
Iteration :  94   Loss :  7078.30705968
Iteration :  95   Loss :  585.418670636
Iteration :  96   Loss :  3075.04878938
Iteration :  97   Loss :  338.672853193
Iteration :  98   Loss :  116.710791982
Iteration :  99   Loss :  40.2200405429
[-0.03964989 -0.00510726 -0.01291697 ...,  0.00570153  0.00711616
  0.00387858]
CROSS VALIDATION 2
Iteration :  0   Loss :  5384.25262776
Iteration :  1   Loss :  2422.39548649
Iteration :  2   Loss :  181.296621525
Iteration :  3   Loss :  62.4770245774
Iteration :  4   Loss :  21.5303438487
Iteration :  5   Loss :  7.41961880191
Iteration :  6   Loss :  2.55689107485
Iteration :  7   Loss :  0.881193468892
Iteration :  8   Loss :  0.304176997763
Iteration :  9   Loss :  0.105467701471
Iteration :  10   Loss :  0.0379656446636
Iteration :  11   Loss :  0.0231664649718
Iteration :  12   Loss :  0.00808434301084
Iteration :  13   Loss :  7441.67527101
Iteration :  14   Loss :  2614.39847505
Iteration :  15   Loss :  212.552656194
Iteration :  16   Loss :  73.248234928
Iteration :  17   Loss :  25.24223817
Iteration :  18   Loss :  8.69910245351
Iteration :  19   Loss :  2.99794332316
Iteration :  20   Loss :  1.03427965348
Iteration :  21   Loss :  0.356447517303
Iteration :  22   Loss :  0.123779937799
Iteration :  23   Loss :  0.046368804965
Iteration :  24   Loss :  0.0223387697195
Iteration :  25   Loss :  8518.93514286
Iteration :  26   Loss :  2909.44473746
Iteration :  27   Loss :  1748.59903437
Iteration :  28   Loss :  607.11597816
Iteration :  29   Loss :  479.71109127
Iteration :  30   Loss :  3169.95334783
Iteration :  31   Loss :  468.917911012
Iteration :  32   Loss :  181.009035394
Iteration :  33   Loss :  62.3779188929
Iteration :  34   Loss :  21.4961908224
Iteration :  35   Loss :  7.40784925295
Iteration :  36   Loss :  2.55283511975
Iteration :  37   Loss :  0.879764626202
Iteration :  38   Loss :  0.303611615485
Iteration :  39   Loss :  0.106402727724
Iteration :  40   Loss :  6.21497229057
Iteration :  41   Loss :  2.17562905831
Iteration :  42   Loss :  6415.7282218
Iteration :  43   Loss :  777.284888768
Iteration :  44   Loss :  1214.29623914
Iteration :  45   Loss :  291.12803432
Iteration :  46   Loss :  100.326267541
Iteration :  47   Loss :  34.5736541043
Iteration :  48   Loss :  11.9145024371
Iteration :  49   Loss :  4.10590423006
Iteration :  50   Loss :  1.41665863582
Iteration :  51   Loss :  0.488039235613
Iteration :  52   Loss :  0.169358290903
Iteration :  53   Loss :  0.0610972331443
Iteration :  54   Loss :  1.85750224203
Iteration :  55   Loss :  7336.61581014
Iteration :  56   Loss :  460.60253552
Iteration :  57   Loss :  158.72924545
Iteration :  58   Loss :  54.7000675592
Iteration :  59   Loss :  18.850409023
Iteration :  60   Loss :  6.49646733072
Iteration :  61   Loss :  33.5195579861
Iteration :  62   Loss :  1702.27270703
Iteration :  63   Loss :  823.311860646
Iteration :  64   Loss :  263.262632428
Iteration :  65   Loss :  107.519081251
Iteration :  66   Loss :  37.0523853442
Iteration :  67   Loss :  12.768712992
Iteration :  68   Loss :  4.40111448144
Iteration :  69   Loss :  1.51652178793
Iteration :  70   Loss :  0.523531245024
Iteration :  71   Loss :  0.181632131862
Iteration :  72   Loss :  0.0647280808537
Iteration :  73   Loss :  0.0380020759283
Iteration :  74   Loss :  3559.77859551
Iteration :  75   Loss :  641.684206519
Iteration :  76   Loss :  133.565367658
Iteration :  77   Loss :  46.0282532679
Iteration :  78   Loss :  15.8620419514
Iteration :  79   Loss :  5.46627647785
Iteration :  80   Loss :  1.88419345362
Iteration :  81   Loss :  0.649286773328
Iteration :  82   Loss :  0.22426034803
Iteration :  83   Loss :  0.0823480291724
Iteration :  84   Loss :  0.076154091241
Iteration :  85   Loss :  2712.27429696
Iteration :  86   Loss :  910.127292907
Iteration :  87   Loss :  2450.16806967
Iteration :  88   Loss :  143.342139215
Iteration :  89   Loss :  49.3974478131
Iteration :  90   Loss :  17.0230949621
Iteration :  91   Loss :  5.86659784171
Iteration :  92   Loss :  2.0216649607
Iteration :  93   Loss :  0.697714984324
Iteration :  94   Loss :  0.240329531859
Iteration :  95   Loss :  0.0851835620964
Iteration :  96   Loss :  0.0338509510643
Iteration :  97   Loss :  0.0148901495886
Iteration :  98   Loss :  1036.67365
Iteration :  99   Loss :  1228.79257596
[ 0.05818268 -0.06250363 -0.00721282 ...,  0.08619944  0.00020831
  0.01886622]
CROSS VALIDATION 3
Iteration :  0   Loss :  3908.61686851
Iteration :  1   Loss :  404.315933377
Iteration :  2   Loss :  1035.98290801
Iteration :  3   Loss :  279.93835173
Iteration :  4   Loss :  96.4701665923
Iteration :  5   Loss :  33.2447947373
Iteration :  6   Loss :  11.4565612993
Iteration :  7   Loss :  3.94807060302
Iteration :  8   Loss :  1.36055323064
Iteration :  9   Loss :  0.468864731523
Iteration :  10   Loss :  0.161946215039
Iteration :  11   Loss :  0.0566192977914
Iteration :  12   Loss :  0.0310783573646
Iteration :  13   Loss :  7552.71845482
Iteration :  14   Loss :  5590.21680586
Iteration :  15   Loss :  1876.57015297
Iteration :  16   Loss :  214.522455177
Iteration :  17   Loss :  73.9270516556
Iteration :  18   Loss :  25.4761626795
Iteration :  19   Loss :  8.7793960443
Iteration :  20   Loss :  3.02548695169
Iteration :  21   Loss :  1.04270313858
Iteration :  22   Loss :  0.361279186506
Iteration :  23   Loss :  0.12444293665
Iteration :  24   Loss :  0.0473116530659
Iteration :  25   Loss :  0.0195038415204
Iteration :  26   Loss :  0.0129302307029
Iteration :  27   Loss :  8494.07404942
Iteration :  28   Loss :  4131.71376139
Iteration :  29   Loss :  1084.12407569
Iteration :  30   Loss :  213.181580221
Iteration :  31   Loss :  73.5082354777
Iteration :  32   Loss :  25.331833522
Iteration :  33   Loss :  8.72965845821
Iteration :  34   Loss :  3.00834666116
Iteration :  35   Loss :  1.03671293371
Iteration :  36   Loss :  0.357307251308
Iteration :  37   Loss :  0.125203146954
Iteration :  38   Loss :  0.0457405492755
Iteration :  39   Loss :  0.0183793928232
Iteration :  40   Loss :  0.00970261650469
Iteration :  41   Loss :  10831.7961493
Iteration :  42   Loss :  7626.75182941
Iteration :  43   Loss :  1507.39601568
Iteration :  44   Loss :  215.45490428
Iteration :  45   Loss :  74.2483849777
Iteration :  46   Loss :  25.5869027593
Iteration :  47   Loss :  8.81759654719
Iteration :  48   Loss :  3.03878417727
Iteration :  49   Loss :  1.04745248846
Iteration :  50   Loss :  0.361213989165
Iteration :  51   Loss :  0.125449693268
Iteration :  52   Loss :  0.0466865502266
Iteration :  53   Loss :  0.0190110690581
Iteration :  54   Loss :  0.0149951153385
Iteration :  55   Loss :  9343.0745563
Iteration :  56   Loss :  6381.06773544
Iteration :  57   Loss :  383.363132436
Iteration :  58   Loss :  132.111606084
Iteration :  59   Loss :  45.527269018
Iteration :  60   Loss :  15.6892515782
Iteration :  61   Loss :  5.40670726349
Iteration :  62   Loss :  1.86324299536
Iteration :  63   Loss :  0.642369763536
Iteration :  64   Loss :  0.221466105939
Iteration :  65   Loss :  0.0784811285712
Iteration :  66   Loss :  0.0376991181929
Iteration :  67   Loss :  0.0228266511092
Iteration :  68   Loss :  4552.40962016
Iteration :  69   Loss :  1329.39333773
Iteration :  70   Loss :  909.516637173
Iteration :  71   Loss :  2150.27263529
Iteration :  72   Loss :  147.742113247
Iteration :  73   Loss :  50.9137322184
Iteration :  74   Loss :  17.5454924225
Iteration :  75   Loss :  6.04639357784
Iteration :  76   Loss :  2.08394468183
Iteration :  77   Loss :  0.719275462018
Iteration :  78   Loss :  0.248053938159
Iteration :  79   Loss :  0.0876843548915
Iteration :  80   Loss :  36.8173754048
Iteration :  81   Loss :  1882.51376963
Iteration :  82   Loss :  2095.37643084
Iteration :  83   Loss :  253.354738312
Iteration :  84   Loss :  87.3093434344
Iteration :  85   Loss :  30.0878635296
Iteration :  86   Loss :  10.3686851087
Iteration :  87   Loss :  3.57342865175
Iteration :  88   Loss :  1.2316905892
Iteration :  89   Loss :  0.425102670545
Iteration :  90   Loss :  0.1473369918
Iteration :  91   Loss :  0.0550385855961
Iteration :  92   Loss :  9145.84611662
Iteration :  93   Loss :  690.821940968
Iteration :  94   Loss :  1229.7307229
Iteration :  95   Loss :  506.615378213
Iteration :  96   Loss :  557.844638517
Iteration :  97   Loss :  4022.1378518
Iteration :  98   Loss :  301.637925103
Iteration :  99   Loss :  103.948107308
[-0.06328371 -0.02350308  0.00078196 ...,  0.03397111 -0.00090507
  0.01538953]
CROSS VALIDATION 4
Iteration :  0   Loss :  3908.61686851
Iteration :  1   Loss :  404.315933377
Iteration :  2   Loss :  1036.65979706
Iteration :  3   Loss :  97.2952969901
Iteration :  4   Loss :  33.5291447252
Iteration :  5   Loss :  11.5545517695
Iteration :  6   Loss :  3.9818393129
Iteration :  7   Loss :  1.37219036249
Iteration :  8   Loss :  0.472917993076
Iteration :  9   Loss :  0.164804900438
Iteration :  10   Loss :  0.057359187435
Iteration :  11   Loss :  0.0215760363258
Iteration :  12   Loss :  0.0268265837762
Iteration :  13   Loss :  8773.04874363
Iteration :  14   Loss :  3413.16084061
Iteration :  15   Loss :  958.106925225
Iteration :  16   Loss :  402.482458358
Iteration :  17   Loss :  168.253119331
Iteration :  18   Loss :  57.9820748078
Iteration :  19   Loss :  19.9813293945
Iteration :  20   Loss :  6.88580954884
Iteration :  21   Loss :  2.37293386323
Iteration :  22   Loss :  0.817745393933
Iteration :  23   Loss :  0.281935337438
Iteration :  24   Loss :  0.0983069434048
Iteration :  25   Loss :  0.0346941233779
Iteration :  26   Loss :  0.0183291759898
Iteration :  27   Loss :  13513.8874709
Iteration :  28   Loss :  6055.65268386
Iteration :  29   Loss :  552.622479786
Iteration :  30   Loss :  190.440439326
Iteration :  31   Loss :  65.6280955938
Iteration :  32   Loss :  22.6162413115
Iteration :  33   Loss :  7.79383230215
Iteration :  34   Loss :  2.68586324068
Iteration :  35   Loss :  0.926627133782
Iteration :  36   Loss :  0.320536115484
Iteration :  37   Loss :  0.110665870071
Iteration :  38   Loss :  0.043160279491
Iteration :  39   Loss :  0.0193165661927
Iteration :  40   Loss :  0.00914901119049
Iteration :  41   Loss :  0.126316469449
Iteration :  42   Loss :  2501.13683639
Iteration :  43   Loss :  2608.22719921
Iteration :  44   Loss :  350.01403933
Iteration :  45   Loss :  120.619102296
Iteration :  46   Loss :  41.5668124981
Iteration :  47   Loss :  14.3245449957
Iteration :  48   Loss :  4.9365450418
Iteration :  49   Loss :  1.7011941076
Iteration :  50   Loss :  0.586492322579
Iteration :  51   Loss :  0.203703336453
Iteration :  52   Loss :  0.0724739535175
Iteration :  53   Loss :  0.0273637935348
Iteration :  54   Loss :  0.0117411970525
Iteration :  55   Loss :  10840.1501372
Iteration :  56   Loss :  6864.15261563
Iteration :  57   Loss :  1506.30140195
Iteration :  58   Loss :  215.500819418
Iteration :  59   Loss :  74.2642078932
Iteration :  60   Loss :  25.5923530985
Iteration :  61   Loss :  8.81953890518
Iteration :  62   Loss :  3.0396590327
Iteration :  63   Loss :  1.04757570523
Iteration :  64   Loss :  0.361401331053
Iteration :  65   Loss :  0.125157823904
Iteration :  66   Loss :  0.0473498576509
Iteration :  67   Loss :  0.0179546659578
Iteration :  68   Loss :  0.0158025879343
Iteration :  69   Loss :  7691.25084171
Iteration :  70   Loss :  5329.1030065
Iteration :  71   Loss :  1881.36158527
Iteration :  72   Loss :  214.327673732
Iteration :  73   Loss :  73.8599275967
Iteration :  74   Loss :  25.453030911
Iteration :  75   Loss :  8.77142455505
Iteration :  76   Loss :  3.02273990556
Iteration :  77   Loss :  1.0417676629
Iteration :  78   Loss :  0.360945724922
Iteration :  79   Loss :  0.124260630866
Iteration :  80   Loss :  0.0472542829074
Iteration :  81   Loss :  0.0231145882616
Iteration :  82   Loss :  0.0187576461098
Iteration :  83   Loss :  3.63064119207
Iteration :  84   Loss :  1850.26698297
Iteration :  85   Loss :  2056.96552943
Iteration :  86   Loss :  253.48998665
Iteration :  87   Loss :  87.3560600335
Iteration :  88   Loss :  30.1039625785
Iteration :  89   Loss :  10.3741992425
Iteration :  90   Loss :  3.5751332282
Iteration :  91   Loss :  1.23235075301
Iteration :  92   Loss :  0.425194438936
Iteration :  93   Loss :  0.14809389759
Iteration :  94   Loss :  0.0543267412516
Iteration :  95   Loss :  0.0241255879909
Iteration :  96   Loss :  0.0100754638736
Iteration :  97   Loss :  6309.06745892
Iteration :  98   Loss :  522.526997788
Iteration :  99   Loss :  1691.53241891
[-0.0063521  -0.02994469  0.08024315 ...,  0.05107484  0.03020574
  0.00370362]
CROSS VALIDATION 5
Iteration :  0   Loss :  3590.28293754
Iteration :  1   Loss :  361.705505346
Iteration :  2   Loss :  124.648123926
Iteration :  3   Loss :  42.9552621363
Iteration :  4   Loss :  14.8029066711
Iteration :  5   Loss :  5.10126198993
Iteration :  6   Loss :  1.75796526977
Iteration :  7   Loss :  0.606047430679
Iteration :  8   Loss :  0.211082752132
Iteration :  9   Loss :  0.0734876057616
Iteration :  10   Loss :  0.0295668495298
Iteration :  11   Loss :  0.0123098273886
Iteration :  12   Loss :  0.0192759090825
Iteration :  13   Loss :  6545.43017805
Iteration :  14   Loss :  2288.80550914
Iteration :  15   Loss :  553.336895405
Iteration :  16   Loss :  430.70125698
Iteration :  17   Loss :  1873.11724008
Iteration :  18   Loss :  851.684092275
Iteration :  19   Loss :  269.760512287
Iteration :  20   Loss :  446.407277808
Iteration :  21   Loss :  1362.66052727
Iteration :  22   Loss :  124.98307834
Iteration :  23   Loss :  43.0706915082
Iteration :  24   Loss :  14.8426850389
Iteration :  25   Loss :  5.11497009801
Iteration :  26   Loss :  1.76268105538
Iteration :  27   Loss :  0.60748280051
Iteration :  28   Loss :  0.209870146235
Iteration :  29   Loss :  0.0763492700667
Iteration :  30   Loss :  0.0283526249106
Iteration :  31   Loss :  0.0292670168248
Iteration :  32   Loss :  7844.68128017
Iteration :  33   Loss :  2384.73304418
Iteration :  34   Loss :  1967.93046562
Iteration :  35   Loss :  190.704037616
Iteration :  36   Loss :  65.7189347759
Iteration :  37   Loss :  22.6475456004
Iteration :  38   Loss :  7.80462013693
Iteration :  39   Loss :  2.68956861444
Iteration :  40   Loss :  0.927125875261
Iteration :  41   Loss :  0.31990580352
Iteration :  42   Loss :  0.110787043383
Iteration :  43   Loss :  0.0406774823397
Iteration :  44   Loss :  0.0164799685887
Iteration :  45   Loss :  0.00914956339144
Iteration :  46   Loss :  5329.43604695
Iteration :  47   Loss :  1316.29994887
Iteration :  48   Loss :  458.549253747
Iteration :  49   Loss :  674.491106427
Iteration :  50   Loss :  746.626662815
Iteration :  51   Loss :  79.0781818461
Iteration :  52   Loss :  27.25131296
Iteration :  53   Loss :  9.39125585947
Iteration :  54   Loss :  3.23655199736
Iteration :  55   Loss :  1.1154011479
Iteration :  56   Loss :  0.385267447958
Iteration :  57   Loss :  0.133467201817
Iteration :  58   Loss :  0.050706036432
Iteration :  59   Loss :  7045.66293771
Iteration :  60   Loss :  4974.73618221
Iteration :  61   Loss :  1513.1894711
Iteration :  62   Loss :  403.838399475
Iteration :  63   Loss :  1003.43384767
Iteration :  64   Loss :  110.197544333
Iteration :  65   Loss :  37.9754163523
Iteration :  66   Loss :  13.0867911428
Iteration :  67   Loss :  4.50986948131
Iteration :  68   Loss :  1.55451548526
Iteration :  69   Loss :  0.536074519966
Iteration :  70   Loss :  0.186778056063
Iteration :  71   Loss :  0.0648506694398
Iteration :  72   Loss :  0.0256900796631
Iteration :  73   Loss :  11891.2047378
Iteration :  74   Loss :  5796.86102787
Iteration :  75   Loss :  447.708018087
Iteration :  76   Loss :  154.285747987
Iteration :  77   Loss :  53.1688475214
Iteration :  78   Loss :  18.3227958204
Iteration :  79   Loss :  6.31439916554
Iteration :  80   Loss :  2.1765709454
Iteration :  81   Loss :  0.7519503365
Iteration :  82   Loss :  0.259506909815
Iteration :  83   Loss :  0.0982897925366
Iteration :  84   Loss :  0.0336681809348
Iteration :  85   Loss :  0.0193782959941
Iteration :  86   Loss :  0.0238959054663
Iteration :  87   Loss :  6608.49192773
Iteration :  88   Loss :  2965.65548946
Iteration :  89   Loss :  470.814925812
Iteration :  90   Loss :  614.063317186
Iteration :  91   Loss :  2131.66980285
Iteration :  92   Loss :  524.805989726
Iteration :  93   Loss :  1173.32803128
Iteration :  94   Loss :  126.165188493
Iteration :  95   Loss :  43.4780610691
Iteration :  96   Loss :  14.9830695527
Iteration :  97   Loss :  5.16334851293
Iteration :  98   Loss :  1.77940901228
Iteration :  99   Loss :  0.61415833407
[-0.00454698 -0.00477308 -0.00145314 ...,  0.00262863  0.00224123
  0.00119954]
CROSS VALIDATION 6
Iteration :  0   Loss :  3845.87328054
Iteration :  1   Loss :  404.315933377
Iteration :  2   Loss :  1039.08548639
Iteration :  3   Loss :  93.6569800733
Iteration :  4   Loss :  32.2753363888
Iteration :  5   Loss :  11.1224741412
Iteration :  6   Loss :  3.83294014585
Iteration :  7   Loss :  1.32092747625
Iteration :  8   Loss :  0.455360731473
Iteration :  9   Loss :  0.15830408657
Iteration :  10   Loss :  0.0562081657587
Iteration :  11   Loss :  0.0232064645003
Iteration :  12   Loss :  2.12423194654
Iteration :  13   Loss :  5276.27198974
Iteration :  14   Loss :  420.19092472
Iteration :  15   Loss :  144.803148172
Iteration :  16   Loss :  49.9010848768
Iteration :  17   Loss :  17.1968186571
Iteration :  18   Loss :  5.92652642121
Iteration :  19   Loss :  2.04491061799
Iteration :  20   Loss :  0.705575528279
Iteration :  21   Loss :  0.244846841926
Iteration :  22   Loss :  0.0893410999868
Iteration :  23   Loss :  0.0341085912875
Iteration :  24   Loss :  0.0170422764056
Iteration :  25   Loss :  76.2920672954
Iteration :  26   Loss :  1919.34033732
Iteration :  27   Loss :  794.441070252
Iteration :  28   Loss :  318.758137958
Iteration :  29   Loss :  138.91149237
Iteration :  30   Loss :  47.8705926777
Iteration :  31   Loss :  16.4967894608
Iteration :  32   Loss :  5.68499463432
Iteration :  33   Loss :  1.95911869943
Iteration :  34   Loss :  0.675235038512
Iteration :  35   Loss :  0.234664611672
Iteration :  36   Loss :  0.0811013384104
Iteration :  37   Loss :  0.0305519790089
Iteration :  38   Loss :  0.0282884152573
Iteration :  39   Loss :  7475.43460149
Iteration :  40   Loss :  2024.35071663
Iteration :  41   Loss :  438.102444497
Iteration :  42   Loss :  150.975487338
Iteration :  43   Loss :  52.0281393237
Iteration :  44   Loss :  17.9295311031
Iteration :  45   Loss :  6.17880671049
Iteration :  46   Loss :  2.13029815371
Iteration :  47   Loss :  0.734047533798
Iteration :  48   Loss :  0.255559626816
Iteration :  49   Loss :  0.0935228341725
Iteration :  50   Loss :  0.0727369924997
Iteration :  51   Loss :  7487.90785343
Iteration :  52   Loss :  468.478467658
Iteration :  53   Loss :  2817.42923217
Iteration :  54   Loss :  641.402596345
Iteration :  55   Loss :  1781.44927168
Iteration :  56   Loss :  838.68243624
Iteration :  57   Loss :  215.727133946
Iteration :  58   Loss :  74.3421986363
Iteration :  59   Loss :  25.6192273869
Iteration :  60   Loss :  8.82869788548
Iteration :  61   Loss :  3.04247685911
Iteration :  62   Loss :  1.04848746357
Iteration :  63   Loss :  0.361670219901
Iteration :  64   Loss :  0.124989775155
Iteration :  65   Loss :  0.102176062508
Iteration :  66   Loss :  0.0201911831991
Iteration :  67   Loss :  0.0306457114187
Iteration :  68   Loss :  3930.66810496
Iteration :  69   Loss :  635.431492327
Iteration :  70   Loss :  218.977433939
Iteration :  71   Loss :  75.4622916134
Iteration :  72   Loss :  26.0052250733
Iteration :  73   Loss :  8.96171739101
Iteration :  74   Loss :  3.0883180087
Iteration :  75   Loss :  1.06457590083
Iteration :  76   Loss :  0.368058688208
Iteration :  77   Loss :  0.127956377918
Iteration :  78   Loss :  0.0497353969102
Iteration :  79   Loss :  0.0196691858079
Iteration :  80   Loss :  0.0108523038008
Iteration :  81   Loss :  0.475801350399
Iteration :  82   Loss :  3684.60980205
Iteration :  83   Loss :  2750.9064946
Iteration :  84   Loss :  294.383093223
Iteration :  85   Loss :  101.448000496
Iteration :  86   Loss :  34.9602169489
Iteration :  87   Loss :  12.0477240897
Iteration :  88   Loss :  4.15280888731
Iteration :  89   Loss :  1.43098834493
Iteration :  90   Loss :  0.493458339444
Iteration :  91   Loss :  0.171873185747
Iteration :  92   Loss :  0.06126706291
Iteration :  93   Loss :  0.0285045504338
Iteration :  94   Loss :  0.0152972724146
Iteration :  95   Loss :  6586.00404883
Iteration :  96   Loss :  1588.82883935
Iteration :  97   Loss :  1499.74608919
Iteration :  98   Loss :  198.581804322
Iteration :  99   Loss :  68.4337552578
[-0.07845907 -0.02298696 -0.04745514 ...,  0.04244333  0.01083354
  0.00403216]
CROSS VALIDATION 7
Iteration :  0   Loss :  3908.61686851
Iteration :  1   Loss :  404.315933377
Iteration :  2   Loss :  1039.08548639
Iteration :  3   Loss :  93.6569800733
Iteration :  4   Loss :  32.2753363888
Iteration :  5   Loss :  11.1224741412
Iteration :  6   Loss :  3.83294014585
Iteration :  7   Loss :  1.32092747625
Iteration :  8   Loss :  0.455360735905
Iteration :  9   Loss :  0.15833450667
Iteration :  10   Loss :  0.0567611258609
Iteration :  11   Loss :  0.0222766207094
Iteration :  12   Loss :  1.67052922008
Iteration :  13   Loss :  5309.99660146
Iteration :  14   Loss :  1271.2406824
Iteration :  15   Loss :  755.374844
Iteration :  16   Loss :  960.534258625
Iteration :  17   Loss :  247.845798446
Iteration :  18   Loss :  85.410681736
Iteration :  19   Loss :  29.4335615142
Iteration :  20   Loss :  10.1431638978
Iteration :  21   Loss :  3.49546198984
Iteration :  22   Loss :  1.20474078857
Iteration :  23   Loss :  0.415599399784
Iteration :  24   Loss :  0.143411532396
Iteration :  25   Loss :  0.0520868346779
Iteration :  26   Loss :  9795.72311985
Iteration :  27   Loss :  6611.27089701
Iteration :  28   Loss :  1389.26279008
Iteration :  29   Loss :  1484.42211428
Iteration :  30   Loss :  88.1366426375
Iteration :  31   Loss :  30.3729608522
Iteration :  32   Loss :  10.466892696
Iteration :  33   Loss :  3.60716175083
Iteration :  34   Loss :  1.24368864879
Iteration :  35   Loss :  0.429096624657
Iteration :  36   Loss :  0.149494368616
Iteration :  37   Loss :  0.0528055571748
Iteration :  38   Loss :  0.0205715445642
Iteration :  39   Loss :  0.0131399007191
Iteration :  40   Loss :  0.0242530555534
Iteration :  41   Loss :  3844.95164455
Iteration :  42   Loss :  2774.04118348
Iteration :  43   Loss :  295.164566336
Iteration :  44   Loss :  101.717308371
Iteration :  45   Loss :  35.0535423304
Iteration :  46   Loss :  12.0797413719
Iteration :  47   Loss :  4.1642427147
Iteration :  48   Loss :  1.43469417142
Iteration :  49   Loss :  0.49560860943
Iteration :  50   Loss :  0.171453258418
Iteration :  51   Loss :  0.0612652048322
Iteration :  52   Loss :  0.0308892690503
Iteration :  53   Loss :  0.0148356873095
Iteration :  54   Loss :  7568.49159661
Iteration :  55   Loss :  5751.12423338
Iteration :  56   Loss :  1886.32785926
Iteration :  57   Loss :  214.787769651
Iteration :  58   Loss :  74.0184822557
Iteration :  59   Loss :  25.5076707782
Iteration :  60   Loss :  8.79025411898
Iteration :  61   Loss :  3.02922877054
Iteration :  62   Loss :  1.04399038887
Iteration :  63   Loss :  0.361785310053
Iteration :  64   Loss :  0.12454034045
Iteration :  65   Loss :  0.0470402096987
Iteration :  66   Loss :  0.0219068436813
Iteration :  67   Loss :  0.0140294830098
Iteration :  68   Loss :  27.9534818693
Iteration :  69   Loss :  3942.96887467
Iteration :  70   Loss :  1241.88252494
Iteration :  71   Loss :  294.110464659
Iteration :  72   Loss :  101.354050584
Iteration :  73   Loss :  34.9281632769
Iteration :  74   Loss :  12.0366040624
Iteration :  75   Loss :  4.14894203637
Iteration :  76   Loss :  1.42970011411
Iteration :  77   Loss :  0.494920457651
Iteration :  78   Loss :  0.171456487017
Iteration :  79   Loss :  0.0620634415507
Iteration :  80   Loss :  0.0241532218961
Iteration :  81   Loss :  0.0239199007157
Iteration :  82   Loss :  10842.5727432
Iteration :  83   Loss :  4306.62425912
Iteration :  84   Loss :  3323.82079152
Iteration :  85   Loss :  989.556506644
Iteration :  86   Loss :  241.535475044
Iteration :  87   Loss :  83.2360674109
Iteration :  88   Loss :  28.6841629237
Iteration :  89   Loss :  9.88491244209
Iteration :  90   Loss :  3.40657612683
Iteration :  91   Loss :  1.17439291898
Iteration :  92   Loss :  0.405152991499
Iteration :  93   Loss :  0.140125196457
Iteration :  94   Loss :  0.0500843768687
Iteration :  95   Loss :  0.0195929047079
Iteration :  96   Loss :  9440.09504917
Iteration :  97   Loss :  3273.23538527
Iteration :  98   Loss :  914.788655478
Iteration :  99   Loss :  479.544427222
[-0.16122103 -0.00694709 -0.08920744 ...,  0.06225278  0.03092638
  0.02279763]
CROSS VALIDATION 8
Iteration :  0   Loss :  3830.57163502
Iteration :  1   Loss :  1399.34177453
Iteration :  2   Loss :  450.229581181
Iteration :  3   Loss :  142.200404311
Iteration :  4   Loss :  49.0039918026
Iteration :  5   Loss :  16.8873726082
Iteration :  6   Loss :  5.81959700237
Iteration :  7   Loss :  2.00553487465
Iteration :  8   Loss :  0.691237476287
Iteration :  9   Loss :  0.239208170398
Iteration :  10   Loss :  0.0823849869976
Iteration :  11   Loss :  0.0291902542312
Iteration :  12   Loss :  0.0229267968586
Iteration :  13   Loss :  9263.46526167
Iteration :  14   Loss :  7729.71319185
Iteration :  15   Loss :  786.255142895
Iteration :  16   Loss :  197.559028482
Iteration :  17   Loss :  68.0812491429
Iteration :  18   Loss :  23.4616814149
Iteration :  19   Loss :  8.08538706301
Iteration :  20   Loss :  2.78632313814
Iteration :  21   Loss :  0.960345914201
Iteration :  22   Loss :  0.331431330511
Iteration :  23   Loss :  0.116955420311
Iteration :  24   Loss :  0.0411401899355
Iteration :  25   Loss :  0.0158133212942
Iteration :  26   Loss :  0.0127869165451
Iteration :  27   Loss :  9362.56896855
Iteration :  28   Loss :  4512.54995622
Iteration :  29   Loss :  461.099217572
Iteration :  30   Loss :  158.900408107
Iteration :  31   Loss :  54.7590165727
Iteration :  32   Loss :  18.8706242591
Iteration :  33   Loss :  6.50304702706
Iteration :  34   Loss :  2.24102923441
Iteration :  35   Loss :  0.772286066573
Iteration :  36   Loss :  0.266242972429
Iteration :  37   Loss :  0.0935525772848
Iteration :  38   Loss :  0.0359149980137
Iteration :  39   Loss :  0.0255792429114
Iteration :  40   Loss :  0.0183973200564
Iteration :  41   Loss :  9513.27458862
Iteration :  42   Loss :  4562.22687533
Iteration :  43   Loss :  460.311086276
Iteration :  44   Loss :  158.62880846
Iteration :  45   Loss :  54.6654200254
Iteration :  46   Loss :  18.8383697486
Iteration :  47   Loss :  6.49193172976
Iteration :  48   Loss :  2.23719876754
Iteration :  49   Loss :  0.770966041674
Iteration :  50   Loss :  0.265790153647
Iteration :  51   Loss :  0.093481229577
Iteration :  52   Loss :  0.0359156997422
Iteration :  53   Loss :  0.0194802958336
Iteration :  54   Loss :  0.0164088681261
Iteration :  55   Loss :  10900.0191196
Iteration :  56   Loss :  2736.98670189
Iteration :  57   Loss :  329.022717457
Iteration :  58   Loss :  360.111866071
Iteration :  59   Loss :  1586.80579652
Iteration :  60   Loss :  694.45561463
Iteration :  61   Loss :  137.495985997
Iteration :  62   Loss :  2396.8106462
Iteration :  63   Loss :  922.016166079
Iteration :  64   Loss :  803.774986406
Iteration :  65   Loss :  797.459367099
Iteration :  66   Loss :  244.043753636
Iteration :  67   Loss :  84.1004507727
Iteration :  68   Loss :  28.9820399613
Iteration :  69   Loss :  9.98756407549
Iteration :  70   Loss :  3.44185220952
Iteration :  71   Loss :  1.18755916315
Iteration :  72   Loss :  0.409267861612
Iteration :  73   Loss :  0.143912553049
Iteration :  74   Loss :  0.0538124222322
Iteration :  75   Loss :  0.0220305963449
Iteration :  76   Loss :  0.904980135884
Iteration :  77   Loss :  3588.5137963
Iteration :  78   Loss :  2007.51854592
Iteration :  79   Loss :  1836.56303393
Iteration :  80   Loss :  945.195285094
Iteration :  81   Loss :  625.747461037
Iteration :  82   Loss :  852.22578744
Iteration :  83   Loss :  1184.00650429
Iteration :  84   Loss :  244.498162285
Iteration :  85   Loss :  84.2570455308
Iteration :  86   Loss :  29.0360107366
Iteration :  87   Loss :  10.0062713531
Iteration :  88   Loss :  3.44854235082
Iteration :  89   Loss :  1.18851330891
Iteration :  90   Loss :  0.409958734677
Iteration :  91   Loss :  0.141725713006
Iteration :  92   Loss :  0.0556534157653
Iteration :  93   Loss :  0.0216738413324
Iteration :  94   Loss :  0.0126372033025
Iteration :  95   Loss :  1.5989865851
Iteration :  96   Loss :  5238.14522519
Iteration :  97   Loss :  1419.39100935
Iteration :  98   Loss :  407.133314989
Iteration :  99   Loss :  343.065891242
[-0.16816179 -0.13221725 -0.06510003 ...,  0.04580701  0.06471901
 -0.00105974]
CROSS VALIDATION 9
Iteration :  0   Loss :  2906.60871134
Iteration :  1   Loss :  1417.5187014
Iteration :  2   Loss :  320.529964172
Iteration :  3   Loss :  138.896844921
Iteration :  4   Loss :  47.8664800071
Iteration :  5   Loss :  16.4952170043
Iteration :  6   Loss :  5.68445347236
Iteration :  7   Loss :  1.95917318987
Iteration :  8   Loss :  0.675121151551
Iteration :  9   Loss :  0.233697241274
Iteration :  10   Loss :  0.0808246206395
Iteration :  11   Loss :  0.0294881000309
Iteration :  12   Loss :  0.0169473925475
Iteration :  13   Loss :  10176.8224527
Iteration :  14   Loss :  7124.16137515
Iteration :  15   Loss :  994.18474471
Iteration :  16   Loss :  186.817618719
Iteration :  17   Loss :  64.3796274745
Iteration :  18   Loss :  22.1860039899
Iteration :  19   Loss :  7.64557690339
Iteration :  20   Loss :  2.63549352861
Iteration :  21   Loss :  0.908143974131
Iteration :  22   Loss :  0.313412284855
Iteration :  23   Loss :  0.108447211834
Iteration :  24   Loss :  0.0383006260397
Iteration :  25   Loss :  0.0200511014568
Iteration :  26   Loss :  10455.0326467
Iteration :  27   Loss :  4278.83080687
Iteration :  28   Loss :  489.338556144
Iteration :  29   Loss :  168.632080762
Iteration :  30   Loss :  58.1145350329
Iteration :  31   Loss :  20.0264671301
Iteration :  32   Loss :  6.90136846495
Iteration :  33   Loss :  2.37895381946
Iteration :  34   Loss :  0.819840750481
Iteration :  35   Loss :  0.283523482021
Iteration :  36   Loss :  0.0986477561104
Iteration :  37   Loss :  0.0356892861753
Iteration :  38   Loss :  0.0164882670982
Iteration :  39   Loss :  0.0170356024951
Iteration :  40   Loss :  7982.5309708
Iteration :  41   Loss :  4521.10933974
Iteration :  42   Loss :  349.221572283
Iteration :  43   Loss :  120.346008497
Iteration :  44   Loss :  41.4727007452
Iteration :  45   Loss :  14.2919979531
Iteration :  46   Loss :  4.92519707951
Iteration :  47   Loss :  1.69743515032
Iteration :  48   Loss :  0.585076959215
Iteration :  49   Loss :  0.202555315367
Iteration :  50   Loss :  0.0709112291856
Iteration :  51   Loss :  0.0298987464606
Iteration :  52   Loss :  0.0351100664143
Iteration :  53   Loss :  2256.34856455
Iteration :  54   Loss :  2018.90655884
Iteration :  55   Loss :  1831.24513431
Iteration :  56   Loss :  977.76715587
Iteration :  57   Loss :  642.173794271
Iteration :  58   Loss :  935.497407613
Iteration :  59   Loss :  1175.14236166
Iteration :  60   Loss :  244.802503862
Iteration :  61   Loss :  84.3619253455
Iteration :  62   Loss :  29.0721521366
Iteration :  63   Loss :  10.0187347579
Iteration :  64   Loss :  3.45289781188
Iteration :  65   Loss :  1.19003817076
Iteration :  66   Loss :  0.410427038677
Iteration :  67   Loss :  0.141824838528
Iteration :  68   Loss :  0.0503736498688
Iteration :  69   Loss :  0.0244675534396
Iteration :  70   Loss :  4344.86289404
Iteration :  71   Loss :  1495.37229797
Iteration :  72   Loss :  1134.17363715
Iteration :  73   Loss :  1099.99454144
Iteration :  74   Loss :  132.938286813
Iteration :  75   Loss :  45.812153269
Iteration :  76   Loss :  15.7874261619
Iteration :  77   Loss :  5.44053950545
Iteration :  78   Loss :  1.87496255756
Iteration :  79   Loss :  0.646375272121
Iteration :  80   Loss :  0.222896167901
Iteration :  81   Loss :  0.0781879897622
Iteration :  82   Loss :  22.7095923158
Iteration :  83   Loss :  3062.87518992
Iteration :  84   Loss :  2144.68471556
Iteration :  85   Loss :  885.366926573
Iteration :  86   Loss :  205.887932385
Iteration :  87   Loss :  70.951581429
Iteration :  88   Loss :  24.4508631403
Iteration :  89   Loss :  8.42635051032
Iteration :  90   Loss :  2.90392692984
Iteration :  91   Loss :  1.00105636633
Iteration :  92   Loss :  0.34581652905
Iteration :  93   Loss :  0.119495835382
Iteration :  94   Loss :  0.0424150423217
Iteration :  95   Loss :  0.0210622689026
Iteration :  96   Loss :  5678.09248469
Iteration :  97   Loss :  3320.39652258
Iteration :  98   Loss :  358.540373456
Iteration :  99   Loss :  123.557380916
[-0.07750111  0.00311742 -0.0095517  ...,  0.02136992  0.01590781
  0.00698127]
CROSS VALIDATION 10
Iteration :  0   Loss :  3909.10791081
Iteration :  1   Loss :  707.604858442
Iteration :  2   Loss :  349.97787793
Iteration :  3   Loss :  399.157635601
Iteration :  4   Loss :  1811.85586176
Iteration :  5   Loss :  86.1412072281
Iteration :  6   Loss :  29.6863184252
Iteration :  7   Loss :  10.230189055
Iteration :  8   Loss :  3.52557555306
Iteration :  9   Loss :  1.21623893425
Iteration :  10   Loss :  0.420379416535
Iteration :  11   Loss :  0.14782123044
Iteration :  12   Loss :  0.0529431557737
Iteration :  13   Loss :  0.0707229404941
Iteration :  14   Loss :  7943.26245915
Iteration :  15   Loss :  5100.35013792
Iteration :  16   Loss :  1629.97554656
Iteration :  17   Loss :  1040.92098217
Iteration :  18   Loss :  100.435937383
Iteration :  19   Loss :  34.6114476684
Iteration :  20   Loss :  11.9275290133
Iteration :  21   Loss :  4.11040510701
Iteration :  22   Loss :  1.41675040162
Iteration :  23   Loss :  0.489248757883
Iteration :  24   Loss :  0.168778715136
Iteration :  25   Loss :  0.0649267493183
Iteration :  26   Loss :  0.031924704885
Iteration :  27   Loss :  5.02905122962
Iteration :  28   Loss :  3852.93939437
Iteration :  29   Loss :  1430.80395592
Iteration :  30   Loss :  292.190221261
Iteration :  31   Loss :  100.692310121
Iteration :  32   Loss :  34.6998256611
Iteration :  33   Loss :  11.9580251451
Iteration :  34   Loss :  4.12102619595
Iteration :  35   Loss :  1.42191753832
Iteration :  36   Loss :  0.491647608114
Iteration :  37   Loss :  0.175140410314
Iteration :  38   Loss :  0.0611319675394
Iteration :  39   Loss :  0.0247698273354
Iteration :  40   Loss :  0.439463819946
Iteration :  41   Loss :  3586.79827016
Iteration :  42   Loss :  2408.74260542
Iteration :  43   Loss :  260.379831211
Iteration :  44   Loss :  89.7300621332
Iteration :  45   Loss :  30.9220726237
Iteration :  46   Loss :  10.6561242436
Iteration :  47   Loss :  3.67256922157
Iteration :  48   Loss :  1.26558318021
Iteration :  49   Loss :  0.437584170183
Iteration :  50   Loss :  0.15261390885
Iteration :  51   Loss :  0.0536674990492
Iteration :  52   Loss :  0.0244695024505
Iteration :  53   Loss :  0.0140821512815
Iteration :  54   Loss :  7578.91148141
Iteration :  55   Loss :  1879.48048859
Iteration :  56   Loss :  2342.37800779
Iteration :  57   Loss :  240.276288745
Iteration :  58   Loss :  82.8021364712
Iteration :  59   Loss :  28.5346270549
Iteration :  60   Loss :  9.83341150244
Iteration :  61   Loss :  3.38878679943
Iteration :  62   Loss :  1.16892223714
Iteration :  63   Loss :  0.402927739796
Iteration :  64   Loss :  0.141049755328
Iteration :  65   Loss :  0.0492864880762
Iteration :  66   Loss :  0.0200390395353
Iteration :  67   Loss :  0.0260337316632
Iteration :  68   Loss :  8.28914757584
Iteration :  69   Loss :  2045.76549664
Iteration :  70   Loss :  474.81085952
Iteration :  71   Loss :  4379.52352676
Iteration :  72   Loss :  368.244047182
Iteration :  73   Loss :  126.901385104
Iteration :  74   Loss :  43.7317633906
Iteration :  75   Loss :  15.0704984637
Iteration :  76   Loss :  5.19347737971
Iteration :  77   Loss :  1.78973604948
Iteration :  78   Loss :  0.616883064559
Iteration :  79   Loss :  0.213195489546
Iteration :  80   Loss :  0.0746904067309
Iteration :  81   Loss :  0.0363318841139
Iteration :  82   Loss :  1.44403841351
Iteration :  83   Loss :  3751.02676759
Iteration :  84   Loss :  2767.77533238
Iteration :  85   Loss :  295.330021299
Iteration :  86   Loss :  101.774323449
Iteration :  87   Loss :  35.0726767708
Iteration :  88   Loss :  12.0871860966
Iteration :  89   Loss :  4.16519076507
Iteration :  90   Loss :  1.43571249411
Iteration :  91   Loss :  0.494927460665
Iteration :  92   Loss :  0.172007389288
Iteration :  93   Loss :  0.0616682363757
Iteration :  94   Loss :  0.0287259111463
Iteration :  95   Loss :  0.0133131119085
Iteration :  96   Loss :  30.8240366764
Iteration :  97   Loss :  2013.26734635
Iteration :  98   Loss :  1730.52822994
Iteration :  99   Loss :  158.195148559
[-0.0783803  -0.03920678 -0.05614612 ...,  0.05409784  0.00901298
 -0.00186825]
CROSS VALIDATION 11
Iteration :  0   Loss :  3909.10791081
Iteration :  1   Loss :  409.691051702
Iteration :  2   Loss :  1041.42669026
Iteration :  3   Loss :  96.9412495616
Iteration :  4   Loss :  33.4071356679
Iteration :  5   Loss :  11.5125059619
Iteration :  6   Loss :  3.96734981533
Iteration :  7   Loss :  1.3671970902
Iteration :  8   Loss :  0.471176504558
Iteration :  9   Loss :  0.163622199241
Iteration :  10   Loss :  0.0571838243005
Iteration :  11   Loss :  0.0219447548411
Iteration :  12   Loss :  0.174937596376
Iteration :  13   Loss :  1667.34573683
Iteration :  14   Loss :  578.356384371
Iteration :  15   Loss :  1271.98734372
Iteration :  16   Loss :  339.431367206
Iteration :  17   Loss :  118.243400449
Iteration :  18   Loss :  40.748116395
Iteration :  19   Loss :  14.0422973582
Iteration :  20   Loss :  4.83914675441
Iteration :  21   Loss :  1.66763785889
Iteration :  22   Loss :  0.574737596599
Iteration :  23   Loss :  0.199213899927
Iteration :  24   Loss :  0.0972118663365
Iteration :  25   Loss :  3713.56505385
Iteration :  26   Loss :  1649.62387819
Iteration :  27   Loss :  259.124279077
Iteration :  28   Loss :  89.2973913432
Iteration :  29   Loss :  30.7738845519
Iteration :  30   Loss :  10.6050334476
Iteration :  31   Loss :  3.65614599135
Iteration :  32   Loss :  1.2600347175
Iteration :  33   Loss :  0.434456149024
Iteration :  34   Loss :  0.151230023357
Iteration :  35   Loss :  0.053185372935
Iteration :  36   Loss :  0.0239806577607
Iteration :  37   Loss :  0.0109065851484
Iteration :  38   Loss :  7562.44602688
Iteration :  39   Loss :  3173.85773654
Iteration :  40   Loss :  353.338696379
Iteration :  41   Loss :  121.764820693
Iteration :  42   Loss :  41.9619502581
Iteration :  43   Loss :  14.4609882463
Iteration :  44   Loss :  4.98343298945
Iteration :  45   Loss :  1.71981790475
Iteration :  46   Loss :  0.592143220739
Iteration :  47   Loss :  0.204410286488
Iteration :  48   Loss :  0.0753774166665
Iteration :  49   Loss :  0.0317561300439
Iteration :  50   Loss :  7251.22944996
Iteration :  51   Loss :  2283.77172166
Iteration :  52   Loss :  367.597837209
Iteration :  53   Loss :  126.678693274
Iteration :  54   Loss :  43.6550210729
Iteration :  55   Loss :  15.0440521261
Iteration :  56   Loss :  5.18436502416
Iteration :  57   Loss :  1.786949029
Iteration :  58   Loss :  0.616062063125
Iteration :  59   Loss :  0.214136064112
Iteration :  60   Loss :  0.0776638562328
Iteration :  61   Loss :  0.0283528369167
Iteration :  62   Loss :  6074.74415936
Iteration :  63   Loss :  1510.72815435
Iteration :  64   Loss :  1899.65335034
Iteration :  65   Loss :  167.973578982
Iteration :  66   Loss :  57.8857418621
Iteration :  67   Loss :  19.9481903939
Iteration :  68   Loss :  6.87440724542
Iteration :  69   Loss :  2.36930438251
Iteration :  70   Loss :  0.816597475069
Iteration :  71   Loss :  0.282028783681
Iteration :  72   Loss :  0.100026033574
Iteration :  73   Loss :  8856.69327165
Iteration :  74   Loss :  2264.32965043
Iteration :  75   Loss :  319.197141381
Iteration :  76   Loss :  382.212718162
Iteration :  77   Loss :  2051.44202952
Iteration :  78   Loss :  1456.3897831
Iteration :  79   Loss :  427.872266164
Iteration :  80   Loss :  496.937996126
Iteration :  81   Loss :  1723.20756964
Iteration :  82   Loss :  1017.99810924
Iteration :  83   Loss :  426.880190593
Iteration :  84   Loss :  204.316094323
Iteration :  85   Loss :  70.4098153578
Iteration :  86   Loss :  24.2640801995
Iteration :  87   Loss :  8.36169765453
Iteration :  88   Loss :  2.8815437166
Iteration :  89   Loss :  0.993306993684
Iteration :  90   Loss :  0.344738809782
Iteration :  91   Loss :  0.120105496086
Iteration :  92   Loss :  0.0468703367564
Iteration :  93   Loss :  0.0269323313424
Iteration :  94   Loss :  9524.67088795
Iteration :  95   Loss :  846.22409418
Iteration :  96   Loss :  1662.09798751
Iteration :  97   Loss :  272.557772875
Iteration :  98   Loss :  93.9267291987
Iteration :  99   Loss :  32.3682952239
[-0.05379947 -0.02259632 -0.01590823 ...,  0.0144629   0.01861519
  0.00167302]
CROSS VALIDATION 12
Iteration :  0   Loss :  2287.61903653
Iteration :  1   Loss :  860.593683022
Iteration :  2   Loss :  136.021866011
Iteration :  3   Loss :  46.8749224566
Iteration :  4   Loss :  16.1538230494
Iteration :  5   Loss :  5.5668044044
Iteration :  6   Loss :  1.91849928841
Iteration :  7   Loss :  0.661394230382
Iteration :  8   Loss :  0.22874200361
Iteration :  9   Loss :  0.08143191726
Iteration :  10   Loss :  0.0392549915873
Iteration :  11   Loss :  6190.77340828
Iteration :  12   Loss :  4432.45120867
Iteration :  13   Loss :  1497.41109766
Iteration :  14   Loss :  173.205025356
Iteration :  15   Loss :  59.6885619547
Iteration :  16   Loss :  20.5694056561
Iteration :  17   Loss :  7.0884677933
Iteration :  18   Loss :  2.44277237198
Iteration :  19   Loss :  0.841828574759
Iteration :  20   Loss :  0.291259616476
Iteration :  21   Loss :  0.10140540427
Iteration :  22   Loss :  0.0366019670807
Iteration :  23   Loss :  0.0221698530712
Iteration :  24   Loss :  0.00964046846934
Iteration :  25   Loss :  8752.25334109
Iteration :  26   Loss :  7351.37903731
Iteration :  27   Loss :  1580.91850853
Iteration :  28   Loss :  271.352355109
Iteration :  29   Loss :  282.845972888
Iteration :  30   Loss :  3346.81018733
Iteration :  31   Loss :  990.939157519
Iteration :  32   Loss :  197.16841693
Iteration :  33   Loss :  67.9466386454
Iteration :  34   Loss :  23.4152394947
Iteration :  35   Loss :  8.06917680644
Iteration :  36   Loss :  2.78074037441
Iteration :  37   Loss :  0.958670604613
Iteration :  38   Loss :  0.330561255828
Iteration :  39   Loss :  0.11545627777
Iteration :  40   Loss :  0.0419257839268
Iteration :  41   Loss :  0.0229590100653
Iteration :  42   Loss :  7116.05131025
Iteration :  43   Loss :  687.84577825
Iteration :  44   Loss :  188.17331159
Iteration :  45   Loss :  64.8468157549
Iteration :  46   Loss :  22.3470027604
Iteration :  47   Loss :  7.70104941343
Iteration :  48   Loss :  2.65388195814
Iteration :  49   Loss :  0.915556326237
Iteration :  50   Loss :  0.316056721852
Iteration :  51   Loss :  0.109485195352
Iteration :  52   Loss :  0.0407994758233
Iteration :  53   Loss :  0.0158882187299
Iteration :  54   Loss :  0.00982669960733
Iteration :  55   Loss :  1.01468516592
Iteration :  56   Loss :  2251.10467818
Iteration :  57   Loss :  2839.99836625
Iteration :  58   Loss :  436.606098002
Iteration :  59   Loss :  337.054741621
Iteration :  60   Loss :  1442.46826308
Iteration :  61   Loss :  963.307135053
Iteration :  62   Loss :  4701.31375261
Iteration :  63   Loss :  760.87913855
Iteration :  64   Loss :  148.454961584
Iteration :  65   Loss :  51.1593885059
Iteration :  66   Loss :  17.6302120512
Iteration :  67   Loss :  6.07689914205
Iteration :  68   Loss :  2.09425970783
Iteration :  69   Loss :  0.722407668367
Iteration :  70   Loss :  0.250339916813
Iteration :  71   Loss :  0.0873806155288
Iteration :  72   Loss :  0.032115603205
Iteration :  73   Loss :  0.0215084400115
Iteration :  74   Loss :  9381.88105676
Iteration :  75   Loss :  2654.98615336
Iteration :  76   Loss :  427.116222008
Iteration :  77   Loss :  147.189510723
Iteration :  78   Loss :  50.7251743942
Iteration :  79   Loss :  17.4800008495
Iteration :  80   Loss :  6.02382896157
Iteration :  81   Loss :  2.07634942985
Iteration :  82   Loss :  0.715645313225
Iteration :  83   Loss :  0.247276473002
Iteration :  84   Loss :  0.0891919569539
Iteration :  85   Loss :  0.0408531608159
Iteration :  86   Loss :  4.54055303005
Iteration :  87   Loss :  4306.7769715
Iteration :  88   Loss :  2016.15347639
Iteration :  89   Loss :  197.878837199
Iteration :  90   Loss :  68.1914581253
Iteration :  91   Loss :  23.4996072702
Iteration :  92   Loss :  8.0982509692
Iteration :  93   Loss :  2.790755991
Iteration :  94   Loss :  0.961775998104
Iteration :  95   Loss :  0.333300729178
Iteration :  96   Loss :  0.115737949366
Iteration :  97   Loss :  0.0408081014316
Iteration :  98   Loss :  0.0171538559947
Iteration :  99   Loss :  0.0144283409457
[ -8.83712631e-04  -3.43224136e-04  -7.56844157e-05 ...,   3.79323733e-04
   1.16966892e-04   3.94768487e-05]
CROSS VALIDATION 13
Iteration :  0   Loss :  3386.1466967
Iteration :  1   Loss :  600.952573539
Iteration :  2   Loss :  199.151697094
Iteration :  3   Loss :  68.630101153
Iteration :  4   Loss :  23.6507690026
Iteration :  5   Loss :  8.15034800786
Iteration :  6   Loss :  2.80935278777
Iteration :  7   Loss :  0.968011141232
Iteration :  8   Loss :  0.333887055591
Iteration :  9   Loss :  0.116020592031
Iteration :  10   Loss :  0.041730408074
Iteration :  11   Loss :  0.0166607224767
Iteration :  12   Loss :  0.0126587688752
Iteration :  13   Loss :  1509.2744763
Iteration :  14   Loss :  4886.67427019
Iteration :  15   Loss :  569.093390035
Iteration :  16   Loss :  314.345683339
Iteration :  17   Loss :  108.327352261
Iteration :  18   Loss :  37.3309253791
Iteration :  19   Loss :  12.8646917016
Iteration :  20   Loss :  4.43333225599
Iteration :  21   Loss :  1.5282517796
Iteration :  22   Loss :  0.526599159729
Iteration :  23   Loss :  0.181625949592
Iteration :  24   Loss :  0.0645661772946
Iteration :  25   Loss :  0.0236018050127
Iteration :  26   Loss :  260.176821755
Iteration :  27   Loss :  3444.80168174
Iteration :  28   Loss :  415.482527677
Iteration :  29   Loss :  143.180341153
Iteration :  30   Loss :  49.3417873945
Iteration :  31   Loss :  17.0049276447
Iteration :  32   Loss :  5.85979408182
Iteration :  33   Loss :  2.01950632794
Iteration :  34   Loss :  0.698909842488
Iteration :  35   Loss :  0.240351275789
Iteration :  36   Loss :  0.085688940281
Iteration :  37   Loss :  0.0344041957029
Iteration :  38   Loss :  3961.51017255
Iteration :  39   Loss :  1017.7473199
Iteration :  40   Loss :  773.430579662
Iteration :  41   Loss :  1252.81985155
Iteration :  42   Loss :  1506.46308663
Iteration :  43   Loss :  3949.40515763
Iteration :  44   Loss :  203.790916947
Iteration :  45   Loss :  70.2288328356
Iteration :  46   Loss :  24.201712944
Iteration :  47   Loss :  8.34044284771
Iteration :  48   Loss :  2.87471272011
Iteration :  49   Loss :  0.991204621169
Iteration :  50   Loss :  0.342978975467
Iteration :  51   Loss :  0.120244264036
Iteration :  52   Loss :  10481.8769523
Iteration :  53   Loss :  1181.34274687
Iteration :  54   Loss :  895.649940243
Iteration :  55   Loss :  343.462732575
Iteration :  56   Loss :  188.660210844
Iteration :  57   Loss :  65.0146071697
Iteration :  58   Loss :  22.4048257262
Iteration :  59   Loss :  7.72097591137
Iteration :  60   Loss :  2.66074532584
Iteration :  61   Loss :  0.91716773493
Iteration :  62   Loss :  0.316673616549
Iteration :  63   Loss :  0.113067746184
Iteration :  64   Loss :  0.0395477653266
Iteration :  65   Loss :  0.173835585851
Iteration :  66   Loss :  7067.83169983
Iteration :  67   Loss :  1316.97414266
Iteration :  68   Loss :  1218.4350689
Iteration :  69   Loss :  178.192320887
Iteration :  70   Loss :  61.4072447568
Iteration :  71   Loss :  21.161684689
Iteration :  72   Loss :  7.29257436398
Iteration :  73   Loss :  2.51310997435
Iteration :  74   Loss :  0.866052179052
Iteration :  75   Loss :  0.299187878563
Iteration :  76   Loss :  0.10429245867
Iteration :  77   Loss :  0.0510301490251
Iteration :  78   Loss :  3631.33749901
Iteration :  79   Loss :  3294.94195881
Iteration :  80   Loss :  1153.91909042
Iteration :  81   Loss :  183.497511153
Iteration :  82   Loss :  63.235478438
Iteration :  83   Loss :  21.79185212
Iteration :  84   Loss :  7.50989947564
Iteration :  85   Loss :  2.5880033844
Iteration :  86   Loss :  0.892165150327
Iteration :  87   Loss :  0.309873644191
Iteration :  88   Loss :  0.107121811661
Iteration :  89   Loss :  0.0391968703907
Iteration :  90   Loss :  0.174356096899
Iteration :  91   Loss :  7353.10854691
Iteration :  92   Loss :  2539.11657165
Iteration :  93   Loss :  347.429319977
Iteration :  94   Loss :  119.728376516
Iteration :  95   Loss :  41.25985724
Iteration :  96   Loss :  14.2186494881
Iteration :  97   Loss :  4.89991984434
Iteration :  98   Loss :  1.68857207715
Iteration :  99   Loss :  0.58191015435
[-0.00368261 -0.00037264 -0.00133839 ...,  0.00063841  0.00091722
  0.00058593]
CROSS VALIDATION 14
Iteration :  0   Loss :  3811.69832407
Iteration :  1   Loss :  408.043256304
Iteration :  2   Loss :  1055.93646107
Iteration :  3   Loss :  107.778111595
Iteration :  4   Loss :  37.1416503538
Iteration :  5   Loss :  12.7994652216
Iteration :  6   Loss :  4.41085166119
Iteration :  7   Loss :  1.52004741064
Iteration :  8   Loss :  0.524063072759
Iteration :  9   Loss :  0.182060271898
Iteration :  10   Loss :  0.0631923766846
Iteration :  11   Loss :  0.0233952602524
Iteration :  12   Loss :  0.0169106859666
Iteration :  13   Loss :  4.24904743181
Iteration :  14   Loss :  3856.34607138
Iteration :  15   Loss :  2020.16687622
Iteration :  16   Loss :  1701.63474124
Iteration :  17   Loss :  924.450831427
Iteration :  18   Loss :  344.861718746
Iteration :  19   Loss :  126.540313646
Iteration :  20   Loss :  43.607333767
Iteration :  21   Loss :  15.0276184994
Iteration :  22   Loss :  5.17870059558
Iteration :  23   Loss :  1.78476023255
Iteration :  24   Loss :  0.615382393206
Iteration :  25   Loss :  0.212780731646
Iteration :  26   Loss :  0.0745622979621
Iteration :  27   Loss :  0.0288295023597
Iteration :  28   Loss :  0.0292429606554
Iteration :  29   Loss :  3581.6736548
Iteration :  30   Loss :  2549.27720994
Iteration :  31   Loss :  273.713591536
Iteration :  32   Loss :  94.3250383911
Iteration :  33   Loss :  32.5055574243
Iteration :  34   Loss :  11.2018111149
Iteration :  35   Loss :  3.86028058593
Iteration :  36   Loss :  1.33030167352
Iteration :  37   Loss :  0.458906773739
Iteration :  38   Loss :  0.159676149494
Iteration :  39   Loss :  0.0569986575942
Iteration :  40   Loss :  0.0218848221217
Iteration :  41   Loss :  0.0102564914449
Iteration :  42   Loss :  9221.25037483
Iteration :  43   Loss :  3312.35117348
Iteration :  44   Loss :  735.572252029
Iteration :  45   Loss :  167.305000216
Iteration :  46   Loss :  57.6553414095
Iteration :  47   Loss :  19.8687330848
Iteration :  48   Loss :  6.84700777034
Iteration :  49   Loss :  2.3596646898
Iteration :  50   Loss :  0.813345556857
Iteration :  51   Loss :  0.280316863463
Iteration :  52   Loss :  0.096931150919
Iteration :  53   Loss :  0.0390939251978
Iteration :  54   Loss :  0.0156285618141
Iteration :  55   Loss :  0.0271751504792
Iteration :  56   Loss :  10470.2688244
Iteration :  57   Loss :  2327.4521785
Iteration :  58   Loss :  2129.52755238
Iteration :  59   Loss :  242.533687302
Iteration :  60   Loss :  83.5800635166
Iteration :  61   Loss :  28.8027081728
Iteration :  62   Loss :  9.92576415
Iteration :  63   Loss :  3.42054165971
Iteration :  64   Loss :  1.17889800548
Iteration :  65   Loss :  0.406758628053
Iteration :  66   Loss :  0.140450912807
Iteration :  67   Loss :  0.0516492681067
Iteration :  68   Loss :  0.0225396337231
Iteration :  69   Loss :  0.0117956973942
Iteration :  70   Loss :  7477.53291309
Iteration :  71   Loss :  5223.28868183
Iteration :  72   Loss :  1886.63521703
Iteration :  73   Loss :  215.006962046
Iteration :  74   Loss :  74.0940186255
Iteration :  75   Loss :  25.5337015315
Iteration :  76   Loss :  8.79922463372
Iteration :  77   Loss :  3.03232010473
Iteration :  78   Loss :  1.04504915256
Iteration :  79   Loss :  0.362179903254
Iteration :  80   Loss :  0.124634999004
Iteration :  81   Loss :  0.0449701607794
Iteration :  82   Loss :  0.0191853809377
Iteration :  83   Loss :  0.00896323955142
Iteration :  84   Loss :  0.0469076403902
Iteration :  85   Loss :  3022.88892562
Iteration :  86   Loss :  2022.40671824
Iteration :  87   Loss :  1761.25734799
Iteration :  88   Loss :  972.389321833
Iteration :  89   Loss :  204.715731237
Iteration :  90   Loss :  70.547535107
Iteration :  91   Loss :  24.3115401045
Iteration :  92   Loss :  8.37805934026
Iteration :  93   Loss :  2.88721357319
Iteration :  94   Loss :  0.995006358476
Iteration :  95   Loss :  0.343024222177
Iteration :  96   Loss :  0.119007251707
Iteration :  97   Loss :  0.0434237206613
Iteration :  98   Loss :  0.0217904020265
Iteration :  99   Loss :  18.6103870129
[ -6.09690460e-04  -1.14633617e-03   1.05030459e-04 ...,   4.91664628e-04
   1.37716957e-04   3.55587130e-06]
CROSS VALIDATION 15
Iteration :  0   Loss :  3811.69832407
Iteration :  1   Loss :  408.043256304
Iteration :  2   Loss :  1055.93646107
Iteration :  3   Loss :  107.778111595
Iteration :  4   Loss :  37.1416503538
Iteration :  5   Loss :  12.7994652216
Iteration :  6   Loss :  4.4108516612
Iteration :  7   Loss :  1.52004759234
Iteration :  8   Loss :  0.52416837466
Iteration :  9   Loss :  0.182160817917
Iteration :  10   Loss :  0.0633327418043
Iteration :  11   Loss :  0.0234734410892
Iteration :  12   Loss :  0.0153269997751
Iteration :  13   Loss :  9528.06516612
Iteration :  14   Loss :  5659.245578
Iteration :  15   Loss :  728.74080987
Iteration :  16   Loss :  410.179255636
Iteration :  17   Loss :  141.352768849
Iteration :  18   Loss :  48.7118862951
Iteration :  19   Loss :  16.7867094911
Iteration :  20   Loss :  5.78490460896
Iteration :  21   Loss :  1.99354860823
Iteration :  22   Loss :  0.687006093789
Iteration :  23   Loss :  0.23751456217
Iteration :  24   Loss :  0.0826028705538
Iteration :  25   Loss :  0.029744345055
Iteration :  26   Loss :  0.0141521623295
Iteration :  27   Loss :  10649.6419089
Iteration :  28   Loss :  2279.20818754
Iteration :  29   Loss :  2131.35711509
Iteration :  30   Loss :  242.598087106
Iteration :  31   Loss :  83.6022564734
Iteration :  32   Loss :  28.8103561361
Iteration :  33   Loss :  9.92839973183
Iteration :  34   Loss :  3.42145240563
Iteration :  35   Loss :  1.17941763535
Iteration :  36   Loss :  0.406873103631
Iteration :  37   Loss :  0.140891218422
Iteration :  38   Loss :  0.0510958076185
Iteration :  39   Loss :  0.0201870369429
Iteration :  40   Loss :  0.0125598753623
Iteration :  41   Loss :  173.479942727
Iteration :  42   Loss :  1777.03986699
Iteration :  43   Loss :  1394.97288543
Iteration :  44   Loss :  213.7624919
Iteration :  45   Loss :  73.6651590515
Iteration :  46   Loss :  25.3859112987
Iteration :  47   Loss :  8.74830097683
Iteration :  48   Loss :  3.01576704662
Iteration :  49   Loss :  1.03915007853
Iteration :  50   Loss :  0.363040856834
Iteration :  51   Loss :  0.124741333771
Iteration :  52   Loss :  0.0450689982799
Iteration :  53   Loss :  0.0221214051616
Iteration :  54   Loss :  0.0101011316947
Iteration :  55   Loss :  9163.64354291
Iteration :  56   Loss :  2530.4339145
Iteration :  57   Loss :  1943.48466942
Iteration :  58   Loss :  213.090690678
Iteration :  59   Loss :  73.4336481657
Iteration :  60   Loss :  25.3061298256
Iteration :  61   Loss :  8.72080071616
Iteration :  62   Loss :  3.00535128167
Iteration :  63   Loss :  1.03588120791
Iteration :  64   Loss :  0.357505657615
Iteration :  65   Loss :  0.123625177622
Iteration :  66   Loss :  0.0475394681433
Iteration :  67   Loss :  0.0209304171812
Iteration :  68   Loss :  8163.15871418
Iteration :  69   Loss :  770.167992749
Iteration :  70   Loss :  442.939804969
Iteration :  71   Loss :  152.642453282
Iteration :  72   Loss :  52.6024490969
Iteration :  73   Loss :  18.1274448327
Iteration :  74   Loss :  6.24693834232
Iteration :  75   Loss :  2.15277110516
Iteration :  76   Loss :  0.741918473867
Iteration :  77   Loss :  0.256318347594
Iteration :  78   Loss :  0.0908987136871
Iteration :  79   Loss :  0.03527632311
Iteration :  80   Loss :  0.0187726140053
Iteration :  81   Loss :  7445.93708824
Iteration :  82   Loss :  461.658323452
Iteration :  83   Loss :  285.059761133
Iteration :  84   Loss :  1006.73295505
Iteration :  85   Loss :  300.886769026
Iteration :  86   Loss :  103.689246415
Iteration :  87   Loss :  35.7325776002
Iteration :  88   Loss :  12.3138925531
Iteration :  89   Loss :  4.24456284965
Iteration :  90   Loss :  1.46247446657
Iteration :  91   Loss :  0.504430814466
Iteration :  92   Loss :  0.17908811571
Iteration :  93   Loss :  0.32857865905
Iteration :  94   Loss :  4337.61163472
Iteration :  95   Loss :  587.999747925
Iteration :  96   Loss :  1383.25600695
Iteration :  97   Loss :  1695.65836676
Iteration :  98   Loss :  131.785696048
Iteration :  99   Loss :  45.4149564511
[-0.04687386 -0.01039394 -0.03444001 ...,  0.01107667  0.01241053
  0.00445751]
CROSS VALIDATION 16
Iteration :  0   Loss :  3812.18642733
Iteration :  1   Loss :  408.042251048
Iteration :  2   Loss :  337.149323934
Iteration :  3   Loss :  1268.81466245
Iteration :  4   Loss :  1923.51567166
Iteration :  5   Loss :  188.593532011
Iteration :  6   Loss :  64.9916288314
Iteration :  7   Loss :  22.3969071111
Iteration :  8   Loss :  7.71824921359
Iteration :  9   Loss :  2.65998415318
Iteration :  10   Loss :  0.917620225888
Iteration :  11   Loss :  0.316402076125
Iteration :  12   Loss :  0.111362752911
Iteration :  13   Loss :  10534.178266
Iteration :  14   Loss :  2702.1583677
Iteration :  15   Loss :  532.583046765
Iteration :  16   Loss :  1204.3482048
Iteration :  17   Loss :  125.419856934
Iteration :  18   Loss :  43.2212107332
Iteration :  19   Loss :  14.8945557977
Iteration :  20   Loss :  5.13284610713
Iteration :  21   Loss :  1.76904491935
Iteration :  22   Loss :  0.610129315848
Iteration :  23   Loss :  0.211168239435
Iteration :  24   Loss :  0.0752899069422
Iteration :  25   Loss :  0.0280010133597
Iteration :  26   Loss :  3.36461133231
Iteration :  27   Loss :  7366.23045688
Iteration :  28   Loss :  1063.34895531
Iteration :  29   Loss :  718.492030821
Iteration :  30   Loss :  687.449107606
Iteration :  31   Loss :  187.404626401
Iteration :  32   Loss :  64.5819174733
Iteration :  33   Loss :  22.2557155853
Iteration :  34   Loss :  7.6695907398
Iteration :  35   Loss :  2.64303445157
Iteration :  36   Loss :  0.910865447021
Iteration :  37   Loss :  0.314291080425
Iteration :  38   Loss :  0.111658349032
Iteration :  39   Loss :  0.0411986598093
Iteration :  40   Loss :  10626.7449959
Iteration :  41   Loss :  6108.86862603
Iteration :  42   Loss :  1654.32882008
Iteration :  43   Loss :  147.271994435
Iteration :  44   Loss :  50.7517235482
Iteration :  45   Loss :  17.4896622606
Iteration :  46   Loss :  6.02715070063
Iteration :  47   Loss :  2.07703974667
Iteration :  48   Loss :  0.715872906932
Iteration :  49   Loss :  0.24730477537
Iteration :  50   Loss :  0.0880622764222
Iteration :  51   Loss :  0.0330814687049
Iteration :  52   Loss :  0.0139739995166
Iteration :  53   Loss :  0.0106947215445
Iteration :  54   Loss :  9405.8957438
Iteration :  55   Loss :  6431.30115731
Iteration :  56   Loss :  484.47256599
Iteration :  57   Loss :  166.955148738
Iteration :  58   Loss :  57.5347783276
Iteration :  59   Loss :  19.8271855779
Iteration :  60   Loss :  6.83268971163
Iteration :  61   Loss :  2.35462811967
Iteration :  62   Loss :  0.811433811418
Iteration :  63   Loss :  0.279743286251
Iteration :  64   Loss :  0.0972556312791
Iteration :  65   Loss :  0.0391263030875
Iteration :  66   Loss :  0.0152496587576
Iteration :  67   Loss :  0.0115179241662
Iteration :  68   Loss :  8.27974269917
Iteration :  69   Loss :  2404.91047207
Iteration :  70   Loss :  1819.94603215
Iteration :  71   Loss :  249.360004572
Iteration :  72   Loss :  85.9324956153
Iteration :  73   Loss :  29.6133849345
Iteration :  74   Loss :  10.2051332386
Iteration :  75   Loss :  3.51681325394
Iteration :  76   Loss :  1.2119570861
Iteration :  77   Loss :  0.418423340524
Iteration :  78   Loss :  0.144427795333
Iteration :  79   Loss :  0.051380507238
Iteration :  80   Loss :  0.0216342563927
Iteration :  81   Loss :  0.00993884724214
Iteration :  82   Loss :  0.454635007149
Iteration :  83   Loss :  1101.59204932
Iteration :  84   Loss :  327.99415152
Iteration :  85   Loss :  113.030780681
Iteration :  86   Loss :  38.9517841158
Iteration :  87   Loss :  13.4232593694
Iteration :  88   Loss :  4.62581874032
Iteration :  89   Loss :  1.59414411789
Iteration :  90   Loss :  0.549459654464
Iteration :  91   Loss :  0.193705302291
Iteration :  92   Loss :  0.0712084306658
Iteration :  93   Loss :  0.0258068643222
Iteration :  94   Loss :  0.0160640797479
Iteration :  95   Loss :  3875.68751196
Iteration :  96   Loss :  2017.20526386
Iteration :  97   Loss :  1651.56865243
Iteration :  98   Loss :  268.908438349
Iteration :  99   Loss :  196.814313811
[-0.1252287  -0.03319794 -0.03424534 ...,  0.04099501  0.04842197
  0.01515783]
CROSS VALIDATION 17
Iteration :  0   Loss :  3404.06593892
Iteration :  1   Loss :  408.043385578
Iteration :  2   Loss :  1054.36198833
Iteration :  3   Loss :  93.76096562
Iteration :  4   Loss :  32.3111710751
Iteration :  5   Loss :  11.1348232114
Iteration :  6   Loss :  3.8372000552
Iteration :  7   Loss :  1.32265220379
Iteration :  8   Loss :  0.456075693592
Iteration :  9   Loss :  0.158260475897
Iteration :  10   Loss :  0.0568520134488
Iteration :  11   Loss :  0.0214183802927
Iteration :  12   Loss :  0.105830916809
Iteration :  13   Loss :  3472.98434034
Iteration :  14   Loss :  1628.9210431
Iteration :  15   Loss :  1246.74847521
Iteration :  16   Loss :  513.501080315
Iteration :  17   Loss :  220.070704376
Iteration :  18   Loss :  75.8390459258
Iteration :  19   Loss :  26.1350592001
Iteration :  20   Loss :  9.00646364503
Iteration :  21   Loss :  3.10427401432
Iteration :  22   Loss :  1.06984574681
Iteration :  23   Loss :  0.369656728277
Iteration :  24   Loss :  0.128161772405
Iteration :  25   Loss :  0.0484627731188
Iteration :  26   Loss :  9803.89893952
Iteration :  27   Loss :  3146.79907066
Iteration :  28   Loss :  1279.6052574
Iteration :  29   Loss :  1731.19387498
Iteration :  30   Loss :  1697.308206
Iteration :  31   Loss :  230.276100923
Iteration :  32   Loss :  79.355949912
Iteration :  33   Loss :  27.3470271608
Iteration :  34   Loss :  9.42411877386
Iteration :  35   Loss :  3.24770949645
Iteration :  36   Loss :  1.11983194467
Iteration :  37   Loss :  0.386325428226
Iteration :  38   Loss :  0.133168810559
Iteration :  39   Loss :  0.050654131529
Iteration :  40   Loss :  9070.85386411
Iteration :  41   Loss :  6495.31839088
Iteration :  42   Loss :  386.87235653
Iteration :  43   Loss :  133.320927461
Iteration :  44   Loss :  45.9440158981
Iteration :  45   Loss :  15.8328676304
Iteration :  46   Loss :  5.45619911999
Iteration :  47   Loss :  1.88030468604
Iteration :  48   Loss :  0.64828112759
Iteration :  49   Loss :  0.223502804588
Iteration :  50   Loss :  0.0790931078136
Iteration :  51   Loss :  0.0388124564388
Iteration :  52   Loss :  0.0324277990683
Iteration :  53   Loss :  4005.62140887
Iteration :  54   Loss :  510.13178371
Iteration :  55   Loss :  1850.17461716
Iteration :  56   Loss :  689.241812646
Iteration :  57   Loss :  134.281575348
Iteration :  58   Loss :  46.2750668641
Iteration :  59   Loss :  15.9469518266
Iteration :  60   Loss :  5.4955138867
Iteration :  61   Loss :  1.89382108316
Iteration :  62   Loss :  0.652678882408
Iteration :  63   Loss :  0.225055076829
Iteration :  64   Loss :  0.078335354391
Iteration :  65   Loss :  0.0787040877632
Iteration :  66   Loss :  4115.42034174
Iteration :  67   Loss :  1739.6358554
Iteration :  68   Loss :  756.322762392
Iteration :  69   Loss :  174.752822484
Iteration :  70   Loss :  60.2219517024
Iteration :  71   Loss :  20.7532183755
Iteration :  72   Loss :  7.15186064539
Iteration :  73   Loss :  2.46491638666
Iteration :  74   Loss :  0.849440221846
Iteration :  75   Loss :  0.292742977288
Iteration :  76   Loss :  0.101311593216
Iteration :  77   Loss :  0.0380420817251
Iteration :  78   Loss :  0.0175168294389
Iteration :  79   Loss :  339.098415216
Iteration :  80   Loss :  2404.26024884
Iteration :  81   Loss :  2367.3764078
Iteration :  82   Loss :  191.787637465
Iteration :  83   Loss :  66.0923566231
Iteration :  84   Loss :  22.7763247603
Iteration :  85   Loss :  7.85099925413
Iteration :  86   Loss :  2.70509130856
Iteration :  87   Loss :  0.933237992931
Iteration :  88   Loss :  0.323782799638
Iteration :  89   Loss :  0.111712283066
Iteration :  90   Loss :  0.0411313399634
Iteration :  91   Loss :  0.0171643392941
Iteration :  92   Loss :  0.0686509202977
Iteration :  93   Loss :  6325.14870908
Iteration :  94   Loss :  1514.11272022
Iteration :  95   Loss :  1738.28035309
Iteration :  96   Loss :  572.554313469
Iteration :  97   Loss :  117.25896039
Iteration :  98   Loss :  40.4088762919
Iteration :  99   Loss :  13.9255008018
[-0.01975496 -0.01593049 -0.01374625 ...,  0.01426106 -0.00828445
 -0.00121776]
CROSS VALIDATION 18
Iteration :  0   Loss :  2160.31397013
Iteration :  1   Loss :  448.69759194
Iteration :  2   Loss :  154.626656821
Iteration :  3   Loss :  53.2862298109
Iteration :  4   Loss :  18.3630839975
Iteration :  5   Loss :  6.32814246939
Iteration :  6   Loss :  2.18075550331
Iteration :  7   Loss :  0.751610396202
Iteration :  8   Loss :  0.263736944572
Iteration :  9   Loss :  0.0976211268919
Iteration :  10   Loss :  0.0385484746759
Iteration :  11   Loss :  0.0154337050953
Iteration :  12   Loss :  0.0117180151646
Iteration :  13   Loss :  239.724143514
Iteration :  14   Loss :  2824.5561666
Iteration :  15   Loss :  3205.18129447
Iteration :  16   Loss :  432.342636611
Iteration :  17   Loss :  148.990539957
Iteration :  18   Loss :  51.3439552726
Iteration :  19   Loss :  17.6937525282
Iteration :  20   Loss :  6.09748270204
Iteration :  21   Loss :  2.10128888565
Iteration :  22   Loss :  0.725632818354
Iteration :  23   Loss :  0.249692691713
Iteration :  24   Loss :  0.0868876009339
Iteration :  25   Loss :  0.031857830272
Iteration :  26   Loss :  0.0130872404694
Iteration :  27   Loss :  8439.0627536
Iteration :  28   Loss :  2073.40178436
Iteration :  29   Loss :  2714.69630317
Iteration :  30   Loss :  206.741996881
Iteration :  31   Loss :  71.2458109349
Iteration :  32   Loss :  24.5521744606
Iteration :  33   Loss :  8.4609784468
Iteration :  34   Loss :  2.91575625924
Iteration :  35   Loss :  1.00481556873
Iteration :  36   Loss :  0.346339947577
Iteration :  37   Loss :  0.120150209412
Iteration :  38   Loss :  0.042468040619
Iteration :  39   Loss :  0.0171120805832
Iteration :  40   Loss :  0.0183034414407
Iteration :  41   Loss :  0.0161352407205
Iteration :  42   Loss :  4873.14166836
Iteration :  43   Loss :  1347.8484259
Iteration :  44   Loss :  1837.49755385
Iteration :  45   Loss :  231.605197961
Iteration :  46   Loss :  79.8139729534
Iteration :  47   Loss :  27.5048868856
Iteration :  48   Loss :  9.47994513245
Iteration :  49   Loss :  3.26655113075
Iteration :  50   Loss :  1.12617785497
Iteration :  51   Loss :  0.388399108257
Iteration :  52   Loss :  0.134686548614
Iteration :  53   Loss :  0.0476355628413
Iteration :  54   Loss :  7.51703423816
Iteration :  55   Loss :  9084.69043221
Iteration :  56   Loss :  594.264865717
Iteration :  57   Loss :  2476.35017561
Iteration :  58   Loss :  224.347925232
Iteration :  59   Loss :  371.899033338
Iteration :  60   Loss :  3751.20360399
Iteration :  61   Loss :  1192.3321021
Iteration :  62   Loss :  192.953967855
Iteration :  63   Loss :  66.4942881482
Iteration :  64   Loss :  22.9147418189
Iteration :  65   Loss :  7.8966992093
Iteration :  66   Loss :  2.72130106668
Iteration :  67   Loss :  0.938274348515
Iteration :  68   Loss :  0.323425012228
Iteration :  69   Loss :  0.114671791866
Iteration :  70   Loss :  0.0573042347138
Iteration :  71   Loss :  0.0358221261937
Iteration :  72   Loss :  3786.51485838
Iteration :  73   Loss :  2693.93299259
Iteration :  74   Loss :  498.406202228
Iteration :  75   Loss :  211.352065806
Iteration :  76   Loss :  72.8345653443
Iteration :  77   Loss :  25.0996981531
Iteration :  78   Loss :  8.64995635687
Iteration :  79   Loss :  2.98099810728
Iteration :  80   Loss :  1.02728839234
Iteration :  81   Loss :  0.354130455763
Iteration :  82   Loss :  0.122641672995
Iteration :  83   Loss :  0.0497765007815
Iteration :  84   Loss :  0.0214437430578
Iteration :  85   Loss :  1.10370879616
Iteration :  86   Loss :  4515.42018699
Iteration :  87   Loss :  913.646470854
Iteration :  88   Loss :  879.727453025
Iteration :  89   Loss :  453.760481585
Iteration :  90   Loss :  113.334639294
Iteration :  91   Loss :  39.0564974959
Iteration :  92   Loss :  13.459344876
Iteration :  93   Loss :  4.63825422414
Iteration :  94   Loss :  1.59839925044
Iteration :  95   Loss :  0.550971553851
Iteration :  96   Loss :  0.190077836234
Iteration :  97   Loss :  0.066683208776
Iteration :  98   Loss :  0.0317010151738
Iteration :  99   Loss :  0.0592379750244
[ -9.30540992e-04  -2.46852727e-04  -8.92231037e-05 ...,   6.79313676e-04
   4.13462814e-04   9.30183140e-05]
CROSS VALIDATION 19
Iteration :  0   Loss :  2453.23152296
Iteration :  1   Loss :  391.129562117
Iteration :  2   Loss :  134.788012378
Iteration :  3   Loss :  46.4496046162
Iteration :  4   Loss :  16.0081241465
Iteration :  5   Loss :  5.51637056346
Iteration :  6   Loss :  1.90131798126
Iteration :  7   Loss :  0.656429446758
Iteration :  8   Loss :  0.229690273386
Iteration :  9   Loss :  2.46073534795
Iteration :  10   Loss :  0.0442197037797
Iteration :  11   Loss :  0.0214797328969
Iteration :  12   Loss :  0.035601752449
Iteration :  13   Loss :  9388.12426347
Iteration :  14   Loss :  5306.76625219
Iteration :  15   Loss :  386.717295475
Iteration :  16   Loss :  133.267491532
Iteration :  17   Loss :  45.9256012261
Iteration :  18   Loss :  15.8265217101
Iteration :  19   Loss :  5.45401223441
Iteration :  20   Loss :  1.87954962773
Iteration :  21   Loss :  0.64801109919
Iteration :  22   Loss :  0.223409973258
Iteration :  23   Loss :  0.0788506349407
Iteration :  24   Loss :  0.0339449162391
Iteration :  25   Loss :  0.0306979594123
Iteration :  26   Loss :  4755.61266113
Iteration :  27   Loss :  1641.61027058
Iteration :  28   Loss :  551.737342171
Iteration :  29   Loss :  547.304164579
Iteration :  30   Loss :  140.945653081
Iteration :  31   Loss :  48.5715892416
Iteration :  32   Loss :  16.7383614172
Iteration :  33   Loss :  5.76824350605
Iteration :  34   Loss :  1.98793298092
Iteration :  35   Loss :  0.685248638645
Iteration :  36   Loss :  0.236858899518
Iteration :  37   Loss :  0.082196002898
Iteration :  38   Loss :  0.0295654309859
Iteration :  39   Loss :  0.0576860444256
Iteration :  40   Loss :  3361.19507469
Iteration :  41   Loss :  955.434767158
Iteration :  42   Loss :  974.051129457
Iteration :  43   Loss :  259.779513991
Iteration :  44   Loss :  812.616327419
Iteration :  45   Loss :  2024.04488199
Iteration :  46   Loss :  899.137051429
Iteration :  47   Loss :  699.496917005
Iteration :  48   Loss :  2036.4566888
Iteration :  49   Loss :  803.316969471
Iteration :  50   Loss :  829.611048139
Iteration :  51   Loss :  800.053702891
Iteration :  52   Loss :  1284.14431627
Iteration :  53   Loss :  1507.85269579
Iteration :  54   Loss :  1688.24118981
Iteration :  55   Loss :  263.336230295
Iteration :  56   Loss :  90.7488756665
Iteration :  57   Loss :  31.2736016659
Iteration :  58   Loss :  10.7772058578
Iteration :  59   Loss :  3.71398609102
Iteration :  60   Loss :  1.28055661192
Iteration :  61   Loss :  0.443710249598
Iteration :  62   Loss :  0.153093835756
Iteration :  63   Loss :  0.0560057190582
Iteration :  64   Loss :  0.0251772423023
Iteration :  65   Loss :  6336.61604646
Iteration :  66   Loss :  1855.60456401
Iteration :  67   Loss :  255.580379698
Iteration :  68   Loss :  88.0761126685
Iteration :  69   Loss :  30.3521029421
Iteration :  70   Loss :  10.4598811464
Iteration :  71   Loss :  3.60551823587
Iteration :  72   Loss :  1.24361374624
Iteration :  73   Loss :  0.430063911491
Iteration :  74   Loss :  0.150922693519
Iteration :  75   Loss :  0.0551345149201
Iteration :  76   Loss :  6179.10097442
Iteration :  77   Loss :  2393.9563878
Iteration :  78   Loss :  518.359366772
Iteration :  79   Loss :  776.854517745
Iteration :  80   Loss :  166.524900223
Iteration :  81   Loss :  57.3865640004
Iteration :  82   Loss :  19.7764139272
Iteration :  83   Loss :  6.815193184
Iteration :  84   Loss :  2.34860940667
Iteration :  85   Loss :  0.809987696934
Iteration :  86   Loss :  0.279899294119
Iteration :  87   Loss :  0.0969166430195
Iteration :  88   Loss :  0.0377763673829
Iteration :  89   Loss :  7713.63552166
Iteration :  90   Loss :  4591.39392846
Iteration :  91   Loss :  2629.7479554
Iteration :  92   Loss :  308.813106587
Iteration :  93   Loss :  227.926377934
Iteration :  94   Loss :  78.5462067424
Iteration :  95   Loss :  27.0679797992
Iteration :  96   Loss :  9.3279556174
Iteration :  97   Loss :  3.21452715827
Iteration :  98   Loss :  1.10778424797
Iteration :  99   Loss :  0.38290910291
[-0.00626538 -0.00030526 -0.00352682 ...,  0.00345544  0.00018923
  0.00056627]
Accuracy (Logistic Loss):	0.8
Hinge Loss
CROSS VALIDATION 0
Iteration :  0   Loss :  1944.60924434
Iteration :  1   Loss :  378.101571493
Iteration :  2   Loss :  130.298408079
Iteration :  3   Loss :  44.9024188949
Iteration :  4   Loss :  15.473920613
Iteration :  5   Loss :  5.33250156738
Iteration :  6   Loss :  1.83764500784
Iteration :  7   Loss :  0.633274858369
Iteration :  8   Loss :  0.218234231602
Iteration :  9   Loss :  0.0752061750334
Iteration :  10   Loss :  0.160499236532
Iteration :  11   Loss :  4470.20224247
Iteration :  12   Loss :  1318.75080819
Iteration :  13   Loss :  883.711588375
Iteration :  14   Loss :  742.76703658
Iteration :  15   Loss :  472.251657283
Iteration :  16   Loss :  970.366583438
Iteration :  17   Loss :  751.087846713
Iteration :  18   Loss :  335.334548967
Iteration :  19   Loss :  196.739171909
Iteration :  20   Loss :  67.7987155816
Iteration :  21   Loss :  23.3642634047
Iteration :  22   Loss :  8.05160982421
Iteration :  23   Loss :  2.77468284098
Iteration :  24   Loss :  0.956189511928
Iteration :  25   Loss :  0.3295145554
Iteration :  26   Loss :  0.113554730381
Iteration :  27   Loss :  0.039132343566
Iteration :  28   Loss :  0.0134854823558
Iteration :  29   Loss :  4837.20051825
Iteration :  30   Loss :  2684.01712418
Iteration :  31   Loss :  663.687926419
Iteration :  32   Loss :  159.150071893
Iteration :  33   Loss :  54.8450537551
Iteration :  34   Loss :  18.9002737203
Iteration :  35   Loss :  6.51326459263
Iteration :  36   Loss :  2.29388176047
Iteration :  37   Loss :  5572.40450869
Iteration :  38   Loss :  661.046198453
Iteration :  39   Loss :  607.937003534
Iteration :  40   Loss :  1796.13196195
Iteration :  41   Loss :  137.438374751
Iteration :  42   Loss :  47.3629383987
Iteration :  43   Loss :  16.3218456113
Iteration :  44   Loss :  5.62470685239
Iteration :  45   Loss :  1.93834250909
Iteration :  46   Loss :  0.667976444133
Iteration :  47   Loss :  0.230192820837
Iteration :  48   Loss :  0.0793272505797
Iteration :  49   Loss :  0.0273371370212
Iteration :  50   Loss :  0.00942071047536
Iteration :  51   Loss :  0.626626951114
Iteration :  52   Loss :  6738.7673681
Iteration :  53   Loss :  1877.98456338
Iteration :  54   Loss :  1281.48023035
Iteration :  55   Loss :  973.113718588
Iteration :  56   Loss :  118.189659591
Iteration :  57   Loss :  40.7295966407
Iteration :  58   Loss :  14.0359152252
Iteration :  59   Loss :  4.83694739104
Iteration :  60   Loss :  1.66687100116
Iteration :  61   Loss :  0.574424055069
Iteration :  62   Loss :  0.197953527785
Iteration :  63   Loss :  0.0682171974113
Iteration :  64   Loss :  0.0235084773417
Iteration :  65   Loss :  10796.2589204
Iteration :  66   Loss :  3105.26798897
Iteration :  67   Loss :  1208.17292141
Iteration :  68   Loss :  191.967700325
Iteration :  69   Loss :  66.1544083411
Iteration :  70   Loss :  22.7976150965
Iteration :  71   Loss :  7.85633591351
Iteration :  72   Loss :  2.70738907227
Iteration :  73   Loss :  0.932999259365
Iteration :  74   Loss :  0.321522911831
Iteration :  75   Loss :  0.110800712642
Iteration :  76   Loss :  0.0381832755002
Iteration :  77   Loss :  0.185179921093
Iteration :  78   Loss :  6681.69476681
Iteration :  79   Loss :  1856.45745979
Iteration :  80   Loss :  1260.2779405
Iteration :  81   Loss :  961.924338249
Iteration :  82   Loss :  118.225838954
Iteration :  83   Loss :  40.74206449
Iteration :  84   Loss :  14.040211798
Iteration :  85   Loss :  4.83842804237
Iteration :  86   Loss :  1.66738125165
Iteration :  87   Loss :  0.574599893604
Iteration :  88   Loss :  0.198014123886
Iteration :  89   Loss :  0.068238079566
Iteration :  90   Loss :  0.0235156735867
Iteration :  91   Loss :  10796.1039794
Iteration :  92   Loss :  3105.23146851
Iteration :  93   Loss :  1208.15779322
Iteration :  94   Loss :  191.967980564
Iteration :  95   Loss :  66.1545049148
Iteration :  96   Loss :  22.797648377
Iteration :  97   Loss :  7.85634738238
Iteration :  98   Loss :  2.70739302458
Iteration :  99   Loss :  0.933000621379
[-0.00497206 -0.00153477 -0.00033187 ...,  0.00345749  0.00162131
  0.00042633]
CROSS VALIDATION 1
Iteration :  0   Loss :  2094.83943344
Iteration :  1   Loss :  435.526047171
Iteration :  2   Loss :  150.08758202
Iteration :  3   Loss :  51.7220093333
Iteration :  4   Loss :  17.8240345635
Iteration :  5   Loss :  6.14237946698
Iteration :  6   Loss :  2.11673879907
Iteration :  7   Loss :  0.729453979122
Iteration :  8   Loss :  0.251378728397
Iteration :  9   Loss :  0.306897037591
Iteration :  10   Loss :  1927.13899935
Iteration :  11   Loss :  919.412288696
Iteration :  12   Loss :  352.958538273
Iteration :  13   Loss :  719.274315513
Iteration :  14   Loss :  139.597965975
Iteration :  15   Loss :  48.1071598457
Iteration :  16   Loss :  16.5783133891
Iteration :  17   Loss :  5.71308877322
Iteration :  18   Loss :  1.9687999958
Iteration :  19   Loss :  0.678472465127
Iteration :  20   Loss :  8217.15825062
Iteration :  21   Loss :  5254.41077523
Iteration :  22   Loss :  478.570954714
Iteration :  23   Loss :  179.156856551
Iteration :  24   Loss :  61.7396354977
Iteration :  25   Loss :  21.2762305879
Iteration :  26   Loss :  7.33204827632
Iteration :  27   Loss :  2.63107100309
Iteration :  28   Loss :  712.84909483
Iteration :  29   Loss :  2098.362914
Iteration :  30   Loss :  461.264063394
Iteration :  31   Loss :  344.680191897
Iteration :  32   Loss :  118.780993486
Iteration :  33   Loss :  40.9333775055
Iteration :  34   Loss :  14.106140594
Iteration :  35   Loss :  4.86114790869
Iteration :  36   Loss :  1.67521079439
Iteration :  37   Loss :  0.577298049422
Iteration :  38   Loss :  0.198943941254
Iteration :  39   Loss :  0.0685585059592
Iteration :  40   Loss :  0.0236260964256
Iteration :  41   Loss :  0.00814184067322
Iteration :  42   Loss :  10775.4675052
Iteration :  43   Loss :  6634.68292705
Iteration :  44   Loss :  1706.65251892
Iteration :  45   Loss :  280.855367397
Iteration :  46   Loss :  96.7861813634
Iteration :  47   Loss :  33.3536972775
Iteration :  48   Loss :  11.4940904415
Iteration :  49   Loss :  3.96100360264
Iteration :  50   Loss :  1.36501009975
Iteration :  51   Loss :  0.470399110768
Iteration :  52   Loss :  0.162105264606
Iteration :  53   Loss :  0.0558634491682
Iteration :  54   Loss :  0.0192512251872
Iteration :  55   Loss :  0.00663420674386
Iteration :  56   Loss :  6516.01733497
Iteration :  57   Loss :  584.999624703
Iteration :  58   Loss :  201.597997926
Iteration :  59   Loss :  69.4731262239
Iteration :  60   Loss :  23.941285712
Iteration :  61   Loss :  8.25045874136
Iteration :  62   Loss :  2.84320860048
Iteration :  63   Loss :  0.979804323524
Iteration :  64   Loss :  0.337652507183
Iteration :  65   Loss :  0.116359167713
Iteration :  66   Loss :  0.0400987868381
Iteration :  67   Loss :  3124.09836673
Iteration :  68   Loss :  359.319797809
Iteration :  69   Loss :  228.054864892
Iteration :  70   Loss :  78.5904849133
Iteration :  71   Loss :  27.0832385963
Iteration :  72   Loss :  9.3332139848
Iteration :  73   Loss :  3.21633924895
Iteration :  74   Loss :  1.10838969097
Iteration :  75   Loss :  0.381964591404
Iteration :  76   Loss :  0.131629651805
Iteration :  77   Loss :  0.0453611817019
Iteration :  78   Loss :  0.0460189996504
Iteration :  79   Loss :  3408.43529013
Iteration :  80   Loss :  1854.38698687
Iteration :  81   Loss :  293.819227337
Iteration :  82   Loss :  455.101057208
Iteration :  83   Loss :  2238.28157952
Iteration :  84   Loss :  173.498221985
Iteration :  85   Loss :  59.7896010853
Iteration :  86   Loss :  20.6042249715
Iteration :  87   Loss :  7.1004669536
Iteration :  88   Loss :  2.44690741967
Iteration :  89   Loss :  0.843234108341
Iteration :  90   Loss :  9809.75513438
Iteration :  91   Loss :  3590.60107244
Iteration :  92   Loss :  519.37292874
Iteration :  93   Loss :  178.982238945
Iteration :  94   Loss :  61.6794601431
Iteration :  95   Loss :  21.2554934276
Iteration :  96   Loss :  7.32490199822
Iteration :  97   Loss :  2.52425047041
Iteration :  98   Loss :  0.869887465925
Iteration :  99   Loss :  0.299773818899
[-0.00449537 -0.00301878 -0.00141323 ...,  0.00243351  0.00196128
  0.00021594]
CROSS VALIDATION 2
Iteration :  0   Loss :  2535.9234439
Iteration :  1   Loss :  308.868134196
Iteration :  2   Loss :  106.439722092
Iteration :  3   Loss :  36.680425025
Iteration :  4   Loss :  12.640521354
Iteration :  5   Loss :  4.35607766245
Iteration :  6   Loss :  1.50115743409
Iteration :  7   Loss :  0.876813500624
Iteration :  8   Loss :  3985.7715306
Iteration :  9   Loss :  1450.31785077
Iteration :  10   Loss :  155.362966896
Iteration :  11   Loss :  53.5399712335
Iteration :  12   Loss :  18.4505263831
Iteration :  13   Loss :  6.3582761808
Iteration :  14   Loss :  2.19113943699
Iteration :  15   Loss :  0.755093345401
Iteration :  16   Loss :  0.26021436639
Iteration :  17   Loss :  0.0896730409399
Iteration :  18   Loss :  0.0309024224257
Iteration :  19   Loss :  10368.7918261
Iteration :  20   Loss :  7874.9643187
Iteration :  21   Loss :  2035.61519241
Iteration :  22   Loss :  254.761234895
Iteration :  23   Loss :  87.7938253893
Iteration :  24   Loss :  30.2548218518
Iteration :  25   Loss :  10.4261802152
Iteration :  26   Loss :  3.59298872792
Iteration :  27   Loss :  1.23818769027
Iteration :  28   Loss :  0.426694563338
Iteration :  29   Loss :  0.147044145095
Iteration :  30   Loss :  0.0506732038895
Iteration :  31   Loss :  12697.532929
Iteration :  32   Loss :  7234.74487652
Iteration :  33   Loss :  653.078435534
Iteration :  34   Loss :  215.240330792
Iteration :  35   Loss :  74.1744403387
Iteration :  36   Loss :  25.5614158337
Iteration :  37   Loss :  8.80877531991
Iteration :  38   Loss :  3.03561129561
Iteration :  39   Loss :  1.04610863637
Iteration :  40   Loss :  0.3605017812
Iteration :  41   Loss :  0.12423330592
Iteration :  42   Loss :  0.0428123107975
Iteration :  43   Loss :  0.0147536438981
Iteration :  44   Loss :  4505.69515609
Iteration :  45   Loss :  1329.55090095
Iteration :  46   Loss :  860.410126657
Iteration :  47   Loss :  584.049006542
Iteration :  48   Loss :  235.568884712
Iteration :  49   Loss :  3654.93975596
Iteration :  50   Loss :  210.79452157
Iteration :  51   Loss :  72.6423603159
Iteration :  52   Loss :  25.033442392
Iteration :  53   Loss :  8.6268292394
Iteration :  54   Loss :  2.97291045956
Iteration :  55   Loss :  1.02450116436
Iteration :  56   Loss :  0.353055583094
Iteration :  57   Loss :  0.121667255333
Iteration :  58   Loss :  0.0419280185022
Iteration :  59   Loss :  0.300408361321
Iteration :  60   Loss :  6651.84162645
Iteration :  61   Loss :  1865.56138341
Iteration :  62   Loss :  1083.00303474
Iteration :  63   Loss :  132.853110166
Iteration :  64   Loss :  45.7828003588
Iteration :  65   Loss :  15.7773107914
Iteration :  66   Loss :  5.43705351918
Iteration :  67   Loss :  1.87367488422
Iteration :  68   Loss :  0.645691192735
Iteration :  69   Loss :  0.602737328129
Iteration :  70   Loss :  2039.50970047
Iteration :  71   Loss :  3788.70222292
Iteration :  72   Loss :  882.032557645
Iteration :  73   Loss :  152.837327611
Iteration :  74   Loss :  52.6696051651
Iteration :  75   Loss :  18.150587632
Iteration :  76   Loss :  6.2549136329
Iteration :  77   Loss :  2.15551944368
Iteration :  78   Loss :  0.742818261736
Iteration :  79   Loss :  0.255984223007
Iteration :  80   Loss :  0.0882152819931
Iteration :  81   Loss :  0.204829780313
Iteration :  82   Loss :  3172.46555705
Iteration :  83   Loss :  1029.06063546
Iteration :  84   Loss :  205.981581048
Iteration :  85   Loss :  70.9837623742
Iteration :  86   Loss :  24.461869334
Iteration :  87   Loss :  8.42985819994
Iteration :  88   Loss :  2.90503183959
Iteration :  89   Loss :  1.32392598307
Iteration :  90   Loss :  3313.55109795
Iteration :  91   Loss :  3287.04296653
Iteration :  92   Loss :  410.020167126
Iteration :  93   Loss :  141.29794501
Iteration :  94   Loss :  48.6929933323
Iteration :  95   Loss :  16.780198746
Iteration :  96   Loss :  5.78266092686
Iteration :  97   Loss :  2.05223120675
Iteration :  98   Loss :  1160.75238685
Iteration :  99   Loss :  555.704245695
[-0.00029578 -0.02044415 -0.01549715 ...,  0.09766132 -0.01090507
  0.02184411]
CROSS VALIDATION 3
Iteration :  0   Loss :  2093.8466099
Iteration :  1   Loss :  435.76675621
Iteration :  2   Loss :  150.170533288
Iteration :  3   Loss :  51.7505953508
Iteration :  4   Loss :  17.8338856534
Iteration :  5   Loss :  6.14577427258
Iteration :  6   Loss :  2.11790869044
Iteration :  7   Loss :  0.729857137942
Iteration :  8   Loss :  0.446009162234
Iteration :  9   Loss :  3344.68476141
Iteration :  10   Loss :  2539.80034608
Iteration :  11   Loss :  692.262123087
Iteration :  12   Loss :  169.786710015
Iteration :  13   Loss :  58.5105688416
Iteration :  14   Loss :  20.1634548774
Iteration :  15   Loss :  6.94857220913
Iteration :  16   Loss :  2.39456264014
Iteration :  17   Loss :  0.873153448151
Iteration :  18   Loss :  4966.70026882
Iteration :  19   Loss :  1378.42292069
Iteration :  20   Loss :  1017.20193861
Iteration :  21   Loss :  342.173607707
Iteration :  22   Loss :  117.917194036
Iteration :  23   Loss :  40.6357016909
Iteration :  24   Loss :  14.0035578816
Iteration :  25   Loss :  4.8257966562
Iteration :  26   Loss :  1.66302832208
Iteration :  27   Loss :  0.573099821042
Iteration :  28   Loss :  0.197497180605
Iteration :  29   Loss :  0.0680599346134
Iteration :  30   Loss :  0.0234542826656
Iteration :  31   Loss :  0.452507283052
Iteration :  32   Loss :  2595.34971141
Iteration :  33   Loss :  437.24021932
Iteration :  34   Loss :  150.678306627
Iteration :  35   Loss :  51.9255802299
Iteration :  36   Loss :  17.8941875746
Iteration :  37   Loss :  6.16655504932
Iteration :  38   Loss :  2.12506999928
Iteration :  39   Loss :  1.07926511833
Iteration :  40   Loss :  876.544556814
Iteration :  41   Loss :  2755.64794525
Iteration :  42   Loss :  808.797317908
Iteration :  43   Loss :  368.112926255
Iteration :  44   Loss :  279.800991189
Iteration :  45   Loss :  2465.23371909
Iteration :  46   Loss :  249.203423197
Iteration :  47   Loss :  85.8785357655
Iteration :  48   Loss :  29.5947897128
Iteration :  49   Loss :  10.1987250987
Iteration :  50   Loss :  3.51460492361
Iteration :  51   Loss :  1.21117567633
Iteration :  52   Loss :  0.417385894237
Iteration :  53   Loss :  0.143836264311
Iteration :  54   Loss :  0.0495677290889
Iteration :  55   Loss :  0.0170816433449
Iteration :  56   Loss :  0.0503383937662
Iteration :  57   Loss :  6713.92063704
Iteration :  58   Loss :  1859.89321604
Iteration :  59   Loss :  1283.88131221
Iteration :  60   Loss :  960.320121287
Iteration :  61   Loss :  118.418302568
Iteration :  62   Loss :  40.8083897963
Iteration :  63   Loss :  14.0630683066
Iteration :  64   Loss :  4.8463046737
Iteration :  65   Loss :  1.67009563477
Iteration :  66   Loss :  0.575535303098
Iteration :  67   Loss :  0.198336477394
Iteration :  68   Loss :  0.0683491665122
Iteration :  69   Loss :  0.0235539555018
Iteration :  70   Loss :  10796.2935233
Iteration :  71   Loss :  3087.28969044
Iteration :  72   Loss :  1196.91503636
Iteration :  73   Loss :  192.228932465
Iteration :  74   Loss :  66.2444321195
Iteration :  75   Loss :  22.8286383873
Iteration :  76   Loss :  7.8670269175
Iteration :  77   Loss :  2.71107332249
Iteration :  78   Loss :  0.93426889688
Iteration :  79   Loss :  0.321960444388
Iteration :  80   Loss :  0.110951491692
Iteration :  81   Loss :  0.0382352357976
Iteration :  82   Loss :  0.181337622488
Iteration :  83   Loss :  6681.38945237
Iteration :  84   Loss :  1840.00382637
Iteration :  85   Loss :  238.357251858
Iteration :  86   Loss :  206.221306838
Iteration :  87   Loss :  71.0663748023
Iteration :  88   Loss :  24.4903385832
Iteration :  89   Loss :  8.43966905008
Iteration :  90   Loss :  2.90841277808
Iteration :  91   Loss :  1.00227447753
Iteration :  92   Loss :  0.345395996015
Iteration :  93   Loss :  0.119027668306
Iteration :  94   Loss :  0.0410183846538
Iteration :  95   Loss :  0.0141354350929
Iteration :  96   Loss :  0.00487124314994
Iteration :  97   Loss :  10841.7474026
Iteration :  98   Loss :  7747.30751533
Iteration :  99   Loss :  1502.3497305
[-0.09611475 -0.01192508 -0.0196248  ...,  0.04279191 -0.01958918
  0.01002762]
CROSS VALIDATION 4
Iteration :  0   Loss :  2093.8466099
Iteration :  1   Loss :  435.76675621
Iteration :  2   Loss :  150.170533288
Iteration :  3   Loss :  51.7505953508
Iteration :  4   Loss :  17.8338856534
Iteration :  5   Loss :  6.14577427258
Iteration :  6   Loss :  2.11790869044
Iteration :  7   Loss :  0.729857137942
Iteration :  8   Loss :  0.446009162234
Iteration :  9   Loss :  1649.39408163
Iteration :  10   Loss :  561.031127974
Iteration :  11   Loss :  352.917104064
Iteration :  12   Loss :  121.619533772
Iteration :  13   Loss :  41.9115730709
Iteration :  14   Loss :  14.4432387035
Iteration :  15   Loss :  4.97731602422
Iteration :  16   Loss :  1.71524374232
Iteration :  17   Loss :  0.591093891013
Iteration :  18   Loss :  0.203698156345
Iteration :  19   Loss :  0.0701968664021
Iteration :  20   Loss :  4735.73842046
Iteration :  21   Loss :  420.671632042
Iteration :  22   Loss :  1383.23501414
Iteration :  23   Loss :  494.497859005
Iteration :  24   Loss :  155.139608484
Iteration :  25   Loss :  75.2600424119
Iteration :  26   Loss :  25.9355275344
Iteration :  27   Loss :  8.9376987699
Iteration :  28   Loss :  3.0800398872
Iteration :  29   Loss :  1.06141927032
Iteration :  30   Loss :  0.365778012194
Iteration :  31   Loss :  0.126051559403
Iteration :  32   Loss :  5340.4717462
Iteration :  33   Loss :  3330.57416463
Iteration :  34   Loss :  2063.38074261
Iteration :  35   Loss :  296.41362566
Iteration :  36   Loss :  102.147746712
Iteration :  37   Loss :  35.2013580186
Iteration :  38   Loss :  12.1308168436
Iteration :  39   Loss :  4.18042727826
Iteration :  40   Loss :  1.44062617169
Iteration :  41   Loss :  0.496457330412
Iteration :  42   Loss :  0.171085244572
Iteration :  43   Loss :  0.0589580596706
Iteration :  44   Loss :  0.020317665669
Iteration :  45   Loss :  10749.4171473
Iteration :  46   Loss :  6944.26017327
Iteration :  47   Loss :  1502.39897139
Iteration :  48   Loss :  177.510340261
Iteration :  49   Loss :  61.1722259241
Iteration :  50   Loss :  21.0806943359
Iteration :  51   Loss :  7.26466410159
Iteration :  52   Loss :  2.50349175733
Iteration :  53   Loss :  0.862733760488
Iteration :  54   Loss :  0.297308564849
Iteration :  55   Loss :  0.102456153661
Iteration :  56   Loss :  0.181548116955
Iteration :  57   Loss :  3786.04445366
Iteration :  58   Loss :  1746.06289587
Iteration :  59   Loss :  317.739152417
Iteration :  60   Loss :  109.496783049
Iteration :  61   Loss :  37.7339254762
Iteration :  62   Loss :  13.0035704446
Iteration :  63   Loss :  4.70737077332
Iteration :  64   Loss :  1769.78977829
Iteration :  65   Loss :  2507.77928978
Iteration :  66   Loss :  1651.22216618
Iteration :  67   Loss :  732.388449463
Iteration :  68   Loss :  1753.84838511
Iteration :  69   Loss :  1065.15849414
Iteration :  70   Loss :  220.41445868
Iteration :  71   Loss :  75.9575078472
Iteration :  72   Loss :  26.1758826209
Iteration :  73   Loss :  9.02052806102
Iteration :  74   Loss :  3.10858387006
Iteration :  75   Loss :  1.07125587458
Iteration :  76   Loss :  0.369167825863
Iteration :  77   Loss :  0.127219730492
Iteration :  78   Loss :  0.0438414690898
Iteration :  79   Loss :  0.161423816277
Iteration :  80   Loss :  2158.59838036
Iteration :  81   Loss :  1274.92568201
Iteration :  82   Loss :  935.211646915
Iteration :  83   Loss :  268.32497518
Iteration :  84   Loss :  92.4680555436
Iteration :  85   Loss :  31.8656185108
Iteration :  86   Loss :  10.9812803687
Iteration :  87   Loss :  3.78428300382
Iteration :  88   Loss :  1.30411002835
Iteration :  89   Loss :  0.449412204195
Iteration :  90   Loss :  0.154872920911
Iteration :  91   Loss :  0.0533710954164
Iteration :  92   Loss :  0.0183923297191
Iteration :  93   Loss :  0.00633822090135
Iteration :  94   Loss :  3213.21510817
Iteration :  95   Loss :  939.52455162
Iteration :  96   Loss :  1561.68949518
Iteration :  97   Loss :  615.527374398
Iteration :  98   Loss :  460.326440662
Iteration :  99   Loss :  1435.84010645
[-0.12957854  0.00234409 -0.08073564 ...,  0.0777549  -0.00097827
  0.00536668]
CROSS VALIDATION 5
Iteration :  0   Loss :  2933.76950881
Iteration :  1   Loss :  1878.22655616
Iteration :  2   Loss :  218.242893298
Iteration :  3   Loss :  75.2091599594
Iteration :  4   Loss :  25.9179928213
Iteration :  5   Loss :  8.93165609414
Iteration :  6   Loss :  3.07795750751
Iteration :  7   Loss :  1.06070165691
Iteration :  8   Loss :  0.36553071387
Iteration :  9   Loss :  0.125966337387
Iteration :  10   Loss :  0.0434095345553
Iteration :  11   Loss :  12723.7919604
Iteration :  12   Loss :  5503.64319235
Iteration :  13   Loss :  1166.18246247
Iteration :  14   Loss :  207.072785291
Iteration :  15   Loss :  71.359804651
Iteration :  16   Loss :  24.591458084
Iteration :  17   Loss :  8.47451606204
Iteration :  18   Loss :  2.92042148295
Iteration :  19   Loss :  1.00641282354
Iteration :  20   Loss :  0.420566595153
Iteration :  21   Loss :  7999.59353682
Iteration :  22   Loss :  1504.02811248
Iteration :  23   Loss :  966.640983796
Iteration :  24   Loss :  952.861967584
Iteration :  25   Loss :  132.838155807
Iteration :  26   Loss :  45.7776469044
Iteration :  27   Loss :  15.7755348482
Iteration :  28   Loss :  5.43644150752
Iteration :  29   Loss :  1.87346397755
Iteration :  30   Loss :  0.645618511726
Iteration :  31   Loss :  0.222488004935
Iteration :  32   Loss :  10191.9824634
Iteration :  33   Loss :  5304.71091838
Iteration :  34   Loss :  599.815470945
Iteration :  35   Loss :  343.430730217
Iteration :  36   Loss :  118.350413769
Iteration :  37   Loss :  40.7849944889
Iteration :  38   Loss :  14.0550059986
Iteration :  39   Loss :  4.84352630415
Iteration :  40   Loss :  1.66913817478
Iteration :  41   Loss :  0.575205350718
Iteration :  42   Loss :  0.21081047244
Iteration :  43   Loss :  1480.15753927
Iteration :  44   Loss :  1699.24685448
Iteration :  45   Loss :  221.467129944
Iteration :  46   Loss :  76.3202711897
Iteration :  47   Loss :  26.3008952884
Iteration :  48   Loss :  9.06360894932
Iteration :  49   Loss :  3.12343006903
Iteration :  50   Loss :  1.07637205562
Iteration :  51   Loss :  0.370930924182
Iteration :  52   Loss :  0.127827315654
Iteration :  53   Loss :  0.0440508503388
Iteration :  54   Loss :  0.0836354380691
Iteration :  55   Loss :  6483.88450798
Iteration :  56   Loss :  890.962195095
Iteration :  57   Loss :  4326.21401664
Iteration :  58   Loss :  507.809709985
Iteration :  59   Loss :  682.675407303
Iteration :  60   Loss :  4471.81471602
Iteration :  61   Loss :  292.421482738
Iteration :  62   Loss :  113.488777794
Iteration :  63   Loss :  39.1096155009
Iteration :  64   Loss :  13.4776499877
Iteration :  65   Loss :  4.64456238864
Iteration :  66   Loss :  1.60057278544
Iteration :  67   Loss :  0.551576882198
Iteration :  68   Loss :  6290.92334548
Iteration :  69   Loss :  3627.14304918
Iteration :  70   Loss :  1069.00064615
Iteration :  71   Loss :  175.814588811
Iteration :  72   Loss :  60.5878493146
Iteration :  73   Loss :  20.8793110366
Iteration :  74   Loss :  7.19526496312
Iteration :  75   Loss :  2.47957596871
Iteration :  76   Loss :  0.854492088352
Iteration :  77   Loss :  0.29446838422
Iteration :  78   Loss :  0.101477392813
Iteration :  79   Loss :  0.259538010381
Iteration :  80   Loss :  4468.55638629
Iteration :  81   Loss :  2666.28389478
Iteration :  82   Loss :  636.007872648
Iteration :  83   Loss :  159.408649986
Iteration :  84   Loss :  54.9341629163
Iteration :  85   Loss :  18.9309818229
Iteration :  86   Loss :  6.52384697892
Iteration :  87   Loss :  2.24819715123
Iteration :  88   Loss :  0.774756128881
Iteration :  89   Loss :  1872.24003261
Iteration :  90   Loss :  1744.47266009
Iteration :  91   Loss :  210.257113751
Iteration :  92   Loss :  72.4571630341
Iteration :  93   Loss :  24.9696211524
Iteration :  94   Loss :  8.60483566268
Iteration :  95   Loss :  2.96533120506
Iteration :  96   Loss :  1.02188926092
Iteration :  97   Loss :  0.352155489347
Iteration :  98   Loss :  0.121357072062
Iteration :  99   Loss :  0.0418211255678
[-0.00084933 -0.00012872  0.00020017 ...,  0.00049515 -0.00022911
  0.00014878]
CROSS VALIDATION 6
Iteration :  0   Loss :  2091.74939365
Iteration :  1   Loss :  435.94306235
Iteration :  2   Loss :  150.231290532
Iteration :  3   Loss :  51.7715330372
Iteration :  4   Loss :  17.8411010352
Iteration :  5   Loss :  6.14826078107
Iteration :  6   Loss :  2.11876557155
Iteration :  7   Loss :  0.730152429615
Iteration :  8   Loss :  0.394649237443
Iteration :  9   Loss :  2183.98620212
Iteration :  10   Loss :  999.185605846
Iteration :  11   Loss :  456.873574414
Iteration :  12   Loss :  376.840657812
Iteration :  13   Loss :  129.863881863
Iteration :  14   Loss :  44.7526758667
Iteration :  15   Loss :  15.4223173411
Iteration :  16   Loss :  5.31471845118
Iteration :  17   Loss :  1.83151672934
Iteration :  18   Loss :  0.631162978935
Iteration :  19   Loss :  0.285672226802
Iteration :  20   Loss :  5957.67082985
Iteration :  21   Loss :  1858.96767397
Iteration :  22   Loss :  1276.61647288
Iteration :  23   Loss :  961.966978668
Iteration :  24   Loss :  119.288094682
Iteration :  25   Loss :  41.1081307557
Iteration :  26   Loss :  14.1663626931
Iteration :  27   Loss :  4.88190117775
Iteration :  28   Loss :  1.68236262375
Iteration :  29   Loss :  0.579762656953
Iteration :  30   Loss :  0.199793275036
Iteration :  31   Loss :  0.0688511967283
Iteration :  32   Loss :  0.0237269612306
Iteration :  33   Loss :  10795.9130481
Iteration :  34   Loss :  3086.96255046
Iteration :  35   Loss :  1198.18530469
Iteration :  36   Loss :  192.423539208
Iteration :  37   Loss :  66.3114959741
Iteration :  38   Loss :  22.8517494087
Iteration :  39   Loss :  7.87499125703
Iteration :  40   Loss :  2.71381793093
Iteration :  41   Loss :  0.935214722385
Iteration :  42   Loss :  0.32228638738
Iteration :  43   Loss :  0.111063815618
Iteration :  44   Loss :  0.0382739439911
Iteration :  45   Loss :  0.175045907753
Iteration :  46   Loss :  6207.45940329
Iteration :  47   Loss :  1839.20137123
Iteration :  48   Loss :  242.070548496
Iteration :  49   Loss :  206.290141177
Iteration :  50   Loss :  71.0900959542
Iteration :  51   Loss :  24.4985131812
Iteration :  52   Loss :  8.44248611613
Iteration :  53   Loss :  2.90938357336
Iteration :  54   Loss :  1.00260902541
Iteration :  55   Loss :  0.345511285291
Iteration :  56   Loss :  0.119067398395
Iteration :  57   Loss :  0.0410320761265
Iteration :  58   Loss :  0.0141401533412
Iteration :  59   Loss :  0.00487286911579
Iteration :  60   Loss :  10841.7580788
Iteration :  61   Loss :  7747.24073068
Iteration :  62   Loss :  1502.34928234
Iteration :  63   Loss :  177.397101887
Iteration :  64   Loss :  61.1332026008
Iteration :  65   Loss :  21.0672464233
Iteration :  66   Loss :  7.26002978704
Iteration :  67   Loss :  2.50189471608
Iteration :  68   Loss :  0.862183400614
Iteration :  69   Loss :  0.297118904132
Iteration :  70   Loss :  0.102390794267
Iteration :  71   Loss :  0.186486492938
Iteration :  72   Loss :  3786.19181255
Iteration :  73   Loss :  1741.57546479
Iteration :  74   Loss :  317.927794983
Iteration :  75   Loss :  109.561791576
Iteration :  76   Loss :  37.7563282065
Iteration :  77   Loss :  13.0112906984
Iteration :  78   Loss :  4.637663634
Iteration :  79   Loss :  1770.09959524
Iteration :  80   Loss :  2505.08481714
Iteration :  81   Loss :  1649.79446957
Iteration :  82   Loss :  731.443409774
Iteration :  83   Loss :  1757.91648256
Iteration :  84   Loss :  1064.73447244
Iteration :  85   Loss :  220.43936379
Iteration :  86   Loss :  75.9660904516
Iteration :  87   Loss :  26.1788402909
Iteration :  88   Loss :  9.02154731015
Iteration :  89   Loss :  3.10893511572
Iteration :  90   Loss :  1.07137691812
Iteration :  91   Loss :  0.36920953894
Iteration :  92   Loss :  0.127234105327
Iteration :  93   Loss :  0.0438464228329
Iteration :  94   Loss :  0.164372697465
Iteration :  95   Loss :  2158.59219736
Iteration :  96   Loss :  1274.8630474
Iteration :  97   Loss :  924.539070299
Iteration :  98   Loss :  268.507544526
Iteration :  99   Loss :  92.5309711644
[-0.060589   -0.03585363 -0.03201876 ...,  0.0623718   0.03970415
  0.00167372]
CROSS VALIDATION 7
Iteration :  0   Loss :  2091.74939365
Iteration :  1   Loss :  435.94306235
Iteration :  2   Loss :  150.231290532
Iteration :  3   Loss :  51.7715330372
Iteration :  4   Loss :  17.8411010352
Iteration :  5   Loss :  6.14826078107
Iteration :  6   Loss :  2.11876557155
Iteration :  7   Loss :  0.730152429615
Iteration :  8   Loss :  0.394649237443
Iteration :  9   Loss :  1651.59432226
Iteration :  10   Loss :  560.162879745
Iteration :  11   Loss :  353.230579278
Iteration :  12   Loss :  121.72756115
Iteration :  13   Loss :  41.9488006215
Iteration :  14   Loss :  14.456067771
Iteration :  15   Loss :  4.98173707718
Iteration :  16   Loss :  1.71676729103
Iteration :  17   Loss :  0.591618924462
Iteration :  18   Loss :  0.203879089269
Iteration :  19   Loss :  0.0702592180917
Iteration :  20   Loss :  4734.66905158
Iteration :  21   Loss :  419.962318055
Iteration :  22   Loss :  1382.65675342
Iteration :  23   Loss :  494.049556971
Iteration :  24   Loss :  155.61058426
Iteration :  25   Loss :  75.266627243
Iteration :  26   Loss :  25.9377967474
Iteration :  27   Loss :  8.93848076835
Iteration :  28   Loss :  3.08030937339
Iteration :  29   Loss :  1.06151213855
Iteration :  30   Loss :  0.365810015716
Iteration :  31   Loss :  0.126062588206
Iteration :  32   Loss :  5340.49612726
Iteration :  33   Loss :  3330.39420928
Iteration :  34   Loss :  2054.76306693
Iteration :  35   Loss :  296.616975944
Iteration :  36   Loss :  102.217823698
Iteration :  37   Loss :  35.2255074016
Iteration :  38   Loss :  12.1391390152
Iteration :  39   Loss :  4.18329519998
Iteration :  40   Loss :  1.44161449245
Iteration :  41   Loss :  0.496797917789
Iteration :  42   Loss :  0.171202615132
Iteration :  43   Loss :  0.0589985069954
Iteration :  44   Loss :  0.0203316043099
Iteration :  45   Loss :  10749.44811
Iteration :  46   Loss :  7424.95603043
Iteration :  47   Loss :  1502.3976722
Iteration :  48   Loss :  177.510348825
Iteration :  49   Loss :  61.1722288753
Iteration :  50   Loss :  21.080695353
Iteration :  51   Loss :  7.26466445207
Iteration :  52   Loss :  2.50349187811
Iteration :  53   Loss :  0.86273380211
Iteration :  54   Loss :  0.297308579193
Iteration :  55   Loss :  0.102456158604
Iteration :  56   Loss :  0.0787845748653
Iteration :  57   Loss :  4297.3318239
Iteration :  58   Loss :  537.165934908
Iteration :  59   Loss :  707.165343081
Iteration :  60   Loss :  405.520099121
Iteration :  61   Loss :  153.915518213
Iteration :  62   Loss :  53.0411627826
Iteration :  63   Loss :  18.2786309138
Iteration :  64   Loss :  6.29903890781
Iteration :  65   Loss :  2.17072555101
Iteration :  66   Loss :  0.748058471582
Iteration :  67   Loss :  0.257790063164
Iteration :  68   Loss :  0.209834695315
Iteration :  69   Loss :  6604.35827402
Iteration :  70   Loss :  1836.69946964
Iteration :  71   Loss :  224.466376833
Iteration :  72   Loss :  206.521314326
Iteration :  73   Loss :  71.1697610377
Iteration :  74   Loss :  24.5259667395
Iteration :  75   Loss :  8.45194694682
Iteration :  76   Loss :  2.91264389088
Iteration :  77   Loss :  1.00373257055
Iteration :  78   Loss :  0.345898472634
Iteration :  79   Loss :  0.119200827871
Iteration :  80   Loss :  0.0410780575495
Iteration :  81   Loss :  0.0141559990998
Iteration :  82   Loss :  0.00487832975725
Iteration :  83   Loss :  10841.6932438
Iteration :  84   Loss :  7492.452235
Iteration :  85   Loss :  1502.34851877
Iteration :  86   Loss :  177.397258556
Iteration :  87   Loss :  61.1332565908
Iteration :  88   Loss :  21.0672650289
Iteration :  89   Loss :  7.26003619878
Iteration :  90   Loss :  2.50189692564
Iteration :  91   Loss :  0.862184162056
Iteration :  92   Loss :  0.297119166534
Iteration :  93   Loss :  0.102390884694
Iteration :  94   Loss :  0.0807672120649
Iteration :  95   Loss :  4297.57733758
Iteration :  96   Loss :  537.168910673
Iteration :  97   Loss :  707.157923657
Iteration :  98   Loss :  405.520230872
Iteration :  99   Loss :  153.91544404
[-0.09960957 -0.00783836 -0.03412105 ...,  0.05166577  0.05983406
  0.01022169]
CROSS VALIDATION 8
Iteration :  0   Loss :  2752.91772769
Iteration :  1   Loss :  1341.60820383
Iteration :  2   Loss :  177.867219946
Iteration :  3   Loss :  61.2952110115
Iteration :  4   Loss :  21.1230764954
Iteration :  5   Loss :  7.2792695101
Iteration :  6   Loss :  2.50852495905
Iteration :  7   Loss :  0.86446826312
Iteration :  8   Loss :  0.297906295589
Iteration :  9   Loss :  0.102662138956
Iteration :  10   Loss :  0.0353786238527
Iteration :  11   Loss :  6674.62238321
Iteration :  12   Loss :  1833.89969402
Iteration :  13   Loss :  235.162676709
Iteration :  14   Loss :  206.460970853
Iteration :  15   Loss :  71.1489659416
Iteration :  16   Loss :  24.518800496
Iteration :  17   Loss :  8.449477372
Iteration :  18   Loss :  2.91179284531
Iteration :  19   Loss :  1.00343928988
Iteration :  20   Loss :  0.345797404542
Iteration :  21   Loss :  0.119165998575
Iteration :  22   Loss :  0.0410660549497
Iteration :  23   Loss :  0.0141518628577
Iteration :  24   Loss :  0.00487690435782
Iteration :  25   Loss :  10841.7143797
Iteration :  26   Loss :  7747.20237426
Iteration :  27   Loss :  1502.347388
Iteration :  28   Loss :  177.397204938
Iteration :  29   Loss :  61.1332381134
Iteration :  30   Loss :  21.0672586614
Iteration :  31   Loss :  7.26003400444
Iteration :  32   Loss :  2.50189616945
Iteration :  33   Loss :  0.862183901462
Iteration :  34   Loss :  0.29711907673
Iteration :  35   Loss :  0.102390853746
Iteration :  36   Loss :  0.186471464921
Iteration :  37   Loss :  3546.930299
Iteration :  38   Loss :  559.856825416
Iteration :  39   Loss :  2460.82672035
Iteration :  40   Loss :  243.853097566
Iteration :  41   Loss :  84.0347483684
Iteration :  42   Loss :  28.9593981123
Iteration :  43   Loss :  9.97976141191
Iteration :  44   Loss :  3.43914736945
Iteration :  45   Loss :  1.18517208384
Iteration :  46   Loss :  0.408424739454
Iteration :  47   Loss :  0.140748141196
Iteration :  48   Loss :  0.0485035242398
Iteration :  49   Loss :  0.0167149053883
Iteration :  50   Loss :  5765.24232022
Iteration :  51   Loss :  4594.93432915
Iteration :  52   Loss :  579.167738587
Iteration :  53   Loss :  196.0967635
Iteration :  54   Loss :  67.577333817
Iteration :  55   Loss :  23.2879725515
Iteration :  56   Loss :  8.02531906671
Iteration :  57   Loss :  2.76562272564
Iteration :  58   Loss :  0.953067285798
Iteration :  59   Loss :  0.328438598236
Iteration :  60   Loss :  0.113183942434
Iteration :  61   Loss :  0.0390045655219
Iteration :  62   Loss :  0.013441448485
Iteration :  63   Loss :  10015.9531717
Iteration :  64   Loss :  928.319643624
Iteration :  65   Loss :  718.882283231
Iteration :  66   Loss :  188.684162286
Iteration :  67   Loss :  65.0228611284
Iteration :  68   Loss :  22.407670141
Iteration :  69   Loss :  7.72195613101
Iteration :  70   Loss :  2.66108016202
Iteration :  71   Loss :  0.917040644697
Iteration :  72   Loss :  0.316023378787
Iteration :  73   Loss :  0.399215864931
Iteration :  74   Loss :  6543.00766793
Iteration :  75   Loss :  1825.78357133
Iteration :  76   Loss :  1235.36856033
Iteration :  77   Loss :  963.991469495
Iteration :  78   Loss :  119.343306852
Iteration :  79   Loss :  41.127157542
Iteration :  80   Loss :  14.1729195554
Iteration :  81   Loss :  4.88416075241
Iteration :  82   Loss :  1.68314130071
Iteration :  83   Loss :  0.580030998519
Iteration :  84   Loss :  0.199885748809
Iteration :  85   Loss :  0.0688830643171
Iteration :  86   Loss :  0.02373794319
Iteration :  87   Loss :  10795.952578
Iteration :  88   Loss :  3086.91160386
Iteration :  89   Loss :  1198.20408327
Iteration :  90   Loss :  193.054209913
Iteration :  91   Loss :  66.5288327827
Iteration :  92   Loss :  22.9266463208
Iteration :  93   Loss :  7.90080164545
Iteration :  94   Loss :  2.72271250523
Iteration :  95   Loss :  0.938279901056
Iteration :  96   Loss :  0.323342685294
Iteration :  97   Loss :  0.111427828749
Iteration :  98   Loss :  0.0383993873507
Iteration :  99   Loss :  0.17055429615
[ -5.91425157e-04  -1.81441601e-04  -3.88114194e-05 ...,   4.13215398e-04
   1.95129470e-04   4.99225366e-05]
CROSS VALIDATION 9
Iteration :  0   Loss :  2071.48804672
Iteration :  1   Loss :  364.415593791
Iteration :  2   Loss :  348.905865755
Iteration :  3   Loss :  666.838517774
Iteration :  4   Loss :  3489.55751834
Iteration :  5   Loss :  268.593693697
Iteration :  6   Loss :  92.5606592181
Iteration :  7   Loss :  31.8975308652
Iteration :  8   Loss :  10.9922777549
Iteration :  9   Loss :  3.78807283707
Iteration :  10   Loss :  1.30541605106
Iteration :  11   Loss :  0.449862275533
Iteration :  12   Loss :  0.155028020977
Iteration :  13   Loss :  0.053424544789
Iteration :  14   Loss :  5284.12918691
Iteration :  15   Loss :  4643.1261395
Iteration :  16   Loss :  734.214476868
Iteration :  17   Loss :  416.434171383
Iteration :  18   Loss :  723.063010475
Iteration :  19   Loss :  68.1352109587
Iteration :  20   Loss :  23.4802238113
Iteration :  21   Loss :  8.09157119311
Iteration :  22   Loss :  2.78845401557
Iteration :  23   Loss :  0.960935226469
Iteration :  24   Loss :  0.331149986449
Iteration :  25   Loss :  0.114118319846
Iteration :  26   Loss :  10393.5102667
Iteration :  27   Loss :  6607.98163986
Iteration :  28   Loss :  786.323436436
Iteration :  29   Loss :  289.834208553
Iteration :  30   Loss :  99.8803994184
Iteration :  31   Loss :  34.4200025172
Iteration :  32   Loss :  11.8615522183
Iteration :  33   Loss :  4.08763540783
Iteration :  34   Loss :  1.40864896262
Iteration :  35   Loss :  0.485437594579
Iteration :  36   Loss :  0.167287709347
Iteration :  37   Loss :  0.0576493827652
Iteration :  38   Loss :  0.0198666796634
Iteration :  39   Loss :  0.301577819976
Iteration :  40   Loss :  3187.87245899
Iteration :  41   Loss :  1122.66566305
Iteration :  42   Loss :  239.696467714
Iteration :  43   Loss :  82.6023230797
Iteration :  44   Loss :  28.4657669061
Iteration :  45   Loss :  9.80965008416
Iteration :  46   Loss :  3.38052493338
Iteration :  47   Loss :  1.16497007815
Iteration :  48   Loss :  0.401462882166
Iteration :  49   Loss :  5016.01996376
Iteration :  50   Loss :  4432.5947289
Iteration :  51   Loss :  357.860707218
Iteration :  52   Loss :  123.323159649
Iteration :  53   Loss :  42.4986633039
Iteration :  54   Loss :  14.6455571505
Iteration :  55   Loss :  5.04703742597
Iteration :  56   Loss :  1.7392705868
Iteration :  57   Loss :  0.59937383435
Iteration :  58   Loss :  0.206551525697
Iteration :  59   Loss :  4574.33787817
Iteration :  60   Loss :  922.485478174
Iteration :  61   Loss :  377.715342218
Iteration :  62   Loss :  156.495796574
Iteration :  63   Loss :  1982.49091819
Iteration :  64   Loss :  199.315908641
Iteration :  65   Loss :  68.6866904526
Iteration :  66   Loss :  23.6702703638
Iteration :  67   Loss :  8.15706355049
Iteration :  68   Loss :  2.81102347984
Iteration :  69   Loss :  0.968712938829
Iteration :  70   Loss :  0.333830280887
Iteration :  71   Loss :  0.115041981964
Iteration :  72   Loss :  0.0396448685812
Iteration :  73   Loss :  3098.29761013
Iteration :  74   Loss :  725.641346464
Iteration :  75   Loss :  3192.7588639
Iteration :  76   Loss :  141.731229188
Iteration :  77   Loss :  48.8423083392
Iteration :  78   Loss :  16.8316545165
Iteration :  79   Loss :  5.80039321228
Iteration :  80   Loss :  1.99888616916
Iteration :  81   Loss :  0.688840527017
Iteration :  82   Loss :  0.552964547806
Iteration :  83   Loss :  1293.01372516
Iteration :  84   Loss :  508.394715241
Iteration :  85   Loss :  175.199012822
Iteration :  86   Loss :  60.3757143288
Iteration :  87   Loss :  20.806206736
Iteration :  88   Loss :  7.17007233047
Iteration :  89   Loss :  2.47089428056
Iteration :  90   Loss :  0.851500272846
Iteration :  91   Loss :  0.293437368147
Iteration :  92   Loss :  0.272722986428
Iteration :  93   Loss :  4522.82390017
Iteration :  94   Loss :  415.163960638
Iteration :  95   Loss :  143.070558923
Iteration :  96   Loss :  49.3038576831
Iteration :  97   Loss :  16.9907100436
Iteration :  98   Loss :  5.85520568475
Iteration :  99   Loss :  2.01777521497
[-0.00562864 -0.00308041 -0.00168827 ...,  0.00755346  0.00338599
  0.00048532]
CROSS VALIDATION 10
Iteration :  0   Loss :  1738.13800801
Iteration :  1   Loss :  312.789124781
Iteration :  2   Loss :  107.790943218
Iteration :  3   Loss :  37.1460722875
Iteration :  4   Loss :  12.8009890738
Iteration :  5   Loss :  4.41137679372
Iteration :  6   Loss :  1.52021418845
Iteration :  7   Loss :  0.523884330639
Iteration :  8   Loss :  0.180536923003
Iteration :  9   Loss :  0.0622152232109
Iteration :  10   Loss :  1927.50618403
Iteration :  11   Loss :  1776.13505706
Iteration :  12   Loss :  698.460212447
Iteration :  13   Loss :  119.527111699
Iteration :  14   Loss :  41.1904989319
Iteration :  15   Loss :  14.1947477701
Iteration :  16   Loss :  4.89168302112
Iteration :  17   Loss :  1.68573356615
Iteration :  18   Loss :  0.580924324774
Iteration :  19   Loss :  0.20019359992
Iteration :  20   Loss :  0.0689891535606
Iteration :  21   Loss :  0.0237745028358
Iteration :  22   Loss :  0.20784377056
Iteration :  23   Loss :  6106.44677406
Iteration :  24   Loss :  816.516516868
Iteration :  25   Loss :  1288.18753175
Iteration :  26   Loss :  159.749561111
Iteration :  27   Loss :  55.0516450431
Iteration :  28   Loss :  18.9714675952
Iteration :  29   Loss :  6.53779886928
Iteration :  30   Loss :  2.25300514263
Iteration :  31   Loss :  0.776413021295
Iteration :  32   Loss :  0.267561386448
Iteration :  33   Loss :  0.0922049135633
Iteration :  34   Loss :  0.0317749365784
Iteration :  35   Loss :  0.237803962942
Iteration :  36   Loss :  4814.75809245
Iteration :  37   Loss :  2436.25977488
Iteration :  38   Loss :  1825.53513341
Iteration :  39   Loss :  269.864602067
Iteration :  40   Loss :  92.9986297264
Iteration :  41   Loss :  32.0484608383
Iteration :  42   Loss :  11.0442900624
Iteration :  43   Loss :  3.80599691194
Iteration :  44   Loss :  1.31159290564
Iteration :  45   Loss :  0.451990894875
Iteration :  46   Loss :  0.155761569136
Iteration :  47   Loss :  0.0536773344216
Iteration :  48   Loss :  0.0894569262073
Iteration :  49   Loss :  1997.52157379
Iteration :  50   Loss :  1795.47307716
Iteration :  51   Loss :  685.422248537
Iteration :  52   Loss :  119.367840643
Iteration :  53   Loss :  41.1356121854
Iteration :  54   Loss :  14.1758331286
Iteration :  55   Loss :  4.8851648052
Iteration :  56   Loss :  1.68348730953
Iteration :  57   Loss :  0.580150237376
Iteration :  58   Loss :  0.199926839972
Iteration :  59   Loss :  0.0688972248324
Iteration :  60   Loss :  0.0237428230761
Iteration :  61   Loss :  0.112740817746
Iteration :  62   Loss :  6106.61069182
Iteration :  63   Loss :  816.51269885
Iteration :  64   Loss :  1288.20422708
Iteration :  65   Loss :  159.749331282
Iteration :  66   Loss :  55.0515658414
Iteration :  67   Loss :  18.9714403013
Iteration :  68   Loss :  6.53778946348
Iteration :  69   Loss :  2.25300190128
Iteration :  70   Loss :  0.776411904285
Iteration :  71   Loss :  0.267561001513
Iteration :  72   Loss :  0.0922047809099
Iteration :  73   Loss :  0.0317748908645
Iteration :  74   Loss :  0.237877210621
Iteration :  75   Loss :  4814.7582392
Iteration :  76   Loss :  2436.25978344
Iteration :  77   Loss :  1825.5351426
Iteration :  78   Loss :  269.864602075
Iteration :  79   Loss :  92.9986297292
Iteration :  80   Loss :  32.0484608393
Iteration :  81   Loss :  11.0442900628
Iteration :  82   Loss :  3.80599691205
Iteration :  83   Loss :  1.31159290568
Iteration :  84   Loss :  0.451990894888
Iteration :  85   Loss :  0.155761569141
Iteration :  86   Loss :  0.0536773344232
Iteration :  87   Loss :  0.0894569263019
Iteration :  88   Loss :  1997.52157385
Iteration :  89   Loss :  1795.47307718
Iteration :  90   Loss :  685.422248525
Iteration :  91   Loss :  119.367840643
Iteration :  92   Loss :  41.1356121854
Iteration :  93   Loss :  14.1758331285
Iteration :  94   Loss :  4.8851648052
Iteration :  95   Loss :  1.68348730953
Iteration :  96   Loss :  0.580150237375
Iteration :  97   Loss :  0.199926839972
Iteration :  98   Loss :  0.0688972248324
Iteration :  99   Loss :  0.0237428230761
[ -3.65163521e-04  -1.56658348e-04  -5.93152176e-05 ...,   7.38475282e-04
   2.88806706e-04   3.65521045e-05]
CROSS VALIDATION 11
Iteration :  0   Loss :  2094.41975751
Iteration :  1   Loss :  436.69670707
Iteration :  2   Loss :  150.491005684
Iteration :  3   Loss :  51.8610340429
Iteration :  4   Loss :  17.8719441722
Iteration :  5   Loss :  6.15888970184
Iteration :  6   Loss :  2.12242842715
Iteration :  7   Loss :  0.731414694279
Iteration :  8   Loss :  0.392144088568
Iteration :  9   Loss :  1654.98924381
Iteration :  10   Loss :  334.880566964
Iteration :  11   Loss :  115.403923342
Iteration :  12   Loss :  39.7695979898
Iteration :  13   Loss :  13.7050879941
Iteration :  14   Loss :  4.72294029662
Iteration :  15   Loss :  1.62758276744
Iteration :  16   Loss :  0.560884851066
Iteration :  17   Loss :  0.457217944086
Iteration :  18   Loss :  2116.54867077
Iteration :  19   Loss :  757.784212495
Iteration :  20   Loss :  295.766171927
Iteration :  21   Loss :  101.924626268
Iteration :  22   Loss :  35.1244679951
Iteration :  23   Loss :  12.1043196047
Iteration :  24   Loss :  4.17129600692
Iteration :  25   Loss :  1.43747942433
Iteration :  26   Loss :  0.495372922937
Iteration :  27   Loss :  0.170711544544
Iteration :  28   Loss :  0.0588292780881
Iteration :  29   Loss :  0.0202732859667
Iteration :  30   Loss :  4774.94257676
Iteration :  31   Loss :  2676.39381771
Iteration :  32   Loss :  666.250339915
Iteration :  33   Loss :  159.506866444
Iteration :  34   Loss :  54.9680095042
Iteration :  35   Loss :  18.9426457695
Iteration :  36   Loss :  6.52786651701
Iteration :  37   Loss :  2.24958233303
Iteration :  38   Loss :  1.01984420714
Iteration :  39   Loss :  3094.71311065
Iteration :  40   Loss :  1039.74795016
Iteration :  41   Loss :  385.362077772
Iteration :  42   Loss :  181.532203915
Iteration :  43   Loss :  144.615718589
Iteration :  44   Loss :  49.8363385293
Iteration :  45   Loss :  17.1742094306
Iteration :  46   Loss :  5.91844180916
Iteration :  47   Loss :  2.03956715387
Iteration :  48   Loss :  0.702859689979
Iteration :  49   Loss :  0.24221401235
Iteration :  50   Loss :  0.0834698996335
Iteration :  51   Loss :  0.0287647443565
Iteration :  52   Loss :  10846.2975955
Iteration :  53   Loss :  2948.51989714
Iteration :  54   Loss :  1171.30551066
Iteration :  55   Loss :  193.182907535
Iteration :  56   Loss :  66.573183551
Iteration :  57   Loss :  22.9419301359
Iteration :  58   Loss :  7.90606863434
Iteration :  59   Loss :  2.72452757377
Iteration :  60   Loss :  0.938905395789
Iteration :  61   Loss :  0.3235582384
Iteration :  62   Loss :  0.111502110975
Iteration :  63   Loss :  0.0384249859109
Iteration :  64   Loss :  0.160337820755
Iteration :  65   Loss :  6670.38772339
Iteration :  66   Loss :  1837.89527574
Iteration :  67   Loss :  238.008207034
Iteration :  68   Loss :  206.38823605
Iteration :  69   Loss :  71.1239006423
Iteration :  70   Loss :  24.5101626885
Iteration :  71   Loss :  8.4465006783
Iteration :  72   Loss :  2.9107670404
Iteration :  73   Loss :  1.00308578501
Iteration :  74   Loss :  0.345675582456
Iteration :  75   Loss :  0.1191240172
Iteration :  76   Loss :  0.0410515876565
Iteration :  77   Loss :  0.0141468772522
Iteration :  78   Loss :  0.00487518625742
Iteration :  79   Loss :  10841.6239461
Iteration :  80   Loss :  6710.10764173
Iteration :  81   Loss :  1500.52965148
Iteration :  82   Loss :  177.523517494
Iteration :  83   Loss :  61.176766959
Iteration :  84   Loss :  21.0822592319
Iteration :  85   Loss :  7.2652033838
Iteration :  86   Loss :  2.50367760056
Iteration :  87   Loss :  0.862797804326
Iteration :  88   Loss :  0.297330635136
Iteration :  89   Loss :  0.10246375935
Iteration :  90   Loss :  0.191207461409
Iteration :  91   Loss :  3401.72016478
Iteration :  92   Loss :  1732.69199092
Iteration :  93   Loss :  161.026283188
Iteration :  94   Loss :  55.4916190256
Iteration :  95   Loss :  19.1230879899
Iteration :  96   Loss :  6.59004910455
Iteration :  97   Loss :  2.27101121029
Iteration :  98   Loss :  0.782618131583
Iteration :  99   Loss :  0.269699743052
[ -5.27153175e-03  -1.13702599e-03  -2.07218554e-03 ...,   2.08034270e-03
   1.73735043e-03   8.61902514e-06]
CROSS VALIDATION 12
Iteration :  0   Loss :  1685.05930688
Iteration :  1   Loss :  2233.13747034
Iteration :  2   Loss :  234.027031762
Iteration :  3   Loss :  80.6485663777
Iteration :  4   Loss :  27.7924785433
Iteration :  5   Loss :  9.57762671143
Iteration :  6   Loss :  3.30056685231
Iteration :  7   Loss :  1.13741554926
Iteration :  8   Loss :  0.391967255804
Iteration :  9   Loss :  0.135076691823
Iteration :  10   Loss :  0.0465490736886
Iteration :  11   Loss :  0.0160413779166
Iteration :  12   Loss :  0.171557426077
Iteration :  13   Loss :  3327.05694496
Iteration :  14   Loss :  2165.61872736
Iteration :  15   Loss :  649.811830623
Iteration :  16   Loss :  132.691011214
Iteration :  17   Loss :  45.7269390852
Iteration :  18   Loss :  15.7580603159
Iteration :  19   Loss :  5.4304195708
Iteration :  20   Loss :  1.87138874479
Iteration :  21   Loss :  0.644903361233
Iteration :  22   Loss :  0.222241555363
Iteration :  23   Loss :  0.0765871476241
Iteration :  24   Loss :  0.0263928641591
Iteration :  25   Loss :  0.00909530254266
Iteration :  26   Loss :  0.341734910689
Iteration :  27   Loss :  4248.69586852
Iteration :  28   Loss :  663.688227608
Iteration :  29   Loss :  202.831681754
Iteration :  30   Loss :  69.8982687013
Iteration :  31   Loss :  24.0877949894
Iteration :  32   Loss :  8.30094762331
Iteration :  33   Loss :  2.86060768432
Iteration :  34   Loss :  0.985800259795
Iteration :  35   Loss :  0.339718779873
Iteration :  36   Loss :  0.117071230456
Iteration :  37   Loss :  0.0403441723346
Iteration :  38   Loss :  0.0139030933136
Iteration :  39   Loss :  0.372052885354
Iteration :  40   Loss :  2171.96635099
Iteration :  41   Loss :  750.9140251
Iteration :  42   Loss :  177.242263665
Iteration :  43   Loss :  61.079843463
Iteration :  44   Loss :  21.0488582143
Iteration :  45   Loss :  7.2536929862
Iteration :  46   Loss :  2.49971097731
Iteration :  47   Loss :  0.861430857628
Iteration :  48   Loss :  0.29685956865
Iteration :  49   Loss :  0.1023014241
Iteration :  50   Loss :  0.0352543171187
Iteration :  51   Loss :  2379.55579644
Iteration :  52   Loss :  2717.52071785
Iteration :  53   Loss :  341.794943995
Iteration :  54   Loss :  117.786701907
Iteration :  55   Loss :  40.5907325135
Iteration :  56   Loss :  13.9880609552
Iteration :  57   Loss :  4.82045622656
Iteration :  58   Loss :  1.66118794496
Iteration :  59   Loss :  0.572465604665
Iteration :  60   Loss :  0.197278621915
Iteration :  61   Loss :  0.0679846166259
Iteration :  62   Loss :  10525.2924435
Iteration :  63   Loss :  7472.15974332
Iteration :  64   Loss :  492.897567685
Iteration :  65   Loss :  169.858506967
Iteration :  66   Loss :  58.5353109459
Iteration :  67   Loss :  20.171981308
Iteration :  68   Loss :  6.95151052099
Iteration :  69   Loss :  2.3955752182
Iteration :  70   Loss :  1.05363573572
Iteration :  71   Loss :  2488.91502124
Iteration :  72   Loss :  3665.14844641
Iteration :  73   Loss :  403.792174928
Iteration :  74   Loss :  139.151703021
Iteration :  75   Loss :  47.9533721945
Iteration :  76   Loss :  16.5253162908
Iteration :  77   Loss :  5.69482532749
Iteration :  78   Loss :  1.96250618989
Iteration :  79   Loss :  0.67630354293
Iteration :  80   Loss :  0.233062440535
Iteration :  81   Loss :  0.080316156489
Iteration :  82   Loss :  0.101264556411
Iteration :  83   Loss :  2349.40646337
Iteration :  84   Loss :  2722.78739974
Iteration :  85   Loss :  341.555997396
Iteration :  86   Loss :  117.704357998
Iteration :  87   Loss :  40.5623557992
Iteration :  88   Loss :  13.9782819937
Iteration :  89   Loss :  4.81708627729
Iteration :  90   Loss :  1.66002661939
Iteration :  91   Loss :  0.572065397723
Iteration :  92   Loss :  0.197140705727
Iteration :  93   Loss :  0.0679370890272
Iteration :  94   Loss :  10525.2658879
Iteration :  95   Loss :  7472.33018505
Iteration :  96   Loss :  492.897306055
Iteration :  97   Loss :  169.858416806
Iteration :  98   Loss :  58.5352798753
Iteration :  99   Loss :  20.1719706007
[-0.04772959 -0.01062789 -0.00354519 ...,  0.0231188   0.0165723
 -0.00125624]
CROSS VALIDATION 13
Iteration :  0   Loss :  2100.46192247
Iteration :  1   Loss :  436.896632657
Iteration :  2   Loss :  150.559902477
Iteration :  3   Loss :  51.8847767172
Iteration :  4   Loss :  17.880126187
Iteration :  5   Loss :  6.16170932384
Iteration :  6   Loss :  2.12340010324
Iteration :  7   Loss :  0.731749545698
Iteration :  8   Loss :  0.511781965148
Iteration :  9   Loss :  1648.32727312
Iteration :  10   Loss :  564.503396885
Iteration :  11   Loss :  354.029256948
Iteration :  12   Loss :  122.002795206
Iteration :  13   Loss :  42.0436496304
Iteration :  14   Loss :  14.488753895
Iteration :  15   Loss :  4.99300111375
Iteration :  16   Loss :  1.72064901527
Iteration :  17   Loss :  0.592956613925
Iteration :  18   Loss :  0.204340073355
Iteration :  19   Loss :  0.0704180788243
Iteration :  20   Loss :  0.347795211177
Iteration :  21   Loss :  4814.8284061
Iteration :  22   Loss :  422.78958737
Iteration :  23   Loss :  1382.98483871
Iteration :  24   Loss :  495.37033021
Iteration :  25   Loss :  155.680491396
Iteration :  26   Loss :  75.2668480122
Iteration :  27   Loss :  25.9378728272
Iteration :  28   Loss :  8.93850698636
Iteration :  29   Loss :  3.08031840844
Iteration :  30   Loss :  1.06151525214
Iteration :  31   Loss :  0.365811088696
Iteration :  32   Loss :  0.126062957968
Iteration :  33   Loss :  5340.36196902
Iteration :  34   Loss :  3332.93172077
Iteration :  35   Loss :  2066.84624773
Iteration :  36   Loss :  297.796927751
Iteration :  37   Loss :  102.624449466
Iteration :  38   Loss :  35.3656355954
Iteration :  39   Loss :  12.187428898
Iteration :  40   Loss :  4.19993648194
Iteration :  41   Loss :  1.44734928096
Iteration :  42   Loss :  0.498774195779
Iteration :  43   Loss :  0.171883664605
Iteration :  44   Loss :  0.0592332049412
Iteration :  45   Loss :  0.0204124840815
Iteration :  46   Loss :  10748.8784637
Iteration :  47   Loss :  7713.52440036
Iteration :  48   Loss :  1502.80664431
Iteration :  49   Loss :  177.695029928
Iteration :  50   Loss :  61.2358722335
Iteration :  51   Loss :  21.1026276296
Iteration :  52   Loss :  7.27222258179
Iteration :  53   Loss :  2.5060965017
Iteration :  54   Loss :  0.863631387131
Iteration :  55   Loss :  0.297617897927
Iteration :  56   Loss :  0.102562753608
Iteration :  57   Loss :  0.185495690608
Iteration :  58   Loss :  1555.27843998
Iteration :  59   Loss :  426.71373246
Iteration :  60   Loss :  3163.91911157
Iteration :  61   Loss :  318.214620506
Iteration :  62   Loss :  959.552053766
Iteration :  63   Loss :  235.804011038
Iteration :  64   Loss :  81.2609350858
Iteration :  65   Loss :  28.0035082608
Iteration :  66   Loss :  9.65035012314
Iteration :  67   Loss :  3.325628226
Iteration :  68   Loss :  1.14605200397
Iteration :  69   Loss :  6280.3047291
Iteration :  70   Loss :  1303.85704952
Iteration :  71   Loss :  556.751986298
Iteration :  72   Loss :  4274.56125863
Iteration :  73   Loss :  453.257245071
Iteration :  74   Loss :  5734.76412593
Iteration :  75   Loss :  191.8122808
Iteration :  76   Loss :  66.1008488795
Iteration :  77   Loss :  22.7791578535
Iteration :  78   Loss :  7.84997532271
Iteration :  79   Loss :  2.70519713518
Iteration :  80   Loss :  0.932243891141
Iteration :  81   Loss :  0.321262602738
Iteration :  82   Loss :  0.173322952618
Iteration :  83   Loss :  4641.20937406
Iteration :  84   Loss :  2669.32303585
Iteration :  85   Loss :  624.548046469
Iteration :  86   Loss :  159.741523486
Iteration :  87   Loss :  55.0488751799
Iteration :  88   Loss :  18.9705130666
Iteration :  89   Loss :  6.53746992709
Iteration :  90   Loss :  2.25289178514
Iteration :  91   Loss :  0.776373956924
Iteration :  92   Loss :  3091.83189462
Iteration :  93   Loss :  1050.30972136
Iteration :  94   Loss :  385.853434836
Iteration :  95   Loss :  247.772949744
Iteration :  96   Loss :  143.231383761
Iteration :  97   Loss :  49.3592798819
Iteration :  98   Loss :  17.0098092086
Iteration :  99   Loss :  5.86178748969
[-0.02017978 -0.00247109 -0.01692959 ...,  0.00648953  0.00997245
  0.0025627 ]
CROSS VALIDATION 14
Iteration :  0   Loss :  2100.46192247
Iteration :  1   Loss :  319.2031121
Iteration :  2   Loss :  110.001281392
Iteration :  3   Loss :  37.9077817515
Iteration :  4   Loss :  13.0634834353
Iteration :  5   Loss :  4.50183554875
Iteration :  6   Loss :  1.55138737753
Iteration :  7   Loss :  0.534626991387
Iteration :  8   Loss :  0.184238974778
Iteration :  9   Loss :  11126.0188559
Iteration :  10   Loss :  1279.28992546
Iteration :  11   Loss :  2330.14490837
Iteration :  12   Loss :  481.200816438
Iteration :  13   Loss :  554.711041957
Iteration :  14   Loss :  4428.46466546
Iteration :  15   Loss :  403.327457545
Iteration :  16   Loss :  156.533349648
Iteration :  17   Loss :  53.9432993888
Iteration :  18   Loss :  18.5895181793
Iteration :  19   Loss :  6.40617444716
Iteration :  20   Loss :  2.20764576314
Iteration :  21   Loss :  0.760781626494
Iteration :  22   Loss :  0.262174617357
Iteration :  23   Loss :  0.0903485673056
Iteration :  24   Loss :  0.0311352170415
Iteration :  25   Loss :  0.0107295751237
Iteration :  26   Loss :  8794.83210852
Iteration :  27   Loss :  1424.14062492
Iteration :  28   Loss :  881.390606575
Iteration :  29   Loss :  326.099779932
Iteration :  30   Loss :  408.422974963
Iteration :  31   Loss :  958.303687181
Iteration :  32   Loss :  169.299817973
Iteration :  33   Loss :  58.3427799121
Iteration :  34   Loss :  20.1056327681
Iteration :  35   Loss :  6.92864600579
Iteration :  36   Loss :  2.3876958277
Iteration :  37   Loss :  0.822829072353
Iteration :  38   Loss :  4792.37359002
Iteration :  39   Loss :  4444.76708633
Iteration :  40   Loss :  568.167727884
Iteration :  41   Loss :  197.044087083
Iteration :  42   Loss :  67.9037930653
Iteration :  43   Loss :  23.4004743858
Iteration :  44   Loss :  8.06408856946
Iteration :  45   Loss :  2.77898316863
Iteration :  46   Loss :  0.957671457227
Iteration :  47   Loss :  0.330025251805
Iteration :  48   Loss :  0.113730722584
Iteration :  49   Loss :  0.0391929926234
Iteration :  50   Loss :  0.0135063827599
Iteration :  51   Loss :  9409.61886735
Iteration :  52   Loss :  933.423674565
Iteration :  53   Loss :  716.181025266
Iteration :  54   Loss :  188.917887251
Iteration :  55   Loss :  65.103405599
Iteration :  56   Loss :  22.4354267468
Iteration :  57   Loss :  7.73152139556
Iteration :  58   Loss :  2.66437646872
Iteration :  59   Loss :  0.918176592147
Iteration :  60   Loss :  0.316414840118
Iteration :  61   Loss :  0.394227978208
Iteration :  62   Loss :  6550.77800947
Iteration :  63   Loss :  1831.27992036
Iteration :  64   Loss :  1227.57138931
Iteration :  65   Loss :  952.416181115
Iteration :  66   Loss :  119.491892199
Iteration :  67   Loss :  41.178361863
Iteration :  68   Loss :  14.1905651883
Iteration :  69   Loss :  4.89024165246
Iteration :  70   Loss :  1.68523685295
Iteration :  71   Loss :  0.580753151354
Iteration :  72   Loss :  0.200134611474
Iteration :  73   Loss :  0.0689688254235
Iteration :  74   Loss :  0.0237674975121
Iteration :  75   Loss :  10795.8340097
Iteration :  76   Loss :  3115.10539533
Iteration :  77   Loss :  1202.49826178
Iteration :  78   Loss :  193.210114973
Iteration :  79   Loss :  66.5825595655
Iteration :  80   Loss :  22.9451612247
Iteration :  81   Loss :  7.90718210692
Iteration :  82   Loss :  2.72491128999
Iteration :  83   Loss :  0.939037629072
Iteration :  84   Loss :  0.3236038076
Iteration :  85   Loss :  0.111517814677
Iteration :  86   Loss :  0.0384303975979
Iteration :  87   Loss :  0.17495371686
Iteration :  88   Loss :  6688.6886601
Iteration :  89   Loss :  1839.82566012
Iteration :  90   Loss :  236.843968104
Iteration :  91   Loss :  206.445736017
Iteration :  92   Loss :  71.1437158317
Iteration :  93   Loss :  24.5169912442
Iteration :  94   Loss :  8.44885388178
Iteration :  95   Loss :  2.91157798298
Iteration :  96   Loss :  1.0033652457
Iteration :  97   Loss :  0.345771888017
Iteration :  98   Loss :  0.11915720527
Iteration :  99   Loss :  0.0410630246695
[ -1.42141133e-03  -5.77936650e-04  -3.66168495e-04 ...,   8.17161811e-04
   4.33065866e-04  -6.20271868e-05]
CROSS VALIDATION 15
Iteration :  0   Loss :  2100.46192247
Iteration :  1   Loss :  437.624404961
Iteration :  2   Loss :  150.81070168
Iteration :  3   Loss :  51.971205178
Iteration :  4   Loss :  17.9099104876
Iteration :  5   Loss :  6.17197335669
Iteration :  6   Loss :  2.12693721401
Iteration :  7   Loss :  0.732968477164
Iteration :  8   Loss :  0.495281513907
Iteration :  9   Loss :  1648.35350157
Iteration :  10   Loss :  564.486865829
Iteration :  11   Loss :  354.029284142
Iteration :  12   Loss :  122.002804577
Iteration :  13   Loss :  42.0436528599
Iteration :  14   Loss :  14.4887550079
Iteration :  15   Loss :  4.99300149728
Iteration :  16   Loss :  1.72064914743
Iteration :  17   Loss :  0.592956659472
Iteration :  18   Loss :  0.204340089051
Iteration :  19   Loss :  0.0704180842334
Iteration :  20   Loss :  0.343352340178
Iteration :  21   Loss :  2664.39536788
Iteration :  22   Loss :  639.571676686
Iteration :  23   Loss :  1426.94555098
Iteration :  24   Loss :  165.723064284
Iteration :  25   Loss :  57.110187014
Iteration :  26   Loss :  19.6808662383
Iteration :  27   Loss :  6.78226628455
Iteration :  28   Loss :  2.33725159236
Iteration :  29   Loss :  0.805445374277
Iteration :  30   Loss :  0.277566288997
Iteration :  31   Loss :  0.0956527248748
Iteration :  32   Loss :  0.0329630943622
Iteration :  33   Loss :  0.0113594839181
Iteration :  34   Loss :  6695.69617214
Iteration :  35   Loss :  1827.96984115
Iteration :  36   Loss :  1269.26423949
Iteration :  37   Loss :  894.901665911
Iteration :  38   Loss :  119.219509015
Iteration :  39   Loss :  41.0844952991
Iteration :  40   Loss :  14.1582176268
Iteration :  41   Loss :  4.87909428867
Iteration :  42   Loss :  1.68139533558
Iteration :  43   Loss :  0.579429317663
Iteration :  44   Loss :  0.199678402255
Iteration :  45   Loss :  0.0688116101686
Iteration :  46   Loss :  0.0237133192199
Iteration :  47   Loss :  10796.1099191
Iteration :  48   Loss :  3074.78830953
Iteration :  49   Loss :  489.477010024
Iteration :  50   Loss :  168.679741123
Iteration :  51   Loss :  58.1290938748
Iteration :  52   Loss :  20.0319939562
Iteration :  53   Loss :  6.90326917405
Iteration :  54   Loss :  2.3789506623
Iteration :  55   Loss :  0.819815381813
Iteration :  56   Loss :  0.282518368668
Iteration :  57   Loss :  0.0973592718626
Iteration :  58   Loss :  0.0335511912457
Iteration :  59   Loss :  2482.7289012
Iteration :  60   Loss :  498.503785569
Iteration :  61   Loss :  1522.57151856
Iteration :  62   Loss :  166.043936631
Iteration :  63   Loss :  57.220763534
Iteration :  64   Loss :  19.7189722543
Iteration :  65   Loss :  6.79539808192
Iteration :  66   Loss :  2.34177696972
Iteration :  67   Loss :  0.80700487445
Iteration :  68   Loss :  0.278103711756
Iteration :  69   Loss :  0.0958379273053
Iteration :  70   Loss :  0.0330269173762
Iteration :  71   Loss :  0.379300306282
Iteration :  72   Loss :  6696.63588356
Iteration :  73   Loss :  1828.77565905
Iteration :  74   Loss :  1269.6732176
Iteration :  75   Loss :  895.059778681
Iteration :  76   Loss :  119.219155122
Iteration :  77   Loss :  41.0843733434
Iteration :  78   Loss :  14.1581755993
Iteration :  79   Loss :  4.8790798055
Iteration :  80   Loss :  1.6813903445
Iteration :  81   Loss :  0.579427597678
Iteration :  82   Loss :  0.199677809527
Iteration :  83   Loss :  0.0688114059073
Iteration :  84   Loss :  0.0237132488289
Iteration :  85   Loss :  10796.1086557
Iteration :  86   Loss :  3074.78901361
Iteration :  87   Loss :  489.476983604
Iteration :  88   Loss :  168.679732018
Iteration :  89   Loss :  58.1290907373
Iteration :  90   Loss :  20.031992875
Iteration :  91   Loss :  6.90326880145
Iteration :  92   Loss :  2.3789505339
Iteration :  93   Loss :  0.819815337564
Iteration :  94   Loss :  0.28251835342
Iteration :  95   Loss :  0.0973592666077
Iteration :  96   Loss :  0.0335511894348
Iteration :  97   Loss :  2482.72890052
Iteration :  98   Loss :  498.503785528
Iteration :  99   Loss :  1522.57151878
[-0.1006721  -0.06189001 -0.00362222 ...,  0.04857131 -0.02739922
  0.0125922 ]
CROSS VALIDATION 16
Iteration :  0   Loss :  2100.46192247
Iteration :  1   Loss :  382.563208901
Iteration :  2   Loss :  159.483648176
Iteration :  3   Loss :  54.9600082063
Iteration :  4   Loss :  18.9398884248
Iteration :  5   Loss :  6.52691630242
Iteration :  6   Loss :  2.2492548775
Iteration :  7   Loss :  0.775120634241
Iteration :  8   Loss :  0.267116014123
Iteration :  9   Loss :  0.0920514328342
Iteration :  10   Loss :  0.391980135335
Iteration :  11   Loss :  3428.02203952
Iteration :  12   Loss :  367.05618857
Iteration :  13   Loss :  126.492034554
Iteration :  14   Loss :  43.5906962038
Iteration :  15   Loss :  15.0218849924
Iteration :  16   Loss :  5.17672458524
Iteration :  17   Loss :  1.78396236191
Iteration :  18   Loss :  0.614775164547
Iteration :  19   Loss :  0.211859011722
Iteration :  20   Loss :  11526.9689936
Iteration :  21   Loss :  7708.22649146
Iteration :  22   Loss :  2706.31999634
Iteration :  23   Loss :  164.91888189
Iteration :  24   Loss :  56.8330559634
Iteration :  25   Loss :  19.5853635019
Iteration :  26   Loss :  6.74935487803
Iteration :  27   Loss :  2.32590992071
Iteration :  28   Loss :  0.801536896044
Iteration :  29   Loss :  0.278384338989
Iteration :  30   Loss :  4357.86155429
Iteration :  31   Loss :  1247.99888778
Iteration :  32   Loss :  258.706586951
Iteration :  33   Loss :  89.1534417757
Iteration :  34   Loss :  30.7233622233
Iteration :  35   Loss :  10.5876449355
Iteration :  36   Loss :  3.64863143775
Iteration :  37   Loss :  1.25736284599
Iteration :  38   Loss :  0.433302555617
Iteration :  39   Loss :  0.149321339741
Iteration :  40   Loss :  0.0514579529086
Iteration :  41   Loss :  0.140522069494
Iteration :  42   Loss :  1709.74218924
Iteration :  43   Loss :  2728.77153477
Iteration :  44   Loss :  267.411892842
Iteration :  45   Loss :  92.1533962455
Iteration :  46   Loss :  31.7571830831
Iteration :  47   Loss :  10.9439121992
Iteration :  48   Loss :  3.77140547734
Iteration :  49   Loss :  1.29967227583
Iteration :  50   Loss :  0.447882900611
Iteration :  51   Loss :  0.154345904264
Iteration :  52   Loss :  0.0531894790591
Iteration :  53   Loss :  0.285306271007
Iteration :  54   Loss :  4024.71209105
Iteration :  55   Loss :  1930.75747673
Iteration :  56   Loss :  239.849188615
Iteration :  57   Loss :  82.6549525628
Iteration :  58   Loss :  28.4839036672
Iteration :  59   Loss :  9.815900233
Iteration :  60   Loss :  3.38267881082
Iteration :  61   Loss :  1.16571233056
Iteration :  62   Loss :  0.401718671389
Iteration :  63   Loss :  0.138437148439
Iteration :  64   Loss :  0.0477071279799
Iteration :  65   Loss :  0.329501944782
Iteration :  66   Loss :  2357.31455128
Iteration :  67   Loss :  1807.43366496
Iteration :  68   Loss :  249.691595184
Iteration :  69   Loss :  86.04676578
Iteration :  70   Loss :  29.6527638254
Iteration :  71   Loss :  10.2187036842
Iteration :  72   Loss :  3.52148978758
Iteration :  73   Loss :  1.21354828434
Iteration :  74   Loss :  0.418203523862
Iteration :  75   Loss :  0.144118029442
Iteration :  76   Loss :  0.0496648287857
Iteration :  77   Loss :  0.0171151050834
Iteration :  78   Loss :  6325.03406849
Iteration :  79   Loss :  521.210352627
Iteration :  80   Loss :  1695.29768196
Iteration :  81   Loss :  519.385240397
Iteration :  82   Loss :  200.109313456
Iteration :  83   Loss :  68.9601074182
Iteration :  84   Loss :  23.7644931813
Iteration :  85   Loss :  8.18953388135
Iteration :  86   Loss :  2.82221315145
Iteration :  87   Loss :  0.972569035993
Iteration :  88   Loss :  0.33515913895
Iteration :  89   Loss :  0.115499922642
Iteration :  90   Loss :  0.0398026805183
Iteration :  91   Loss :  0.0137164886365
Iteration :  92   Loss :  9878.54850083
Iteration :  93   Loss :  6386.1240141
Iteration :  94   Loss :  484.068709159
Iteration :  95   Loss :  166.815974753
Iteration :  96   Loss :  57.486817277
Iteration :  97   Loss :  19.8106576155
Iteration :  98   Loss :  6.82699397444
Iteration :  99   Loss :  2.35266529923
[-0.01206356 -0.0056614  -0.00571412 ...,  0.00700917  0.00229291
  0.0017177 ]
CROSS VALIDATION 17
Iteration :  0   Loss :  2100.46192247
Iteration :  1   Loss :  484.02046655
Iteration :  2   Loss :  166.799349763
Iteration :  3   Loss :  57.4810881027
Iteration :  4   Loss :  19.8086832723
Iteration :  5   Loss :  6.82631359169
Iteration :  6   Loss :  2.35243083105
Iteration :  7   Loss :  0.810676324862
Iteration :  8   Loss :  0.279368938299
Iteration :  9   Loss :  0.0962739397866
Iteration :  10   Loss :  0.427444555254
Iteration :  11   Loss :  1948.30048382
Iteration :  12   Loss :  455.499388205
Iteration :  13   Loss :  280.411719217
Iteration :  14   Loss :  96.6332947954
Iteration :  15   Loss :  33.3010107035
Iteration :  16   Loss :  11.4759340062
Iteration :  17   Loss :  3.95474667384
Iteration :  18   Loss :  1.36285388586
Iteration :  19   Loss :  0.469656053189
Iteration :  20   Loss :  0.161849197912
Iteration :  21   Loss :  0.0557752054654
Iteration :  22   Loss :  0.32899174749
Iteration :  23   Loss :  4353.48174601
Iteration :  24   Loss :  1813.34581015
Iteration :  25   Loss :  1260.59119341
Iteration :  26   Loss :  398.075332619
Iteration :  27   Loss :  127.98570211
Iteration :  28   Loss :  44.1054322414
Iteration :  29   Loss :  15.1992693022
Iteration :  30   Loss :  5.23785337952
Iteration :  31   Loss :  1.80502808917
Iteration :  32   Loss :  0.622034670812
Iteration :  33   Loss :  0.214360726027
Iteration :  34   Loss :  0.07387131782
Iteration :  35   Loss :  0.0254569561207
Iteration :  36   Loss :  0.00877277723014
Iteration :  37   Loss :  5812.80728239
Iteration :  38   Loss :  4655.59938148
Iteration :  39   Loss :  559.34172868
Iteration :  40   Loss :  196.784813203
Iteration :  41   Loss :  67.8144441275
Iteration :  42   Loss :  23.3696836533
Iteration :  43   Loss :  8.0534777079
Iteration :  44   Loss :  2.77532653646
Iteration :  45   Loss :  0.956411337232
Iteration :  46   Loss :  0.329590999102
Iteration :  47   Loss :  0.113581073812
Iteration :  48   Loss :  0.0391414218334
Iteration :  49   Loss :  0.0134886108373
Iteration :  50   Loss :  10015.8868329
Iteration :  51   Loss :  938.583871506
Iteration :  52   Loss :  723.041128338
Iteration :  53   Loss :  189.003326533
Iteration :  54   Loss :  65.1328490166
Iteration :  55   Loss :  22.4455733072
Iteration :  56   Loss :  7.7350180239
Iteration :  57   Loss :  2.66558144945
Iteration :  58   Loss :  0.918591843185
Iteration :  59   Loss :  0.316557940685
Iteration :  60   Loss :  0.411497257771
Iteration :  61   Loss :  4256.74926791
Iteration :  62   Loss :  2753.62933687
Iteration :  63   Loss :  360.26947262
Iteration :  64   Loss :  124.153249553
Iteration :  65   Loss :  42.7847223982
Iteration :  66   Loss :  14.7441365996
Iteration :  67   Loss :  5.08100910515
Iteration :  68   Loss :  1.75097764133
Iteration :  69   Loss :  0.603408227973
Iteration :  70   Loss :  0.207941826892
Iteration :  71   Loss :  0.0716592869744
Iteration :  72   Loss :  0.0246946633414
Iteration :  73   Loss :  0.0085100818511
Iteration :  74   Loss :  8603.18136582
Iteration :  75   Loss :  2504.89613926
Iteration :  76   Loss :  1050.90813686
Iteration :  77   Loss :  164.772151843
Iteration :  78   Loss :  56.7824910015
Iteration :  79   Loss :  19.5679381999
Iteration :  80   Loss :  6.74334990669
Iteration :  81   Loss :  2.32384053442
Iteration :  82   Loss :  0.800823760319
Iteration :  83   Loss :  0.27597362452
Iteration :  84   Loss :  0.0951038732922
Iteration :  85   Loss :  0.20464961958
Iteration :  86   Loss :  4323.4113083
Iteration :  87   Loss :  2862.42914849
Iteration :  88   Loss :  359.593042121
Iteration :  89   Loss :  123.920143362
Iteration :  90   Loss :  42.7043911647
Iteration :  91   Loss :  14.7164534778
Iteration :  92   Loss :  5.07146916412
Iteration :  93   Loss :  1.74769006143
Iteration :  94   Loss :  0.602275287883
Iteration :  95   Loss :  0.207551401933
Iteration :  96   Loss :  0.0715247417771
Iteration :  97   Loss :  0.0246482974272
Iteration :  98   Loss :  0.00849410359218
Iteration :  99   Loss :  8603.39569137
[-0.00798725 -0.06061252  0.01413423 ...,  0.03253523  0.00432532
 -0.00010964]
CROSS VALIDATION 18
Iteration :  0   Loss :  1874.44472444
Iteration :  1   Loss :  3226.92623622
Iteration :  2   Loss :  202.844735011
Iteration :  3   Loss :  69.9027670128
Iteration :  4   Loss :  24.0893451623
Iteration :  5   Loss :  8.3014818318
Iteration :  6   Loss :  3.1407648186
Iteration :  7   Loss :  3497.55182117
Iteration :  8   Loss :  1899.57905102
Iteration :  9   Loss :  727.404196605
Iteration :  10   Loss :  666.01153704
Iteration :  11   Loss :  302.346723155
Iteration :  12   Loss :  1018.34809091
Iteration :  13   Loss :  1733.19633905
Iteration :  14   Loss :  988.416739575
Iteration :  15   Loss :  346.914172877
Iteration :  16   Loss :  146.392109415
Iteration :  17   Loss :  50.4485044505
Iteration :  18   Loss :  17.3851692653
Iteration :  19   Loss :  5.99114113838
Iteration :  20   Loss :  2.06462022844
Iteration :  21   Loss :  0.711493284706
Iteration :  22   Loss :  0.24518925428
Iteration :  23   Loss :  0.0844952042507
Iteration :  24   Loss :  0.0291180768192
Iteration :  25   Loss :  0.415942232334
Iteration :  26   Loss :  1182.49693368
Iteration :  27   Loss :  511.572369068
Iteration :  28   Loss :  461.420467109
Iteration :  29   Loss :  139.229844546
Iteration :  30   Loss :  47.9803007161
Iteration :  31   Loss :  16.5345961874
Iteration :  32   Loss :  5.69802329292
Iteration :  33   Loss :  1.96360824774
Iteration :  34   Loss :  0.676683325492
Iteration :  35   Loss :  0.233193318233
Iteration :  36   Loss :  0.227413513407
Iteration :  37   Loss :  4825.48127471
Iteration :  38   Loss :  913.230579304
Iteration :  39   Loss :  869.972340517
Iteration :  40   Loss :  1769.49097301
Iteration :  41   Loss :  799.951648111
Iteration :  42   Loss :  2029.58242676
Iteration :  43   Loss :  296.712314259
Iteration :  44   Loss :  102.250678443
Iteration :  45   Loss :  35.236829547
Iteration :  46   Loss :  12.1430407644
Iteration :  47   Loss :  4.18463979031
Iteration :  48   Loss :  1.44207785466
Iteration :  49   Loss :  0.496957598052
Iteration :  50   Loss :  8010.80837952
Iteration :  51   Loss :  1312.39115986
Iteration :  52   Loss :  2878.63878219
Iteration :  53   Loss :  357.100562075
Iteration :  54   Loss :  123.061204371
Iteration :  55   Loss :  42.4083903238
Iteration :  56   Loss :  14.6144479817
Iteration :  57   Loss :  5.0363168274
Iteration :  58   Loss :  1.73557613792
Iteration :  59   Loss :  0.598100682255
Iteration :  60   Loss :  0.206112781974
Iteration :  61   Loss :  0.0710289758117
Iteration :  62   Loss :  0.0244774504354
Iteration :  63   Loss :  8707.92184737
Iteration :  64   Loss :  1362.92871316
Iteration :  65   Loss :  2945.43619515
Iteration :  66   Loss :  356.471343617
Iteration :  67   Loss :  122.844368025
Iteration :  68   Loss :  42.333665877
Iteration :  69   Loss :  14.588697027
Iteration :  70   Loss :  5.02744273466
Iteration :  71   Loss :  1.73251801745
Iteration :  72   Loss :  0.597046816681
Iteration :  73   Loss :  0.205749607057
Iteration :  74   Loss :  0.0709038213108
Iteration :  75   Loss :  0.0244343206696
Iteration :  76   Loss :  8708.62948368
Iteration :  77   Loss :  1362.9867709
Iteration :  78   Loss :  2945.50188871
Iteration :  79   Loss :  356.470730903
Iteration :  80   Loss :  122.844156876
Iteration :  81   Loss :  42.3335931125
Iteration :  82   Loss :  14.5886719514
Iteration :  83   Loss :  5.02743409333
Iteration :  84   Loss :  1.73251503954
Iteration :  85   Loss :  0.597045790457
Iteration :  86   Loss :  0.205749253408
Iteration :  87   Loss :  0.070903699439
Iteration :  88   Loss :  0.0244342786711
Iteration :  89   Loss :  8708.63017944
Iteration :  90   Loss :  1362.98682798
Iteration :  91   Loss :  2945.5019533
Iteration :  92   Loss :  356.4707303
Iteration :  93   Loss :  122.844156668
Iteration :  94   Loss :  42.333593041
Iteration :  95   Loss :  14.5886719268
Iteration :  96   Loss :  5.02743408483
Iteration :  97   Loss :  1.73251503661
Iteration :  98   Loss :  0.597045789448
Iteration :  99   Loss :  0.20574925306
[-0.0058503  -0.00236491 -0.00199104 ...,  0.00278092  0.00074717
  0.00028368]
CROSS VALIDATION 19
Iteration :  0   Loss :  2570.42717482
Iteration :  1   Loss :  1362.19500487
Iteration :  2   Loss :  375.120776439
Iteration :  3   Loss :  129.271189788
Iteration :  4   Loss :  44.5484269567
Iteration :  5   Loss :  15.3519306782
Iteration :  6   Loss :  5.29046234963
Iteration :  7   Loss :  1.82315778122
Iteration :  8   Loss :  0.628282383574
Iteration :  9   Loss :  0.216513763962
Iteration :  10   Loss :  0.0746132809238
Iteration :  11   Loss :  0.0693191095102
Iteration :  12   Loss :  4542.14751746
Iteration :  13   Loss :  788.799395922
Iteration :  14   Loss :  1107.87809993
Iteration :  15   Loss :  255.851537325
Iteration :  16   Loss :  88.1695568903
Iteration :  17   Loss :  30.3843035047
Iteration :  18   Loss :  10.4708011703
Iteration :  19   Loss :  3.60836565268
Iteration :  20   Loss :  1.24348676589
Iteration :  21   Loss :  0.563781583032
Iteration :  22   Loss :  6001.48059903
Iteration :  23   Loss :  653.952469354
Iteration :  24   Loss :  1952.31075547
Iteration :  25   Loss :  212.418112819
Iteration :  26   Loss :  73.2018696411
Iteration :  27   Loss :  25.2262561221
Iteration :  28   Loss :  8.69327519993
Iteration :  29   Loss :  2.99580854709
Iteration :  30   Loss :  1.0323921243
Iteration :  31   Loss :  0.355774904027
Iteration :  32   Loss :  0.122604366457
Iteration :  33   Loss :  0.17693791879
Iteration :  34   Loss :  2331.8533518
Iteration :  35   Loss :  575.323542627
Iteration :  36   Loss :  1977.82422248
Iteration :  37   Loss :  669.61174924
Iteration :  38   Loss :  111.104565318
Iteration :  39   Loss :  38.287986834
Iteration :  40   Loss :  13.1945067388
Iteration :  41   Loss :  4.54698777547
Iteration :  42   Loss :  1.56694738496
Iteration :  43   Loss :  0.539989159521
Iteration :  44   Loss :  0.186086843247
Iteration :  45   Loss :  0.0641277933434
Iteration :  46   Loss :  3715.17962274
Iteration :  47   Loss :  724.391417815
Iteration :  48   Loss :  324.807443971
Iteration :  49   Loss :  111.932602434
Iteration :  50   Loss :  38.5733385127
Iteration :  51   Loss :  13.2928424039
Iteration :  52   Loss :  4.58087544369
Iteration :  53   Loss :  1.57862548829
Iteration :  54   Loss :  0.544013576209
Iteration :  55   Loss :  0.187473706268
Iteration :  56   Loss :  0.0646057232372
Iteration :  57   Loss :  0.0222639193415
Iteration :  58   Loss :  1768.65161195
Iteration :  59   Loss :  1319.82367261
Iteration :  60   Loss :  185.90937004
Iteration :  61   Loss :  64.0666339142
Iteration :  62   Loss :  22.0781425929
Iteration :  63   Loss :  7.60839692319
Iteration :  64   Loss :  1824.41477956
Iteration :  65   Loss :  1790.52910372
Iteration :  66   Loss :  347.436002056
Iteration :  67   Loss :  565.04098511
Iteration :  68   Loss :  1480.31254921
Iteration :  69   Loss :  1242.70693562
Iteration :  70   Loss :  1317.55514798
Iteration :  71   Loss :  120.353742503
Iteration :  72   Loss :  41.4753659779
Iteration :  73   Loss :  14.2929164248
Iteration :  74   Loss :  4.92551313554
Iteration :  75   Loss :  1.69739183575
Iteration :  76   Loss :  0.584941906513
Iteration :  77   Loss :  0.201578107535
Iteration :  78   Loss :  0.408150516276
Iteration :  79   Loss :  3385.24419033
Iteration :  80   Loss :  580.598568362
Iteration :  81   Loss :  134.282048644
Iteration :  82   Loss :  46.2752299674
Iteration :  83   Loss :  15.947008034
Iteration :  84   Loss :  5.49553325645
Iteration :  85   Loss :  1.89382771416
Iteration :  86   Loss :  0.652636103462
Iteration :  87   Loss :  0.224906352546
Iteration :  88   Loss :  0.0775054691994
Iteration :  89   Loss :  3644.56995985
Iteration :  90   Loss :  698.468896413
Iteration :  91   Loss :  407.966792141
Iteration :  92   Loss :  172.574721513
Iteration :  93   Loss :  59.4713515712
Iteration :  94   Loss :  20.4945523116
Iteration :  95   Loss :  7.06267241882
Iteration :  96   Loss :  2.43388296252
Iteration :  97   Loss :  0.838745721726
Iteration :  98   Loss :  0.289041994437
Iteration :  99   Loss :  0.0996073927822
[-0.00295107 -0.00114307 -0.00180022 ...,  0.00075639  0.00054344
  0.00027756]
Accuracy (Hinge Loss):	0.75
lmda : 1  eta : 0.001
Logistic Loss
CROSS VALIDATION 0
Iteration :  0   Loss :  111.654209534
Iteration :  1   Loss :  18.0299416388
Iteration :  2   Loss :  14.5569141631
Iteration :  3   Loss :  11.7735300768
Iteration :  4   Loss :  9.5223485502
Iteration :  5   Loss :  7.7016087242
Iteration :  6   Loss :  6.2290071463
Iteration :  7   Loss :  5.03797744232
Iteration :  8   Loss :  4.07468152113
Iteration :  9   Loss :  3.29557660016
Iteration :  10   Loss :  2.6654486109
Iteration :  11   Loss :  2.15582337976
Iteration :  12   Loss :  1.74368068882
Iteration :  13   Loss :  1.41039925978
Iteration :  14   Loss :  1.14088005942
Iteration :  15   Loss :  0.922895306987
Iteration :  16   Loss :  0.746633088613
Iteration :  17   Loss :  0.604214757712
Iteration :  18   Loss :  0.489264727697
Iteration :  19   Loss :  0.39658028384
Iteration :  20   Loss :  0.321901904982
Iteration :  21   Loss :  0.261712289274
Iteration :  22   Loss :  0.213124600036
Iteration :  23   Loss :  0.173865200762
Iteration :  24   Loss :  0.142151741079
Iteration :  25   Loss :  0.116581870638
Iteration :  26   Loss :  0.0959784327947
Iteration :  27   Loss :  0.0793689303414
Iteration :  28   Loss :  0.0659921856171
Iteration :  29   Loss :  0.0552503431248
Iteration :  30   Loss :  0.0466760969169
Iteration :  31   Loss :  0.0399195692236
Iteration :  32   Loss :  0.0346402668539
Iteration :  33   Loss :  0.0305252508971
Iteration :  34   Loss :  0.0273849571125
Iteration :  35   Loss :  0.0250206843246
Iteration :  36   Loss :  0.0231684191751
Iteration :  37   Loss :  0.0216691842327
Iteration :  38   Loss :  0.0204851399942
Iteration :  39   Loss :  0.0196534443471
Iteration :  40   Loss :  0.0191267246144
Iteration :  41   Loss :  0.0186201881483
Iteration :  42   Loss :  0.0180875627112
Iteration :  43   Loss :  0.0177774380822
Iteration :  44   Loss :  0.0178254427731
[ -2.77111492e-04  -9.33697146e-05   6.51868160e-06 ...,   1.22865840e-04
   8.89002370e-05   3.21742322e-05]
CROSS VALIDATION 1
Iteration :  0   Loss :  81.7017894408
Iteration :  1   Loss :  19.6402038928
Iteration :  2   Loss :  15.8848591573
Iteration :  3   Loss :  12.8475626743
Iteration :  4   Loss :  10.3910185816
Iteration :  5   Loss :  8.40418294898
Iteration :  6   Loss :  6.79724422499
Iteration :  7   Loss :  5.49756345983
Iteration :  8   Loss :  4.4463907998
Iteration :  9   Loss :  3.59620987166
Iteration :  10   Loss :  2.90859016122
Iteration :  11   Loss :  2.3524506955
Iteration :  12   Loss :  1.90265727206
Iteration :  13   Loss :  1.53889011496
Iteration :  14   Loss :  1.24472619457
Iteration :  15   Loss :  1.00688100733
Iteration :  16   Loss :  0.814589717336
Iteration :  17   Loss :  0.659142588931
Iteration :  18   Loss :  0.533460744023
Iteration :  19   Loss :  0.431807797626
Iteration :  20   Loss :  0.349670034841
Iteration :  21   Loss :  0.28346845045
Iteration :  22   Loss :  0.230268504372
Iteration :  23   Loss :  0.187564135674
Iteration :  24   Loss :  0.153173618151
Iteration :  25   Loss :  0.125319141665
Iteration :  26   Loss :  0.102736779939
Iteration :  27   Loss :  0.0845139753003
Iteration :  28   Loss :  0.0699053912767
Iteration :  29   Loss :  0.0582448425345
Iteration :  30   Loss :  0.048976641868
Iteration :  31   Loss :  0.0416242256286
Iteration :  32   Loss :  0.0357582841068
Iteration :  33   Loss :  0.0310651189978
Iteration :  34   Loss :  0.0273775233761
Iteration :  35   Loss :  0.0245662111623
Iteration :  36   Loss :  0.0224002649367
Iteration :  37   Loss :  0.0206737858925
Iteration :  38   Loss :  0.0193338674079
Iteration :  39   Loss :  0.018459980741
Iteration :  40   Loss :  0.0180684176218
Iteration :  41   Loss :  0.0178148009962
Iteration :  42   Loss :  0.0175002987027
Iteration :  43   Loss :  0.0172904600633
Iteration :  44   Loss :  0.017355895293
[ -2.68677775e-04  -9.64273780e-05  -2.41922490e-05 ...,   1.09811112e-04
   8.63646262e-05   4.58481867e-05]
CROSS VALIDATION 2
Iteration :  0   Loss :  354.698351936
Iteration :  1   Loss :  85.2090876228
Iteration :  2   Loss :  19.3302325259
Iteration :  3   Loss :  15.6345422298
Iteration :  4   Loss :  12.6454149527
Iteration :  5   Loss :  10.2277891604
Iteration :  6   Loss :  8.27242636399
Iteration :  7   Loss :  6.69096303286
Iteration :  8   Loss :  5.41192876466
Iteration :  9   Loss :  4.37750060909
Iteration :  10   Loss :  3.54088453804
Iteration :  11   Loss :  2.86424119254
Iteration :  12   Loss :  2.31696969094
Iteration :  13   Loss :  1.87432320751
Iteration :  14   Loss :  1.51630787844
Iteration :  15   Loss :  1.22676150479
Iteration :  16   Loss :  0.992609959307
Iteration :  17   Loss :  0.803259177587
Iteration :  18   Loss :  0.650110063909
Iteration :  19   Loss :  0.52622570199
Iteration :  20   Loss :  0.426030894366
Iteration :  21   Loss :  0.345022645469
Iteration :  22   Loss :  0.279566385739
Iteration :  23   Loss :  0.226723066843
Iteration :  24   Loss :  0.184100456537
Iteration :  25   Loss :  0.149796284586
Iteration :  26   Loss :  0.122331753034
Iteration :  27   Loss :  0.100465760667
Iteration :  28   Loss :  0.0830555759484
Iteration :  29   Loss :  0.0691188071232
Iteration :  30   Loss :  0.0579388316937
Iteration :  31   Loss :  0.0489080984465
Iteration :  32   Loss :  0.0415823310181
Iteration :  33   Loss :  0.0356719889328
Iteration :  34   Loss :  0.0309507795959
Iteration :  35   Loss :  0.0272230385916
Iteration :  36   Loss :  0.0243445660865
Iteration :  37   Loss :  0.0222191195125
Iteration :  38   Loss :  0.0206771350916
Iteration :  39   Loss :  0.0194913106797
Iteration :  40   Loss :  0.0186088844175
Iteration :  41   Loss :  0.0179577562663
Iteration :  42   Loss :  0.017352301919
Iteration :  43   Loss :  0.0167870539189
Iteration :  44   Loss :  0.0164607870794
Iteration :  45   Loss :  0.0164907250415
[ -3.34277246e-04  -1.70231717e-04  -5.69727596e-05 ...,   2.18545587e-04
   6.60008518e-05   2.13055278e-05]
CROSS VALIDATION 3
Iteration :  0   Loss :  81.9597226602
Iteration :  1   Loss :  19.6652386277
Iteration :  2   Loss :  15.9051070753
Iteration :  3   Loss :  12.8639390483
Iteration :  4   Loss :  10.4042636781
Iteration :  5   Loss :  8.41489549011
Iteration :  6   Loss :  6.80590845288
Iteration :  7   Loss :  5.50457102475
Iteration :  8   Loss :  4.45205845794
Iteration :  9   Loss :  3.6007937774
Iteration :  10   Loss :  2.91229735538
Iteration :  11   Loss :  2.35544821012
Iteration :  12   Loss :  1.90507914553
Iteration :  13   Loss :  1.54084293802
Iteration :  14   Loss :  1.24629573393
Iteration :  15   Loss :  1.00814341515
Iteration :  16   Loss :  0.815614608573
Iteration :  17   Loss :  0.659978769515
Iteration :  18   Loss :  0.534144256005
Iteration :  19   Loss :  0.43237785011
Iteration :  20   Loss :  0.350180828954
Iteration :  21   Loss :  0.283991351228
Iteration :  22   Loss :  0.230843656129
Iteration :  23   Loss :  0.188118083869
Iteration :  24   Loss :  0.153612654999
Iteration :  25   Loss :  0.125660021171
Iteration :  26   Loss :  0.10303594348
Iteration :  27   Loss :  0.0847918449033
Iteration :  28   Loss :  0.0701615631549
Iteration :  29   Loss :  0.0584769495241
Iteration :  30   Loss :  0.0491870694755
Iteration :  31   Loss :  0.0418189006667
Iteration :  32   Loss :  0.0359445161328
Iteration :  33   Loss :  0.0312520865804
Iteration :  34   Loss :  0.0275756144726
Iteration :  35   Loss :  0.0247796685875
Iteration :  36   Loss :  0.0226208034309
Iteration :  37   Loss :  0.0208914485313
Iteration :  38   Loss :  0.0195287086542
Iteration :  39   Loss :  0.0186064537831
Iteration :  40   Loss :  0.0181741328833
Iteration :  41   Loss :  0.0179400376339
Iteration :  42   Loss :  0.017650680872
Iteration :  43   Loss :  0.0174685917077
Iteration :  44   Loss :  0.0175958022005
Iteration :  45   Loss :  0.0179478438858
Iteration :  46   Loss :  0.0182739401469
Iteration :  47   Loss :  0.0186016678093
Iteration :  48   Loss :  0.0189707468357
Iteration :  49   Loss :  0.019347348579
Iteration :  50   Loss :  0.0196911998481
Iteration :  51   Loss :  0.0200477959531
Iteration :  52   Loss :  0.0204260984749
Iteration :  53   Loss :  0.0207568018212
Iteration :  54   Loss :  0.0209856305504
Iteration :  55   Loss :  0.0211104906617
Iteration :  56   Loss :  0.0211633066017
[ -2.99386887e-04  -1.67216617e-04  -5.83110040e-05 ...,   1.49492021e-04
   7.48484059e-05   2.48778598e-05]
CROSS VALIDATION 4
Iteration :  0   Loss :  81.9597226602
Iteration :  1   Loss :  19.6652386277
Iteration :  2   Loss :  15.9051070753
Iteration :  3   Loss :  12.8639390483
Iteration :  4   Loss :  10.4042636781
Iteration :  5   Loss :  8.41489549011
Iteration :  6   Loss :  6.80590845288
Iteration :  7   Loss :  5.50457102475
Iteration :  8   Loss :  4.45205845794
Iteration :  9   Loss :  3.6007937774
Iteration :  10   Loss :  2.91229735538
Iteration :  11   Loss :  2.35544821012
Iteration :  12   Loss :  1.90507914554
Iteration :  13   Loss :  1.54084293816
Iteration :  14   Loss :  1.24629573535
Iteration :  15   Loss :  1.00814342588
Iteration :  16   Loss :  0.815614673589
Iteration :  17   Loss :  0.659979090134
Iteration :  18   Loss :  0.534145575384
Iteration :  19   Loss :  0.432382480681
Iteration :  20   Loss :  0.350194882927
Iteration :  21   Loss :  0.284028247868
Iteration :  22   Loss :  0.230926220853
Iteration :  23   Loss :  0.188270754073
Iteration :  24   Loss :  0.15383655691
Iteration :  25   Loss :  0.125915157005
Iteration :  26   Loss :  0.103264620883
Iteration :  27   Loss :  0.0849510235447
Iteration :  28   Loss :  0.0702386831392
Iteration :  29   Loss :  0.0584938657191
Iteration :  30   Loss :  0.0491746370431
Iteration :  31   Loss :  0.0417906731728
Iteration :  32   Loss :  0.0359033555178
Iteration :  33   Loss :  0.031204803379
Iteration :  34   Loss :  0.0275271753146
Iteration :  35   Loss :  0.0247287251873
Iteration :  36   Loss :  0.0225660319631
Iteration :  37   Loss :  0.0208340660683
Iteration :  38   Loss :  0.0194745539574
Iteration :  39   Loss :  0.0185643471445
Iteration :  40   Loss :  0.0181445243415
Iteration :  41   Loss :  0.0179049730642
Iteration :  42   Loss :  0.0176027434128
Iteration :  43   Loss :  0.0173977521723
Iteration :  44   Loss :  0.0174873219569
[ -2.81984044e-04  -9.13847704e-05  -3.76999279e-05 ...,   1.46876340e-04
   9.11455046e-05   4.28701367e-05]
CROSS VALIDATION 5
Iteration :  0   Loss :  81.7670598125
Iteration :  1   Loss :  19.6474890639
Iteration :  2   Loss :  15.8907513526
Iteration :  3   Loss :  12.8523282405
Iteration :  4   Loss :  10.3948729381
Iteration :  5   Loss :  8.4073003255
Iteration :  6   Loss :  6.79976553718
Iteration :  7   Loss :  5.49960267884
Iteration :  8   Loss :  4.44804010221
Iteration :  9   Loss :  3.59754379787
Iteration :  10   Loss :  2.90966895657
Iteration :  11   Loss :  2.35332295473
Iteration :  12   Loss :  1.90336197227
Iteration :  13   Loss :  1.53945822907
Iteration :  14   Loss :  1.24518265414
Iteration :  15   Loss :  1.00724787477
Iteration :  16   Loss :  0.814886686238
Iteration :  17   Loss :  0.659384300054
Iteration :  18   Loss :  0.533661985974
Iteration :  19   Loss :  0.431988421454
Iteration :  20   Loss :  0.349862080116
Iteration :  21   Loss :  0.283718804946
Iteration :  22   Loss :  0.230590194875
Iteration :  23   Loss :  0.187847976958
Iteration :  24   Loss :  0.153292887042
Iteration :  25   Loss :  0.12531435804
Iteration :  26   Loss :  0.102689166991
Iteration :  27   Loss :  0.0844431420567
Iteration :  28   Loss :  0.0698137087471
Iteration :  29   Loss :  0.058138835919
Iteration :  30   Loss :  0.0488633215028
Iteration :  31   Loss :  0.0415054839391
Iteration :  32   Loss :  0.0356353740505
Iteration :  33   Loss :  0.0309454836992
Iteration :  34   Loss :  0.0272731307217
Iteration :  35   Loss :  0.0244811912134
Iteration :  36   Loss :  0.0223242547636
Iteration :  37   Loss :  0.0205977515067
Iteration :  38   Loss :  0.0192413641497
Iteration :  39   Loss :  0.018330748817
Iteration :  40   Loss :  0.0179083720487
Iteration :  41   Loss :  0.0176666963289
Iteration :  42   Loss :  0.0173605495284
Iteration :  43   Loss :  0.0171527509549
Iteration :  44   Loss :  0.017247098601
[ -2.65709562e-04  -8.99301728e-05  -2.74167557e-05 ...,   1.22529333e-04
   6.11945706e-05   4.63502301e-05]
CROSS VALIDATION 6
Iteration :  0   Loss :  81.9627227214
Iteration :  1   Loss :  19.6655143868
Iteration :  2   Loss :  15.9053301074
Iteration :  3   Loss :  12.8641194351
Iteration :  4   Loss :  10.4044095736
Iteration :  5   Loss :  8.41501348939
Iteration :  6   Loss :  6.80600388988
Iteration :  7   Loss :  5.5046482135
Iteration :  8   Loss :  4.45212088756
Iteration :  9   Loss :  3.60084426952
Iteration :  10   Loss :  2.91233819085
Iteration :  11   Loss :  2.35548122958
Iteration :  12   Loss :  1.9051058272
Iteration :  13   Loss :  1.54086445846
Iteration :  14   Loss :  1.24631303724
Iteration :  15   Loss :  1.00815733936
Iteration :  16   Loss :  0.815625975241
Iteration :  17   Loss :  0.65998832461
Iteration :  18   Loss :  0.534153077986
Iteration :  19   Loss :  0.432388574779
Iteration :  20   Loss :  0.350199976472
Iteration :  21   Loss :  0.284032915858
Iteration :  22   Loss :  0.230931430239
Iteration :  23   Loss :  0.188278470499
Iteration :  24   Loss :  0.153851607065
Iteration :  25   Loss :  0.125949838816
Iteration :  26   Loss :  0.103339697357
Iteration :  27   Loss :  0.0850857938482
Iteration :  28   Loss :  0.0704353978046
Iteration :  29   Loss :  0.058726343197
Iteration :  30   Loss :  0.0494014091178
Iteration :  31   Loss :  0.0419787642116
Iteration :  32   Loss :  0.0360388774978
Iteration :  33   Loss :  0.0312963237434
Iteration :  34   Loss :  0.0275934624302
Iteration :  35   Loss :  0.0247808870509
Iteration :  36   Loss :  0.022608603335
Iteration :  37   Loss :  0.020873028372
Iteration :  38   Loss :  0.0195095262603
Iteration :  39   Loss :  0.018585011438
Iteration :  40   Loss :  0.0181457130456
Iteration :  41   Loss :  0.0179013680245
Iteration :  42   Loss :  0.0175993178223
Iteration :  43   Loss :  0.0173882761069
Iteration :  44   Loss :  0.0174640944789
[ -2.95050206e-04  -9.95630554e-05  -3.61754997e-05 ...,   1.38567525e-04
   8.40569708e-05   4.01021894e-05]
CROSS VALIDATION 7
Iteration :  0   Loss :  81.9627227214
Iteration :  1   Loss :  19.6655143868
Iteration :  2   Loss :  15.9053301074
Iteration :  3   Loss :  12.8641194351
Iteration :  4   Loss :  10.4044095736
Iteration :  5   Loss :  8.41501348939
Iteration :  6   Loss :  6.80600388988
Iteration :  7   Loss :  5.5046482135
Iteration :  8   Loss :  4.45212088756
Iteration :  9   Loss :  3.60084426952
Iteration :  10   Loss :  2.91233819085
Iteration :  11   Loss :  2.35548122957
Iteration :  12   Loss :  1.90510582704
Iteration :  13   Loss :  1.54086445696
Iteration :  14   Loss :  1.24631302574
Iteration :  15   Loss :  1.00815726764
Iteration :  16   Loss :  0.815625606872
Iteration :  17   Loss :  0.659986738751
Iteration :  18   Loss :  0.534147255021
Iteration :  19   Loss :  0.43237011279
Iteration :  20   Loss :  0.350149417067
Iteration :  21   Loss :  0.283915740968
Iteration :  22   Loss :  0.230711118086
Iteration :  23   Loss :  0.187953943791
Iteration :  24   Loss :  0.15346705143
Iteration :  25   Loss :  0.125550345771
Iteration :  26   Loss :  0.102950638971
Iteration :  27   Loss :  0.0847210512641
Iteration :  28   Loss :  0.0700975528558
Iteration :  29   Loss :  0.0584126416372
Iteration :  30   Loss :  0.0491150188904
Iteration :  31   Loss :  0.0417346860857
Iteration :  32   Loss :  0.035851303213
Iteration :  33   Loss :  0.0311590446356
Iteration :  34   Loss :  0.0274909854965
Iteration :  35   Loss :  0.0247023447747
Iteration :  36   Loss :  0.0225432656299
Iteration :  37   Loss :  0.0208106822897
Iteration :  38   Loss :  0.0194442449553
Iteration :  39   Loss :  0.0185158409384
Iteration :  40   Loss :  0.0180769571508
Iteration :  41   Loss :  0.0178422970223
Iteration :  42   Loss :  0.0175513426369
Iteration :  43   Loss :  0.0173581490827
Iteration :  44   Loss :  0.0174675105432
Iteration :  45   Loss :  0.017808945535
Iteration :  46   Loss :  0.0181252573024
Iteration :  47   Loss :  0.0184268207464
Iteration :  48   Loss :  0.0187766522998
Iteration :  49   Loss :  0.019150182072
Iteration :  50   Loss :  0.0195149249284
Iteration :  51   Loss :  0.019898502712
Iteration :  52   Loss :  0.0202990425541
Iteration :  53   Loss :  0.0206414323122
Iteration :  54   Loss :  0.0208704813932
Iteration :  55   Loss :  0.0209890008858
Iteration :  56   Loss :  0.0210352718514
[ -3.08713226e-04  -1.72045176e-04  -6.04282426e-05 ...,   1.38131629e-04
   7.25301606e-05   2.57117264e-05]
CROSS VALIDATION 8
Iteration :  0   Loss :  81.9627227214
Iteration :  1   Loss :  19.6655143868
Iteration :  2   Loss :  15.9053301074
Iteration :  3   Loss :  12.8641194351
Iteration :  4   Loss :  10.4044095736
Iteration :  5   Loss :  8.41501348939
Iteration :  6   Loss :  6.80600388988
Iteration :  7   Loss :  5.5046482135
Iteration :  8   Loss :  4.45212088756
Iteration :  9   Loss :  3.60084426952
Iteration :  10   Loss :  2.91233819085
Iteration :  11   Loss :  2.35548122958
Iteration :  12   Loss :  1.9051058272
Iteration :  13   Loss :  1.54086445846
Iteration :  14   Loss :  1.24631303724
Iteration :  15   Loss :  1.00815733938
Iteration :  16   Loss :  0.815625975306
Iteration :  17   Loss :  0.659988324831
Iteration :  18   Loss :  0.534153078263
Iteration :  19   Loss :  0.432388570576
Iteration :  20   Loss :  0.35019992571
Iteration :  21   Loss :  0.284032587102
Iteration :  22   Loss :  0.230930078066
Iteration :  23   Loss :  0.188275028647
Iteration :  24   Loss :  0.153845891689
Iteration :  25   Loss :  0.125941955358
Iteration :  26   Loss :  0.103327505456
Iteration :  27   Loss :  0.0850621998653
Iteration :  28   Loss :  0.0703846796224
Iteration :  29   Loss :  0.0586234812623
Iteration :  30   Loss :  0.0492293836845
Iteration :  31   Loss :  0.041765283068
Iteration :  32   Loss :  0.0358434584607
Iteration :  33   Loss :  0.0311412053048
Iteration :  34   Loss :  0.0274667798866
Iteration :  35   Loss :  0.0246722776403
Iteration :  36   Loss :  0.0225104982764
Iteration :  37   Loss :  0.020778344719
Iteration :  38   Loss :  0.0194101705341
Iteration :  39   Loss :  0.0184723910418
Iteration :  40   Loss :  0.0180167993226
Iteration :  41   Loss :  0.0177666941028
Iteration :  42   Loss :  0.0174632624951
Iteration :  43   Loss :  0.0172540185487
Iteration :  44   Loss :  0.0173458156047
[ -2.79224169e-04  -9.01601498e-05  -1.54115720e-05 ...,   1.41576519e-04
   7.70682144e-05   3.94256200e-05]
CROSS VALIDATION 9
Iteration :  0   Loss :  164.837280255
Iteration :  1   Loss :  185.850581238
Iteration :  2   Loss :  21.0939738473
Iteration :  3   Loss :  17.0606581002
Iteration :  4   Loss :  13.7985406125
Iteration :  5   Loss :  11.1601628681
Iteration :  6   Loss :  9.02626145332
Iteration :  7   Loss :  7.30037695578
Iteration :  8   Loss :  5.9044936802
Iteration :  9   Loss :  4.77551308579
Iteration :  10   Loss :  3.86240155117
Iteration :  11   Loss :  3.12388360624
Iteration :  12   Loss :  2.52657657138
Iteration :  13   Loss :  2.04348280281
Iteration :  14   Loss :  1.65277227026
Iteration :  15   Loss :  1.33680050669
Iteration :  16   Loss :  1.08131065421
Iteration :  17   Loss :  0.874762354001
Iteration :  18   Loss :  0.707773817956
Iteration :  19   Loss :  0.572755255734
Iteration :  20   Loss :  0.46362294454
Iteration :  21   Loss :  0.375466523519
Iteration :  22   Loss :  0.304276862955
Iteration :  23   Loss :  0.246774030025
Iteration :  24   Loss :  0.20029100967
Iteration :  25   Loss :  0.162700318573
Iteration :  26   Loss :  0.132400760394
Iteration :  27   Loss :  0.108088467202
Iteration :  28   Loss :  0.0885950201872
Iteration :  29   Loss :  0.0729592631903
Iteration :  30   Loss :  0.0604439960037
Iteration :  31   Loss :  0.0504924576125
Iteration :  32   Loss :  0.0426703052013
Iteration :  33   Loss :  0.0365970247533
Iteration :  34   Loss :  0.0318883182241
Iteration :  35   Loss :  0.0281606844819
Iteration :  36   Loss :  0.0251439472237
Iteration :  37   Loss :  0.0226697617984
Iteration :  38   Loss :  0.0206545489743
Iteration :  39   Loss :  0.0191148065799
Iteration :  40   Loss :  0.0180824662675
Iteration :  41   Loss :  0.0173505047443
Iteration :  42   Loss :  0.0166200805337
Iteration :  43   Loss :  0.0160847509645
Iteration :  44   Loss :  0.0159323634542
Iteration :  45   Loss :  0.0159889658036
[ -3.76397354e-04  -9.98160737e-05  -6.94165068e-05 ...,   2.28698411e-04
   1.15049979e-04   2.45703140e-05]
CROSS VALIDATION 10
Iteration :  0   Loss :  82.0485111825
Iteration :  1   Loss :  19.6785207287
Iteration :  2   Loss :  15.9158495455
Iteration :  3   Loss :  12.8726274828
Iteration :  4   Loss :  10.4112908229
Iteration :  5   Loss :  8.42057899559
Iteration :  6   Loss :  6.81050523218
Iteration :  7   Loss :  5.50828886702
Iteration :  8   Loss :  4.45506541894
Iteration :  9   Loss :  3.60322576682
Iteration :  10   Loss :  2.91426424688
Iteration :  11   Loss :  2.35703871035
Iteration :  12   Loss :  1.90636458609
Iteration :  13   Loss :  1.54188023595
Iteration :  14   Loss :  1.24713049156
Iteration :  15   Loss :  1.00881506925
Iteration :  16   Loss :  0.816159619596
Iteration :  17   Loss :  0.660423743462
Iteration :  18   Loss :  0.5345065126
Iteration :  19   Loss :  0.432674621816
Iteration :  20   Loss :  0.350433364001
Iteration :  21   Loss :  0.284223877924
Iteration :  22   Loss :  0.231075562488
Iteration :  23   Loss :  0.188348150963
Iteration :  24   Loss :  0.153798008853
Iteration :  25   Loss :  0.125723844259
Iteration :  26   Loss :  0.102964773408
Iteration :  27   Loss :  0.0846425861117
Iteration :  28   Loss :  0.0699709159154
Iteration :  29   Loss :  0.0582611740873
Iteration :  30   Loss :  0.0489581564492
Iteration :  31   Loss :  0.0415803886638
Iteration :  32   Loss :  0.0356930971642
Iteration :  33   Loss :  0.0309939628233
Iteration :  34   Loss :  0.0273215584087
Iteration :  35   Loss :  0.0245307040595
Iteration :  36   Loss :  0.0223712742424
Iteration :  37   Loss :  0.0206405697433
Iteration :  38   Loss :  0.0192761771681
Iteration :  39   Loss :  0.0183509948035
Iteration :  40   Loss :  0.0179146244182
Iteration :  41   Loss :  0.0176757699903
Iteration :  42   Loss :  0.0173839161738
Iteration :  43   Loss :  0.0172119622486
Iteration :  44   Loss :  0.0173653931013
Iteration :  45   Loss :  0.0177423707544
Iteration :  46   Loss :  0.018084853594
Iteration :  47   Loss :  0.0184105825203
Iteration :  48   Loss :  0.0187540501843
Iteration :  49   Loss :  0.0191085903538
Iteration :  50   Loss :  0.0194397698994
Iteration :  51   Loss :  0.0197954087749
Iteration :  52   Loss :  0.0201820991118
Iteration :  53   Loss :  0.0205258916031
Iteration :  54   Loss :  0.0207615832692
Iteration :  55   Loss :  0.0208847481488
Iteration :  56   Loss :  0.0209324853666
[ -2.80266170e-04  -1.61009889e-04  -5.25460949e-05 ...,   1.40439464e-04
   6.67851654e-05   2.83504778e-05]
CROSS VALIDATION 11
Iteration :  0   Loss :  82.0484086128
Iteration :  1   Loss :  19.678512283
Iteration :  2   Loss :  15.9158427146
Iteration :  3   Loss :  12.8726219581
Iteration :  4   Loss :  10.4112863546
Iteration :  5   Loss :  8.42057538161
Iteration :  6   Loss :  6.81050230922
Iteration :  7   Loss :  5.50828650295
Iteration :  8   Loss :  4.4550635069
Iteration :  9   Loss :  3.60322422039
Iteration :  10   Loss :  2.9142629962
Iteration :  11   Loss :  2.35703769903
Iteration :  12   Loss :  1.90636376886
Iteration :  13   Loss :  1.54187957677
Iteration :  14   Loss :  1.24712996172
Iteration :  15   Loss :  1.00881464381
Iteration :  16   Loss :  0.816159275644
Iteration :  17   Loss :  0.660423471231
Iteration :  18   Loss :  0.534506346502
Iteration :  19   Loss :  0.432674770466
Iteration :  20   Loss :  0.350434711632
Iteration :  21   Loss :  0.284229549581
Iteration :  22   Loss :  0.231095043943
Iteration :  23   Loss :  0.18840580293
Iteration :  24   Loss :  0.153941862026
Iteration :  25   Loss :  0.126005631202
Iteration :  26   Loss :  0.103367103446
Iteration :  27   Loss :  0.0850767827997
Iteration :  28   Loss :  0.0703688687719
Iteration :  29   Loss :  0.0585856316846
Iteration :  30   Loss :  0.0491988637498
Iteration :  31   Loss :  0.0417641728967
Iteration :  32   Loss :  0.0358592592425
Iteration :  33   Loss :  0.0311553158807
Iteration :  34   Loss :  0.0274697766283
Iteration :  35   Loss :  0.0246609744364
Iteration :  36   Loss :  0.0224879799967
Iteration :  37   Loss :  0.0207486722063
Iteration :  38   Loss :  0.0193872072278
Iteration :  39   Loss :  0.018478307587
Iteration :  40   Loss :  0.0180581466107
Iteration :  41   Loss :  0.0178113669375
Iteration :  42   Loss :  0.017498417018
Iteration :  43   Loss :  0.0172778530089
Iteration :  44   Loss :  0.0173446919219
[ -2.91724258e-04  -8.75704908e-05  -2.20468636e-05 ...,   1.42662322e-04
   8.53223246e-05   3.98685954e-05]
CROSS VALIDATION 12
Iteration :  0   Loss :  130.386033022
Iteration :  1   Loss :  75.3781205095
Iteration :  2   Loss :  16.0556816128
Iteration :  3   Loss :  12.9857226782
Iteration :  4   Loss :  10.5027614237
Iteration :  5   Loss :  8.49455977591
Iteration :  6   Loss :  6.87034036817
Iteration :  7   Loss :  5.55668310304
Iteration :  8   Loss :  4.49420632064
Iteration :  9   Loss :  3.63488255097
Iteration :  10   Loss :  2.93986751315
Iteration :  11   Loss :  2.37774422547
Iteration :  12   Loss :  1.92310285531
Iteration :  13   Loss :  1.55539212031
Iteration :  14   Loss :  1.25799035222
Iteration :  15   Loss :  1.01745434862
Iteration :  16   Loss :  0.822912404553
Iteration :  17   Loss :  0.665574902828
Iteration :  18   Loss :  0.538341619497
Iteration :  19   Loss :  0.435487392531
Iteration :  20   Loss :  0.352399451005
Iteration :  21   Loss :  0.285329687264
Iteration :  22   Loss :  0.231209565894
Iteration :  23   Loss :  0.187621105573
Iteration :  24   Loss :  0.152676319517
Iteration :  25   Loss :  0.124699171665
Iteration :  26   Loss :  0.102209194556
Iteration :  27   Loss :  0.0841434698043
Iteration :  28   Loss :  0.0697199992736
Iteration :  29   Loss :  0.0582810985577
Iteration :  30   Loss :  0.0491504591642
Iteration :  31   Loss :  0.0418190753947
Iteration :  32   Loss :  0.0359913716117
Iteration :  33   Loss :  0.0314233884773
Iteration :  34   Loss :  0.0278045911516
Iteration :  35   Loss :  0.0248683498747
Iteration :  36   Loss :  0.022524123853
Iteration :  37   Loss :  0.0206701241269
Iteration :  38   Loss :  0.0191817459641
Iteration :  39   Loss :  0.0180666666786
Iteration :  40   Loss :  0.0174494913885
Iteration :  41   Loss :  0.0173177947093
Iteration :  42   Loss :  0.0172615065954
[ -5.27756734e-04  -1.77155709e-04  -1.20504268e-04 ...,   1.41243734e-04
   5.78458433e-05   2.12886831e-05]
CROSS VALIDATION 13
Iteration :  0   Loss :  240.340965987
Iteration :  1   Loss :  91.1255192557
Iteration :  2   Loss :  18.664880527
Iteration :  3   Loss :  15.0960244597
Iteration :  4   Loss :  12.2095587035
Iteration :  5   Loss :  9.87500544482
Iteration :  6   Loss :  7.98683514312
Iteration :  7   Loss :  6.45969624642
Iteration :  8   Loss :  5.22455701933
Iteration :  9   Loss :  4.2255850871
Iteration :  10   Loss :  3.41762366306
Iteration :  11   Loss :  2.76415045974
Iteration :  12   Loss :  2.23562725624
Iteration :  13   Loss :  1.80816601331
Iteration :  14   Loss :  1.46245165162
Iteration :  15   Loss :  1.1828703709
Iteration :  16   Loss :  0.956793425909
Iteration :  17   Loss :  0.773987280838
Iteration :  18   Loss :  0.626179147217
Iteration :  19   Loss :  0.506692474321
Iteration :  20   Loss :  0.410091432859
Iteration :  21   Loss :  0.332020271027
Iteration :  22   Loss :  0.269051677536
Iteration :  23   Loss :  0.218408272152
Iteration :  24   Loss :  0.177772127151
Iteration :  25   Loss :  0.14517188683
Iteration :  26   Loss :  0.118981769193
Iteration :  27   Loss :  0.0979783189719
Iteration :  28   Loss :  0.0811848950434
Iteration :  29   Loss :  0.0677169844132
Iteration :  30   Loss :  0.0568156856886
Iteration :  31   Loss :  0.0479501278857
Iteration :  32   Loss :  0.040786156798
Iteration :  33   Loss :  0.0350310218085
Iteration :  34   Loss :  0.0304229134279
Iteration :  35   Loss :  0.0267749795503
Iteration :  36   Loss :  0.0239270879636
Iteration :  37   Loss :  0.0216874717557
Iteration :  38   Loss :  0.0199005256432
Iteration :  39   Loss :  0.0185731105144
Iteration :  40   Loss :  0.0178121875767
Iteration :  41   Loss :  0.0175172001089
Iteration :  42   Loss :  0.0172312832676
Iteration :  43   Loss :  0.0168542854536
Iteration :  44   Loss :  0.0166993841293
Iteration :  45   Loss :  0.0168040521902
Iteration :  46   Loss :  0.0170592897736
Iteration :  47   Loss :  0.0174488806843
Iteration :  48   Loss :  0.0178805976377
Iteration :  49   Loss :  0.0183589638812
Iteration :  50   Loss :  0.0188805545747
Iteration :  51   Loss :  0.0195317318229
Iteration :  52   Loss :  0.0202250412448
Iteration :  53   Loss :  0.0207144413946
Iteration :  54   Loss :  0.0209538046848
Iteration :  55   Loss :  0.0210448881797
[ -3.09508551e-04  -1.76598204e-04  -5.08303321e-05 ...,   1.39996086e-04
   7.43909119e-05   2.12939759e-05]
CROSS VALIDATION 14
Iteration :  0   Loss :  81.8177844613
Iteration :  1   Loss :  19.6639449647
Iteration :  2   Loss :  15.9040607698
Iteration :  3   Loss :  12.8630928037
Iteration :  4   Loss :  10.4035792414
Iteration :  5   Loss :  8.41434192251
Iteration :  6   Loss :  6.8054607314
Iteration :  7   Loss :  5.50420891158
Iteration :  8   Loss :  4.4517655882
Iteration :  9   Loss :  3.60055692889
Iteration :  10   Loss :  2.91210587595
Iteration :  11   Loss :  2.35529356785
Iteration :  12   Loss :  1.90495444463
Iteration :  13   Loss :  1.5407416935
Iteration :  14   Loss :  1.24620753324
Iteration :  15   Loss :  1.00804296103
Iteration :  16   Loss :  0.815452588613
Iteration :  17   Loss :  0.659700187766
Iteration :  18   Loss :  0.533741356583
Iteration :  19   Loss :  0.431912726242
Iteration :  20   Loss :  0.349708315089
Iteration :  21   Loss :  0.283538582728
Iteration :  22   Loss :  0.230442244142
Iteration :  23   Loss :  0.187800022017
Iteration :  24   Loss :  0.153377473956
Iteration :  25   Loss :  0.125473709978
Iteration :  26   Loss :  0.102858897455
Iteration :  27   Loss :  0.0846079118645
Iteration :  28   Loss :  0.0699733904794
Iteration :  29   Loss :  0.0582952098494
Iteration :  30   Loss :  0.0490140185881
Iteration :  31   Loss :  0.0416413295111
Iteration :  32   Loss :  0.0357512381039
Iteration :  33   Loss :  0.0310468548333
Iteration :  34   Loss :  0.0273653764922
Iteration :  35   Loss :  0.02456429466
Iteration :  36   Loss :  0.0223982253218
Iteration :  37   Loss :  0.0206658473948
Iteration :  38   Loss :  0.0193072096678
Iteration :  39   Loss :  0.0183879800722
Iteration :  40   Loss :  0.0179451606067
Iteration :  41   Loss :  0.0176787733314
Iteration :  42   Loss :  0.0173558167522
Iteration :  43   Loss :  0.0171342538298
Iteration :  44   Loss :  0.0172231221124
[ -2.58274832e-04  -7.78779156e-05  -1.78631168e-05 ...,   1.22178183e-04
   6.73117758e-05   3.74511309e-05]
CROSS VALIDATION 15
Iteration :  0   Loss :  81.8177844613
Iteration :  1   Loss :  19.6639449647
Iteration :  2   Loss :  15.9040607698
Iteration :  3   Loss :  12.8630928037
Iteration :  4   Loss :  10.4035792414
Iteration :  5   Loss :  8.41434192251
Iteration :  6   Loss :  6.80546073141
Iteration :  7   Loss :  5.50420891165
Iteration :  8   Loss :  4.45176558894
Iteration :  9   Loss :  3.60055693501
Iteration :  10   Loss :  2.91210591707
Iteration :  11   Loss :  2.355293796
Iteration :  12   Loss :  1.90495550953
Iteration :  13   Loss :  1.54074594402
Iteration :  14   Loss :  1.24622221369
Iteration :  15   Loss :  1.0080869263
Iteration :  16   Loss :  0.815564640761
Iteration :  17   Loss :  0.659930818123
Iteration :  18   Loss :  0.5340990705
Iteration :  19   Loss :  0.432332382033
Iteration :  20   Loss :  0.350122823024
Iteration :  21   Loss :  0.283902799366
Iteration :  22   Loss :  0.230717514441
Iteration :  23   Loss :  0.187981256435
Iteration :  24   Loss :  0.153508974532
Iteration :  25   Loss :  0.125600628805
Iteration :  26   Loss :  0.102993255127
Iteration :  27   Loss :  0.0847488932751
Iteration :  28   Loss :  0.0701152603411
Iteration :  29   Loss :  0.0584296714996
Iteration :  30   Loss :  0.049139046925
Iteration :  31   Loss :  0.0417652733252
Iteration :  32   Loss :  0.0358836406237
Iteration :  33   Loss :  0.0311895891939
Iteration :  34   Loss :  0.0275173709747
Iteration :  35   Loss :  0.0247248711066
Iteration :  36   Loss :  0.0225639850357
Iteration :  37   Loss :  0.020830748375
Iteration :  38   Loss :  0.0194636926293
Iteration :  39   Loss :  0.0185336580199
Iteration :  40   Loss :  0.0180930732273
Iteration :  41   Loss :  0.0178587303895
Iteration :  42   Loss :  0.0175667925767
Iteration :  43   Loss :  0.0173663603929
Iteration :  44   Loss :  0.0174655415591
[ -2.90030067e-04  -7.57902163e-05  -3.29247633e-05 ...,   1.42820327e-04
   8.34042713e-05   3.96750823e-05]
CROSS VALIDATION 16
Iteration :  0   Loss :  196.241314777
Iteration :  1   Loss :  19.1110650537
Iteration :  2   Loss :  15.4569026435
Iteration :  3   Loss :  12.5014564011
Iteration :  4   Loss :  10.1111465021
Iteration :  5   Loss :  8.177939821
Iteration :  6   Loss :  6.61443018512
Iteration :  7   Loss :  5.34986726618
Iteration :  8   Loss :  4.3270540479
Iteration :  9   Loss :  3.49978914271
Iteration :  10   Loss :  2.83069889089
Iteration :  11   Loss :  2.28954279049
Iteration :  12   Loss :  1.85185961149
Iteration :  13   Loss :  1.49786526774
Iteration :  14   Loss :  1.2115599964
Iteration :  15   Loss :  0.980007702764
Iteration :  16   Loss :  0.792756157123
Iteration :  17   Loss :  0.641368992408
Iteration :  18   Loss :  0.519030532552
Iteration :  19   Loss :  0.42019010788
Iteration :  20   Loss :  0.34033127277
Iteration :  21   Loss :  0.275873212591
Iteration :  22   Loss :  0.22392293727
Iteration :  23   Loss :  0.182043743782
Iteration :  24   Loss :  0.148309011551
Iteration :  25   Loss :  0.121222990693
Iteration :  26   Loss :  0.0995192167471
Iteration :  27   Loss :  0.082156810338
Iteration :  28   Loss :  0.0682374161155
Iteration :  29   Loss :  0.057046217349
Iteration :  30   Loss :  0.0480890737265
Iteration :  31   Loss :  0.0409557457581
Iteration :  32   Loss :  0.0352608533928
Iteration :  33   Loss :  0.0307447356661
Iteration :  34   Loss :  0.027257724259
Iteration :  35   Loss :  0.0246136379673
Iteration :  36   Loss :  0.0225401539146
Iteration :  37   Loss :  0.0208568261502
Iteration :  38   Loss :  0.0195397635165
Iteration :  39   Loss :  0.0186852383837
Iteration :  40   Loss :  0.0182925239961
Iteration :  41   Loss :  0.0180223541563
Iteration :  42   Loss :  0.0176955725836
Iteration :  43   Loss :  0.0175067769115
Iteration :  44   Loss :  0.017570876632
[ -3.16109149e-04  -1.01996907e-04  -8.30785040e-05 ...,   1.88217186e-04
   5.59190773e-05   3.98694099e-05]
CROSS VALIDATION 17
Iteration :  0   Loss :  81.8396897055
Iteration :  1   Loss :  19.6658277889
Iteration :  2   Loss :  15.9055835848
Iteration :  3   Loss :  12.8643244458
Iteration :  4   Loss :  10.4045753849
Iteration :  5   Loss :  8.41514759638
Iteration :  6   Loss :  6.80611235476
Iteration :  7   Loss :  5.5047359399
Iteration :  8   Loss :  4.45219184477
Iteration :  9   Loss :  3.60090168311
Iteration :  10   Loss :  2.9123847275
Iteration :  11   Loss :  2.35551922724
Iteration :  12   Loss :  1.90513763264
Iteration :  13   Loss :  1.54089275761
Iteration :  14   Loss :  1.24634016309
Iteration :  15   Loss :  1.00818183534
Iteration :  16   Loss :  0.81564204696
Iteration :  17   Loss :  0.659995037907
Iteration :  18   Loss :  0.534154649323
Iteration :  19   Loss :  0.432387651509
Iteration :  20   Loss :  0.350194590662
Iteration :  21   Loss :  0.284019457437
Iteration :  22   Loss :  0.230910525107
Iteration :  23   Loss :  0.188257345388
Iteration :  24   Loss :  0.153835395147
Iteration :  25   Loss :  0.12593755335
Iteration :  26   Loss :  0.103321931861
Iteration :  27   Loss :  0.0850617012513
Iteration :  28   Loss :  0.0704093003483
Iteration :  29   Loss :  0.0587021344541
Iteration :  30   Loss :  0.0493854068765
Iteration :  31   Loss :  0.0419799662353
Iteration :  32   Loss :  0.0360635264571
Iteration :  33   Loss :  0.0313400138516
Iteration :  34   Loss :  0.0276465142573
Iteration :  35   Loss :  0.0248381953806
Iteration :  36   Loss :  0.0226681687714
Iteration :  37   Loss :  0.0209333547851
Iteration :  38   Loss :  0.0195697422501
Iteration :  39   Loss :  0.0186461295972
Iteration :  40   Loss :  0.0182085671107
Iteration :  41   Loss :  0.0179630417982
Iteration :  42   Loss :  0.0176577031928
Iteration :  43   Loss :  0.0174440638394
Iteration :  44   Loss :  0.0175193641888
[ -2.85906074e-04  -8.89724119e-05  -2.58239707e-05 ...,   1.46314892e-04
   8.00174993e-05   4.03522528e-05]
CROSS VALIDATION 18
Iteration :  0   Loss :  134.056211557
Iteration :  1   Loss :  150.929206606
Iteration :  2   Loss :  18.0381627193
Iteration :  3   Loss :  14.589315269
Iteration :  4   Loss :  11.7999453226
Iteration :  5   Loss :  9.54391028061
Iteration :  6   Loss :  7.71925580442
Iteration :  7   Loss :  6.2435020178
Iteration :  8   Loss :  5.04990583938
Iteration :  9   Loss :  4.08453443755
Iteration :  10   Loss :  3.30379428804
Iteration :  11   Loss :  2.67241646028
Iteration :  12   Loss :  2.16182642152
Iteration :  13   Loss :  1.74885772665
Iteration :  14   Loss :  1.41480549138
Iteration :  15   Loss :  1.14459671382
Iteration :  16   Loss :  0.926046059538
Iteration :  17   Loss :  0.749286613689
Iteration :  18   Loss :  0.606337025146
Iteration :  19   Loss :  0.49075010016
Iteration :  20   Loss :  0.397322003762
Iteration :  21   Loss :  0.321841084134
Iteration :  22   Loss :  0.260875706722
Iteration :  23   Loss :  0.211653472293
Iteration :  24   Loss :  0.171986986482
Iteration :  25   Loss :  0.140115128225
Iteration :  26   Loss :  0.114553800013
Iteration :  27   Loss :  0.094060741489
Iteration :  28   Loss :  0.0775802952944
Iteration :  29   Loss :  0.0642444892629
Iteration :  30   Loss :  0.0534815880649
Iteration :  31   Loss :  0.0449328006362
Iteration :  32   Loss :  0.0382381546288
Iteration :  33   Loss :  0.0329399479943
Iteration :  34   Loss :  0.0287089247964
Iteration :  35   Loss :  0.0254978271873
Iteration :  36   Loss :  0.0231727356482
Iteration :  37   Loss :  0.0213562353055
Iteration :  38   Loss :  0.0198353300781
Iteration :  39   Loss :  0.018722153069
Iteration :  40   Loss :  0.0181306990447
Iteration :  41   Loss :  0.0178102489789
Iteration :  42   Loss :  0.0174042931362
Iteration :  43   Loss :  0.0170562115823
Iteration :  44   Loss :  0.0170246302063
[ -3.25715408e-04  -2.05917204e-04  -1.00710806e-04 ...,   2.07783187e-04
   8.33284166e-05   3.00421904e-05]
CROSS VALIDATION 19
Iteration :  0   Loss :  145.563945883
Iteration :  1   Loss :  40.3506784197
Iteration :  2   Loss :  19.8715336632
Iteration :  3   Loss :  16.0719570531
Iteration :  4   Loss :  12.9988861401
Iteration :  5   Loss :  10.5134079394
Iteration :  6   Loss :  8.50317060318
Iteration :  7   Loss :  6.87730474507
Iteration :  8   Loss :  5.56231584407
Iteration :  9   Loss :  4.49876205563
Iteration :  10   Loss :  3.63856727869
Iteration :  11   Loss :  2.94284809993
Iteration :  12   Loss :  2.38015661128
Iteration :  13   Loss :  1.9250601574
Iteration :  14   Loss :  1.55699441664
Iteration :  15   Loss :  1.25933687003
Iteration :  16   Loss :  1.01864962417
Iteration :  17   Loss :  0.82404356682
Iteration :  18   Loss :  0.666682609664
Iteration :  19   Loss :  0.539451768477
Iteration :  20   Loss :  0.436600158271
Iteration :  21   Loss :  0.353455538049
Iteration :  22   Loss :  0.286297684687
Iteration :  23   Loss :  0.23209534293
Iteration :  24   Loss :  0.188327466444
Iteration :  25   Loss :  0.153029929796
Iteration :  26   Loss :  0.124665439738
Iteration :  27   Loss :  0.101922057049
Iteration :  28   Loss :  0.0837246728635
Iteration :  29   Loss :  0.0691726550799
Iteration :  30   Loss :  0.0575047929896
Iteration :  31   Loss :  0.0481858901302
Iteration :  32   Loss :  0.040804856121
Iteration :  33   Loss :  0.0350038309934
Iteration :  34   Loss :  0.0305168230239
Iteration :  35   Loss :  0.027109086098
Iteration :  36   Loss :  0.0244655820695
Iteration :  37   Loss :  0.0223454897661
Iteration :  38   Loss :  0.0207133119825
Iteration :  39   Loss :  0.0196551695687
Iteration :  40   Loss :  0.0190380792241
Iteration :  41   Loss :  0.0184338782921
Iteration :  42   Loss :  0.0177955503075
Iteration :  43   Loss :  0.0173387042395
Iteration :  44   Loss :  0.0172108190703
Iteration :  45   Loss :  0.0172899630108
[ -2.96313519e-04  -1.47978384e-04  -5.63534886e-05 ...,   1.93847025e-04
   6.90617326e-05   2.43179929e-05]
Accuracy (Logistic Loss):	0.85
Hinge Loss
CROSS VALIDATION 0
Iteration :  0   Loss :  119.86482135
Iteration :  1   Loss :  21.8692742567
Iteration :  2   Loss :  17.6877156335
Iteration :  3   Loss :  14.3057003475
Iteration :  4   Loss :  11.5703501047
Iteration :  5   Loss :  9.35801801329
Iteration :  6   Loss :  7.5686993345
Iteration :  7   Loss :  6.1215109369
Iteration :  8   Loss :  4.95103511112
Iteration :  9   Loss :  4.00436247263
Iteration :  10   Loss :  3.23870028233
Iteration :  11   Loss :  2.61943807297
Iteration :  12   Loss :  2.11858314137
Iteration :  13   Loss :  1.71349518556
Iteration :  14   Loss :  1.38586288808
Iteration :  15   Loss :  1.12087618381
Iteration :  16   Loss :  0.906556795938
Iteration :  17   Loss :  0.733216778206
Iteration :  18   Loss :  0.593020587626
Iteration :  19   Loss :  0.479630891983
Iteration :  20   Loss :  0.387922101433
Iteration :  21   Loss :  0.313748674856
Iteration :  22   Loss :  0.253757727674
Iteration :  23   Loss :  0.205237470354
Iteration :  24   Loss :  0.165994626541
Iteration :  25   Loss :  0.134255289704
Iteration :  26   Loss :  0.108584736682
Iteration :  27   Loss :  0.0878225734438
Iteration :  28   Loss :  0.0710302814372
Iteration :  29   Loss :  0.0574487934389
Iteration :  30   Loss :  0.0464641812028
Iteration :  31   Loss :  0.0375799038694
Iteration :  32   Loss :  0.0616169690103
Iteration :  33   Loss :  327.448334928
Iteration :  34   Loss :  46.5448776602
Iteration :  35   Loss :  21.2898616745
Iteration :  36   Loss :  17.2190908009
Iteration :  37   Loss :  13.9266798697
Iteration :  38   Loss :  11.2638010006
Iteration :  39   Loss :  9.11008324795
Iteration :  40   Loss :  7.36817143524
Iteration :  41   Loss :  5.95932537843
Iteration :  42   Loss :  4.81986056895
Iteration :  43   Loss :  3.89826942295
Iteration :  44   Loss :  3.15289296786
Iteration :  45   Loss :  2.55003771885
Iteration :  46   Loss :  2.06245262172
Iteration :  47   Loss :  1.66809721495
Iteration :  48   Loss :  1.34914532786
Iteration :  49   Loss :  1.09117927863
Iteration :  50   Loss :  0.882538147315
Iteration :  51   Loss :  0.71379066366
Iteration :  52   Loss :  0.577308882431
Iteration :  53   Loss :  0.466923375019
Iteration :  54   Loss :  0.377644350839
Iteration :  55   Loss :  0.305436102262
Iteration :  56   Loss :  0.247034577262
Iteration :  57   Loss :  0.199799833454
Iteration :  58   Loss :  0.161596703955
Iteration :  59   Loss :  0.130698280763
Iteration :  60   Loss :  0.105707852798
Iteration :  61   Loss :  0.0854957699361
Iteration :  62   Loss :  0.0691483790797
Iteration :  63   Loss :  0.0559267240114
Iteration :  64   Loss :  0.0452331421252
Iteration :  65   Loss :  0.0365842480977
Iteration :  66   Loss :  0.0295890832692
Iteration :  67   Loss :  0.0239314430182
Iteration :  68   Loss :  0.0193555832643
Iteration :  69   Loss :  0.0156546599893
Iteration :  70   Loss :  0.0126613792017
Iteration :  71   Loss :  0.0102404346948
Iteration :  72   Loss :  0.00828239175746
Iteration :  73   Loss :  0.00669874036295
Iteration :  74   Loss :  0.00541789422237
Iteration :  75   Loss :  0.00438195484738
Iteration :  76   Loss :  0.0035440943467
Iteration :  77   Loss :  0.00286643865027
Iteration :  78   Loss :  0.00628100699163
Iteration :  79   Loss :  629.625752738
Iteration :  80   Loss :  479.927479876
Iteration :  81   Loss :  121.779392236
Iteration :  82   Loss :  16.1212704519
Iteration :  83   Loss :  13.0387704712
Iteration :  84   Loss :  10.5456661066
Iteration :  85   Loss :  8.52926078256
Iteration :  86   Loss :  6.89840629897
Iteration :  87   Loss :  5.57938263102
Iteration :  88   Loss :  4.51256553966
Iteration :  89   Loss :  3.64973135854
Iteration :  90   Loss :  2.95187712455
Iteration :  91   Loss :  2.38745751465
Iteration :  92   Loss :  1.93095889286
Iteration :  93   Loss :  1.56174600931
Iteration :  94   Loss :  1.26312921866
Iteration :  95   Loss :  1.0216100528
Iteration :  96   Loss :  0.826271045404
Iteration :  97   Loss :  0.668282226277
Iteration :  98   Loss :  0.540501977459
Iteration :  99   Loss :  0.437154208432
[ -5.18893712e-04  -1.37074108e-03  -5.75499673e-04 ...,   2.68103551e-03
   4.96359482e-05  -1.89249458e-05]
CROSS VALIDATION 1
Iteration :  0   Loss :  114.169505724
Iteration :  1   Loss :  18.5855030882
Iteration :  2   Loss :  15.0318245439
Iteration :  3   Loss :  12.1576342619
Iteration :  4   Loss :  9.83300932063
Iteration :  5   Loss :  7.95286897241
Iteration :  6   Loss :  6.43222464558
Iteration :  7   Loss :  5.20233817943
Iteration :  8   Loss :  4.20761463171
Iteration :  9   Loss :  3.40308920304
Iteration :  10   Loss :  2.75239467905
Iteration :  11   Loss :  2.22611751186
Iteration :  12   Loss :  1.80046823019
Iteration :  13   Loss :  1.45620607657
Iteration :  14   Loss :  1.1940183536
Iteration :  15   Loss :  151.658945353
Iteration :  16   Loss :  11.7620459227
Iteration :  17   Loss :  9.51306024643
Iteration :  18   Loss :  7.69409640524
Iteration :  19   Loss :  6.22293120821
Iteration :  20   Loss :  5.03306311522
Iteration :  21   Loss :  4.07070614702
Iteration :  22   Loss :  3.29235858086
Iteration :  23   Loss :  2.66283652848
Iteration :  24   Loss :  2.1536835078
Iteration :  25   Loss :  1.74188411575
Iteration :  26   Loss :  1.40882365571
Iteration :  27   Loss :  1.13944669162
Iteration :  28   Loss :  0.921576492407
Iteration :  29   Loss :  0.745364603366
Iteration :  30   Loss :  0.602845663414
Iteration :  31   Loss :  0.48757734437
Iteration :  32   Loss :  0.394349136389
Iteration :  33   Loss :  0.318946815651
Iteration :  34   Loss :  0.257961947491
Iteration :  35   Loss :  0.208637813854
Iteration :  36   Loss :  0.168744800515
Iteration :  37   Loss :  0.136479611125
Iteration :  38   Loss :  0.110383752246
Iteration :  39   Loss :  0.0892776046147
Iteration :  40   Loss :  0.072207100443
Iteration :  41   Loss :  0.0584005963969
Iteration :  42   Loss :  0.0472339927596
Iteration :  43   Loss :  0.0382025220573
Iteration :  44   Loss :  0.0308979318976
Iteration :  45   Loss :  0.0249900306089
Iteration :  46   Loss :  0.0202117614831
Iteration :  47   Loss :  0.0163471309276
Iteration :  48   Loss :  0.0132214448398
Iteration :  49   Loss :  0.0106934118547
Iteration :  50   Loss :  0.00864875650736
Iteration :  51   Loss :  0.00699505360308
Iteration :  52   Loss :  1187.43728927
Iteration :  53   Loss :  415.031576829
Iteration :  54   Loss :  154.954527117
Iteration :  55   Loss :  17.471510889
Iteration :  56   Loss :  14.1308354665
Iteration :  57   Loss :  11.4289206154
Iteration :  58   Loss :  9.24363083433
Iteration :  59   Loss :  7.47618378644
Iteration :  60   Loss :  6.04668501051
Iteration :  61   Loss :  4.89051642666
Iteration :  62   Loss :  3.9554153851
Iteration :  63   Loss :  3.19911222124
Iteration :  64   Loss :  2.58741952682
Iteration :  65   Loss :  2.09268676582
Iteration :  66   Loss :  1.69255037864
Iteration :  67   Loss :  1.36892287515
Iteration :  68   Loss :  1.10717522016
Iteration :  69   Loss :  0.895475552633
Iteration :  70   Loss :  0.724254346342
Iteration :  71   Loss :  0.585771835594
Iteration :  72   Loss :  0.473768152181
Iteration :  73   Loss :  0.3831803586
Iteration :  74   Loss :  0.309913586511
Iteration :  75   Loss :  0.250655935118
Iteration :  76   Loss :  0.20272876229
Iteration :  77   Loss :  0.163965601054
Iteration :  78   Loss :  0.132614228122
Iteration :  79   Loss :  0.107257457584
Iteration :  80   Loss :  0.0867490794185
Iteration :  81   Loss :  0.0701620469988
Iteration :  82   Loss :  0.0567465715147
Iteration :  83   Loss :  0.0458962290357
Iteration :  84   Loss :  0.0371205481401
Iteration :  85   Loss :  0.0300228389821
Iteration :  86   Loss :  0.0242822615965
Iteration :  87   Loss :  0.0196393228699
Iteration :  88   Loss :  0.0158841465922
Iteration :  89   Loss :  0.0128469863566
Iteration :  90   Loss :  0.0103905524599
Iteration :  91   Loss :  0.0512635632774
Iteration :  92   Loss :  237.510814231
Iteration :  93   Loss :  17.5789180287
Iteration :  94   Loss :  14.2177056077
Iteration :  95   Loss :  11.4991805763
Iteration :  96   Loss :  9.30045659794
Iteration :  97   Loss :  7.52214406549
Iteration :  98   Loss :  6.08385736185
Iteration :  99   Loss :  4.92058116371
[ 0.00495713 -0.00468275 -0.00574594 ...,  0.00421744 -0.00240316
  0.00133074]
CROSS VALIDATION 2
Iteration :  0   Loss :  201.070360337
Iteration :  1   Loss :  21.0805850538
Iteration :  2   Loss :  17.0498293379
Iteration :  3   Loss :  13.7897823855
Iteration :  4   Loss :  11.1530792755
Iteration :  5   Loss :  9.02053229333
Iteration :  6   Loss :  7.29574325123
Iteration :  7   Loss :  5.90074597119
Iteration :  8   Loss :  4.77248195524
Iteration :  9   Loss :  3.85994993248
Iteration :  10   Loss :  3.12190043273
Iteration :  11   Loss :  2.5345014557
Iteration :  12   Loss :  18.8798010318
Iteration :  13   Loss :  50.2829737376
Iteration :  14   Loss :  11.7551224746
Iteration :  15   Loss :  9.50746061016
Iteration :  16   Loss :  7.6895674587
Iteration :  17   Loss :  6.21926822801
Iteration :  18   Loss :  5.03010052252
Iteration :  19   Loss :  4.06831002283
Iteration :  20   Loss :  3.29042061242
Iteration :  21   Loss :  2.66126911319
Iteration :  22   Loss :  2.15241579332
Iteration :  23   Loss :  1.74085879716
Iteration :  24   Loss :  1.40799438522
Iteration :  25   Loss :  1.13877598347
Iteration :  26   Loss :  0.92103402836
Iteration :  27   Loss :  0.74492586225
Iteration :  28   Loss :  0.602490812676
Iteration :  29   Loss :  0.487290343582
Iteration :  30   Loss :  0.394117012164
Iteration :  31   Loss :  0.318759075207
Iteration :  32   Loss :  0.257810104336
Iteration :  33   Loss :  0.208515004175
Iteration :  34   Loss :  0.168645472908
Iteration :  35   Loss :  0.136399275654
Iteration :  36   Loss :  0.11031877748
Iteration :  37   Loss :  0.0892250534797
Iteration :  38   Loss :  0.0721645974536
Iteration :  39   Loss :  0.0583662202772
Iteration :  40   Loss :  0.047206189595
Iteration :  41   Loss :  0.0381800350528
Iteration :  42   Loss :  0.0308797445661
Iteration :  43   Loss :  0.0249753208227
Iteration :  44   Loss :  926.488254927
Iteration :  45   Loss :  220.481694083
Iteration :  46   Loss :  127.123020139
Iteration :  47   Loss :  21.6967761204
Iteration :  48   Loss :  17.5482003508
Iteration :  49   Loss :  14.1928613653
Iteration :  50   Loss :  11.4790867273
Iteration :  51   Loss :  9.28420483377
Iteration :  52   Loss :  7.50899975261
Iteration :  53   Loss :  6.07322633379
Iteration :  54   Loss :  4.91198286277
Iteration :  55   Loss :  3.97277728807
Iteration :  56   Loss :  3.21315440659
Iteration :  57   Loss :  2.59877674784
Iteration :  58   Loss :  2.10187240652
Iteration :  59   Loss :  1.69997966042
Iteration :  60   Loss :  1.37493162614
Iteration :  61   Loss :  1.11203505582
Iteration :  62   Loss :  0.89940615363
Iteration :  63   Loss :  0.72743338886
Iteration :  64   Loss :  0.588343022886
Iteration :  65   Loss :  0.475847710429
Iteration :  66   Loss :  0.384862290726
Iteration :  67   Loss :  0.311273921417
Iteration :  68   Loss :  0.25175616445
Iteration :  69   Loss :  0.203618620056
Iteration :  70   Loss :  0.164685311775
Iteration :  71   Loss :  0.133196325104
Iteration :  72   Loss :  0.107728253541
Iteration :  73   Loss :  0.0871298558865
Iteration :  74   Loss :  0.0827670116072
Iteration :  75   Loss :  110.86437555
Iteration :  76   Loss :  20.3266309517
Iteration :  77   Loss :  111.401489454
Iteration :  78   Loss :  15.404297034
Iteration :  79   Loss :  10.2000636432
Iteration :  80   Loss :  8.24973993407
Iteration :  81   Loss :  6.67233179719
Iteration :  82   Loss :  5.39653515961
Iteration :  83   Loss :  4.36467978723
Iteration :  84   Loss :  3.53012239921
Iteration :  85   Loss :  2.85513823714
Iteration :  86   Loss :  2.30921578102
Iteration :  87   Loss :  1.86767752746
Iteration :  88   Loss :  1.51056448482
Iteration :  89   Loss :  1.22173396062
Iteration :  90   Loss :  0.988129858436
Iteration :  91   Loss :  0.799192499023
Iteration :  92   Loss :  0.646381287886
Iteration :  93   Loss :  0.522788652096
Iteration :  94   Loss :  0.422827795115
Iteration :  95   Loss :  0.341980155087
Iteration :  96   Loss :  0.276591150877
Iteration :  97   Loss :  0.223704982892
Iteration :  98   Loss :  0.180931021156
Iteration :  99   Loss :  0.146335740909
[-0.00055637 -0.00033262 -0.00117245 ...,  0.00070087 -0.00070621
  0.0003623 ]
CROSS VALIDATION 3
Iteration :  0   Loss :  196.86325262
Iteration :  1   Loss :  25.7642510213
Iteration :  2   Loss :  20.2582505962
Iteration :  3   Loss :  16.3847309962
Iteration :  4   Loss :  13.2518555116
Iteration :  5   Loss :  10.7180077928
Iteration :  6   Loss :  8.66864952956
Iteration :  7   Loss :  7.0111429399
Iteration :  8   Loss :  5.6705632355
Iteration :  9   Loss :  4.58631177305
Iteration :  10   Loss :  3.70937679488
Iteration :  11   Loss :  3.00011793511
Iteration :  12   Loss :  2.42647434389
Iteration :  13   Loss :  1.96251543069
Iteration :  14   Loss :  1.58726871578
Iteration :  15   Loss :  1.28377180465
Iteration :  16   Loss :  1.03830563156
Iteration :  17   Loss :  0.839774312409
Iteration :  18   Loss :  0.679203573923
Iteration :  19   Loss :  0.549335086836
Iteration :  20   Loss :  0.444298365342
Iteration :  21   Loss :  0.359345401698
Iteration :  22   Loss :  0.290636040541
Iteration :  23   Loss :  0.235064391145
Iteration :  24   Loss :  0.190118430879
Iteration :  25   Loss :  0.153766453455
Iteration :  26   Loss :  0.124365229077
Iteration :  27   Loss :  0.100585725013
Iteration :  28   Loss :  0.0813530289092
Iteration :  29   Loss :  0.0657977591933
Iteration :  30   Loss :  0.0532167661475
Iteration :  31   Loss :  0.072886948827
Iteration :  32   Loss :  342.521619931
Iteration :  33   Loss :  40.7690009619
Iteration :  34   Loss :  40.9070729365
Iteration :  35   Loss :  15.5272944151
Iteration :  36   Loss :  12.5583668186
Iteration :  37   Loss :  10.1571190018
Iteration :  38   Loss :  8.21500660935
Iteration :  39   Loss :  6.64423972777
Iteration :  40   Loss :  5.37381449089
Iteration :  41   Loss :  4.34630346973
Iteration :  42   Loss :  3.5152597625
Iteration :  43   Loss :  2.84311744081
Iteration :  44   Loss :  2.29949344526
Iteration :  45   Loss :  1.85981417049
Iteration :  46   Loss :  1.50420465685
Iteration :  47   Loss :  1.21659017637
Iteration :  48   Loss :  0.983969601809
Iteration :  49   Loss :  0.795827712642
Iteration :  50   Loss :  0.643659872261
Iteration :  51   Loss :  0.520587590226
Iteration :  52   Loss :  0.421047591712
Iteration :  53   Loss :  0.340540339061
Iteration :  54   Loss :  0.275426637773
Iteration :  55   Loss :  0.222763132862
Iteration :  56   Loss :  0.180169259457
Iteration :  57   Loss :  0.14571963339
Iteration :  58   Loss :  0.117857017447
Iteration :  59   Loss :  0.0953219290932
Iteration :  60   Loss :  0.0770957076879
Iteration :  61   Loss :  0.0623544676492
Iteration :  62   Loss :  0.050431856097
Iteration :  63   Loss :  0.0407889314957
Iteration :  64   Loss :  0.032989801711
Iteration :  65   Loss :  0.0266819202421
Iteration :  66   Loss :  0.021580149952
Iteration :  67   Loss :  0.0174538739237
Iteration :  68   Loss :  1064.53794375
Iteration :  69   Loss :  343.913154186
Iteration :  70   Loss :  130.814602695
Iteration :  71   Loss :  17.6471432917
Iteration :  72   Loss :  14.2728857219
Iteration :  73   Loss :  11.5438098657
Iteration :  74   Loss :  9.33655245415
Iteration :  75   Loss :  7.55133814082
Iteration :  76   Loss :  6.10746932522
Iteration :  77   Loss :  4.93967835407
Iteration :  78   Loss :  3.99517720718
Iteration :  79   Loss :  3.23127130406
Iteration :  80   Loss :  2.61342956745
Iteration :  81   Loss :  2.11372350425
Iteration :  82   Loss :  1.70956474514
Iteration :  83   Loss :  1.38268397544
Iteration :  84   Loss :  1.11830510156
Iteration :  85   Loss :  0.904477322646
Iteration :  86   Loss :  0.731534914793
Iteration :  87   Loss :  0.591660308293
Iteration :  88   Loss :  0.4785307076
Iteration :  89   Loss :  0.387032280019
Iteration :  90   Loss :  0.313028993538
Iteration :  91   Loss :  0.253175654472
Iteration :  92   Loss :  0.20476669363
Iteration :  93   Loss :  0.165613865629
Iteration :  94   Loss :  0.133947332949
Iteration :  95   Loss :  0.108335663418
Iteration :  96   Loss :  0.0876211247351
Iteration :  97   Loss :  0.0708673511345
Iteration :  98   Loss :  0.0573170165529
Iteration :  99   Loss :  0.0463576009816
[ -9.06589729e-04  -2.74311646e-04  -4.91282172e-05 ...,   4.06464764e-04
   3.54195395e-04  -5.03353038e-05]
CROSS VALIDATION 4
Iteration :  0   Loss :  196.86325262
Iteration :  1   Loss :  25.7642510213
Iteration :  2   Loss :  20.2582505962
Iteration :  3   Loss :  16.3847309962
Iteration :  4   Loss :  13.2518555116
Iteration :  5   Loss :  10.7180077928
Iteration :  6   Loss :  8.66864952956
Iteration :  7   Loss :  7.0111429399
Iteration :  8   Loss :  5.6705632355
Iteration :  9   Loss :  4.58631177305
Iteration :  10   Loss :  3.70937679488
Iteration :  11   Loss :  3.00011793511
Iteration :  12   Loss :  2.42647434389
Iteration :  13   Loss :  1.96251543069
Iteration :  14   Loss :  1.58726871578
Iteration :  15   Loss :  1.28377180465
Iteration :  16   Loss :  1.03830563156
Iteration :  17   Loss :  0.839774312409
Iteration :  18   Loss :  0.679203573923
Iteration :  19   Loss :  0.549335086836
Iteration :  20   Loss :  0.444298365342
Iteration :  21   Loss :  0.359345401698
Iteration :  22   Loss :  0.290636040541
Iteration :  23   Loss :  0.235064391145
Iteration :  24   Loss :  0.190118430879
Iteration :  25   Loss :  0.153766453455
Iteration :  26   Loss :  0.124365229077
Iteration :  27   Loss :  0.100585725013
Iteration :  28   Loss :  0.0813530289092
Iteration :  29   Loss :  0.0657977591933
Iteration :  30   Loss :  0.0532167661475
Iteration :  31   Loss :  0.072886948827
Iteration :  32   Loss :  342.521619931
Iteration :  33   Loss :  40.7690009619
Iteration :  34   Loss :  40.9070729365
Iteration :  35   Loss :  15.5272944151
Iteration :  36   Loss :  12.5583668186
Iteration :  37   Loss :  10.1571190018
Iteration :  38   Loss :  8.21500660935
Iteration :  39   Loss :  6.64423972777
Iteration :  40   Loss :  5.37381449089
Iteration :  41   Loss :  4.34630346973
Iteration :  42   Loss :  3.5152597625
Iteration :  43   Loss :  2.84311744081
Iteration :  44   Loss :  2.29949344526
Iteration :  45   Loss :  1.85981417049
Iteration :  46   Loss :  1.50420465685
Iteration :  47   Loss :  1.21659017637
Iteration :  48   Loss :  0.983969601809
Iteration :  49   Loss :  0.795827712642
Iteration :  50   Loss :  0.643659872261
Iteration :  51   Loss :  0.520587590226
Iteration :  52   Loss :  0.421047591712
Iteration :  53   Loss :  0.340540339061
Iteration :  54   Loss :  0.275426637773
Iteration :  55   Loss :  0.222763132862
Iteration :  56   Loss :  0.180169259457
Iteration :  57   Loss :  0.14571963339
Iteration :  58   Loss :  0.117857017447
Iteration :  59   Loss :  0.0953219290932
Iteration :  60   Loss :  0.0770957076879
Iteration :  61   Loss :  0.0623544676492
Iteration :  62   Loss :  0.050431856097
Iteration :  63   Loss :  0.0407889314957
Iteration :  64   Loss :  0.032989801711
Iteration :  65   Loss :  0.0266819202421
Iteration :  66   Loss :  0.021580149952
Iteration :  67   Loss :  0.0174538739237
Iteration :  68   Loss :  1064.53794375
Iteration :  69   Loss :  343.913154186
Iteration :  70   Loss :  130.814602695
Iteration :  71   Loss :  17.6471432917
Iteration :  72   Loss :  14.2728857219
Iteration :  73   Loss :  11.5438098657
Iteration :  74   Loss :  9.33655245415
Iteration :  75   Loss :  7.55133814082
Iteration :  76   Loss :  6.10746932522
Iteration :  77   Loss :  4.93967835407
Iteration :  78   Loss :  3.99517720718
Iteration :  79   Loss :  3.23127130406
Iteration :  80   Loss :  2.61342956745
Iteration :  81   Loss :  2.11372350425
Iteration :  82   Loss :  1.70956474514
Iteration :  83   Loss :  1.38268397544
Iteration :  84   Loss :  1.11830510156
Iteration :  85   Loss :  0.904477322646
Iteration :  86   Loss :  0.731534914793
Iteration :  87   Loss :  0.591660308293
Iteration :  88   Loss :  0.4785307076
Iteration :  89   Loss :  0.387032280019
Iteration :  90   Loss :  0.313028993538
Iteration :  91   Loss :  0.253175654472
Iteration :  92   Loss :  0.20476669363
Iteration :  93   Loss :  0.165613865629
Iteration :  94   Loss :  0.133947332949
Iteration :  95   Loss :  0.108335663418
Iteration :  96   Loss :  0.0876211247351
Iteration :  97   Loss :  0.0708673511345
Iteration :  98   Loss :  0.0573170165529
Iteration :  99   Loss :  0.0463576009816
[ -9.06589729e-04  -2.74311646e-04  -4.91282172e-05 ...,   4.06464764e-04
   3.54195395e-04  -5.03353038e-05]
CROSS VALIDATION 5
Iteration :  0   Loss :  175.576834675
Iteration :  1   Loss :  21.4415937027
Iteration :  2   Loss :  17.3418106011
Iteration :  3   Loss :  14.0259347833
Iteration :  4   Loss :  11.3440776786
Iteration :  5   Loss :  9.1750104621
Iteration :  6   Loss :  7.42068411066
Iteration :  7   Loss :  6.001797262
Iteration :  8   Loss :  4.8542115305
Iteration :  9   Loss :  3.92605223972
Iteration :  10   Loss :  3.1753635152
Iteration :  11   Loss :  2.56821173994
Iteration :  12   Loss :  2.07715164251
Iteration :  13   Loss :  1.67998567987
Iteration :  14   Loss :  1.35876063491
Iteration :  15   Loss :  1.09895607153
Iteration :  16   Loss :  0.888827962866
Iteration :  17   Loss :  0.718877822363
Iteration :  18   Loss :  0.581423340709
Iteration :  19   Loss :  0.470251120017
Iteration :  20   Loss :  0.380335807654
Iteration :  21   Loss :  0.307612933656
Iteration :  22   Loss :  0.248795183226
Iteration :  23   Loss :  0.20122379921
Iteration :  24   Loss :  0.162748397471
Iteration :  25   Loss :  0.1316297624
Iteration :  26   Loss :  0.106461228613
Iteration :  27   Loss :  0.0861050950116
Iteration :  28   Loss :  0.0696411969272
Iteration :  29   Loss :  0.0563253116299
Iteration :  30   Loss :  0.0455555169955
Iteration :  31   Loss :  0.0368449826317
Iteration :  32   Loss :  0.0297999635316
Iteration :  33   Loss :  0.0241020014953
Iteration :  34   Loss :  0.0194935297644
Iteration :  35   Loss :  0.015766230151
Iteration :  36   Loss :  0.0479814392156
Iteration :  37   Loss :  313.967802144
Iteration :  38   Loss :  101.467944756
Iteration :  39   Loss :  33.5917778603
Iteration :  40   Loss :  15.4408936875
Iteration :  41   Loss :  12.4884865161
Iteration :  42   Loss :  10.1006003033
Iteration :  43   Loss :  8.16929468239
Iteration :  44   Loss :  6.60726824186
Iteration :  45   Loss :  5.34391221239
Iteration :  46   Loss :  4.32211871661
Iteration :  47   Loss :  3.4956993038
Iteration :  48   Loss :  2.82729707901
Iteration :  49   Loss :  2.28669804759
Iteration :  50   Loss :  1.8494653426
Iteration :  51   Loss :  1.49583459744
Iteration :  52   Loss :  1.20982053102
Iteration :  53   Loss :  0.978494360131
Iteration :  54   Loss :  0.791399375575
Iteration :  55   Loss :  0.640078264301
Iteration :  56   Loss :  0.517690810828
Iteration :  57   Loss :  0.418704696852
Iteration :  58   Loss :  0.33864542213
Iteration :  59   Loss :  0.27389404225
Iteration :  60   Loss :  0.221523580352
Iteration :  61   Loss :  0.179166718081
Iteration :  62   Loss :  0.144908784956
Iteration :  63   Loss :  0.117201208921
Iteration :  64   Loss :  0.0947915157573
Iteration :  65   Loss :  0.076666713102
Iteration :  66   Loss :  0.062007499837
Iteration :  67   Loss :  0.050151230964
Iteration :  68   Loss :  0.0405619638563
Iteration :  69   Loss :  0.0328062318762
Iteration :  70   Loss :  0.0265334502473
Iteration :  71   Loss :  0.0214600684614
Iteration :  72   Loss :  0.0629243499074
Iteration :  73   Loss :  265.756593205
Iteration :  74   Loss :  50.7927940819
Iteration :  75   Loss :  16.074986794
Iteration :  76   Loss :  13.0013365733
Iteration :  77   Loss :  10.5153898326
Iteration :  78   Loss :  8.50477354445
Iteration :  79   Loss :  6.87860119253
Iteration :  80   Loss :  5.56336439984
Iteration :  81   Loss :  4.49961010664
Iteration :  82   Loss :  3.63925309518
Iteration :  83   Loss :  2.94340237863
Iteration :  84   Loss :  2.38060319959
Iteration :  85   Loss :  1.92541517091
Iteration :  86   Loss :  1.55726228588
Iteration :  87   Loss :  1.2595028146
Iteration :  88   Loss :  1.01867704263
Iteration :  89   Loss :  0.82389884735
Iteration :  90   Loss :  0.66636360913
Iteration :  91   Loss :  0.538950213367
Iteration :  92   Loss :  0.435899152518
Iteration :  93   Loss :  0.352552177276
Iteration :  94   Loss :  0.285141728273
Iteration :  95   Loss :  0.230620629919
Iteration :  96   Loss :  0.186524347967
Iteration :  97   Loss :  0.150859584404
Iteration :  98   Loss :  0.122014173777
Iteration :  99   Loss :  0.0986842079756
[  8.37948569e-06  -7.73221544e-04  -1.63458575e-04 ...,   5.34608585e-04
   3.31004935e-04   1.29659918e-04]
CROSS VALIDATION 6
Iteration :  0   Loss :  196.588357219
Iteration :  1   Loss :  23.8381279864
Iteration :  2   Loss :  18.7546101424
Iteration :  3   Loss :  15.1685971433
Iteration :  4   Loss :  12.268254981
Iteration :  5   Loss :  9.92247858238
Iteration :  6   Loss :  8.02523108381
Iteration :  7   Loss :  6.49075061375
Iteration :  8   Loss :  5.249673572
Iteration :  9   Loss :  4.24589916522
Iteration :  10   Loss :  3.43405346522
Iteration :  11   Loss :  2.77743835714
Iteration :  12   Loss :  2.24637266304
Iteration :  13   Loss :  1.8168504544
Iteration :  14   Loss :  1.46945590462
Iteration :  15   Loss :  1.18848562928
Iteration :  16   Loss :  0.961238841241
Iteration :  17   Loss :  0.777443232922
Iteration :  18   Loss :  0.628790634007
Iteration :  19   Loss :  0.508561454615
Iteration :  20   Loss :  0.411320937578
Iteration :  21   Loss :  0.332673489417
Iteration :  22   Loss :  351.888343671
Iteration :  23   Loss :  332.034453925
Iteration :  24   Loss :  26.3266320665
Iteration :  25   Loss :  15.1259187034
Iteration :  26   Loss :  104.145996993
Iteration :  27   Loss :  11.9284110425
Iteration :  28   Loss :  9.64761518849
Iteration :  29   Loss :  7.80292349864
Iteration :  30   Loss :  6.31094979807
Iteration :  31   Loss :  5.10425193336
Iteration :  32   Loss :  4.12828316384
Iteration :  33   Loss :  3.33892646823
Iteration :  34   Loss :  2.70050030916
Iteration :  35   Loss :  2.18414570946
Iteration :  36   Loss :  1.76652173079
Iteration :  37   Loss :  1.42875038595
Iteration :  38   Loss :  1.15556329128
Iteration :  39   Loss :  0.934611485177
Iteration :  40   Loss :  0.755907214095
Iteration :  41   Loss :  0.611372453028
Iteration :  42   Loss :  0.494473752005
Iteration :  43   Loss :  0.399926902514
Iteration :  44   Loss :  0.323458073773
Iteration :  45   Loss :  0.261610621419
Iteration :  46   Loss :  0.211588835736
Iteration :  47   Loss :  0.171131566316
Iteration :  48   Loss :  0.138410010565
Iteration :  49   Loss :  0.111945045774
Iteration :  50   Loss :  0.090540367869
Iteration :  51   Loss :  0.0732284145064
Iteration :  52   Loss :  0.0592266280482
Iteration :  53   Loss :  0.0479020813656
Iteration :  54   Loss :  0.038742867436
Iteration :  55   Loss :  0.0313349594501
Iteration :  56   Loss :  0.0728094209721
Iteration :  57   Loss :  540.799513369
Iteration :  58   Loss :  56.6169979318
Iteration :  59   Loss :  20.0275213812
Iteration :  60   Loss :  16.1981188254
Iteration :  61   Loss :  13.1009248967
Iteration :  62   Loss :  10.59593617
Iteration :  63   Loss :  8.56991885724
Iteration :  64   Loss :  6.9312902646
Iteration :  65   Loss :  5.60597895178
Iteration :  66   Loss :  4.53407645736
Iteration :  67   Loss :  3.6671292379
Iteration :  68   Loss :  2.96594840735
Iteration :  69   Loss :  2.39883826949
Iteration :  70   Loss :  1.94016356755
Iteration :  71   Loss :  1.56919068564
Iteration :  72   Loss :  1.26915042066
Iteration :  73   Loss :  1.02647995875
Iteration :  74   Loss :  0.830209791179
Iteration :  75   Loss :  0.671467856235
Iteration :  76   Loss :  0.543078492626
Iteration :  77   Loss :  0.439238075828
Iteration :  78   Loss :  0.35525267503
Iteration :  79   Loss :  0.287325871917
Iteration :  80   Loss :  0.232387149979
Iteration :  81   Loss :  0.187953097001
Iteration :  82   Loss :  0.152015146602
Iteration :  83   Loss :  0.122948784378
Iteration :  84   Loss :  0.0994401144746
Iteration :  85   Loss :  0.0804264671406
Iteration :  86   Loss :  0.0650483625335
Iteration :  87   Loss :  0.0526106593852
Iteration :  88   Loss :  0.0425511323135
Iteration :  89   Loss :  0.0344150573729
Iteration :  90   Loss :  0.0278346570251
Iteration :  91   Loss :  0.123574499252
Iteration :  92   Loss :  273.101111264
Iteration :  93   Loss :  34.3314281336
Iteration :  94   Loss :  19.0490118279
Iteration :  95   Loss :  15.4067071617
Iteration :  96   Loss :  12.4608366939
Iteration :  97   Loss :  10.078237321
Iteration :  98   Loss :  8.15120765916
Iteration :  99   Loss :  6.59263958432
[-0.01054124 -0.00159514 -0.00903111 ...,  0.00989049  0.00495315
  0.00046974]
CROSS VALIDATION 7
Iteration :  0   Loss :  196.588357219
Iteration :  1   Loss :  25.7128141773
Iteration :  2   Loss :  20.2673140793
Iteration :  3   Loss :  16.3920614778
Iteration :  4   Loss :  13.2577843537
Iteration :  5   Loss :  10.7228029987
Iteration :  6   Loss :  8.67252785845
Iteration :  7   Loss :  7.01427970511
Iteration :  8   Loss :  5.6731002292
Iteration :  9   Loss :  4.58836367576
Iteration :  10   Loss :  3.71103635939
Iteration :  11   Loss :  3.00146017926
Iteration :  12   Loss :  2.42755994154
Iteration :  13   Loss :  1.96339345446
Iteration :  14   Loss :  1.58797885525
Iteration :  15   Loss :  1.28434616046
Iteration :  16   Loss :  1.03877016652
Iteration :  17   Loss :  0.840150025023
Iteration :  18   Loss :  0.67950744765
Iteration :  19   Loss :  0.549580857775
Iteration :  20   Loss :  0.444497143154
Iteration :  21   Loss :  0.359506171799
Iteration :  22   Loss :  0.29076607027
Iteration :  23   Loss :  0.235169558279
Iteration :  24   Loss :  0.190203489319
Iteration :  25   Loss :  0.153835248125
Iteration :  26   Loss :  0.124420869724
Iteration :  27   Loss :  0.100630726778
Iteration :  28   Loss :  0.0813894260206
Iteration :  29   Loss :  0.0658271969216
Iteration :  30   Loss :  0.0532405751758
Iteration :  31   Loss :  0.0709417946481
Iteration :  32   Loss :  342.430139739
Iteration :  33   Loss :  40.9582665721
Iteration :  34   Loss :  40.8110221838
Iteration :  35   Loss :  15.5308446184
Iteration :  36   Loss :  12.5612381981
Iteration :  37   Loss :  10.1594413535
Iteration :  38   Loss :  8.21688491107
Iteration :  39   Loss :  6.64575888503
Iteration :  40   Loss :  5.37504317463
Iteration :  41   Loss :  4.34729722051
Iteration :  42   Loss :  3.51606350117
Iteration :  43   Loss :  2.84376749901
Iteration :  44   Loss :  2.30001920777
Iteration :  45   Loss :  1.86023940352
Iteration :  46   Loss :  1.50454858234
Iteration :  47   Loss :  1.2168683409
Iteration :  48   Loss :  0.984194579326
Iteration :  49   Loss :  0.79600967288
Iteration :  50   Loss :  0.643807040425
Iteration :  51   Loss :  0.520706618805
Iteration :  52   Loss :  0.421143861192
Iteration :  53   Loss :  0.340618201141
Iteration :  54   Loss :  0.275489612077
Iteration :  55   Loss :  0.222814066037
Iteration :  56   Loss :  0.180210453852
Iteration :  57   Loss :  0.145752951128
Iteration :  58   Loss :  0.1178839646
Iteration :  59   Loss :  0.0953437237614
Iteration :  60   Loss :  0.0771133350627
Iteration :  61   Loss :  0.0623687245463
Iteration :  62   Loss :  0.0504433869754
Iteration :  63   Loss :  0.0407982575892
Iteration :  64   Loss :  0.03299734459
Iteration :  65   Loss :  0.0266880208698
Iteration :  66   Loss :  0.0215850840968
Iteration :  67   Loss :  0.0174578646255
Iteration :  68   Loss :  1064.54520583
Iteration :  69   Loss :  343.839161668
Iteration :  70   Loss :  130.754473139
Iteration :  71   Loss :  17.6522135158
Iteration :  72   Loss :  14.2769864837
Iteration :  73   Loss :  11.547126533
Iteration :  74   Loss :  9.33923495142
Iteration :  75   Loss :  7.5535077258
Iteration :  76   Loss :  6.10922407033
Iteration :  77   Loss :  4.94109757961
Iteration :  78   Loss :  3.99632506685
Iteration :  79   Loss :  3.2321996849
Iteration :  80   Loss :  2.61418043536
Iteration :  81   Loss :  2.11433080095
Iteration :  82   Loss :  1.71005592246
Iteration :  83   Loss :  1.38308123621
Iteration :  84   Loss :  1.11862640328
Iteration :  85   Loss :  0.904737189225
Iteration :  86   Loss :  0.73174509306
Iteration :  87   Loss :  0.591830299001
Iteration :  88   Loss :  0.478668194892
Iteration :  89   Loss :  0.387143478777
Iteration :  90   Loss :  0.313118930314
Iteration :  91   Loss :  0.253248394705
Iteration :  92   Loss :  0.20482552542
Iteration :  93   Loss :  0.165661448368
Iteration :  94   Loss :  0.133985817535
Iteration :  95   Loss :  0.108366789481
Iteration :  96   Loss :  0.0876462992767
Iteration :  97   Loss :  0.0708877121272
Iteration :  98   Loss :  0.0573334843809
Iteration :  99   Loss :  0.0463709200455
[ -9.06998758e-04  -2.74321496e-04  -4.91678407e-05 ...,   4.06866673e-04
   3.54696619e-04  -5.04302484e-05]
CROSS VALIDATION 8
Iteration :  0   Loss :  242.173163489
Iteration :  1   Loss :  25.3649087605
Iteration :  2   Loss :  20.8831821665
Iteration :  3   Loss :  18.0418569591
Iteration :  4   Loss :  14.5921273677
Iteration :  5   Loss :  11.8020102697
Iteration :  6   Loss :  9.54538313
Iteration :  7   Loss :  7.72023892677
Iteration :  8   Loss :  6.24407509627
Iteration :  9   Loss :  5.05016414358
Iteration :  10   Loss :  4.08453733882
Iteration :  11   Loss :  3.30354515178
Iteration :  12   Loss :  2.67188414857
Iteration :  13   Loss :  2.16100115948
Iteration :  14   Loss :  1.7478025811
Iteration :  15   Loss :  1.41361046897
Iteration :  16   Loss :  1.14331823262
Iteration :  17   Loss :  0.92470776761
Iteration :  18   Loss :  0.747897156789
Iteration :  19   Loss :  0.604893974859
Iteration :  20   Loss :  0.489234004301
Iteration :  21   Loss :  0.395689031321
Iteration :  22   Loss :  0.320030513275
Iteration :  23   Loss :  0.258838434528
Iteration :  24   Loss :  0.209346710422
Iteration :  25   Loss :  0.169318151087
Iteration :  26   Loss :  0.136943333047
Iteration :  27   Loss :  0.110758807284
Iteration :  28   Loss :  0.0895809464988
Iteration :  29   Loss :  0.0724524412313
Iteration :  30   Loss :  0.0585990263057
Iteration :  31   Loss :  0.0473944814781
Iteration :  32   Loss :  0.0383323242072
Iteration :  33   Loss :  0.0310029149661
Iteration :  34   Loss :  0.0250749401784
Iteration :  35   Loss :  0.0202804357473
Iteration :  36   Loss :  0.0164026741908
Iteration :  37   Loss :  203.423199372
Iteration :  38   Loss :  19.7783039322
Iteration :  39   Loss :  15.9965535006
Iteration :  40   Loss :  12.9379002757
Iteration :  41   Loss :  10.464083
Iteration :  42   Loss :  8.46327693812
Iteration :  43   Loss :  6.84503902838
Iteration :  44   Loss :  5.53621955687
Iteration :  45   Loss :  4.47765554802
Iteration :  46   Loss :  3.62149640216
Iteration :  47   Loss :  2.92904088985
Iteration :  48   Loss :  2.42707995252
Iteration :  49   Loss :  10.5382504421
Iteration :  50   Loss :  8.52326304508
Iteration :  51   Loss :  6.89355537096
Iteration :  52   Loss :  5.58277659303
Iteration :  53   Loss :  139.894372478
Iteration :  54   Loss :  101.581085131
Iteration :  55   Loss :  11.2330644281
Iteration :  56   Loss :  9.08522372369
Iteration :  57   Loss :  7.34806522638
Iteration :  58   Loss :  5.94306361772
Iteration :  59   Loss :  4.80670817094
Iteration :  60   Loss :  3.88763185569
Iteration :  61   Loss :  3.14428937806
Iteration :  62   Loss :  2.54307919577
Iteration :  63   Loss :  2.05682461706
Iteration :  64   Loss :  1.66354532426
Iteration :  65   Loss :  1.34546379059
Iteration :  66   Loss :  1.08820167711
Iteration :  67   Loss :  0.880129884087
Iteration :  68   Loss :  0.711842877253
Iteration :  69   Loss :  0.575733526445
Iteration :  70   Loss :  0.465649238147
Iteration :  71   Loss :  0.376613837874
Iteration :  72   Loss :  0.304602630603
Iteration :  73   Loss :  0.24636047123
Iteration :  74   Loss :  0.199254621224
Iteration :  75   Loss :  0.161155740127
Iteration :  76   Loss :  290.648621758
Iteration :  77   Loss :  37.197097072
Iteration :  78   Loss :  26.2517559539
Iteration :  79   Loss :  16.4567115571
Iteration :  80   Loss :  13.3100728844
Iteration :  81   Loss :  10.7650935956
Iteration :  82   Loss :  8.70673219661
Iteration :  83   Loss :  7.04194393392
Iteration :  84   Loss :  5.69547486343
Iteration :  85   Loss :  4.606460123
Iteration :  86   Loss :  3.725672639
Iteration :  87   Loss :  3.01329789954
Iteration :  88   Loss :  2.43713420668
Iteration :  89   Loss :  1.97113705295
Iteration :  90   Loss :  1.5942418234
Iteration :  91   Loss :  1.28941160518
Iteration :  92   Loss :  1.04286706268
Iteration :  93   Loss :  0.8434635659
Iteration :  94   Loss :  0.682187416271
Iteration :  95   Loss :  0.551748397599
Iteration :  96   Loss :  0.446250234162
Iteration :  97   Loss :  0.360924059509
Iteration :  98   Loss :  0.291912847906
Iteration :  99   Loss :  0.236097063987
[ -1.52640348e-03  -1.34770362e-03  -1.28187569e-05 ...,   1.02186104e-03
   6.37220728e-04  -4.53130556e-05]
CROSS VALIDATION 9
Iteration :  0   Loss :  176.057144972
Iteration :  1   Loss :  25.5351221877
Iteration :  2   Loss :  20.2710745124
Iteration :  3   Loss :  16.3951028897
Iteration :  4   Loss :  13.2602442263
Iteration :  5   Loss :  10.7247925264
Iteration :  6   Loss :  8.67413697454
Iteration :  7   Loss :  7.01558114692
Iteration :  8   Loss :  5.67415282621
Iteration :  9   Loss :  4.58921500884
Iteration :  10   Loss :  3.71172491162
Iteration :  11   Loss :  3.00201707546
Iteration :  12   Loss :  2.42801035529
Iteration :  13   Loss :  1.96375774594
Iteration :  14   Loss :  1.58827349164
Iteration :  15   Loss :  1.28458446031
Iteration :  16   Loss :  1.03896290177
Iteration :  17   Loss :  0.840305907938
Iteration :  18   Loss :  0.679633524659
Iteration :  19   Loss :  0.549682827975
Iteration :  20   Loss :  0.444579615937
Iteration :  21   Loss :  0.359572875207
Iteration :  22   Loss :  0.290820019519
Iteration :  23   Loss :  0.235213192053
Iteration :  24   Loss :  0.190238780009
Iteration :  25   Loss :  0.153863790987
Iteration :  26   Loss :  0.124443954991
Iteration :  27   Loss :  0.10064939798
Iteration :  28   Loss :  0.081404527158
Iteration :  29   Loss :  0.0658394106157
Iteration :  30   Loss :  0.0532504535259
Iteration :  31   Loss :  0.0430685933272
Iteration :  32   Loss :  0.0348335762114
Iteration :  33   Loss :  0.0281731521263
Iteration :  34   Loss :  0.0227862478408
Iteration :  35   Loss :  0.018429357437
Iteration :  36   Loss :  0.0149055350364
Iteration :  37   Loss :  0.012055492194
Iteration :  38   Loss :  691.987833845
Iteration :  39   Loss :  106.227487855
Iteration :  40   Loss :  148.731304296
Iteration :  41   Loss :  150.682244588
Iteration :  42   Loss :  15.4287202866
Iteration :  43   Loss :  12.4786407549
Iteration :  44   Loss :  10.0926371207
Iteration :  45   Loss :  8.16285411612
Iteration :  46   Loss :  6.60205915703
Iteration :  47   Loss :  5.33969914112
Iteration :  48   Loss :  4.31871121411
Iteration :  49   Loss :  3.49294333968
Iteration :  50   Loss :  2.82506807456
Iteration :  51   Loss :  2.28489524443
Iteration :  52   Loss :  1.84800724805
Iteration :  53   Loss :  1.49465530079
Iteration :  54   Loss :  1.20886672416
Iteration :  55   Loss :  0.977722927833
Iteration :  56   Loss :  0.790775446543
Iteration :  57   Loss :  0.639573634875
Iteration :  58   Loss :  0.517282670088
Iteration :  59   Loss :  0.418374595484
Iteration :  60   Loss :  0.338378438459
Iteration :  61   Loss :  0.27367810773
Iteration :  62   Loss :  0.221348934026
Iteration :  63   Loss :  0.179025465358
Iteration :  64   Loss :  0.144794540745
Iteration :  65   Loss :  0.117108809004
Iteration :  66   Loss :  0.0947167833522
Iteration :  67   Loss :  0.109821183298
Iteration :  68   Loss :  254.618253388
Iteration :  69   Loss :  53.8146008728
Iteration :  70   Loss :  18.2356747438
Iteration :  71   Loss :  14.7488858326
Iteration :  72   Loss :  11.9287954166
Iteration :  73   Loss :  9.64792606759
Iteration :  74   Loss :  7.80317493548
Iteration :  75   Loss :  6.31115315842
Iteration :  76   Loss :  5.10441640978
Iteration :  77   Loss :  4.12841619121
Iteration :  78   Loss :  3.33903405984
Iteration :  79   Loss :  2.7005873285
Iteration :  80   Loss :  2.18421609009
Iteration :  81   Loss :  1.76657865416
Iteration :  82   Loss :  1.42879642517
Iteration :  83   Loss :  1.15560052748
Iteration :  84   Loss :  0.93464160155
Iteration :  85   Loss :  0.755931572006
Iteration :  86   Loss :  0.611392153535
Iteration :  87   Loss :  0.494489685638
Iteration :  88   Loss :  0.399939789525
Iteration :  89   Loss :  0.323468496696
Iteration :  90   Loss :  0.261619051406
Iteration :  91   Loss :  0.211595653851
Iteration :  92   Loss :  0.171137080759
Iteration :  93   Loss :  0.138414470608
Iteration :  94   Loss :  0.111948653025
Iteration :  95   Loss :  0.0905432853888
Iteration :  96   Loss :  0.122857280892
Iteration :  97   Loss :  163.976444407
Iteration :  98   Loss :  105.637745657
Iteration :  99   Loss :  15.5346622357
[-0.0134227  -0.00300444  0.00882334 ..., -0.00123642  0.00088756
  0.00193287]
CROSS VALIDATION 10
Iteration :  0   Loss :  102.231615605
Iteration :  1   Loss :  21.1836780949
Iteration :  2   Loss :  17.1332102665
Iteration :  3   Loss :  13.8572202958
Iteration :  4   Loss :  11.2076225843
Iteration :  5   Loss :  9.06464653894
Iteration :  6   Loss :  7.33142254367
Iteration :  7   Loss :  5.92960313268
Iteration :  8   Loss :  4.79582142505
Iteration :  9   Loss :  3.87882673197
Iteration :  10   Loss :  3.13716785575
Iteration :  11   Loss :  2.53731935847
Iteration :  12   Loss :  2.05216610105
Iteration :  13   Loss :  1.65977754918
Iteration :  14   Loss :  1.34241644054
Iteration :  15   Loss :  1.08573700176
Iteration :  16   Loss :  0.878136471951
Iteration :  17   Loss :  0.710230619499
Iteration :  18   Loss :  0.574429543682
Iteration :  19   Loss :  0.464594586034
Iteration :  20   Loss :  0.375760842641
Iteration :  21   Loss :  0.303912734042
Iteration :  22   Loss :  0.24580248773
Iteration :  23   Loss :  0.198803328083
Iteration :  24   Loss :  0.1607907374
Iteration :  25   Loss :  0.130046420665
Iteration :  26   Loss :  0.105180632923
Iteration :  27   Loss :  0.0850693581993
Iteration :  28   Loss :  0.0688035002577
Iteration :  29   Loss :  0.055647788439
Iteration :  30   Loss :  0.0450075409907
Iteration :  31   Loss :  0.0364017834823
Iteration :  32   Loss :  0.0867431697874
Iteration :  33   Loss :  144.577629763
Iteration :  34   Loss :  40.5459932288
Iteration :  35   Loss :  64.129193929
Iteration :  36   Loss :  16.0527958732
Iteration :  37   Loss :  12.9833887122
Iteration :  38   Loss :  10.5008737284
Iteration :  39   Loss :  8.49303302121
Iteration :  40   Loss :  6.86910553968
Iteration :  41   Loss :  5.55568438242
Iteration :  42   Loss :  4.49339856241
Iteration :  43   Loss :  3.63422924177
Iteration :  44   Loss :  2.93933912123
Iteration :  45   Loss :  2.37731686551
Iteration :  46   Loss :  1.92275720695
Iteration :  47   Loss :  1.55511254328
Iteration :  48   Loss :  1.25776411786
Iteration :  49   Loss :  1.0172707969
Iteration :  50   Loss :  0.822761485666
Iteration :  51   Loss :  0.665443718976
Iteration :  52   Loss :  0.538206212663
Iteration :  53   Loss :  0.435297409966
Iteration :  54   Loss :  109.759168927
Iteration :  55   Loss :  26.9225388757
Iteration :  56   Loss :  47.2615100375
Iteration :  57   Loss :  73.4772806393
Iteration :  58   Loss :  10.9409512296
Iteration :  59   Loss :  8.84896461759
Iteration :  60   Loss :  7.15698051842
Iteration :  61   Loss :  5.78851564614
Iteration :  62   Loss :  4.68171085549
Iteration :  63   Loss :  3.78653490365
Iteration :  64   Loss :  3.06252287232
Iteration :  65   Loss :  2.47694701941
Iteration :  66   Loss :  2.00333737665
Iteration :  67   Loss :  1.62028521936
Iteration :  68   Loss :  1.31047532117
Iteration :  69   Loss :  1.05990324844
Iteration :  70   Loss :  0.857242313457
Iteration :  71   Loss :  0.693331570655
Iteration :  72   Loss :  0.560761711502
Iteration :  73   Loss :  0.45354013923
Iteration :  74   Loss :  0.366820083599
Iteration :  75   Loss :  0.296681510836
Iteration :  76   Loss :  0.239953925119
Iteration :  77   Loss :  0.194073051664
Iteration :  78   Loss :  0.156964923009
Iteration :  79   Loss :  0.126952128819
Iteration :  80   Loss :  0.102677991381
Iteration :  81   Loss :  0.0830452392725
Iteration :  82   Loss :  0.0671664070664
Iteration :  83   Loss :  0.054323718948
Iteration :  84   Loss :  0.043936642873
Iteration :  85   Loss :  0.0355356485958
Iteration :  86   Loss :  0.0287409833467
Iteration :  87   Loss :  0.0232455057492
Iteration :  88   Loss :  0.0188008020121
Iteration :  89   Loss :  0.0152059568036
Iteration :  90   Loss :  0.0233088874898
Iteration :  91   Loss :  298.900450191
Iteration :  92   Loss :  191.433436073
Iteration :  93   Loss :  19.557673814
Iteration :  94   Loss :  15.8181094084
Iteration :  95   Loss :  12.7935759455
Iteration :  96   Loss :  10.347354494
Iteration :  97   Loss :  8.36886774116
Iteration :  98   Loss :  6.76868153204
Iteration :  99   Loss :  5.47446214938
[ -6.94657008e-03  -6.93558552e-04  -5.23906699e-03 ...,   6.11509363e-03
  -2.76793456e-05   1.11369305e-03]
CROSS VALIDATION 11
Iteration :  0   Loss :  196.788356382
Iteration :  1   Loss :  25.5804729967
Iteration :  2   Loss :  20.2747802214
Iteration :  3   Loss :  16.3981000412
Iteration :  4   Loss :  13.2626683014
Iteration :  5   Loss :  10.7267531013
Iteration :  6   Loss :  8.67572267378
Iteration :  7   Loss :  7.01686364934
Iteration :  8   Loss :  5.67519010517
Iteration :  9   Loss :  4.59005395279
Iteration :  10   Loss :  3.7124034436
Iteration :  11   Loss :  3.00256586737
Iteration :  12   Loss :  2.42845421433
Iteration :  13   Loss :  1.96411673602
Iteration :  14   Loss :  1.5885638403
Iteration :  15   Loss :  1.28481929227
Iteration :  16   Loss :  1.03915283221
Iteration :  17   Loss :  0.840459522349
Iteration :  18   Loss :  0.679757766916
Iteration :  19   Loss :  0.549783314242
Iteration :  20   Loss :  0.444660888525
Iteration :  21   Loss :  0.359638607906
Iteration :  22   Loss :  0.290873183665
Iteration :  23   Loss :  0.235256190841
Iteration :  24   Loss :  0.190273557128
Iteration :  25   Loss :  0.153891918477
Iteration :  26   Loss :  0.124466704307
Iteration :  27   Loss :  0.100667797467
Iteration :  28   Loss :  0.0814194085344
Iteration :  29   Loss :  0.0658514465685
Iteration :  30   Loss :  0.0532601881201
Iteration :  31   Loss :  0.0716068153091
Iteration :  32   Loss :  215.598615104
Iteration :  33   Loss :  18.1120249476
Iteration :  34   Loss :  14.6488787447
Iteration :  35   Loss :  11.8479103854
Iteration :  36   Loss :  9.5825068216
Iteration :  37   Loss :  7.75026430815
Iteration :  38   Loss :  6.26835941414
Iteration :  39   Loss :  5.06980512955
Iteration :  40   Loss :  4.10042283052
Iteration :  41   Loss :  3.31639322605
Iteration :  42   Loss :  2.68227558093
Iteration :  43   Loss :  2.16940567709
Iteration :  44   Loss :  1.75460009599
Iteration :  45   Loss :  1.41910825134
Iteration :  46   Loss :  1.14776480044
Iteration :  47   Loss :  0.928304120477
Iteration :  48   Loss :  0.750805861764
Iteration :  49   Loss :  0.607246515042
Iteration :  50   Loss :  0.4911367223
Iteration :  51   Loss :  0.397227936294
Iteration :  52   Loss :  0.321275168824
Iteration :  53   Loss :  0.259845103206
Iteration :  54   Loss :  0.210160896989
Iteration :  55   Loss :  0.169976659472
Iteration :  56   Loss :  0.13747593001
Iteration :  57   Loss :  0.111189567972
Iteration :  58   Loss :  0.0899293427219
Iteration :  59   Loss :  0.072734221653
Iteration :  60   Loss :  0.0977279529711
Iteration :  61   Loss :  147.899778614
Iteration :  62   Loss :  195.793216376
Iteration :  63   Loss :  16.2205643035
Iteration :  64   Loss :  13.1190786419
Iteration :  65   Loss :  10.6106187919
Iteration :  66   Loss :  8.58179405881
Iteration :  67   Loss :  6.94089484434
Iteration :  68   Loss :  5.61374706852
Iteration :  69   Loss :  4.54035925569
Iteration :  70   Loss :  3.67221072113
Iteration :  71   Loss :  2.97005827534
Iteration :  72   Loss :  2.4021623019
Iteration :  73   Loss :  1.94285202165
Iteration :  74   Loss :  1.57136508846
Iteration :  75   Loss :  37.6107048194
Iteration :  76   Loss :  6.14808826957
Iteration :  77   Loss :  4.97253067137
Iteration :  78   Loss :  4.021747931
Iteration :  79   Loss :  3.25276151912
Iteration :  80   Loss :  2.63081070266
Iteration :  81   Loss :  2.12778124451
Iteration :  82   Loss :  1.72093454686
Iteration :  83   Loss :  1.39187979133
Iteration :  84   Loss :  1.12574261295
Iteration :  85   Loss :  0.910492729695
Iteration :  86   Loss :  0.736400133824
Iteration :  87   Loss :  0.595595263322
Iteration :  88   Loss :  0.481713271628
Iteration :  89   Loss :  0.389606315484
Iteration :  90   Loss :  0.315110855369
Iteration :  91   Loss :  0.254859449718
Iteration :  92   Loss :  0.206128535416
Iteration :  93   Loss :  0.166715313714
Iteration :  94   Loss :  0.134838176435
Iteration :  95   Loss :  0.109056171381
Iteration :  96   Loss :  0.1218653044
Iteration :  97   Loss :  551.269908924
Iteration :  98   Loss :  18.6688576705
Iteration :  99   Loss :  16.0869915138
[-0.00691854 -0.01118691 -0.00893299 ...,  0.01783243 -0.00385665
  0.00237744]
CROSS VALIDATION 12
Iteration :  0   Loss :  137.809475855
Iteration :  1   Loss :  59.0760447008
Iteration :  2   Loss :  42.2964849624
Iteration :  3   Loss :  15.8268073659
Iteration :  4   Loss :  12.8006107925
Iteration :  5   Loss :  10.3530442289
Iteration :  6   Loss :  8.37346955883
Iteration :  7   Loss :  6.77240344989
Iteration :  8   Loss :  5.47747241044
Iteration :  9   Loss :  4.43014126803
Iteration :  10   Loss :  3.58306718575
Iteration :  11   Loss :  2.89795960915
Iteration :  12   Loss :  2.34384940636
Iteration :  13   Loss :  1.89568896073
Iteration :  14   Loss :  1.53321993558
Iteration :  15   Loss :  1.2400575303
Iteration :  16   Loss :  1.00294983307
Iteration :  17   Loss :  0.811178790567
Iteration :  18   Loss :  0.656075716419
Iteration :  19   Loss :  0.530629437899
Iteration :  20   Loss :  0.441766442331
Iteration :  21   Loss :  136.655699786
Iteration :  22   Loss :  93.5344446245
Iteration :  23   Loss :  11.7556566281
Iteration :  24   Loss :  9.50789262979
Iteration :  25   Loss :  7.68991687315
Iteration :  26   Loss :  6.21955083197
Iteration :  27   Loss :  5.03032909062
Iteration :  28   Loss :  4.06849488711
Iteration :  29   Loss :  3.29057012935
Iteration :  30   Loss :  2.66139004144
Iteration :  31   Loss :  2.15251359923
Iteration :  32   Loss :  1.7409379019
Iteration :  33   Loss :  1.40805836458
Iteration :  34   Loss :  1.13882772953
Iteration :  35   Loss :  0.921075880207
Iteration :  36   Loss :  0.74495971173
Iteration :  37   Loss :  0.602518189898
Iteration :  38   Loss :  0.487312486087
Iteration :  39   Loss :  364.24934349
Iteration :  40   Loss :  183.918092168
Iteration :  41   Loss :  86.2545453555
Iteration :  42   Loss :  17.2457256581
Iteration :  43   Loss :  13.9482219554
Iteration :  44   Loss :  11.2812240884
Iteration :  45   Loss :  9.12417491918
Iteration :  46   Loss :  7.37956868009
Iteration :  47   Loss :  5.96854339013
Iteration :  48   Loss :  4.82731603216
Iteration :  49   Loss :  3.9042993493
Iteration :  50   Loss :  3.15776993
Iteration :  51   Loss :  2.55398217162
Iteration :  52   Loss :  2.06564286746
Iteration :  53   Loss :  1.67067746333
Iteration :  54   Loss :  1.35123221465
Iteration :  55   Loss :  1.09286713803
Iteration :  56   Loss :  0.883903276159
Iteration :  57   Loss :  99.336199898
Iteration :  58   Loss :  6.38774225752
Iteration :  59   Loss :  5.16636113596
Iteration :  60   Loss :  4.17851665128
Iteration :  61   Loss :  3.37955496055
Iteration :  62   Loss :  2.73336034879
Iteration :  63   Loss :  2.21072267904
Iteration :  64   Loss :  1.78801699739
Iteration :  65   Loss :  1.44613560681
Iteration :  66   Loss :  1.16962433598
Iteration :  67   Loss :  0.945983959506
Iteration :  68   Loss :  0.765105191569
Iteration :  69   Loss :  0.618811712697
Iteration :  70   Loss :  0.500490573048
Iteration :  71   Loss :  0.404793265174
Iteration :  72   Loss :  0.32739395376
Iteration :  73   Loss :  239.013137269
Iteration :  74   Loss :  302.983694186
Iteration :  75   Loss :  51.8526564745
Iteration :  76   Loss :  16.2163201498
Iteration :  77   Loss :  13.1156460002
Iteration :  78   Loss :  10.6078424953
Iteration :  79   Loss :  8.57954860963
Iteration :  80   Loss :  6.9390787408
Iteration :  81   Loss :  5.6122782167
Iteration :  82   Loss :  4.53917125864
Iteration :  83   Loss :  3.67124987745
Iteration :  84   Loss :  2.96928115171
Iteration :  85   Loss :  2.40153376976
Iteration :  86   Loss :  1.94234366926
Iteration :  87   Loss :  1.57095393662
Iteration :  88   Loss :  1.27057652568
Iteration :  89   Loss :  1.02763338249
Iteration :  90   Loss :  0.831142672208
Iteration :  91   Loss :  0.672222363868
Iteration :  92   Loss :  0.543688733108
Iteration :  93   Loss :  0.439731634049
Iteration :  94   Loss :  0.355651861459
Iteration :  95   Loss :  0.2876487311
Iteration :  96   Loss :  334.675213407
Iteration :  97   Loss :  171.064933538
Iteration :  98   Loss :  12.4977983817
Iteration :  99   Loss :  10.1081316749
[-0.02116412 -0.00314782  0.00373234 ...,  0.00562225  0.0053829
  0.00223058]
CROSS VALIDATION 13
Iteration :  0   Loss :  197.416754612
Iteration :  1   Loss :  25.6628471338
Iteration :  2   Loss :  20.2779096875
Iteration :  3   Loss :  16.4006311314
Iteration :  4   Loss :  13.2647154294
Iteration :  5   Loss :  10.7284088041
Iteration :  6   Loss :  8.67706179458
Iteration :  7   Loss :  7.01794672086
Iteration :  8   Loss :  5.67606608582
Iteration :  9   Loss :  4.59076243979
Iteration :  10   Loss :  3.71297646292
Iteration :  11   Loss :  3.00302932138
Iteration :  12   Loss :  2.42882905268
Iteration :  13   Loss :  1.96441990265
Iteration :  14   Loss :  1.58880903934
Iteration :  15   Loss :  1.28501760754
Iteration :  16   Loss :  1.0393132282
Iteration :  17   Loss :  0.840589249505
Iteration :  18   Loss :  0.679862689331
Iteration :  19   Loss :  0.549868174755
Iteration :  20   Loss :  0.444729523112
Iteration :  21   Loss :  0.359694119079
Iteration :  22   Loss :  0.290918080712
Iteration :  23   Loss :  0.235292503258
Iteration :  24   Loss :  0.190302926356
Iteration :  25   Loss :  0.153915672103
Iteration :  26   Loss :  0.124485916073
Iteration :  27   Loss :  0.100683335809
Iteration :  28   Loss :  0.081431975836
Iteration :  29   Loss :  0.0658616109139
Iteration :  30   Loss :  0.0532684089714
Iteration :  31   Loss :  0.0736130927807
Iteration :  32   Loss :  342.18923232
Iteration :  33   Loss :  41.1050203579
Iteration :  34   Loss :  40.8639655758
Iteration :  35   Loss :  15.5385579939
Iteration :  36   Loss :  12.5674767222
Iteration :  37   Loss :  10.1644870279
Iteration :  38   Loss :  8.22096581719
Iteration :  39   Loss :  6.64905949328
Iteration :  40   Loss :  5.37771268343
Iteration :  41   Loss :  4.34945630051
Iteration :  42   Loss :  3.51780974992
Iteration :  43   Loss :  2.84517985276
Iteration :  44   Loss :  2.30116150957
Iteration :  45   Loss :  1.86116328921
Iteration :  46   Loss :  1.50529581461
Iteration :  47   Loss :  1.21747269711
Iteration :  48   Loss :  0.984683378394
Iteration :  49   Loss :  0.796405010138
Iteration :  50   Loss :  0.644126786427
Iteration :  51   Loss :  0.520965227128
Iteration :  52   Loss :  0.42135302179
Iteration :  53   Loss :  0.340787368765
Iteration :  54   Loss :  0.275626433666
Iteration :  55   Loss :  0.222924726378
Iteration :  56   Loss :  0.18029995516
Iteration :  57   Loss :  0.145825339157
Iteration :  58   Loss :  0.11794251153
Iteration :  59   Loss :  0.0953910761084
Iteration :  60   Loss :  0.0771516333089
Iteration :  61   Loss :  0.0623996998992
Iteration :  62   Loss :  0.0504684396236
Iteration :  63   Loss :  0.0408185199953
Iteration :  64   Loss :  0.033013732682
Iteration :  65   Loss :  0.0267012754436
Iteration :  66   Loss :  0.0215958043032
Iteration :  67   Loss :  0.0174665350532
Iteration :  68   Loss :  1064.47015884
Iteration :  69   Loss :  322.856347808
Iteration :  70   Loss :  130.975733067
Iteration :  71   Loss :  17.6590426684
Iteration :  72   Loss :  14.2825098544
Iteration :  73   Loss :  11.5515937965
Iteration :  74   Loss :  9.3428480428
Iteration :  75   Loss :  7.55642996876
Iteration :  76   Loss :  6.11158755995
Iteration :  77   Loss :  4.94300915344
Iteration :  78   Loss :  3.99787113435
Iteration :  79   Loss :  3.23345013346
Iteration :  80   Loss :  2.61519178938
Iteration :  81   Loss :  2.11514877699
Iteration :  82   Loss :  1.71071749574
Iteration :  83   Loss :  1.38361631204
Iteration :  84   Loss :  1.11905916886
Iteration :  85   Loss :  0.90508720699
Iteration :  86   Loss :  0.732028184973
Iteration :  87   Loss :  0.592059261755
Iteration :  88   Loss :  0.478853378361
Iteration :  89   Loss :  0.387293253868
Iteration :  90   Loss :  0.313240067357
Iteration :  91   Loss :  0.253346369496
Iteration :  92   Loss :  0.204904766744
Iteration :  93   Loss :  0.165725538195
Iteration :  94   Loss :  0.134037652937
Iteration :  95   Loss :  0.108408713591
Iteration :  96   Loss :  0.0876802072028
Iteration :  97   Loss :  0.07091513662
Iteration :  98   Loss :  0.0573556651183
Iteration :  99   Loss :  0.0463888596703
[ -9.06794454e-04  -2.74196748e-04  -4.91046086e-05 ...,   4.06917661e-04
   3.54802239e-04  -5.04282138e-05]
CROSS VALIDATION 14
Iteration :  0   Loss :  197.416754612
Iteration :  1   Loss :  23.4755831513
Iteration :  2   Loss :  18.9262081522
Iteration :  3   Loss :  15.3073844101
Iteration :  4   Loss :  12.3805051489
Iteration :  5   Loss :  10.0132657309
Iteration :  6   Loss :  8.09865909277
Iteration :  7   Loss :  6.55013867239
Iteration :  8   Loss :  5.2977062173
Iteration :  9   Loss :  4.28474763185
Iteration :  10   Loss :  3.46547383256
Iteration :  11   Loss :  2.80285092986
Iteration :  12   Loss :  2.26692617362
Iteration :  13   Loss :  1.83347398961
Iteration :  14   Loss :  1.4829009033
Iteration :  15   Loss :  1.19935984991
Iteration :  16   Loss :  0.97003383462
Iteration :  17   Loss :  0.784556561881
Iteration :  18   Loss :  0.634543844578
Iteration :  19   Loss :  0.513214610973
Iteration :  20   Loss :  0.415084377805
Iteration :  21   Loss :  0.335717333477
Iteration :  22   Loss :  0.271525824684
Iteration :  23   Loss :  0.219608182595
Iteration :  24   Loss :  0.177617557809
Iteration :  25   Loss :  0.143655834994
Iteration :  26   Loss :  0.116187831779
Iteration :  27   Loss :  0.0939719034324
Iteration :  28   Loss :  0.0760038164022
Iteration :  29   Loss :  0.0614713536356
Iteration :  30   Loss :  0.0497175996767
Iteration :  31   Loss :  0.0402112459125
Iteration :  32   Loss :  0.0325225736631
Iteration :  33   Loss :  0.026304029474
Iteration :  34   Loss :  208.043625999
Iteration :  35   Loss :  50.6207944656
Iteration :  36   Loss :  16.0647804208
Iteration :  37   Loss :  12.9930817302
Iteration :  38   Loss :  10.5087133734
Iteration :  39   Loss :  8.49937367108
Iteration :  40   Loss :  6.87423381283
Iteration :  41   Loss :  5.55983209377
Iteration :  42   Loss :  4.49675320226
Iteration :  43   Loss :  3.63694245095
Iteration :  44   Loss :  2.94153354579
Iteration :  45   Loss :  2.40135197515
Iteration :  46   Loss :  38.9401267488
Iteration :  47   Loss :  57.63313095
Iteration :  48   Loss :  7.62166815564
Iteration :  49   Loss :  6.16435174793
Iteration :  50   Loss :  4.9856844586
Iteration :  51   Loss :  4.03238662185
Iteration :  52   Loss :  3.26136601767
Iteration :  53   Loss :  2.63776996074
Iteration :  54   Loss :  2.13340984363
Iteration :  55   Loss :  1.72548691836
Iteration :  56   Loss :  1.39556171746
Iteration :  57   Loss :  1.12872052898
Iteration :  58   Loss :  0.912901247296
Iteration :  59   Loss :  0.738348125966
Iteration :  60   Loss :  0.597170785704
Iteration :  61   Loss :  0.482987543081
Iteration :  62   Loss :  0.390636937298
Iteration :  63   Loss :  0.315944415063
Iteration :  64   Loss :  0.255533626953
Iteration :  65   Loss :  0.206673805234
Iteration :  66   Loss :  0.167156324119
Iteration :  67   Loss :  0.135194862557
Iteration :  68   Loss :  0.109344656615
Iteration :  69   Loss :  0.0884371913562
Iteration :  70   Loss :  0.0715273800939
Iteration :  71   Loss :  0.057850843346
Iteration :  72   Loss :  0.0467893563478
Iteration :  73   Loss :  0.037842903246
Iteration :  74   Loss :  0.0306070747253
Iteration :  75   Loss :  0.024754787368
Iteration :  76   Loss :  0.0200214983997
Iteration :  77   Loss :  0.0161932474802
Iteration :  78   Loss :  0.0130969849869
Iteration :  79   Loss :  0.0105927495986
Iteration :  80   Loss :  978.231178104
Iteration :  81   Loss :  316.507295913
Iteration :  82   Loss :  22.6830093724
Iteration :  83   Loss :  18.3458588878
Iteration :  84   Loss :  14.8380019955
Iteration :  85   Loss :  12.0008719442
Iteration :  86   Loss :  9.70622105751
Iteration :  87   Loss :  7.85032351446
Iteration :  88   Loss :  6.34928659842
Iteration :  89   Loss :  5.13525846859
Iteration :  90   Loss :  4.15336103206
Iteration :  91   Loss :  3.35920927216
Iteration :  92   Loss :  2.71690489872
Iteration :  93   Loss :  2.19741362644
Iteration :  94   Loss :  1.77725272899
Iteration :  95   Loss :  1.43742954203
Iteration :  96   Loss :  1.16258293185
Iteration :  97   Loss :  0.940288921229
Iteration :  98   Loss :  0.760499084554
Iteration :  99   Loss :  0.615086325649
[-0.00305552 -0.00184363 -0.00174526 ...,  0.00210106 -0.00026512
  0.0001224 ]
CROSS VALIDATION 15
Iteration :  0   Loss :  197.416754612
Iteration :  1   Loss :  25.6628471338
Iteration :  2   Loss :  20.2833971763
Iteration :  3   Loss :  16.4050693738
Iteration :  4   Loss :  13.2683050488
Iteration :  5   Loss :  10.7313120631
Iteration :  6   Loss :  8.67940993005
Iteration :  7   Loss :  7.01984587635
Iteration :  8   Loss :  5.67760210946
Iteration :  9   Loss :  4.59200476494
Iteration :  10   Loss :  3.71398124679
Iteration :  11   Loss :  3.00384198352
Iteration :  12   Loss :  2.42948632813
Iteration :  13   Loss :  1.96495150242
Iteration :  14   Loss :  1.58923899351
Iteration :  15   Loss :  1.28536535145
Iteration :  16   Loss :  1.03959448104
Iteration :  17   Loss :  0.840816724828
Iteration :  18   Loss :  0.680046669776
Iteration :  19   Loss :  0.550016976848
Iteration :  20   Loss :  0.444849873202
Iteration :  21   Loss :  0.359791457387
Iteration :  22   Loss :  0.290996807253
Iteration :  23   Loss :  0.235356176732
Iteration :  24   Loss :  0.190354425015
Iteration :  25   Loss :  0.153957323857
Iteration :  26   Loss :  0.12451960372
Iteration :  27   Loss :  0.100710582141
Iteration :  28   Loss :  0.0814540124787
Iteration :  29   Loss :  0.065879433996
Iteration :  30   Loss :  0.0532828241551
Iteration :  31   Loss :  0.0741957842785
Iteration :  32   Loss :  342.179890341
Iteration :  33   Loss :  41.1072039499
Iteration :  34   Loss :  40.8633420427
Iteration :  35   Loss :  15.5385736242
Iteration :  36   Loss :  12.5674893638
Iteration :  37   Loss :  10.1644972524
Iteration :  38   Loss :  8.2209740867
Iteration :  39   Loss :  6.6490661816
Iteration :  40   Loss :  5.37771809289
Iteration :  41   Loss :  4.34946067565
Iteration :  42   Loss :  3.5178132885
Iteration :  43   Loss :  2.84518271474
Iteration :  44   Loss :  2.30116382432
Iteration :  45   Loss :  1.86116516136
Iteration :  46   Loss :  1.50529732879
Iteration :  47   Loss :  1.21747392177
Iteration :  48   Loss :  0.984684368891
Iteration :  49   Loss :  0.796405811245
Iteration :  50   Loss :  0.644127434357
Iteration :  51   Loss :  0.520965751169
Iteration :  52   Loss :  0.42135344563
Iteration :  53   Loss :  0.340787711565
Iteration :  54   Loss :  0.275626710919
Iteration :  55   Loss :  0.222924950619
Iteration :  56   Loss :  0.180300136524
Iteration :  57   Loss :  0.145825485844
Iteration :  58   Loss :  0.117942630169
Iteration :  59   Loss :  0.0953911720627
Iteration :  60   Loss :  0.077151710916
Iteration :  61   Loss :  0.0623997626673
Iteration :  62   Loss :  0.05046849039
Iteration :  63   Loss :  0.0408185610549
Iteration :  64   Loss :  0.0330137658906
Iteration :  65   Loss :  0.0267013023026
Iteration :  66   Loss :  0.0215958260265
Iteration :  67   Loss :  0.0174665526229
Iteration :  68   Loss :  1064.46748846
Iteration :  69   Loss :  345.303871075
Iteration :  70   Loss :  130.975704471
Iteration :  71   Loss :  17.6590428266
Iteration :  72   Loss :  14.2825099823
Iteration :  73   Loss :  11.5515939
Iteration :  74   Loss :  9.3428481265
Iteration :  75   Loss :  7.55643003646
Iteration :  76   Loss :  6.1115876147
Iteration :  77   Loss :  4.94300919772
Iteration :  78   Loss :  3.99787117016
Iteration :  79   Loss :  3.23345016242
Iteration :  80   Loss :  2.61519181281
Iteration :  81   Loss :  2.11514879594
Iteration :  82   Loss :  1.71071751107
Iteration :  83   Loss :  1.38361632443
Iteration :  84   Loss :  1.11905917888
Iteration :  85   Loss :  0.905087215098
Iteration :  86   Loss :  0.73202819153
Iteration :  87   Loss :  0.592059267059
Iteration :  88   Loss :  0.478853382651
Iteration :  89   Loss :  0.387293257338
Iteration :  90   Loss :  0.313240070163
Iteration :  91   Loss :  0.253346371765
Iteration :  92   Loss :  0.20490476858
Iteration :  93   Loss :  0.16572553968
Iteration :  94   Loss :  0.134037654138
Iteration :  95   Loss :  0.108408714563
Iteration :  96   Loss :  0.0876802079883
Iteration :  97   Loss :  0.0709151372553
Iteration :  98   Loss :  0.0573556656322
Iteration :  99   Loss :  0.0463888600859
[ -9.06794852e-04  -2.74196942e-04  -4.91047837e-05 ...,   4.06917931e-04
   3.54802508e-04  -5.04281510e-05]
CROSS VALIDATION 16
Iteration :  0   Loss :  197.416754612
Iteration :  1   Loss :  19.4016130122
Iteration :  2   Loss :  15.6918885265
Iteration :  3   Loss :  12.6914893815
Iteration :  4   Loss :  10.264787597
Iteration :  5   Loss :  8.30208821398
Iteration :  6   Loss :  6.71467071883
Iteration :  7   Loss :  5.43077858248
Iteration :  8   Loss :  4.39237562747
Iteration :  9   Loss :  3.55252260055
Iteration :  10   Loss :  2.87325536287
Iteration :  11   Loss :  2.32386878524
Iteration :  12   Loss :  1.87952877451
Iteration :  13   Loss :  1.52014969031
Iteration :  14   Loss :  1.22948640759
Iteration :  15   Loss :  0.994399983172
Iteration :  16   Loss :  0.804263731932
Iteration :  17   Loss :  0.650482865495
Iteration :  18   Loss :  0.526105979299
Iteration :  19   Loss :  0.425510826089
Iteration :  20   Loss :  0.344150171721
Iteration :  21   Loss :  0.278346245111
Iteration :  22   Loss :  0.225124490799
Iteration :  23   Loss :  635.958791237
Iteration :  24   Loss :  64.2173034594
Iteration :  25   Loss :  71.2432279146
Iteration :  26   Loss :  39.2709293128
Iteration :  27   Loss :  12.62321797
Iteration :  28   Loss :  10.2095701582
Iteration :  29   Loss :  8.25742873675
Iteration :  30   Loss :  6.67855044687
Iteration :  31   Loss :  5.40156475985
Iteration :  32   Loss :  4.36874769263
Iteration :  33   Loss :  3.53341249257
Iteration :  34   Loss :  2.85779924157
Iteration :  35   Loss :  2.31136798274
Iteration :  36   Loss :  1.86941821312
Iteration :  37   Loss :  1.51197233917
Iteration :  38   Loss :  1.22287262335
Iteration :  39   Loss :  0.989050800862
Iteration :  40   Loss :  0.799937350798
Iteration :  41   Loss :  0.646983718777
Iteration :  42   Loss :  0.523275893975
Iteration :  43   Loss :  0.423221872929
Iteration :  44   Loss :  0.34229888246
Iteration :  45   Loss :  0.27684893534
Iteration :  46   Loss :  0.223913477158
Iteration :  47   Loss :  0.181099649856
Iteration :  48   Loss :  0.146472126619
Iteration :  49   Loss :  0.118465628693
Iteration :  50   Loss :  0.0958141696003
Iteration :  51   Loss :  0.0774938283577
Iteration :  52   Loss :  0.0626764648546
Iteration :  53   Loss :  0.0506922851783
Iteration :  54   Loss :  0.04099956471
Iteration :  55   Loss :  0.0331601603774
Iteration :  56   Loss :  0.0268197051367
Iteration :  57   Loss :  0.0216915894083
Iteration :  58   Loss :  0.0175440053744
Iteration :  59   Loss :  0.0141894684978
Iteration :  60   Loss :  0.0114763425999
Iteration :  61   Loss :  0.00928198540278
Iteration :  62   Loss :  0.00750720469239
Iteration :  63   Loss :  0.00607177450167
Iteration :  64   Loss :  931.054397624
Iteration :  65   Loss :  43.3312470529
Iteration :  66   Loss :  18.2279525906
Iteration :  67   Loss :  14.7426402093
Iteration :  68   Loss :  11.9237440003
Iteration :  69   Loss :  9.64384051746
Iteration :  70   Loss :  7.79987057119
Iteration :  71   Loss :  6.30848061176
Iteration :  72   Loss :  5.10225487279
Iteration :  73   Loss :  4.12666795525
Iteration :  74   Loss :  3.33762009886
Iteration :  75   Loss :  2.69944372678
Iteration :  76   Loss :  2.18329115304
Iteration :  77   Loss :  1.76583057155
Iteration :  78   Loss :  1.42819138121
Iteration :  79   Loss :  1.15511117217
Iteration :  80   Loss :  0.934245814405
Iteration :  81   Loss :  0.755611462139
Iteration :  82   Loss :  0.61113325092
Iteration :  83   Loss :  0.494280287018
Iteration :  84   Loss :  0.39977042939
Iteration :  85   Loss :  0.323331519407
Iteration :  86   Loss :  0.261508265135
Iteration :  87   Loss :  0.211506050692
Iteration :  88   Loss :  0.171064610353
Iteration :  89   Loss :  0.138355857052
Iteration :  90   Loss :  0.111901246792
Iteration :  91   Loss :  0.0905049435599
Iteration :  92   Loss :  0.0731997635736
Iteration :  93   Loss :  0.0592034553747
Iteration :  94   Loss :  351.700908057
Iteration :  95   Loss :  149.432030365
Iteration :  96   Loss :  19.8368918583
Iteration :  97   Loss :  16.0439390043
Iteration :  98   Loss :  12.9762253388
Iteration :  99   Loss :  10.4950800422
[-0.01336451 -0.00633122 -0.00496793 ...,  0.00762332 -0.00541546
  0.00249833]
CROSS VALIDATION 17
Iteration :  0   Loss :  197.416754612
Iteration :  1   Loss :  23.4101133774
Iteration :  2   Loss :  18.9339355074
Iteration :  3   Loss :  15.3136342408
Iteration :  4   Loss :  12.3855599682
Iteration :  5   Loss :  10.0173540332
Iteration :  6   Loss :  8.10196568303
Iteration :  7   Loss :  6.5528130194
Iteration :  8   Loss :  5.2998692104
Iteration :  9   Loss :  4.2864970455
Iteration :  10   Loss :  3.46688874605
Iteration :  11   Loss :  2.80399530197
Iteration :  12   Loss :  2.26785173376
Iteration :  13   Loss :  1.83422257615
Iteration :  14   Loss :  1.48350635484
Iteration :  15   Loss :  1.19984953488
Iteration :  16   Loss :  0.970429888385
Iteration :  17   Loss :  0.784876887389
Iteration :  18   Loss :  0.634802921602
Iteration :  19   Loss :  12.578617338
Iteration :  20   Loss :  43.1552879958
Iteration :  21   Loss :  47.6969121725
Iteration :  22   Loss :  13.158720334
Iteration :  23   Loss :  10.6426807144
Iteration :  24   Loss :  8.60772551688
Iteration :  25   Loss :  6.96186802576
Iteration :  26   Loss :  5.63071003054
Iteration :  27   Loss :  4.5540787804
Iteration :  28   Loss :  3.68330697649
Iteration :  29   Loss :  2.97903284885
Iteration :  30   Loss :  2.40942087401
Iteration :  31   Loss :  1.94872270386
Iteration :  32   Loss :  1.57611325506
Iteration :  33   Loss :  1.27474934625
Iteration :  34   Loss :  1.0310083305
Iteration :  35   Loss :  0.833872306488
Iteration :  36   Loss :  0.674430072929
Iteration :  37   Loss :  0.545474312712
Iteration :  38   Loss :  0.441175798309
Iteration :  39   Loss :  0.356819891383
Iteration :  40   Loss :  0.288593425511
Iteration :  41   Loss :  0.233412338436
Iteration :  42   Loss :  0.188782262235
Iteration :  43   Loss :  0.152685769627
Iteration :  44   Loss :  0.123491179577
Iteration :  45   Loss :  0.0998787999075
Iteration :  46   Loss :  0.0807812728418
Iteration :  47   Loss :  0.0653353268961
Iteration :  48   Loss :  0.052842754149
Iteration :  49   Loss :  0.0427388489307
Iteration :  50   Loss :  0.0345668812562
Iteration :  51   Loss :  0.0279574511171
Iteration :  52   Loss :  0.0226117903774
Iteration :  53   Loss :  405.985504436
Iteration :  54   Loss :  71.9209419893
Iteration :  55   Loss :  153.729897857
Iteration :  56   Loss :  25.7579022841
Iteration :  57   Loss :  41.295027951
Iteration :  58   Loss :  14.0240476494
Iteration :  59   Loss :  11.3425513779
Iteration :  60   Loss :  9.17377600079
Iteration :  61   Loss :  7.41968568701
Iteration :  62   Loss :  6.00098974396
Iteration :  63   Loss :  4.85355841557
Iteration :  64   Loss :  3.92552400494
Iteration :  65   Loss :  3.17493628262
Iteration :  66   Loss :  2.56786619722
Iteration :  67   Loss :  2.07687216998
Iteration :  68   Loss :  1.67975964445
Iteration :  69   Loss :  1.35857781905
Iteration :  70   Loss :  1.09880821134
Iteration :  71   Loss :  0.888708374585
Iteration :  72   Loss :  0.7187811002
Iteration :  73   Loss :  0.581345112502
Iteration :  74   Loss :  0.470187849592
Iteration :  75   Loss :  0.380284634977
Iteration :  76   Loss :  0.307571545555
Iteration :  77   Loss :  0.24876170882
Iteration :  78   Loss :  0.201196725345
Iteration :  79   Loss :  0.162726500319
Iteration :  80   Loss :  0.131612052137
Iteration :  81   Loss :  0.106446904677
Iteration :  82   Loss :  0.0860935099122
Iteration :  83   Loss :  0.0696318269801
Iteration :  84   Loss :  0.0563177332824
Iteration :  85   Loss :  0.0455493876812
Iteration :  86   Loss :  0.0368400252852
Iteration :  87   Loss :  0.101732882863
Iteration :  88   Loss :  349.192480283
Iteration :  89   Loss :  20.9543805098
Iteration :  90   Loss :  16.9477559879
Iteration :  91   Loss :  13.707226176
Iteration :  92   Loss :  11.0863083923
Iteration :  93   Loss :  8.96652847126
Iteration :  94   Loss :  7.25206533871
Iteration :  95   Loss :  5.86541958189
Iteration :  96   Loss :  4.74391021934
Iteration :  97   Loss :  3.83684131288
Iteration :  98   Loss :  3.10321034327
Iteration :  99   Loss :  2.50985476055
[ -2.88962908e-03  -4.77500189e-06  -3.23426616e-03 ...,   2.14046959e-03
   1.81311878e-03   7.77630347e-04]
CROSS VALIDATION 18
Iteration :  0   Loss :  33.9185327871
Iteration :  1   Loss :  26.2027520567
Iteration :  2   Loss :  30.9291386763
Iteration :  3   Loss :  19.1757222534
Iteration :  4   Loss :  15.5091896651
Iteration :  5   Loss :  12.5437238238
Iteration :  6   Loss :  10.1452758504
Iteration :  7   Loss :  8.2054279515
Iteration :  8   Loss :  6.63649257649
Iteration :  9   Loss :  5.36754864927
Iteration :  10   Loss :  4.34123570097
Iteration :  11   Loss :  3.51116098667
Iteration :  12   Loss :  2.83980237967
Iteration :  13   Loss :  2.29681224706
Iteration :  14   Loss :  1.85764563619
Iteration :  15   Loss :  1.50245076152
Iteration :  16   Loss :  1.21517163813
Iteration :  17   Loss :  0.982822298027
Iteration :  18   Loss :  0.79489978139
Iteration :  19   Loss :  0.642909367973
Iteration :  20   Loss :  0.519980587621
Iteration :  21   Loss :  0.420556652262
Iteration :  22   Loss :  0.340143270677
Iteration :  23   Loss :  0.275105491649
Iteration :  24   Loss :  0.222503392129
Iteration :  25   Loss :  0.179959182975
Iteration :  26   Loss :  0.145549725004
Iteration :  27   Loss :  0.117719596736
Iteration :  28   Loss :  0.0952107841863
Iteration :  29   Loss :  0.0770058144668
Iteration :  30   Loss :  0.0622817626425
Iteration :  31   Loss :  0.0503730527975
Iteration :  32   Loss :  0.0407413717994
Iteration :  33   Loss :  276.988147602
Iteration :  34   Loss :  128.008002374
Iteration :  35   Loss :  14.8312478828
Iteration :  36   Loss :  11.9954092652
Iteration :  37   Loss :  9.70180288109
Iteration :  38   Loss :  7.84675012436
Iteration :  39   Loss :  6.34639646557
Iteration :  40   Loss :  5.13292094942
Iteration :  41   Loss :  4.15147046295
Iteration :  42   Loss :  3.35768019312
Iteration :  43   Loss :  2.71566819032
Iteration :  44   Loss :  2.19641338536
Iteration :  45   Loss :  1.77644374102
Iteration :  46   Loss :  1.43677523823
Iteration :  47   Loss :  1.16205373552
Iteration :  48   Loss :  0.939860910955
Iteration :  49   Loss :  0.760152912846
Iteration :  50   Loss :  0.614806344399
Iteration :  51   Loss :  0.497251059262
Iteration :  52   Loss :  0.402173169145
Iteration :  53   Loss :  0.325274838469
Iteration :  54   Loss :  0.263080007962
Iteration :  55   Loss :  91.6258831807
Iteration :  56   Loss :  43.9984526736
Iteration :  57   Loss :  96.0963827993
Iteration :  58   Loss :  14.2164834312
Iteration :  59   Loss :  11.4981920885
Iteration :  60   Loss :  9.29965711595
Iteration :  61   Loss :  7.52149745006
Iteration :  62   Loss :  6.08333438383
Iteration :  63   Loss :  4.92015818275
Iteration :  64   Loss :  3.97938942953
Iteration :  65   Loss :  3.21850226022
Iteration :  66   Loss :  2.60310205435
Iteration :  67   Loss :  2.10537068409
Iteration :  68   Loss :  1.70280904278
Iteration :  69   Loss :  1.37722001075
Iteration :  70   Loss :  1.11388588524
Iteration :  71   Loss :  0.900903091486
Iteration :  72   Loss :  0.728644101699
Iteration :  73   Loss :  0.589322238939
Iteration :  74   Loss :  0.476639693504
Iteration :  75   Loss :  0.385502841082
Iteration :  76   Loss :  0.311791994053
Iteration :  77   Loss :  0.252175177964
Iteration :  78   Loss :  0.203957515248
Iteration :  79   Loss :  0.164959407828
Iteration :  80   Loss :  0.13341801207
Iteration :  81   Loss :  0.107907552404
Iteration :  82   Loss :  0.0872748715495
Iteration :  83   Loss :  0.0705873039865
Iteration :  84   Loss :  0.0570905163837
Iteration :  85   Loss :  0.0461744092335
Iteration :  86   Loss :  0.037345538333
Iteration :  87   Loss :  0.0302048094721
Iteration :  88   Loss :  0.0244294380525
Iteration :  89   Loss :  0.0197583581553
Iteration :  90   Loss :  0.0159804214961
Iteration :  91   Loss :  0.0129248528236
Iteration :  92   Loss :  0.0112397035391
Iteration :  93   Loss :  79.7771852302
Iteration :  94   Loss :  181.028705279
Iteration :  95   Loss :  20.0442814322
Iteration :  96   Loss :  16.2116742371
Iteration :  97   Loss :  13.1118884186
Iteration :  98   Loss :  10.6048033897
Iteration :  99   Loss :  8.57709060228
[-0.00756443 -0.00195256 -0.00410621 ...,  0.00500766  0.00067373
  0.00206526]
CROSS VALIDATION 19
Iteration :  0   Loss :  208.176070194
Iteration :  1   Loss :  74.6202277142
Iteration :  2   Loss :  17.4550396851
Iteration :  3   Loss :  14.1175136723
Iteration :  4   Loss :  11.4181460417
Iteration :  5   Loss :  9.23491643467
Iteration :  6   Loss :  7.46913564112
Iteration :  7   Loss :  6.04098452
Iteration :  8   Loss :  4.88590590992
Iteration :  9   Loss :  3.9516864315
Iteration :  10   Loss :  3.19609626972
Iteration :  11   Loss :  2.58498024638
Iteration :  12   Loss :  2.09071389291
Iteration :  13   Loss :  1.69095473287
Iteration :  14   Loss :  1.36763232804
Iteration :  15   Loss :  1.10613143471
Iteration :  16   Loss :  0.894631346283
Iteration :  17   Loss :  0.723571558167
Iteration :  18   Loss :  0.585219601307
Iteration :  19   Loss :  0.473321508963
Iteration :  20   Loss :  0.382819116698
Iteration :  21   Loss :  0.309621416594
Iteration :  22   Loss :  0.250419630139
Iteration :  23   Loss :  0.20253764048
Iteration :  24   Loss :  0.163811023075
Iteration :  25   Loss :  0.132489206536
Iteration :  26   Loss :  0.107156341002
Iteration :  27   Loss :  0.0866672970366
Iteration :  28   Loss :  0.0700959019818
Iteration :  29   Loss :  0.0566930738889
Iteration :  30   Loss :  0.0458529605312
Iteration :  31   Loss :  0.0545755951144
Iteration :  32   Loss :  162.916199719
Iteration :  33   Loss :  68.4877790115
Iteration :  34   Loss :  106.139797024
Iteration :  35   Loss :  14.215816564
Iteration :  36   Loss :  11.497652731
Iteration :  37   Loss :  9.29922088737
Iteration :  38   Loss :  7.52114463144
Iteration :  39   Loss :  6.0830490266
Iteration :  40   Loss :  4.91992738783
Iteration :  41   Loss :  3.97920276421
Iteration :  42   Loss :  3.21835128663
Iteration :  43   Loss :  2.60297994796
Iteration :  44   Loss :  2.1052719253
Iteration :  45   Loss :  1.70272916737
Iteration :  46   Loss :  1.37715540808
Iteration :  47   Loss :  1.11383363505
Iteration :  48   Loss :  0.900860831898
Iteration :  49   Loss :  0.728609922441
Iteration :  50   Loss :  0.589294594994
Iteration :  51   Loss :  0.476617335276
Iteration :  52   Loss :  0.385484757904
Iteration :  53   Loss :  0.311777368505
Iteration :  54   Loss :  0.252163348923
Iteration :  55   Loss :  0.203947948003
Iteration :  56   Loss :  0.164951669908
Iteration :  57   Loss :  0.133411753694
Iteration :  58   Loss :  0.107902490673
Iteration :  59   Loss :  0.0872707776573
Iteration :  60   Loss :  0.0705839928752
Iteration :  61   Loss :  0.0570878383801
Iteration :  62   Loss :  0.0461722432829
Iteration :  63   Loss :  0.0373437865274
Iteration :  64   Loss :  0.030203392624
Iteration :  65   Loss :  483.80634815
Iteration :  66   Loss :  165.714985681
Iteration :  67   Loss :  17.0622743246
Iteration :  68   Loss :  13.7998478035
Iteration :  69   Loss :  11.1784463628
Iteration :  70   Loss :  10.3913699495
Iteration :  71   Loss :  8.40446713288
Iteration :  72   Loss :  6.79747407038
Iteration :  73   Loss :  5.49774935245
Iteration :  74   Loss :  4.44654111651
Iteration :  75   Loss :  3.59633126817
Iteration :  76   Loss :  2.90868750599
Iteration :  77   Loss :  2.35252605408
Iteration :  78   Loss :  1.90270657255
Iteration :  79   Loss :  1.5388957308
Iteration :  80   Loss :  1.24464807367
Iteration :  81   Loss :  1.00666263235
Iteration :  82   Loss :  0.814181676582
Iteration :  83   Loss :  0.658504429566
Iteration :  84   Loss :  0.532593764058
Iteration :  85   Loss :  0.430758100899
Iteration :  86   Loss :  269.054532354
Iteration :  87   Loss :  62.3694109183
Iteration :  88   Loss :  13.8111395154
Iteration :  89   Loss :  11.1703527724
Iteration :  90   Loss :  9.03450297641
Iteration :  91   Loss :  7.30704264172
Iteration :  92   Loss :  5.90988483897
Iteration :  93   Loss :  62.8028408731
Iteration :  94   Loss :  5.73435774484
Iteration :  95   Loss :  4.63790832477
Iteration :  96   Loss :  3.75110772402
Iteration :  97   Loss :  3.03386961791
Iteration :  98   Loss :  2.45377246822
Iteration :  99   Loss :  1.98459396219
[-0.00385361 -0.00620286 -0.00563381 ...,  0.0028657   0.00304499
  0.00177665]
Accuracy (Hinge Loss):	0.9
lmda : 0.3  eta : 0.001
Logistic Loss
CROSS VALIDATION 0
Iteration :  0   Loss :  141.327629424
Iteration :  1   Loss :  50.7550364899
Iteration :  2   Loss :  7.35312290345
Iteration :  3   Loss :  6.89989380691
Iteration :  4   Loss :  6.47460068489
Iteration :  5   Loss :  6.07552163328
Iteration :  6   Loss :  5.70104088396
Iteration :  7   Loss :  5.34964226379
Iteration :  8   Loss :  5.01990305718
Iteration :  9   Loss :  4.71048824744
Iteration :  10   Loss :  4.42014511389
Iteration :  11   Loss :  4.14769816273
Iteration :  12   Loss :  3.89204437146
Iteration :  13   Loss :  3.65214872759
Iteration :  14   Loss :  3.42704004336
Iteration :  15   Loss :  3.21580702939
Iteration :  16   Loss :  3.01759461013
Iteration :  17   Loss :  2.83160046478
Iteration :  18   Loss :  2.65707177655
Iteration :  19   Loss :  2.49330217287
Iteration :  20   Loss :  2.33962883884
Iteration :  21   Loss :  2.19542978754
Iteration :  22   Loss :  2.06012127597
Iteration :  23   Loss :  1.93315536609
Iteration :  24   Loss :  1.81401764529
Iteration :  25   Loss :  1.70222513225
Iteration :  26   Loss :  1.59732438793
Iteration :  27   Loss :  1.49888982101
Iteration :  28   Loss :  1.40652213392
Iteration :  29   Loss :  1.31984683566
Iteration :  30   Loss :  1.23851276932
Iteration :  31   Loss :  1.16219065093
Iteration :  32   Loss :  1.09057165192
Iteration :  33   Loss :  1.02336606195
Iteration :  34   Loss :  0.960302051364
Iteration :  35   Loss :  0.901124533868
Iteration :  36   Loss :  0.845594118996
Iteration :  37   Loss :  0.793486141941
Iteration :  38   Loss :  0.744589759931
Iteration :  39   Loss :  0.698707107281
Iteration :  40   Loss :  0.655652503552
Iteration :  41   Loss :  0.615251710828
Iteration :  42   Loss :  0.577341237054
Iteration :  43   Loss :  0.5417676829
Iteration :  44   Loss :  0.508387129898
Iteration :  45   Loss :  0.477064567725
Iteration :  46   Loss :  0.447673358484
Iteration :  47   Loss :  0.420094735706
Iteration :  48   Loss :  0.394217335372
Iteration :  49   Loss :  0.36993675564
Iteration :  50   Loss :  0.347155140922
Iteration :  51   Loss :  0.32578078464
Iteration :  52   Loss :  0.305727743718
Iteration :  53   Loss :  0.28691545723
Iteration :  54   Loss :  0.269268363099
Iteration :  55   Loss :  0.252715511776
Iteration :  56   Loss :  0.237190185303
Iteration :  57   Loss :  0.222629541955
Iteration :  58   Loss :  0.20897431413
Iteration :  59   Loss :  0.196168580505
Iteration :  60   Loss :  0.184159609023
Iteration :  61   Loss :  0.172897736893
Iteration :  62   Loss :  0.162336244331
Iteration :  63   Loss :  0.152431206489
Iteration :  64   Loss :  0.143141351796
Iteration :  65   Loss :  0.13442796678
Iteration :  66   Loss :  0.12625484782
Iteration :  67   Loss :  0.118588250809
Iteration :  68   Loss :  0.111396787468
Iteration :  69   Loss :  0.104651260667
Iteration :  70   Loss :  0.0983244669287
Iteration :  71   Loss :  0.0923909904394
Iteration :  72   Loss :  0.0868269923808
Iteration :  73   Loss :  0.0816099981319
Iteration :  74   Loss :  0.0767187069856
Iteration :  75   Loss :  0.0721328688166
Iteration :  76   Loss :  0.0678332608083
Iteration :  77   Loss :  0.0638017518079
Iteration :  78   Loss :  0.0600213964829
Iteration :  79   Loss :  0.0564764959556
Iteration :  80   Loss :  0.0531525941391
Iteration :  81   Loss :  0.0500364127621
Iteration :  82   Loss :  0.0471157401441
Iteration :  83   Loss :  0.0443792879471
Iteration :  84   Loss :  0.0418165316276
Iteration :  85   Loss :  0.039417555627
Iteration :  86   Loss :  0.0371729268255
Iteration :  87   Loss :  0.0350736158835
Iteration :  88   Loss :  0.0331109751642
Iteration :  89   Loss :  0.031276762506
Iteration :  90   Loss :  0.0295631753578
Iteration :  91   Loss :  0.0279628457896
Iteration :  92   Loss :  0.0264687646648
Iteration :  93   Loss :  0.025074153448
Iteration :  94   Loss :  0.0237723512793
Iteration :  95   Loss :  0.0225567898152
Iteration :  96   Loss :  0.0214210793958
Iteration :  97   Loss :  0.0203591656449
Iteration :  98   Loss :  0.019365485243
Iteration :  99   Loss :  0.0184350655985
[ -6.32928115e-04   2.01992053e-04   1.13705757e-04 ...,   2.76204191e-04
   1.13276809e-04   7.59265711e-05]
CROSS VALIDATION 1
Iteration :  0   Loss :  66.6561668447
Iteration :  1   Loss :  7.31926428017
Iteration :  2   Loss :  6.86812214213
Iteration :  3   Loss :  6.4447873384
Iteration :  4   Loss :  6.04754589066
Iteration :  5   Loss :  5.67478946617
Iteration :  6   Loss :  5.325008866
Iteration :  7   Loss :  4.9967879147
Iteration :  8   Loss :  4.68879772649
Iteration :  9   Loss :  4.39979132501
Iteration :  10   Loss :  4.12859859452
Iteration :  11   Loss :  3.87412154249
Iteration :  12   Loss :  3.63532985404
Iteration :  13   Loss :  3.41125672045
Iteration :  14   Loss :  3.20099492482
Iteration :  15   Loss :  3.00369316894
Iteration :  16   Loss :  2.81855262661
Iteration :  17   Loss :  2.64482370941
Iteration :  18   Loss :  2.48180303174
Iteration :  19   Loss :  2.32883056305
Iteration :  20   Loss :  2.18528695551
Iteration :  21   Loss :  2.05059103642
Iteration :  22   Loss :  1.92419745519
Iteration :  23   Loss :  1.80559447536
Iteration :  24   Loss :  1.69430190271
Iteration :  25   Loss :  1.58986914106
Iteration :  26   Loss :  1.49187336793
Iteration :  27   Loss :  1.39991782269
Iteration :  28   Loss :  1.31363020017
Iteration :  29   Loss :  1.23266114337
Iteration :  30   Loss :  1.15668282911
Iteration :  31   Loss :  1.08538764089
Iteration :  32   Loss :  1.01848692375
Iteration :  33   Loss :  0.955709815919
Iteration :  34   Loss :  0.896802152889
Iteration :  35   Loss :  0.841525439267
Iteration :  36   Loss :  0.789655884605
Iteration :  37   Loss :  0.740983499372
Iteration :  38   Loss :  0.695311247646
Iteration :  39   Loss :  0.652454253356
Iteration :  40   Loss :  0.612239057173
Iteration :  41   Loss :  0.574502921302
Iteration :  42   Loss :  0.539093179619
Iteration :  43   Loss :  0.505866630509
Iteration :  44   Loss :  0.474688969516
Iteration :  45   Loss :  0.445434258227
Iteration :  46   Loss :  0.417984424557
Iteration :  47   Loss :  0.392228787721
Iteration :  48   Loss :  0.368063598811
Iteration :  49   Loss :  0.345391585997
Iteration :  50   Loss :  0.324121493823
Iteration :  51   Loss :  0.304167611716
Iteration :  52   Loss :  0.285449299745
Iteration :  53   Loss :  0.2678905378
Iteration :  54   Loss :  0.251419539279
Iteration :  55   Loss :  0.235968470547
Iteration :  56   Loss :  0.221473297962
Iteration :  57   Loss :  0.207873751976
Iteration :  58   Loss :  0.19511336383
Iteration :  59   Loss :  0.183139505021
Iteration :  60   Loss :  0.171903356749
Iteration :  61   Loss :  0.161359766952
Iteration :  62   Loss :  0.151467003497
Iteration :  63   Loss :  0.142186449589
Iteration :  64   Loss :  0.133482288877
Iteration :  65   Loss :  0.125321201132
Iteration :  66   Loss :  0.117672059758
Iteration :  67   Loss :  0.110505609028
Iteration :  68   Loss :  0.10379410809
Iteration :  69   Loss :  0.0975109549779
Iteration :  70   Loss :  0.0916303342764
Iteration :  71   Loss :  0.0861269519477
Iteration :  72   Loss :  0.0809759230049
Iteration :  73   Loss :  0.0761528646607
Iteration :  74   Loss :  0.0716342149256
Iteration :  75   Loss :  0.0673977288694
Iteration :  76   Loss :  0.0634230108073
Iteration :  77   Loss :  0.0596918835528
Iteration :  78   Loss :  0.0561884428235
Iteration :  79   Loss :  0.052898782836
Iteration :  80   Loss :  0.0498105161278
Iteration :  81   Loss :  0.0469122670233
Iteration :  82   Loss :  0.0441932884705
Iteration :  83   Loss :  0.0416432767931
Iteration :  84   Loss :  0.0392523730331
Iteration :  85   Loss :  0.0370112666655
Iteration :  86   Loss :  0.0349112877749
Iteration :  87   Loss :  0.0329444077223
Iteration :  88   Loss :  0.0311031424783
Iteration :  89   Loss :  0.0293804126974
Iteration :  90   Loss :  0.0277694271597
Iteration :  91   Loss :  0.0262636303422
Iteration :  92   Loss :  0.0248567162487
Iteration :  93   Loss :  0.0235426781172
Iteration :  94   Loss :  0.022315849052
Iteration :  95   Loss :  0.0211708983621
Iteration :  96   Loss :  0.0201027764071
Iteration :  97   Loss :  0.0191066292386
Iteration :  98   Loss :  0.0181777182792
Iteration :  99   Loss :  0.0173113778597
[ -3.84987135e-04   2.57403158e-04  -1.70809442e-04 ...,   1.53757410e-04
   8.49699087e-06   2.08334574e-04]
CROSS VALIDATION 2
Iteration :  0   Loss :  306.013361422
Iteration :  1   Loss :  68.5642606553
Iteration :  2   Loss :  17.414751628
Iteration :  3   Loss :  7.8833706081
Iteration :  4   Loss :  7.35296590794
Iteration :  5   Loss :  6.89974648128
Iteration :  6   Loss :  6.47446242809
Iteration :  7   Loss :  6.07539187801
Iteration :  8   Loss :  5.70091909273
Iteration :  9   Loss :  5.3495279242
Iteration :  10   Loss :  5.01979567617
Iteration :  11   Loss :  4.71038734403
Iteration :  12   Loss :  4.42005020971
Iteration :  13   Loss :  4.14760876973
Iteration :  14   Loss :  3.89195997592
Iteration :  15   Loss :  3.65206876949
Iteration :  16   Loss :  3.42696389035
Iteration :  17   Loss :  3.21573394481
Iteration :  18   Loss :  3.01752371573
Iteration :  19   Loss :  2.83153070017
Iteration :  20   Loss :  2.65700186051
Iteration :  21   Loss :  2.49323057614
Iteration :  22   Loss :  2.33955378316
Iteration :  23   Loss :  2.1953492908
Iteration :  24   Loss :  2.06003326372
Iteration :  25   Loss :  1.93305786008
Iteration :  26   Loss :  1.81390901606
Iteration :  27   Loss :  1.70210436798
Iteration :  28   Loss :  1.59719130369
Iteration :  29   Loss :  1.49874513571
Iteration :  30   Loss :  1.4063673886
Iteration :  31   Loss :  1.31968419375
Iteration :  32   Loss :  1.23834478467
Iteration :  33   Loss :  1.16202008606
Iteration :  34   Loss :  1.09040138934
Iteration :  35   Loss :  1.02319910674
Iteration :  36   Loss :  0.960141594833
Iteration :  37   Loss :  0.900974037176
Iteration :  38   Loss :  0.845457374817
Iteration :  39   Loss :  0.793367274431
Iteration :  40   Loss :  0.744493128622
Iteration :  41   Loss :  0.698637093155
Iteration :  42   Loss :  0.65561318136
Iteration :  43   Loss :  0.615246450862
Iteration :  44   Loss :  0.577372320671
Iteration :  45   Loss :  0.541836035651
Iteration :  46   Loss :  0.508492251467
Iteration :  47   Loss :  0.477204668877
Iteration :  48   Loss :  0.447845634303
Iteration :  49   Loss :  0.420295658036
Iteration :  50   Loss :  0.394442860983
Iteration :  51   Loss :  0.37018240655
Iteration :  52   Loss :  0.347415981066
Iteration :  53   Loss :  0.326051357677
Iteration :  54   Loss :  0.306002037814
Iteration :  55   Loss :  0.287186936719
Iteration :  56   Loss :  0.269530077949
Iteration :  57   Loss :  0.252960280333
Iteration :  58   Loss :  0.237410841808
Iteration :  59   Loss :  0.222819234368
Iteration :  60   Loss :  0.209126821825
Iteration :  61   Loss :  0.196278604043
Iteration :  62   Loss :  0.184222984418
Iteration :  63   Loss :  0.172911554035
Iteration :  64   Loss :  0.162298886105
Iteration :  65   Loss :  0.152342337094
Iteration :  66   Loss :  0.143001855754
Iteration :  67   Loss :  0.134239806858
Iteration :  68   Loss :  0.126020819849
Iteration :  69   Loss :  0.118311669761
Iteration :  70   Loss :  0.111081186879
Iteration :  71   Loss :  0.104300177906
Iteration :  72   Loss :  0.097941336457
Iteration :  73   Loss :  0.0919791319206
Iteration :  74   Loss :  0.0863896851492
Iteration :  75   Loss :  0.0811506479959
Iteration :  76   Loss :  0.0762410917021
Iteration :  77   Loss :  0.071641390952
Iteration :  78   Loss :  0.0673330905836
Iteration :  79   Loss :  0.0632987631413
Iteration :  80   Loss :  0.059521881831
Iteration :  81   Loss :  0.0559867257331
Iteration :  82   Loss :  0.0526783176175
Iteration :  83   Loss :  0.0495823940932
Iteration :  84   Loss :  0.0466854178591
Iteration :  85   Loss :  0.043974638775
Iteration :  86   Loss :  0.0414381907943
Iteration :  87   Loss :  0.039065191976
Iteration :  88   Loss :  0.0368458033893
Iteration :  89   Loss :  0.03477119724
Iteration :  90   Loss :  0.0328333879547
Iteration :  91   Loss :  0.0310249054919
Iteration :  92   Loss :  0.0293383554846
Iteration :  93   Loss :  0.0277660165019
Iteration :  94   Loss :  0.0262996993254
Iteration :  95   Loss :  0.0249309967112
Iteration :  96   Loss :  0.0236517743559
Iteration :  97   Loss :  0.0224545713736
Iteration :  98   Loss :  0.0213327263682
Iteration :  99   Loss :  0.020280320104
[ -1.02643710e-03  -2.73314744e-04  -3.79123328e-04 ...,   9.98046124e-04
   2.39958188e-04   8.78955321e-05]
CROSS VALIDATION 3
Iteration :  0   Loss :  66.7004090752
Iteration :  1   Loss :  7.32066476293
Iteration :  2   Loss :  6.86943630245
Iteration :  3   Loss :  6.44602049698
Iteration :  4   Loss :  6.04870304027
Iteration :  5   Loss :  5.67587529181
Iteration :  6   Loss :  5.32602776392
Iteration :  7   Loss :  4.99774401016
Iteration :  8   Loss :  4.68969489049
Iteration :  9   Loss :  4.40063318994
Iteration :  10   Loss :  4.12938856889
Iteration :  11   Loss :  3.87486282471
Iteration :  12   Loss :  3.63602544537
Iteration :  13   Loss :  3.41190943718
Iteration :  14   Loss :  3.20160740963
Iteration :  15   Loss :  3.00426790164
Iteration :  16   Loss :  2.81909193415
Iteration :  17   Loss :  2.6453297753
Iteration :  18   Loss :  2.48227790493
Iteration :  19   Loss :  2.32927616618
Iteration :  20   Loss :  2.18570509271
Iteration :  21   Loss :  2.05098340063
Iteration :  22   Loss :  1.924565635
Iteration :  23   Loss :  1.80593996144
Iteration :  24   Loss :  1.69462609384
Iteration :  25   Loss :  1.59017334981
Iteration :  26   Loss :  1.49215882596
Iteration :  27   Loss :  1.40018568574
Iteration :  28   Loss :  1.31388155273
Iteration :  29   Loss :  1.23289700308
Iteration :  30   Loss :  1.15690415084
Iteration :  31   Loss :  1.08559532063
Iteration :  32   Loss :  1.01868180218
Iteration :  33   Loss :  0.955892681819
Iteration :  34   Loss :  0.896973746229
Iteration :  35   Loss :  0.841686454164
Iteration :  36   Loss :  0.789806972019
Iteration :  37   Loss :  0.741125269597
Iteration :  38   Loss :  0.695444272589
Iteration :  39   Loss :  0.652579068609
Iteration :  40   Loss :  0.612356163878
Iteration :  41   Loss :  0.574612787846
Iteration :  42   Loss :  0.539196243188
Iteration :  43   Loss :  0.505963298607
Iteration :  44   Loss :  0.474779621614
Iteration :  45   Loss :  0.445519247843
Iteration :  46   Loss :  0.418064082238
Iteration :  47   Loss :  0.392303425638
Iteration :  48   Loss :  0.368133517881
Iteration :  49   Loss :  0.345457086539
Iteration :  50   Loss :  0.324182890461
Iteration :  51   Loss :  0.304225252408
Iteration :  52   Loss :  0.28550358754
Iteration :  53   Loss :  0.267941953023
Iteration :  54   Loss :  0.251468660572
Iteration :  55   Loss :  0.236015995997
Iteration :  56   Loss :  0.22152007075
Iteration :  57   Loss :  0.207920795042
Iteration :  58   Loss :  0.195161922877
Iteration :  59   Loss :  0.183191089782
Iteration :  60   Loss :  0.171959759587
Iteration :  61   Loss :  0.161423028595
Iteration :  62   Loss :  0.151539292504
Iteration :  63   Loss :  0.142269832743
Iteration :  64   Loss :  0.133578402196
Iteration :  65   Loss :  0.125430883997
Iteration :  66   Loss :  0.117795063581
Iteration :  67   Loss :  0.110640495507
Iteration :  68   Loss :  0.103938387407
Iteration :  69   Loss :  0.097661415053
Iteration :  70   Loss :  0.0917834488343
Iteration :  71   Loss :  0.0862792650584
Iteration :  72   Loss :  0.0811243615269
Iteration :  73   Loss :  0.0762949720143
Iteration :  74   Loss :  0.0717683051832
Iteration :  75   Loss :  0.0675229480751
Iteration :  76   Loss :  0.0635392944614
Iteration :  77   Loss :  0.0597998228208
Iteration :  78   Loss :  0.0562890960889
Iteration :  79   Loss :  0.052993473184
Iteration :  80   Loss :  0.0499006423714
Iteration :  81   Loss :  0.0469991434989
Iteration :  82   Loss :  0.0442780261894
Iteration :  83   Loss :  0.0417267188344
Iteration :  84   Loss :  0.0393350886057
Iteration :  85   Loss :  0.0370935898386
Iteration :  86   Loss :  0.0349933719192
Iteration :  87   Loss :  0.0330262695885
Iteration :  88   Loss :  0.0311846885187
Iteration :  89   Loss :  0.0294614579903
Iteration :  90   Loss :  0.0278497207907
Iteration :  91   Loss :  0.0263428915414
Iteration :  92   Loss :  0.0249346739355
Iteration :  93   Loss :  0.0236191023298
Iteration :  94   Loss :  0.0223905672147
Iteration :  95   Loss :  0.0212437971992
Iteration :  96   Loss :  0.0201737968736
Iteration :  97   Loss :  0.0191757654265
Iteration :  98   Loss :  0.0182450296302
Iteration :  99   Loss :  0.0173770145742
[ -4.06344444e-04   2.59466445e-04  -1.85631524e-04 ...,   1.86310040e-04
   7.09430228e-06   2.05456146e-04]
CROSS VALIDATION 4
Iteration :  0   Loss :  66.7004090752
Iteration :  1   Loss :  7.32066476293
Iteration :  2   Loss :  6.86943630245
Iteration :  3   Loss :  6.44602049698
Iteration :  4   Loss :  6.04870304027
Iteration :  5   Loss :  5.67587529181
Iteration :  6   Loss :  5.32602776392
Iteration :  7   Loss :  4.99774401016
Iteration :  8   Loss :  4.68969489049
Iteration :  9   Loss :  4.40063318994
Iteration :  10   Loss :  4.12938856889
Iteration :  11   Loss :  3.87486282471
Iteration :  12   Loss :  3.63602544537
Iteration :  13   Loss :  3.41190943718
Iteration :  14   Loss :  3.20160740963
Iteration :  15   Loss :  3.00426790164
Iteration :  16   Loss :  2.81909193415
Iteration :  17   Loss :  2.6453297753
Iteration :  18   Loss :  2.48227790493
Iteration :  19   Loss :  2.32927616618
Iteration :  20   Loss :  2.18570509271
Iteration :  21   Loss :  2.05098340063
Iteration :  22   Loss :  1.924565635
Iteration :  23   Loss :  1.80593996144
Iteration :  24   Loss :  1.69462609384
Iteration :  25   Loss :  1.59017334981
Iteration :  26   Loss :  1.49215882596
Iteration :  27   Loss :  1.40018568574
Iteration :  28   Loss :  1.31388155273
Iteration :  29   Loss :  1.23289700308
Iteration :  30   Loss :  1.15690415084
Iteration :  31   Loss :  1.08559532063
Iteration :  32   Loss :  1.01868180218
Iteration :  33   Loss :  0.955892681819
Iteration :  34   Loss :  0.896973746229
Iteration :  35   Loss :  0.841686454164
Iteration :  36   Loss :  0.789806972019
Iteration :  37   Loss :  0.741125269598
Iteration :  38   Loss :  0.695444272589
Iteration :  39   Loss :  0.65257906861
Iteration :  40   Loss :  0.61235616388
Iteration :  41   Loss :  0.574612787851
Iteration :  42   Loss :  0.539196243198
Iteration :  43   Loss :  0.505963298629
Iteration :  44   Loss :  0.474779621662
Iteration :  45   Loss :  0.445519247944
Iteration :  46   Loss :  0.418064082445
Iteration :  47   Loss :  0.392303426053
Iteration :  48   Loss :  0.368133518694
Iteration :  49   Loss :  0.345457088099
Iteration :  50   Loss :  0.324182893389
Iteration :  51   Loss :  0.304225257787
Iteration :  52   Loss :  0.285503597213
Iteration :  53   Loss :  0.267941970062
Iteration :  54   Loss :  0.251468689981
Iteration :  55   Loss :  0.236016045758
Iteration :  56   Loss :  0.22152015334
Iteration :  57   Loss :  0.207920929595
Iteration :  58   Loss :  0.195162138204
Iteration :  59   Loss :  0.183191428513
Iteration :  60   Loss :  0.171960283746
Iteration :  61   Loss :  0.161423826929
Iteration :  62   Loss :  0.151540489875
Iteration :  63   Loss :  0.142271601724
Iteration :  64   Loss :  0.133580976761
Iteration :  65   Loss :  0.125434574609
Iteration :  66   Loss :  0.117800272206
Iteration :  67   Loss :  0.110647728002
Iteration :  68   Loss :  0.103948259627
Iteration :  69   Loss :  0.0976746480302
Iteration :  70   Loss :  0.091800847998
Iteration :  71   Loss :  0.0863016790192
Iteration :  72   Loss :  0.0811526178781
Iteration :  73   Loss :  0.0763297912861
Iteration :  74   Loss :  0.0718101997711
Iteration :  75   Loss :  0.0675721201288
Iteration :  76   Loss :  0.0635955529807
Iteration :  77   Loss :  0.0598625414375
Iteration :  78   Loss :  0.0563572259081
Iteration :  79   Loss :  0.0530656106936
Iteration :  80   Loss :  0.0499751369539
Iteration :  81   Loss :  0.0470742198925
Iteration :  82   Loss :  0.0443518981629
Iteration :  83   Loss :  0.0417976800813
Iteration :  84   Loss :  0.0394015803482
Iteration :  85   Loss :  0.0371542552619
Iteration :  86   Loss :  0.0350471098068
Iteration :  87   Loss :  0.0330722920822
Iteration :  88   Loss :  0.0312225753001
Iteration :  89   Loss :  0.0294911893986
Iteration :  90   Loss :  0.0278716717894
Iteration :  91   Loss :  0.0263577775475
Iteration :  92   Loss :  0.0249434532655
Iteration :  93   Loss :  0.0236228517296
Iteration :  94   Loss :  0.022390352744
Iteration :  95   Loss :  0.021240562578
Iteration :  96   Loss :  0.0201682868354
Iteration :  97   Loss :  0.0191684938968
Iteration :  98   Loss :  0.0182362928627
Iteration :  99   Loss :  0.0173669400224
[ -4.01064256e-04   2.61061569e-04  -1.81423983e-04 ...,   1.83669768e-04
   9.98821835e-06   2.05048880e-04]
CROSS VALIDATION 5
Iteration :  0   Loss :  66.6045164033
Iteration :  1   Loss :  7.31892567362
Iteration :  2   Loss :  6.86780440649
Iteration :  3   Loss :  6.44448918723
Iteration :  4   Loss :  6.04726611682
Iteration :  5   Loss :  5.67452693693
Iteration :  6   Loss :  5.32476251844
Iteration :  7   Loss :  4.99655675142
Iteration :  8   Loss :  4.68858081157
Iteration :  9   Loss :  4.39958778021
Iteration :  10   Loss :  4.12840759574
Iteration :  11   Loss :  3.87394231643
Iteration :  12   Loss :  3.63516167505
Iteration :  13   Loss :  3.41109890761
Iteration :  14   Loss :  3.20084683919
Iteration :  15   Loss :  3.00355421096
Iteration :  16   Loss :  2.81842223367
Iteration :  17   Loss :  2.64470135358
Iteration :  18   Loss :  2.48168821763
Iteration :  19   Loss :  2.32872282582
Iteration :  20   Loss :  2.18518585894
Iteration :  21   Loss :  2.0504961712
Iteration :  22   Loss :  1.92410843724
Iteration :  23   Loss :  1.80551094428
Iteration :  24   Loss :  1.69422352028
Iteration :  25   Loss :  1.58979558994
Iteration :  26   Loss :  1.49180435034
Iteration :  27   Loss :  1.39985305919
Iteration :  28   Loss :  1.31356942856
Iteration :  29   Loss :  1.23260411762
Iteration :  30   Loss :  1.15662931835
Iteration :  31   Loss :  1.08533742853
Iteration :  32   Loss :  1.01843980654
Iteration :  33   Loss :  0.955665603229
Iteration :  34   Loss :  0.896760665891
Iteration :  35   Loss :  0.841486510277
Iteration :  36   Loss :  0.789619356452
Iteration :  37   Loss :  0.740949224834
Iteration :  38   Loss :  0.695279088957
Iteration :  39   Loss :  0.6524240818
Iteration :  40   Loss :  0.61221075276
Iteration :  41   Loss :  0.574476372562
Iteration :  42   Loss :  0.539068283525
Iteration :  43   Loss :  0.505843292542
Iteration :  44   Loss :  0.474667103911
Iteration :  45   Loss :  0.44541378839
Iteration :  46   Loss :  0.417965283669
Iteration :  47   Loss :  0.392210919535
Iteration :  48   Loss :  0.368046958691
Iteration :  49   Loss :  0.345376142461
Iteration :  50   Loss :  0.324107231253
Iteration :  51   Loss :  0.30415453565
Iteration :  52   Loss :  0.285437447045
Iteration :  53   Loss :  0.267879994546
Iteration :  54   Loss :  0.25141046858
Iteration :  55   Loss :  0.235961150254
Iteration :  56   Loss :  0.22146816518
Iteration :  57   Loss :  0.207871448781
Iteration :  58   Loss :  0.195114777297
Iteration :  59   Loss :  0.183145793101
Iteration :  60   Loss :  0.171915947196
Iteration :  61   Loss :  0.161380308505
Iteration :  62   Loss :  0.151497240661
Iteration :  63   Loss :  0.142227994192
Iteration :  64   Loss :  0.133536284839
Iteration :  65   Loss :  0.125387927887
Iteration :  66   Loss :  0.117750577934
Iteration :  67   Loss :  0.110593580035
Iteration :  68   Loss :  0.103887881758
Iteration :  69   Loss :  0.0976059281322
Iteration :  70   Loss :  0.0917214984736
Iteration :  71   Loss :  0.0862095184876
Iteration :  72   Loss :  0.081045923568
Iteration :  73   Loss :  0.0762076316327
Iteration :  74   Loss :  0.0716726397417
Iteration :  75   Loss :  0.0674202218008
Iteration :  76   Loss :  0.0634311706439
Iteration :  77   Loss :  0.0596879915561
Iteration :  78   Loss :  0.0561749465898
Iteration :  79   Loss :  0.0528779017684
Iteration :  80   Loss :  0.0497840204447
Iteration :  81   Loss :  0.0468814148223
Iteration :  82   Loss :  0.0441588753181
Iteration :  83   Loss :  0.0416057488931
Iteration :  84   Loss :  0.0392119599984
Iteration :  85   Loss :  0.0369680979651
Iteration :  86   Loss :  0.0348654717674
Iteration :  87   Loss :  0.0328960717495
Iteration :  88   Loss :  0.0310524465147
Iteration :  89   Loss :  0.0293275508972
Iteration :  90   Loss :  0.0277146249464
Iteration :  91   Loss :  0.0262071375107
Iteration :  92   Loss :  0.0247987938493
Iteration :  93   Loss :  0.0234835787204
Iteration :  94   Loss :  0.0222557933773
Iteration :  95   Loss :  0.0211100535584
Iteration :  96   Loss :  0.0200412425308
Iteration :  97   Loss :  0.0190444413338
Iteration :  98   Loss :  0.0181148692923
Iteration :  99   Loss :  0.0172478591334
[ -3.87751826e-04   2.62766800e-04  -1.76535296e-04 ...,   1.67649361e-04
  -9.80757908e-06   2.08313677e-04]
CROSS VALIDATION 6
Iteration :  0   Loss :  66.7007906535
Iteration :  1   Loss :  7.3206675327
Iteration :  2   Loss :  6.86943890149
Iteration :  3   Loss :  6.44602293583
Iteration :  4   Loss :  6.04870532879
Iteration :  5   Loss :  5.67587743927
Iteration :  6   Loss :  5.32602977902
Iteration :  7   Loss :  4.99774590105
Iteration :  8   Loss :  4.68969666483
Iteration :  9   Loss :  4.40063485491
Iteration :  10   Loss :  4.12939013124
Iteration :  11   Loss :  3.87486429076
Iteration :  12   Loss :  3.63602682106
Iteration :  13   Loss :  3.41191072807
Iteration :  14   Loss :  3.20160862096
Iteration :  15   Loss :  3.0042690383
Iteration :  16   Loss :  2.81909300075
Iteration :  17   Loss :  2.64533077616
Iteration :  18   Loss :  2.48227884409
Iteration :  19   Loss :  2.32927704746
Iteration :  20   Loss :  2.18570591967
Iteration :  21   Loss :  2.05098417662
Iteration :  22   Loss :  1.92456636316
Iteration :  23   Loss :  1.80594064472
Iteration :  24   Loss :  1.694626735
Iteration :  25   Loss :  1.59017395145
Iteration :  26   Loss :  1.49215939052
Iteration :  27   Loss :  1.4001862155
Iteration :  28   Loss :  1.31388204984
Iteration :  29   Loss :  1.23289746954
Iteration :  30   Loss :  1.15690458855
Iteration :  31   Loss :  1.08559573137
Iteration :  32   Loss :  1.0186821876
Iteration :  33   Loss :  0.955893043473
Iteration :  34   Loss :  0.896974085588
Iteration :  35   Loss :  0.841686772598
Iteration :  36   Loss :  0.789807270813
Iteration :  37   Loss :  0.741125549956
Iteration :  38   Loss :  0.695444535639
Iteration :  39   Loss :  0.652579315402
Iteration :  40   Loss :  0.612356395397
Iteration :  41   Loss :  0.574613005005
Iteration :  42   Loss :  0.539196446836
Iteration :  43   Loss :  0.50596348953
Iteration :  44   Loss :  0.474779800543
Iteration :  45   Loss :  0.445519415458
Iteration :  46   Loss :  0.41806423919
Iteration :  47   Loss :  0.392303572579
Iteration :  48   Loss :  0.368133655517
Iteration :  49   Loss :  0.345457215726
Iteration :  50   Loss :  0.324183012354
Iteration :  51   Loss :  0.304225368691
Iteration :  52   Loss :  0.285503700763
Iteration :  53   Loss :  0.26794206709
Iteration :  54   Loss :  0.251468781432
Iteration :  55   Loss :  0.236016132642
Iteration :  56   Loss :  0.221520236664
Iteration :  57   Loss :  0.207921010302
Iteration :  58   Loss :  0.195162217164
Iteration :  59   Loss :  0.183191506559
Iteration :  60   Loss :  0.171960361748
Iteration :  61   Loss :  0.161423905866
Iteration :  62   Loss :  0.151540570885
Iteration :  63   Loss :  0.14227168619
Iteration :  64   Loss :  0.133581066497
Iteration :  65   Loss :  0.125434672282
Iteration :  66   Loss :  0.117800382057
Iteration :  67   Loss :  0.110647856873
Iteration :  68   Loss :  0.103948418206
Iteration :  69   Loss :  0.0976748523585
Iteration :  70   Loss :  0.091801121527
Iteration :  71   Loss :  0.0863020558572
Iteration :  72   Loss :  0.0811531480059
Iteration :  73   Loss :  0.0763305482163
Iteration :  74   Loss :  0.0718112902795
Iteration :  75   Loss :  0.0675736947218
Iteration :  76   Loss :  0.0635978154522
Iteration :  77   Loss :  0.0598657565412
Iteration :  78   Loss :  0.0563617250877
Iteration :  79   Loss :  0.0530717959766
Iteration :  80   Loss :  0.0499834822823
Iteration :  81   Loss :  0.0470852679009
Iteration :  82   Loss :  0.044366249427
Iteration :  83   Loss :  0.0418159715874
Iteration :  84   Loss :  0.039424450671
Iteration :  85   Loss :  0.0371822958724
Iteration :  86   Loss :  0.0350808050438
Iteration :  87   Loss :  0.0331119542046
Iteration :  88   Loss :  0.031268284671
Iteration :  89   Loss :  0.0295427516227
Iteration :  90   Loss :  0.0279286020526
Iteration :  91   Loss :  0.0264193171992
Iteration :  92   Loss :  0.0250086163024
Iteration :  93   Loss :  0.0236904919258
Iteration :  94   Loss :  0.0224592383285
Iteration :  95   Loss :  0.0213094460637
Iteration :  96   Loss :  0.0202359626681
Iteration :  97   Loss :  0.0192338438927
Iteration :  98   Loss :  0.0182983253336
Iteration :  99   Loss :  0.017424829576
[ -4.04078110e-04   2.60148903e-04  -1.79540746e-04 ...,   1.81862020e-04
   7.78285199e-06   2.04226597e-04]
CROSS VALIDATION 7
Iteration :  0   Loss :  66.7007906535
Iteration :  1   Loss :  7.3206675327
Iteration :  2   Loss :  6.86943890149
Iteration :  3   Loss :  6.44602293583
Iteration :  4   Loss :  6.04870532879
Iteration :  5   Loss :  5.67587743927
Iteration :  6   Loss :  5.32602977902
Iteration :  7   Loss :  4.99774590105
Iteration :  8   Loss :  4.68969666483
Iteration :  9   Loss :  4.40063485491
Iteration :  10   Loss :  4.12939013124
Iteration :  11   Loss :  3.87486429076
Iteration :  12   Loss :  3.63602682106
Iteration :  13   Loss :  3.41191072807
Iteration :  14   Loss :  3.20160862096
Iteration :  15   Loss :  3.0042690383
Iteration :  16   Loss :  2.81909300075
Iteration :  17   Loss :  2.64533077616
Iteration :  18   Loss :  2.48227884409
Iteration :  19   Loss :  2.32927704746
Iteration :  20   Loss :  2.18570591967
Iteration :  21   Loss :  2.05098417662
Iteration :  22   Loss :  1.92456636316
Iteration :  23   Loss :  1.80594064472
Iteration :  24   Loss :  1.694626735
Iteration :  25   Loss :  1.59017395145
Iteration :  26   Loss :  1.49215939052
Iteration :  27   Loss :  1.4001862155
Iteration :  28   Loss :  1.31388204984
Iteration :  29   Loss :  1.23289746954
Iteration :  30   Loss :  1.15690458855
Iteration :  31   Loss :  1.08559573137
Iteration :  32   Loss :  1.0186821876
Iteration :  33   Loss :  0.955893043473
Iteration :  34   Loss :  0.896974085587
Iteration :  35   Loss :  0.841686772597
Iteration :  36   Loss :  0.789807270811
Iteration :  37   Loss :  0.741125549951
Iteration :  38   Loss :  0.695444535628
Iteration :  39   Loss :  0.652579315378
Iteration :  40   Loss :  0.612356395346
Iteration :  41   Loss :  0.574613004897
Iteration :  42   Loss :  0.539196446614
Iteration :  43   Loss :  0.505963489085
Iteration :  44   Loss :  0.474779799669
Iteration :  45   Loss :  0.445519413779
Iteration :  46   Loss :  0.418064236031
Iteration :  47   Loss :  0.392303566752
Iteration :  48   Loss :  0.368133644979
Iteration :  49   Loss :  0.345457197028
Iteration :  50   Loss :  0.324182979793
Iteration :  51   Loss :  0.304225313018
Iteration :  52   Loss :  0.285503607264
Iteration :  53   Loss :  0.267941912792
Iteration :  54   Loss :  0.251468531123
Iteration :  55   Loss :  0.236015733311
Iteration :  56   Loss :  0.22151960989
Iteration :  57   Loss :  0.207920042065
Iteration :  58   Loss :  0.195160744523
Iteration :  59   Loss :  0.183189300744
Iteration :  60   Loss :  0.171957107464
Iteration :  61   Loss :  0.161419177183
Iteration :  62   Loss :  0.151533805119
Iteration :  63   Loss :  0.142262158774
Iteration :  64   Loss :  0.133567872174
Iteration :  65   Loss :  0.12541672077
Iteration :  66   Loss :  0.117776419078
Iteration :  67   Loss :  0.110616522366
Iteration :  68   Loss :  0.103908350006
Iteration :  69   Loss :  0.0976248327457
Iteration :  70   Loss :  0.0917402460958
Iteration :  71   Loss :  0.0862298856756
Iteration :  72   Loss :  0.0810697993963
Iteration :  73   Loss :  0.0762366892237
Iteration :  74   Loss :  0.071708044633
Iteration :  75   Loss :  0.0674624845672
Iteration :  76   Loss :  0.0634801869325
Iteration :  77   Loss :  0.0597432266376
Iteration :  78   Loss :  0.0562356739814
Iteration :  79   Loss :  0.0529434143746
Iteration :  80   Loss :  0.0498537700923
Iteration :  81   Loss :  0.0469550701489
Iteration :  82   Loss :  0.0442363081302
Iteration :  83   Loss :  0.0416869697286
Iteration :  84   Loss :  0.0392970292028
Iteration :  85   Loss :  0.0370570378408
Iteration :  86   Loss :  0.034958197024
Iteration :  87   Loss :  0.0329923451393
Iteration :  88   Loss :  0.0311518617464
Iteration :  89   Loss :  0.0294295449659
Iteration :  90   Loss :  0.0278185193641
Iteration :  91   Loss :  0.0263121996217
Iteration :  92   Loss :  0.0249043016875
Iteration :  93   Loss :  0.0235888735283
Iteration :  94   Loss :  0.0223603130739
Iteration :  95   Loss :  0.0212133504093
Iteration :  96   Loss :  0.0201429919305
Iteration :  97   Loss :  0.0191444459889
Iteration :  98   Loss :  0.0182130594295
Iteration :  99   Loss :  0.0173442883783
[ -4.14964258e-04   2.55972412e-04  -1.88891505e-04 ...,   1.77177547e-04
   5.51441818e-06   2.06197872e-04]
CROSS VALIDATION 8
Iteration :  0   Loss :  66.7007906535
Iteration :  1   Loss :  7.3206675327
Iteration :  2   Loss :  6.86943890149
Iteration :  3   Loss :  6.44602293583
Iteration :  4   Loss :  6.04870532879
Iteration :  5   Loss :  5.67587743927
Iteration :  6   Loss :  5.32602977902
Iteration :  7   Loss :  4.99774590105
Iteration :  8   Loss :  4.68969666483
Iteration :  9   Loss :  4.40063485491
Iteration :  10   Loss :  4.12939013124
Iteration :  11   Loss :  3.87486429076
Iteration :  12   Loss :  3.63602682106
Iteration :  13   Loss :  3.41191072807
Iteration :  14   Loss :  3.20160862096
Iteration :  15   Loss :  3.0042690383
Iteration :  16   Loss :  2.81909300075
Iteration :  17   Loss :  2.64533077616
Iteration :  18   Loss :  2.48227884409
Iteration :  19   Loss :  2.32927704746
Iteration :  20   Loss :  2.18570591967
Iteration :  21   Loss :  2.05098417662
Iteration :  22   Loss :  1.92456636316
Iteration :  23   Loss :  1.80594064472
Iteration :  24   Loss :  1.694626735
Iteration :  25   Loss :  1.59017395145
Iteration :  26   Loss :  1.49215939052
Iteration :  27   Loss :  1.4001862155
Iteration :  28   Loss :  1.31388204984
Iteration :  29   Loss :  1.23289746954
Iteration :  30   Loss :  1.15690458855
Iteration :  31   Loss :  1.08559573137
Iteration :  32   Loss :  1.0186821876
Iteration :  33   Loss :  0.955893043473
Iteration :  34   Loss :  0.896974085588
Iteration :  35   Loss :  0.841686772598
Iteration :  36   Loss :  0.789807270813
Iteration :  37   Loss :  0.741125549956
Iteration :  38   Loss :  0.695444535639
Iteration :  39   Loss :  0.652579315402
Iteration :  40   Loss :  0.612356395398
Iteration :  41   Loss :  0.574613005005
Iteration :  42   Loss :  0.539196446836
Iteration :  43   Loss :  0.50596348953
Iteration :  44   Loss :  0.474779800543
Iteration :  45   Loss :  0.445519415458
Iteration :  46   Loss :  0.418064239191
Iteration :  47   Loss :  0.39230357258
Iteration :  48   Loss :  0.368133655518
Iteration :  49   Loss :  0.345457215728
Iteration :  50   Loss :  0.324183012358
Iteration :  51   Loss :  0.304225368697
Iteration :  52   Loss :  0.285503700771
Iteration :  53   Loss :  0.267942067102
Iteration :  54   Loss :  0.251468781448
Iteration :  55   Loss :  0.236016132659
Iteration :  56   Loss :  0.221520236678
Iteration :  57   Loss :  0.207921010303
Iteration :  58   Loss :  0.19516221713
Iteration :  59   Loss :  0.183191506441
Iteration :  60   Loss :  0.171960361449
Iteration :  61   Loss :  0.161423905189
Iteration :  62   Loss :  0.151540569456
Iteration :  63   Loss :  0.142271683308
Iteration :  64   Loss :  0.133581060915
Iteration :  65   Loss :  0.125434661872
Iteration :  66   Loss :  0.117800363371
Iteration :  67   Loss :  0.110647824633
Iteration :  68   Loss :  0.103948364874
Iteration :  69   Loss :  0.0976747680109
Iteration :  70   Loss :  0.0918009943271
Iteration :  71   Loss :  0.0863018732848
Iteration :  72   Loss :  0.0811528986534
Iteration :  73   Loss :  0.0763302235525
Iteration :  74   Loss :  0.0718108856038
Iteration :  75   Loss :  0.0675732087909
Iteration :  76   Loss :  0.0635972487716
Iteration :  77   Loss :  0.0598651086865
Iteration :  78   Loss :  0.0563609914414
Iteration :  79   Loss :  0.0530709640272
Iteration :  80   Loss :  0.0499825273511
Iteration :  81   Loss :  0.0470841479609
Iteration :  82   Loss :  0.044364898567
Iteration :  83   Loss :  0.0418142916384
Iteration :  84   Loss :  0.0394223005377
Iteration :  85   Loss :  0.0371794782341
Iteration :  86   Loss :  0.035077050387
Iteration :  87   Loss :  0.0331069027475
Iteration :  88   Loss :  0.0312614677698
Iteration :  89   Loss :  0.0295335758641
Iteration :  90   Loss :  0.0279163415218
Iteration :  91   Loss :  0.0264031222138
Iteration :  92   Loss :  0.0249875494382
Iteration :  93   Loss :  0.0236636026175
Iteration :  94   Loss :  0.0224256827539
Iteration :  95   Loss :  0.0212686464719
Iteration :  96   Loss :  0.0201877797514
Iteration :  97   Loss :  0.0191787138233
Iteration :  98   Loss :  0.0182373035987
Iteration :  99   Loss :  0.0173595007589
[ -4.02807244e-04   2.60677621e-04  -1.77542709e-04 ...,   1.81975426e-04
   6.67925334e-06   2.03983982e-04]
CROSS VALIDATION 9
Iteration :  0   Loss :  162.427331344
Iteration :  1   Loss :  78.1709607466
Iteration :  2   Loss :  8.19314149274
Iteration :  3   Loss :  7.68813594327
Iteration :  4   Loss :  7.21425790338
Iteration :  5   Loss :  6.76958881996
Iteration :  6   Loss :  6.352328422
Iteration :  7   Loss :  5.96078743672
Iteration :  8   Loss :  5.59338075422
Iteration :  9   Loss :  5.24862101112
Iteration :  10   Loss :  4.92511256413
Iteration :  11   Loss :  4.62154582491
Iteration :  12   Loss :  4.33669192847
Iteration :  13   Loss :  4.06939771065
Iteration :  14   Loss :  3.81858097716
Iteration :  15   Loss :  3.58322605901
Iteration :  16   Loss :  3.36237966296
Iteration :  17   Loss :  3.15514703036
Iteration :  18   Loss :  2.96068840386
Iteration :  19   Loss :  2.77821576665
Iteration :  20   Loss :  2.60698978829
Iteration :  21   Loss :  2.44631691089
Iteration :  22   Loss :  2.29554654386
Iteration :  23   Loss :  2.15406837612
Iteration :  24   Loss :  2.02130983266
Iteration :  25   Loss :  1.89673369542
Iteration :  26   Loss :  1.77983589107
Iteration :  27   Loss :  1.67014343537
Iteration :  28   Loss :  1.56721251865
Iteration :  29   Loss :  1.47062671662
Iteration :  30   Loss :  1.37999531245
Iteration :  31   Loss :  1.29495171728
Iteration :  32   Loss :  1.21515197723
Iteration :  33   Loss :  1.14027335606
Iteration :  34   Loss :  1.07001298565
Iteration :  35   Loss :  1.00408658312
Iteration :  36   Loss :  0.942227242729
Iteration :  37   Loss :  0.884184319557
Iteration :  38   Loss :  0.829722420489
Iteration :  39   Loss :  0.778620502778
Iteration :  40   Loss :  0.730671057085
Iteration :  41   Loss :  0.68567934242
Iteration :  42   Loss :  0.643462656339
Iteration :  43   Loss :  0.603849648561
Iteration :  44   Loss :  0.566679688022
Iteration :  45   Loss :  0.531802266107
Iteration :  46   Loss :  0.499076393685
Iteration :  47   Loss :  0.468369957742
Iteration :  48   Loss :  0.43955903966
Iteration :  49   Loss :  0.412527232634
Iteration :  50   Loss :  0.38716501219
Iteration :  51   Loss :  0.363369211947
Iteration :  52   Loss :  0.341042639194
Iteration :  53   Loss :  0.320093830362
Iteration :  54   Loss :  0.300436902342
Iteration :  55   Loss :  0.281991424482
Iteration :  56   Loss :  0.264682240018
Iteration :  57   Loss :  0.248439202278
Iteration :  58   Loss :  0.233196834721
Iteration :  59   Loss :  0.218893949441
Iteration :  60   Loss :  0.205473260933
Iteration :  61   Loss :  0.192881021469
Iteration :  62   Loss :  0.181066694477
Iteration :  63   Loss :  0.169982678197
Iteration :  64   Loss :  0.159584090263
Iteration :  65   Loss :  0.149828617127
Iteration :  66   Loss :  0.14067641753
Iteration :  67   Loss :  0.132090053717
Iteration :  68   Loss :  0.124034420719
Iteration :  69   Loss :  0.116476657077
Iteration :  70   Loss :  0.109386039564
Iteration :  71   Loss :  0.102733875013
Iteration :  72   Loss :  0.0964933996047
Iteration :  73   Loss :  0.0906396861386
Iteration :  74   Loss :  0.0851495507748
Iteration :  75   Loss :  0.0800014456317
Iteration :  76   Loss :  0.0751753225319
Iteration :  77   Loss :  0.0706524569722
Iteration :  78   Loss :  0.0664152328928
Iteration :  79   Loss :  0.0624469088727
Iteration :  80   Loss :  0.058731406847
Iteration :  81   Loss :  0.0552531662921
Iteration :  82   Loss :  0.0519970772323
Iteration :  83   Loss :  0.0489484667607
Iteration :  84   Loss :  0.0460931156513
Iteration :  85   Loss :  0.0434173317933
Iteration :  86   Loss :  0.0409081337394
Iteration :  87   Loss :  0.0385535357716
Iteration :  88   Loss :  0.0363428221451
Iteration :  89   Loss :  0.0342666657667
Iteration :  90   Loss :  0.0323170260542
Iteration :  91   Loss :  0.0304868775149
Iteration :  92   Loss :  0.0287698803401
Iteration :  93   Loss :  0.0271600879121
Iteration :  94   Loss :  0.0256517395153
Iteration :  95   Loss :  0.0242391536121
Iteration :  96   Loss :  0.0229167245121
Iteration :  97   Loss :  0.0216790153873
Iteration :  98   Loss :  0.0205209190186
Iteration :  99   Loss :  0.0194378303928
[-0.00078222  0.00023042 -0.00014788 ...,  0.00091274  0.00036272
  0.00015132]
CROSS VALIDATION 10
Iteration :  0   Loss :  66.7027231138
Iteration :  1   Loss :  7.32089359303
Iteration :  2   Loss :  6.869651028
Iteration :  3   Loss :  6.44622198736
Iteration :  4   Loss :  6.04889211125
Iteration :  5   Loss :  5.6760527089
Iteration :  6   Loss :  5.32619424545
Iteration :  7   Loss :  4.99790023016
Iteration :  8   Loss :  4.68984148146
Iteration :  9   Loss :  4.40077074538
Iteration :  10   Loss :  4.12951764574
Iteration :  11   Loss :  3.87498394557
Iteration :  12   Loss :  3.63613910063
Iteration :  13   Loss :  3.412016087
Iteration :  14   Loss :  3.20170748581
Iteration :  15   Loss :  3.00436180935
Iteration :  16   Loss :  2.81918005361
Iteration :  17   Loss :  2.64541246329
Iteration :  18   Loss :  2.48235549623
Iteration :  19   Loss :  2.32934897493
Iteration :  20   Loss :  2.18577341371
Iteration :  21   Loss :  2.05104751048
Iteration :  22   Loss :  1.92462579327
Iteration :  23   Loss :  1.8059964117
Iteration :  24   Loss :  1.69467906464
Iteration :  25   Loss :  1.59022305561
Iteration :  26   Loss :  1.49220546801
Iteration :  27   Loss :  1.40022945288
Iteration :  28   Loss :  1.31392262217
Iteration :  29   Loss :  1.23293554108
Iteration :  30   Loss :  1.15694031344
Iteration :  31   Loss :  1.08562925423
Iteration :  32   Loss :  1.01871364414
Iteration :  33   Loss :  0.955922561033
Iteration :  34   Loss :  0.897001783628
Iteration :  35   Loss :  0.841712763192
Iteration :  36   Loss :  0.789831659085
Iteration :  37   Loss :  0.741148434487
Iteration :  38   Loss :  0.695466008847
Iteration :  39   Loss :  0.652599463888
Iteration :  40   Loss :  0.612375300254
Iteration :  41   Loss :  0.574630742105
Iteration :  42   Loss :  0.539213087085
Iteration :  43   Loss :  0.505979099101
Iteration :  44   Loss :  0.474794441101
Iteration :  45   Loss :  0.445533144398
Iteration :  46   Loss :  0.418077109922
Iteration :  47   Loss :  0.392315634909
Iteration :  48   Loss :  0.368144956212
Iteration :  49   Loss :  0.345467799319
Iteration :  50   Loss :  0.324192922243
Iteration :  51   Loss :  0.304234648474
Iteration :  52   Loss :  0.285512395584
Iteration :  53   Loss :  0.267950224595
Iteration :  54   Loss :  0.251476451857
Iteration :  55   Loss :  0.236023367777
Iteration :  56   Loss :  0.22152708791
Iteration :  57   Loss :  0.207927526591
Iteration :  58   Loss :  0.195168443637
Iteration :  59   Loss :  0.183197484299
Iteration :  60   Loss :  0.171966127774
Iteration :  61   Loss :  0.161429491844
Iteration :  62   Loss :  0.15154599939
Iteration :  63   Loss :  0.142276964853
Iteration :  64   Loss :  0.133586182115
Iteration :  65   Loss :  0.125439587613
Iteration :  66   Loss :  0.117805036445
Iteration :  67   Loss :  0.110652168142
Iteration :  68   Loss :  0.103952280043
Iteration :  69   Loss :  0.0976781218165
Iteration :  70   Loss :  0.0918035953719
Iteration :  71   Loss :  0.0863034386023
Iteration :  72   Loss :  0.0811530156016
Iteration :  73   Loss :  0.0763283095763
Iteration :  74   Loss :  0.0718061466066
Iteration :  75   Loss :  0.0675645968061
Iteration :  76   Loss :  0.0635834239958
Iteration :  77   Loss :  0.0598444201275
Iteration :  78   Loss :  0.0563315011449
Iteration :  79   Loss :  0.0530305454502
Iteration :  80   Loss :  0.0499290607872
Iteration :  81   Loss :  0.0470158079171
Iteration :  82   Loss :  0.0442804810615
Iteration :  83   Loss :  0.0417134840398
Iteration :  84   Loss :  0.0393057902081
Iteration :  85   Loss :  0.0370488526514
Iteration :  86   Loss :  0.0349345346596
Iteration :  87   Loss :  0.0329550459985
Iteration :  88   Loss :  0.0311028859002
Iteration :  89   Loss :  0.0293708026955
Iteration :  90   Loss :  0.0277517811024
Iteration :  91   Loss :  0.0262390618502
Iteration :  92   Loss :  0.0248261855824
Iteration :  93   Loss :  0.0235070378958
Iteration :  94   Loss :  0.0222758649981
Iteration :  95   Loss :  0.0211272405216
Iteration :  96   Loss :  0.0200559916576
Iteration :  97   Loss :  0.0190571183523
Iteration :  98   Loss :  0.0181257433994
Iteration :  99   Loss :  0.0172571131506
[ -3.96503433e-04   2.62303991e-04  -1.81622151e-04 ...,   1.80023707e-04
   3.51591582e-06   2.06642727e-04]
CROSS VALIDATION 11
Iteration :  0   Loss :  66.7026824699
Iteration :  1   Loss :  7.32089365999
Iteration :  2   Loss :  6.86965109084
Iteration :  3   Loss :  6.44622204632
Iteration :  4   Loss :  6.04889216658
Iteration :  5   Loss :  5.67605276083
Iteration :  6   Loss :  5.32619429417
Iteration :  7   Loss :  4.99790027587
Iteration :  8   Loss :  4.68984152436
Iteration :  9   Loss :  4.40077078564
Iteration :  10   Loss :  4.12951768352
Iteration :  11   Loss :  3.87498398102
Iteration :  12   Loss :  3.63613913389
Iteration :  13   Loss :  3.41201611821
Iteration :  14   Loss :  3.20170751509
Iteration :  15   Loss :  3.00436183683
Iteration :  16   Loss :  2.8191800794
Iteration :  17   Loss :  2.64541248749
Iteration :  18   Loss :  2.48235551893
Iteration :  19   Loss :  2.32934899624
Iteration :  20   Loss :  2.1857734337
Iteration :  21   Loss :  2.05104752925
Iteration :  22   Loss :  1.92462581088
Iteration :  23   Loss :  1.80599642822
Iteration :  24   Loss :  1.69467908014
Iteration :  25   Loss :  1.59022307016
Iteration :  26   Loss :  1.49220548166
Iteration :  27   Loss :  1.40022946569
Iteration :  28   Loss :  1.31392263419
Iteration :  29   Loss :  1.23293555236
Iteration :  30   Loss :  1.15694032402
Iteration :  31   Loss :  1.08562926416
Iteration :  32   Loss :  1.01871365346
Iteration :  33   Loss :  0.955922569778
Iteration :  34   Loss :  0.897001791834
Iteration :  35   Loss :  0.841712770893
Iteration :  36   Loss :  0.789831666312
Iteration :  37   Loss :  0.74114844127
Iteration :  38   Loss :  0.695466015215
Iteration :  39   Loss :  0.652599469868
Iteration :  40   Loss :  0.612375305872
Iteration :  41   Loss :  0.574630747386
Iteration :  42   Loss :  0.539213092053
Iteration :  43   Loss :  0.505979103781
Iteration :  44   Loss :  0.474794445517
Iteration :  45   Loss :  0.445533148576
Iteration :  46   Loss :  0.418077113887
Iteration :  47   Loss :  0.392315638686
Iteration :  48   Loss :  0.368144959826
Iteration :  49   Loss :  0.345467802792
Iteration :  50   Loss :  0.324192925593
Iteration :  51   Loss :  0.304234651711
Iteration :  52   Loss :  0.285512398713
Iteration :  53   Loss :  0.267950227616
Iteration :  54   Loss :  0.251476454783
Iteration :  55   Loss :  0.236023370656
Iteration :  56   Loss :  0.221527090874
Iteration :  57   Loss :  0.207927529938
Iteration :  58   Loss :  0.195168447949
Iteration :  59   Loss :  0.183197490654
Iteration :  60   Loss :  0.171966138073
Iteration :  61   Loss :  0.161429509347
Iteration :  62   Loss :  0.151546029577
Iteration :  63   Loss :  0.142277016756
Iteration :  64   Loss :  0.133586270353
Iteration :  65   Loss :  0.125439735444
Iteration :  66   Loss :  0.117805280274
Iteration :  67   Loss :  0.110652564086
Iteration :  68   Loss :  0.103952913347
Iteration :  69   Loss :  0.097679120118
Iteration :  70   Loss :  0.0918051469584
Iteration :  71   Loss :  0.0863058168838
Iteration :  72   Loss :  0.0811566108017
Iteration :  73   Loss :  0.0763336679632
Iteration :  74   Loss :  0.0718140160209
Iteration :  75   Loss :  0.0675759743002
Iteration :  76   Loss :  0.0635995965074
Iteration :  77   Loss :  0.0598669821413
Iteration :  78   Loss :  0.0563623255675
Iteration :  79   Loss :  0.0530716796213
Iteration :  80   Loss :  0.0499825263289
Iteration :  81   Loss :  0.0470833093573
Iteration :  82   Loss :  0.0443630739916
Iteration :  83   Loss :  0.0418112994021
Iteration :  84   Loss :  0.0394179198334
Iteration :  85   Loss :  0.0371734474525
Iteration :  86   Loss :  0.0350690750231
Iteration :  87   Loss :  0.0330966775409
Iteration :  88   Loss :  0.0312487155437
Iteration :  89   Loss :  0.0295181029452
Iteration :  90   Loss :  0.0278981061545
Iteration :  91   Loss :  0.0263823067594
Iteration :  92   Loss :  0.024964619286
Iteration :  93   Loss :  0.0236393274244
Iteration :  94   Loss :  0.0224010947071
Iteration :  95   Loss :  0.0212449220372
Iteration :  96   Loss :  0.0201660573316
Iteration :  97   Loss :  0.019159891585
Iteration :  98   Loss :  0.0182218817897
Iteration :  99   Loss :  0.0173475240342
[ -4.04299081e-04   2.61270936e-04  -1.77499162e-04 ...,   1.82183102e-04
   7.81066261e-06   2.04089773e-04]
CROSS VALIDATION 12
Iteration :  0   Loss :  133.036994294
Iteration :  1   Loss :  7.29726346728
Iteration :  2   Loss :  6.52895473058
Iteration :  3   Loss :  6.12356779749
Iteration :  4   Loss :  5.74613398761
Iteration :  5   Loss :  5.39196652405
Iteration :  6   Loss :  5.05963146805
Iteration :  7   Loss :  4.74778306065
Iteration :  8   Loss :  4.45515823553
Iteration :  9   Loss :  4.18057153034
Iteration :  10   Loss :  3.92291039025
Iteration :  11   Loss :  3.6811308194
Iteration :  12   Loss :  3.45425330289
Iteration :  13   Loss :  3.24135892391
Iteration :  14   Loss :  3.04158563987
Iteration :  15   Loss :  2.85412472663
Iteration :  16   Loss :  2.67821741845
Iteration :  17   Loss :  2.51315176307
Iteration :  18   Loss :  2.35825969203
Iteration :  19   Loss :  2.2129142928
Iteration :  20   Loss :  2.07652726394
Iteration :  21   Loss :  1.94854653561
Iteration :  22   Loss :  1.82845404051
Iteration :  23   Loss :  1.71576362316
Iteration :  24   Loss :  1.61001907787
Iteration :  25   Loss :  1.51079230668
Iteration :  26   Loss :  1.41768159014
Iteration :  27   Loss :  1.3303099637
Iteration :  28   Loss :  1.2483236937
Iteration :  29   Loss :  1.17139084672
Iteration :  30   Loss :  1.09919994676
Iteration :  31   Loss :  1.03145871496
Iteration :  32   Loss :  0.967892886771
Iteration :  33   Loss :  0.908245101944
Iteration :  34   Loss :  0.852273862791
Iteration :  35   Loss :  0.799752556648
Iteration :  36   Loss :  0.750468538527
Iteration :  37   Loss :  0.704222270307
Iteration :  38   Loss :  0.660826512993
Iteration :  39   Loss :  0.620105568788
Iteration :  40   Loss :  0.581894569958
Iteration :  41   Loss :  0.546038811632
Iteration :  42   Loss :  0.512393125901
Iteration :  43   Loss :  0.480821294742
Iteration :  44   Loss :  0.451195499482
Iteration :  45   Loss :  0.42339580471
Iteration :  46   Loss :  0.397309674669
Iteration :  47   Loss :  0.372831520368
Iteration :  48   Loss :  0.349862275769
Iteration :  49   Loss :  0.328309001511
Iteration :  50   Loss :  0.308084514663
Iteration :  51   Loss :  0.289107042895
Iteration :  52   Loss :  0.271299901134
Iteration :  53   Loss :  0.254591188086
Iteration :  54   Loss :  0.238913498926
Iteration :  55   Loss :  0.224203649008
Iteration :  56   Loss :  0.210402402194
Iteration :  57   Loss :  0.197454197796
Iteration :  58   Loss :  0.18530687445
Iteration :  59   Loss :  0.173911399905
Iteration :  60   Loss :  0.163221631211
Iteration :  61   Loss :  0.153194140808
Iteration :  62   Loss :  0.143788134145
Iteration :  63   Loss :  0.134965445391
Iteration :  64   Loss :  0.126690548412
Iteration :  65   Loss :  0.118930500047
Iteration :  66   Loss :  0.111654761816
Iteration :  67   Loss :  0.104834900853
Iteration :  68   Loss :  0.0984442141327
Iteration :  69   Loss :  0.0924573425014
Iteration :  70   Loss :  0.0868499532934
Iteration :  71   Loss :  0.0815985705647
Iteration :  72   Loss :  0.0766805983896
Iteration :  73   Loss :  0.0720745070874
Iteration :  74   Loss :  0.067760071782
Iteration :  75   Loss :  0.0637185299024
Iteration :  76   Loss :  0.0599325821132
Iteration :  77   Loss :  0.056386251294
Iteration :  78   Loss :  0.0530646660824
Iteration :  79   Loss :  0.0499538290001
Iteration :  80   Loss :  0.0470404025781
Iteration :  81   Loss :  0.0443115418873
Iteration :  82   Loss :  0.0417548186354
Iteration :  83   Loss :  0.0393582844173
Iteration :  84   Loss :  0.0371106753196
Iteration :  85   Loss :  0.0350016789037
Iteration :  86   Loss :  0.0330221318443
Iteration :  87   Loss :  0.0311640486506
Iteration :  88   Loss :  0.0294204781194
Iteration :  89   Loss :  0.0277852649767
Iteration :  90   Loss :  0.0262528079681
Iteration :  91   Loss :  0.0248178683451
Iteration :  92   Loss :  0.0234754385813
Iteration :  93   Loss :  0.022220656496
Iteration :  94   Loss :  0.0210487459588
Iteration :  95   Loss :  0.0199549740098
Iteration :  96   Loss :  0.0189346288093
Iteration :  97   Loss :  0.0179830365358
Iteration :  98   Loss :  0.0170956372422
Iteration :  99   Loss :  0.0162681187215
[ -1.30521456e-03  -3.47435987e-04  -4.42805234e-04 ...,   1.25882827e-04
   2.47979270e-04   4.90167193e-05]
CROSS VALIDATION 13
Iteration :  0   Loss :  224.595872785
Iteration :  1   Loss :  66.8373600761
Iteration :  2   Loss :  7.81530354529
Iteration :  3   Loss :  7.33358672023
Iteration :  4   Loss :  6.88156178086
Iteration :  5   Loss :  6.45739858959
Iteration :  6   Loss :  6.05937981415
Iteration :  7   Loss :  5.68589397461
Iteration :  8   Loss :  5.33542891881
Iteration :  9   Loss :  5.0065657001
Iteration :  10   Loss :  4.69797283233
Iteration :  11   Loss :  4.40840089902
Iteration :  12   Loss :  4.13667749475
Iteration :  13   Loss :  3.88170247843
Iteration :  14   Loss :  3.64244351902
Iteration :  15   Loss :  3.41793191595
Iteration :  16   Loss :  3.20725867707
Iteration :  17   Loss :  3.00957083836
Iteration :  18   Loss :  2.82406801046
Iteration :  19   Loss :  2.64999913819
Iteration :  20   Loss :  2.48665945961
Iteration :  21   Loss :  2.33338765274
Iteration :  22   Loss :  2.18956315798
Iteration :  23   Loss :  2.05460366575
Iteration :  24   Loss :  1.9279627589
Iteration :  25   Loss :  1.8091277006
Iteration :  26   Loss :  1.69761735859
Iteration :  27   Loss :  1.59298025759
Iteration :  28   Loss :  1.49479275189
Iteration :  29   Loss :  1.4026573109
Iteration :  30   Loss :  1.31620091071
Iteration :  31   Loss :  1.23507352537
Iteration :  32   Loss :  1.15894671175
Iteration :  33   Loss :  1.08751228252
Iteration :  34   Loss :  1.02048106186
Iteration :  35   Loss :  0.957581718927
Iteration :  36   Loss :  0.898559674179
Iteration :  37   Loss :  0.843176073558
Iteration :  38   Loss :  0.791206825247
Iteration :  39   Loss :  0.742441693026
Iteration :  40   Loss :  0.696683439325
Iteration :  41   Loss :  0.653747010373
Iteration :  42   Loss :  0.61345875633
Iteration :  43   Loss :  0.575655682854
Iteration :  44   Loss :  0.540184738621
Iteration :  45   Loss :  0.506902154926
Iteration :  46   Loss :  0.475672861802
Iteration :  47   Loss :  0.446369998484
Iteration :  48   Loss :  0.418874508754
Iteration :  49   Loss :  0.393074776952
Iteration :  50   Loss :  0.368866246459
Iteration :  51   Loss :  0.346150984272
Iteration :  52   Loss :  0.324837195849
Iteration :  53   Loss :  0.304838724874
Iteration :  54   Loss :  0.286074582155
Iteration :  55   Loss :  0.268468544329
Iteration :  56   Loss :  0.251948851785
Iteration :  57   Loss :  0.236448011939
Iteration :  58   Loss :  0.221902678908
Iteration :  59   Loss :  0.208253551875
Iteration :  60   Loss :  0.195445235788
Iteration :  61   Loss :  0.183426041674
Iteration :  62   Loss :  0.172147743481
Iteration :  63   Loss :  0.161565326513
Iteration :  64   Loss :  0.151636754862
Iteration :  65   Loss :  0.142322765876
Iteration :  66   Loss :  0.133586682968
Iteration :  67   Loss :  0.12539422963
Iteration :  68   Loss :  0.117713327939
Iteration :  69   Loss :  0.110513873744
Iteration :  70   Loss :  0.103767497322
Iteration :  71   Loss :  0.0974473370766
Iteration :  72   Loss :  0.0915278619369
Iteration :  73   Loss :  0.0859847637522
Iteration :  74   Loss :  0.0807949102797
Iteration :  75   Loss :  0.0759363309598
Iteration :  76   Loss :  0.0713882228014
Iteration :  77   Loss :  0.0671309912291
Iteration :  78   Loss :  0.0631463339269
Iteration :  79   Loss :  0.0594173243945
Iteration :  80   Loss :  0.0559284083152
Iteration :  81   Loss :  0.0526652495257
Iteration :  82   Loss :  0.0496144490701
Iteration :  83   Loss :  0.0467632429384
Iteration :  84   Loss :  0.0440992977602
Iteration :  85   Loss :  0.0416106610699
Iteration :  86   Loss :  0.0392858280533
Iteration :  87   Loss :  0.037113822647
Iteration :  88   Loss :  0.0350842028235
Iteration :  89   Loss :  0.033186982912
Iteration :  90   Loss :  0.0314125529078
Iteration :  91   Loss :  0.0297516848675
Iteration :  92   Loss :  0.0281956414127
Iteration :  93   Loss :  0.0267363232616
Iteration :  94   Loss :  0.0253663864254
Iteration :  95   Loss :  0.0240793048819
Iteration :  96   Loss :  0.0228693814522
Iteration :  97   Loss :  0.0217317013621
Iteration :  98   Loss :  0.020662020195
Iteration :  99   Loss :  0.0196566025519
[ -1.14880263e-03  -4.52604310e-04  -5.89561226e-04 ...,   5.17512751e-04
  -5.15599550e-05   1.33104391e-04]
CROSS VALIDATION 14
Iteration :  0   Loss :  66.6775115349
Iteration :  1   Loss :  7.32177756945
Iteration :  2   Loss :  6.8704805182
Iteration :  3   Loss :  6.44700034975
Iteration :  4   Loss :  6.04962249722
Iteration :  5   Loss :  5.67673807562
Iteration :  6   Loss :  5.32683736777
Iteration :  7   Loss :  4.99850371195
Iteration :  8   Loss :  4.69040776606
Iteration :  9   Loss :  4.40130212554
Iteration :  10   Loss :  4.13001627288
Iteration :  11   Loss :  3.87545183851
Iteration :  12   Loss :  3.63657815376
Iteration :  13   Loss :  3.41242807793
Iteration :  14   Loss :  3.20209408259
Iteration :  15   Loss :  3.00472457723
Iteration :  16   Loss :  2.81952046133
Iteration :  17   Loss :  2.64573188909
Iteration :  18   Loss :  2.48265523337
Iteration :  19   Loss :  2.32963023699
Iteration :  20   Loss :  2.18603733944
Iteration :  21   Loss :  2.05129516845
Iteration :  22   Loss :  1.92485818619
Iteration :  23   Loss :  1.80621448046
Iteration :  24   Loss :  1.69488369215
Iteration :  25   Loss :  1.59041507034
Iteration :  26   Loss :  1.49238564737
Iteration :  27   Loss :  1.40039852632
Iteration :  28   Loss :  1.31408127415
Iteration :  29   Loss :  1.23308441379
Iteration :  30   Loss :  1.15708000933
Iteration :  31   Loss :  1.08576033839
Iteration :  32   Loss :  1.01883664637
Iteration :  33   Loss :  0.95603797772
Iteration :  34   Loss :  0.897110079306
Iteration :  35   Loss :  0.841814371692
Iteration :  36   Loss :  0.78992698417
Iteration :  37   Loss :  0.741237849773
Iteration :  38   Loss :  0.695549856755
Iteration :  39   Loss :  0.652678053273
Iteration :  40   Loss :  0.612448902245
Iteration :  41   Loss :  0.574699583546
Iteration :  42   Loss :  0.539277340851
Iteration :  43   Loss :  0.506038870403
Iteration :  44   Loss :  0.474849748846
Iteration :  45   Loss :  0.445583896747
Iteration :  46   Loss :  0.418123073538
Iteration :  47   Loss :  0.392356398334
Iteration :  48   Loss :  0.368179889574
Iteration :  49   Loss :  0.345496015698
Iteration :  50   Loss :  0.32421325069
Iteration :  51   Loss :  0.304245634473
Iteration :  52   Loss :  0.285512349716
Iteration :  53   Loss :  0.267937339801
Iteration :  54   Loss :  0.251448997885
Iteration :  55   Loss :  0.235979944253
Iteration :  56   Loss :  0.221466881242
Iteration :  57   Loss :  0.207850490775
Iteration :  58   Loss :  0.195075335715
Iteration :  59   Loss :  0.183089737362
Iteration :  60   Loss :  0.171845610433
Iteration :  61   Loss :  0.161298241001
Iteration :  62   Loss :  0.151406003216
Iteration :  63   Loss :  0.142130031413
Iteration :  64   Loss :  0.13343388747
Iteration :  65   Loss :  0.12528327557
Iteration :  66   Loss :  0.117645842418
Iteration :  67   Loss :  0.110491052019
Iteration :  68   Loss :  0.10379006459
Iteration :  69   Loss :  0.0975155357316
Iteration :  70   Loss :  0.0916413153865
Iteration :  71   Loss :  0.0861421213336
Iteration :  72   Loss :  0.0809933138263
Iteration :  73   Loss :  0.076170880029
Iteration :  74   Loss :  0.0716516710385
Iteration :  75   Loss :  0.0674138413643
Iteration :  76   Loss :  0.0634373420742
Iteration :  77   Loss :  0.0597042667027
Iteration :  78   Loss :  0.0561988978152
Iteration :  79   Loss :  0.0529074369417
Iteration :  80   Loss :  0.0498175348288
Iteration :  81   Loss :  0.0469177970348
Iteration :  82   Loss :  0.0441974152338
Iteration :  83   Loss :  0.0416460022051
Iteration :  84   Loss :  0.039253618795
Iteration :  85   Loss :  0.0370109011919
Iteration :  86   Loss :  0.0349091657919
Iteration :  87   Loss :  0.0329404099864
Iteration :  88   Loss :  0.0310972088268
Iteration :  89   Loss :  0.0293725671312
Iteration :  90   Loss :  0.0277597931422
Iteration :  91   Loss :  0.0262524300795
Iteration :  92   Loss :  0.0248442458633
Iteration :  93   Loss :  0.0235292556827
Iteration :  94   Loss :  0.0223017430653
Iteration :  95   Loss :  0.0211562552442
Iteration :  96   Loss :  0.020087572406
Iteration :  97   Loss :  0.0190906707382
Iteration :  98   Loss :  0.0181607003613
Iteration :  99   Loss :  0.0172929849133
[ -3.77466439e-04   2.73264991e-04  -1.68064364e-04 ...,   1.64821399e-04
  -8.50862229e-06   2.00885349e-04]
CROSS VALIDATION 15
Iteration :  0   Loss :  66.6775115349
Iteration :  1   Loss :  7.32177756945
Iteration :  2   Loss :  6.8704805182
Iteration :  3   Loss :  6.44700034975
Iteration :  4   Loss :  6.04962249722
Iteration :  5   Loss :  5.67673807562
Iteration :  6   Loss :  5.32683736777
Iteration :  7   Loss :  4.99850371195
Iteration :  8   Loss :  4.69040776606
Iteration :  9   Loss :  4.40130212554
Iteration :  10   Loss :  4.13001627288
Iteration :  11   Loss :  3.87545183851
Iteration :  12   Loss :  3.63657815376
Iteration :  13   Loss :  3.41242807793
Iteration :  14   Loss :  3.20209408259
Iteration :  15   Loss :  3.00472457723
Iteration :  16   Loss :  2.81952046133
Iteration :  17   Loss :  2.64573188909
Iteration :  18   Loss :  2.48265523337
Iteration :  19   Loss :  2.32963023699
Iteration :  20   Loss :  2.18603733944
Iteration :  21   Loss :  2.05129516845
Iteration :  22   Loss :  1.92485818619
Iteration :  23   Loss :  1.80621448047
Iteration :  24   Loss :  1.69488369217
Iteration :  25   Loss :  1.59041507038
Iteration :  26   Loss :  1.49238564745
Iteration :  27   Loss :  1.4003985265
Iteration :  28   Loss :  1.3140812745
Iteration :  29   Loss :  1.23308441449
Iteration :  30   Loss :  1.15708001068
Iteration :  31   Loss :  1.08576034095
Iteration :  32   Loss :  1.01883665113
Iteration :  33   Loss :  0.956037986396
Iteration :  34   Loss :  0.897110094828
Iteration :  35   Loss :  0.84181439896
Iteration :  36   Loss :  0.789927031232
Iteration :  37   Loss :  0.741237929619
Iteration :  38   Loss :  0.69554998999
Iteration :  39   Loss :  0.652678272039
Iteration :  40   Loss :  0.612449255864
Iteration :  41   Loss :  0.574700146487
Iteration :  42   Loss :  0.539278223759
Iteration :  43   Loss :  0.50604023505
Iteration :  44   Loss :  0.474851827888
Iteration :  45   Loss :  0.445587019079
Iteration :  46   Loss :  0.418127695601
Iteration :  47   Loss :  0.39236314076
Iteration :  48   Loss :  0.368189576679
Iteration :  49   Loss :  0.34550971228
Iteration :  50   Loss :  0.324232286047
Iteration :  51   Loss :  0.304271598188
Iteration :  52   Loss :  0.285547039371
Iteration :  53   Loss :  0.267982641597
Iteration :  54   Loss :  0.251506692684
Iteration :  55   Loss :  0.236051457319
Iteration :  56   Loss :  0.22155302821
Iteration :  57   Loss :  0.207951296209
Iteration :  58   Loss :  0.195189990589
Iteration :  59   Loss :  0.183216712191
Iteration :  60   Loss :  0.171982877675
Iteration :  61   Loss :  0.16144352395
Iteration :  62   Loss :  0.151556977666
Iteration :  63   Loss :  0.14228444526
Iteration :  64   Loss :  0.133589602791
Iteration :  65   Loss :  0.125438259963
Iteration :  66   Loss :  0.117798141539
Iteration :  67   Loss :  0.110638772051
Iteration :  68   Loss :  0.103931387888
Iteration :  69   Loss :  0.0976487848963
Iteration :  70   Loss :  0.0917650673823
Iteration :  71   Loss :  0.0862553549364
Iteration :  72   Loss :  0.0810955561467
Iteration :  73   Loss :  0.0762623076152
Iteration :  74   Loss :  0.0717331231153
Iteration :  75   Loss :  0.0674867215764
Iteration :  76   Loss :  0.0635034204763
Iteration :  77   Loss :  0.0597654325694
Iteration :  78   Loss :  0.0562569322622
Iteration :  79   Loss :  0.0529638584218
Iteration :  80   Loss :  0.0498735329891
Iteration :  81   Loss :  0.0469742381207
Iteration :  82   Loss :  0.0442548898356
Iteration :  83   Loss :  0.0417048883722
Iteration :  84   Loss :  0.0393141391895
Iteration :  85   Loss :  0.0370731584794
Iteration :  86   Loss :  0.0349731494875
Iteration :  87   Loss :  0.033005981895
Iteration :  88   Loss :  0.0311640868847
Iteration :  89   Loss :  0.0294403300207
Iteration :  90   Loss :  0.0278279175039
Iteration :  91   Loss :  0.026320354738
Iteration :  92   Loss :  0.0249114447115
Iteration :  93   Loss :  0.0235953007842
Iteration :  94   Loss :  0.0223663494076
Iteration :  95   Loss :  0.0212193071959
Iteration :  96   Loss :  0.0201491316995
Iteration :  97   Loss :  0.0191509616
Iteration :  98   Loss :  0.0182200699421
Iteration :  99   Loss :  0.0173518481083
[ -4.05360838e-04   2.73346220e-04  -1.81326878e-04 ...,   1.82371710e-04
   6.47902967e-06   2.03435893e-04]
CROSS VALIDATION 16
Iteration :  0   Loss :  105.726856975
Iteration :  1   Loss :  6.82204522632
Iteration :  2   Loss :  6.40155209427
Iteration :  3   Loss :  6.00697784366
Iteration :  4   Loss :  5.63672512912
Iteration :  5   Loss :  5.28929511084
Iteration :  6   Loss :  4.96328137371
Iteration :  7   Loss :  4.65736420582
Iteration :  8   Loss :  4.37030520875
Iteration :  9   Loss :  4.10094221833
Iteration :  10   Loss :  3.84818452623
Iteration :  11   Loss :  3.61100840791
Iteration :  12   Loss :  3.38845297336
Iteration :  13   Loss :  3.17961634942
Iteration :  14   Loss :  2.98365217056
Iteration :  15   Loss :  2.79976631478
Iteration :  16   Loss :  2.62721380581
Iteration :  17   Loss :  2.46529582926
Iteration :  18   Loss :  2.31335685793
Iteration :  19   Loss :  2.17078191413
Iteration :  20   Loss :  2.03699399745
Iteration :  21   Loss :  1.91145168933
Iteration :  22   Loss :  1.7936469282
Iteration :  23   Loss :  1.68310293985
Iteration :  24   Loss :  1.57937230674
Iteration :  25   Loss :  1.48203516205
Iteration :  26   Loss :  1.39069749717
Iteration :  27   Loss :  1.30498957399
Iteration :  28   Loss :  1.22456443476
Iteration :  29   Loss :  1.14909650339
Iteration :  30   Loss :  1.0782802728
Iteration :  31   Loss :  1.01182907334
Iteration :  32   Loss :  0.949473917433
Iteration :  33   Loss :  0.890962416122
Iteration :  34   Loss :  0.836057762957
Iteration :  35   Loss :  0.784537780739
Iteration :  36   Loss :  0.736194026175
Iteration :  37   Loss :  0.690830946827
Iteration :  38   Loss :  0.648265083709
Iteration :  39   Loss :  0.608324311929
Iteration :  40   Loss :  0.570847111699
Iteration :  41   Loss :  0.53568186464
Iteration :  42   Loss :  0.502686177421
Iteration :  43   Loss :  0.471726246772
Iteration :  44   Loss :  0.442676291825
Iteration :  45   Loss :  0.415418080527
Iteration :  46   Loss :  0.389840555967
Iteration :  47   Loss :  0.365839531433
Iteration :  48   Loss :  0.343317396232
Iteration :  49   Loss :  0.322182783186
Iteration :  50   Loss :  0.302350187634
Iteration :  51   Loss :  0.2837395641
Iteration :  52   Loss :  0.266275936852
Iteration :  53   Loss :  0.249889047869
Iteration :  54   Loss :  0.234513047861
Iteration :  55   Loss :  0.220086224257
Iteration :  56   Loss :  0.206550756212
Iteration :  57   Loss :  0.193852487406
Iteration :  58   Loss :  0.181940710023
Iteration :  59   Loss :  0.170767956101
Iteration :  60   Loss :  0.160289794153
Iteration :  61   Loss :  0.150464627603
Iteration :  62   Loss :  0.14125348507
Iteration :  63   Loss :  0.13261978294
Iteration :  64   Loss :  0.124529037624
Iteration :  65   Loss :  0.116948524225
Iteration :  66   Loss :  0.109846926138
Iteration :  67   Loss :  0.103194072571
Iteration :  68   Loss :  0.0969608672581
Iteration :  69   Loss :  0.0911194396933
Iteration :  70   Loss :  0.0856434380042
Iteration :  71   Loss :  0.0805083169662
Iteration :  72   Loss :  0.0756915047988
Iteration :  73   Loss :  0.071172421079
Iteration :  74   Loss :  0.0669323853761
Iteration :  75   Loss :  0.0629544584488
Iteration :  76   Loss :  0.0592232265041
Iteration :  77   Loss :  0.055724532966
Iteration :  78   Loss :  0.0524451886919
Iteration :  79   Loss :  0.0493727089248
Iteration :  80   Loss :  0.0464951120538
Iteration :  81   Loss :  0.0438007936338
Iteration :  82   Loss :  0.0412784850199
Iteration :  83   Loss :  0.0389173128234
Iteration :  84   Loss :  0.0367069604568
Iteration :  85   Loss :  0.0346378790886
Iteration :  86   Loss :  0.0327014370143
Iteration :  87   Loss :  0.0308898988015
Iteration :  88   Loss :  0.0291962120812
Iteration :  89   Loss :  0.0276136965358
Iteration :  90   Loss :  0.0261357908178
Iteration :  91   Loss :  0.0247559714117
Iteration :  92   Loss :  0.0234678394853
Iteration :  93   Loss :  0.0222652665176
Iteration :  94   Loss :  0.0211424786149
Iteration :  95   Loss :  0.0200940375464
Iteration :  96   Loss :  0.0191147626313
Iteration :  97   Loss :  0.0181996675936
Iteration :  98   Loss :  0.0173439609479
Iteration :  99   Loss :  0.0165431109197
[-0.00056131  0.00022846 -0.00042014 ...,  0.00045656 -0.00011222
  0.00017509]
CROSS VALIDATION 17
Iteration :  0   Loss :  66.6822839646
Iteration :  1   Loss :  7.32181694216
Iteration :  2   Loss :  6.87051746407
Iteration :  3   Loss :  6.44703501836
Iteration :  4   Loss :  6.04965502895
Iteration :  5   Loss :  5.67676860216
Iteration :  6   Loss :  5.32686601273
Iteration :  7   Loss :  4.9985305913
Iteration :  8   Loss :  4.69043298863
Iteration :  9   Loss :  4.40132579345
Iteration :  10   Loss :  4.13003848196
Iteration :  11   Loss :  3.87547267868
Iteration :  12   Loss :  3.63659770938
Iteration :  13   Loss :  3.41244642819
Iteration :  14   Loss :  3.20211130179
Iteration :  15   Loss :  3.00474073507
Iteration :  16   Loss :  2.81953562325
Iteration :  17   Loss :  2.64574611645
Iteration :  18   Loss :  2.4826685838
Iteration :  19   Loss :  2.32964276453
Iteration :  20   Loss :  2.18604909481
Iteration :  21   Loss :  2.05130619925
Iteration :  22   Loss :  1.92486853708
Iteration :  23   Loss :  1.80622419335
Iteration :  24   Loss :  1.69489280637
Iteration :  25   Loss :  1.59042362281
Iteration :  26   Loss :  1.49239367273
Iteration :  27   Loss :  1.40040605711
Iteration :  28   Loss :  1.31408834094
Iteration :  29   Loss :  1.23309104536
Iteration :  30   Loss :  1.15708623284
Iteration :  31   Loss :  1.08576617957
Iteration :  32   Loss :  1.01884212984
Iteration :  33   Loss :  0.956043127369
Iteration :  34   Loss :  0.897114918848
Iteration :  35   Loss :  0.841818925517
Iteration :  36   Loss :  0.78993127859
Iteration :  37   Loss :  0.741241914879
Iteration :  38   Loss :  0.695553729147
Iteration :  39   Loss :  0.652681780031
Iteration :  40   Loss :  0.612452546608
Iteration :  41   Loss :  0.574703232916
Iteration :  42   Loss :  0.539281117845
Iteration :  43   Loss :  0.506042947835
Iteration :  44   Loss :  0.474854369527
Iteration :  45   Loss :  0.445589398917
Iteration :  46   Loss :  0.418129922328
Iteration :  47   Loss :  0.392365222676
Iteration :  48   Loss :  0.368191522181
Iteration :  49   Loss :  0.345511530622
Iteration :  50   Loss :  0.324233988478
Iteration :  51   Loss :  0.304273199501
Iteration :  52   Loss :  0.285548559842
Iteration :  53   Loss :  0.267984109241
Iteration :  54   Loss :  0.251508145854
Iteration :  55   Loss :  0.23605294791
Iteration :  56   Loss :  0.221554626078
Iteration :  57   Loss :  0.207953095635
Iteration :  58   Loss :  0.195192119557
Iteration :  59   Loss :  0.183219344891
Iteration :  60   Loss :  0.171986250185
Iteration :  61   Loss :  0.161447952634
Iteration :  62   Loss :  0.151562879408
Iteration :  63   Loss :  0.142292358144
Iteration :  64   Loss :  0.133600204793
Iteration :  65   Loss :  0.125452381344
Iteration :  66   Loss :  0.117816763966
Iteration :  67   Loss :  0.11066300478
Iteration :  68   Loss :  0.103962411044
Iteration :  69   Loss :  0.0976877547371
Iteration :  70   Loss :  0.0918129895047
Iteration :  71   Loss :  0.0863129456891
Iteration :  72   Loss :  0.0811631234646
Iteration :  73   Loss :  0.0763396831263
Iteration :  74   Loss :  0.0718196655101
Iteration :  75   Loss :  0.0675813916414
Iteration :  76   Loss :  0.0636049091205
Iteration :  77   Loss :  0.0598723109037
Iteration :  78   Loss :  0.056367790289
Iteration :  79   Loss :  0.0530774066612
Iteration :  80   Loss :  0.0499886562958
Iteration :  81   Loss :  0.0470900063452
Iteration :  82   Loss :  0.0443705400566
Iteration :  83   Loss :  0.0418197967892
Iteration :  84   Loss :  0.0394277986909
Iteration :  85   Loss :  0.0371851714251
Iteration :  86   Loss :  0.0350832355092
Iteration :  87   Loss :  0.0331139905383
Iteration :  88   Loss :  0.0312699993072
Iteration :  89   Loss :  0.0295442366364
Iteration :  90   Loss :  0.027929969758
Iteration :  91   Loss :  0.0264207037024
Iteration :  92   Loss :  0.0250101872627
Iteration :  93   Loss :  0.0236924489858
Iteration :  94   Loss :  0.0224618241059
Iteration :  95   Loss :  0.0213129451583
Iteration :  96   Loss :  0.0202406958556
Iteration :  97   Loss :  0.0192401526907
Iteration :  98   Loss :  0.0183065448747
Iteration :  99   Loss :  0.0174352499874
[ -4.03483719e-04   2.60723246e-04  -1.79141818e-04 ...,   1.82141881e-04
   7.72581506e-06   2.04357705e-04]
CROSS VALIDATION 18
Iteration :  0   Loss :  137.939696507
Iteration :  1   Loss :  135.205236773
Iteration :  2   Loss :  6.87823398543
Iteration :  3   Loss :  6.45424866969
Iteration :  4   Loss :  6.05641673621
Iteration :  5   Loss :  5.68311387814
Iteration :  6   Loss :  5.33282438696
Iteration :  7   Loss :  5.00412823389
Iteration :  8   Loss :  4.69569376899
Iteration :  9   Loss :  4.40627182379
Iteration :  10   Loss :  4.13469045049
Iteration :  11   Loss :  3.87985007868
Iteration :  12   Loss :  3.64071900198
Iteration :  13   Loss :  3.41632914664
Iteration :  14   Loss :  3.20577209156
Iteration :  15   Loss :  3.00819532089
Iteration :  16   Loss :  2.82279870496
Iteration :  17   Loss :  2.64883121973
Iteration :  18   Loss :  2.4855879208
Iteration :  19   Loss :  2.33240717379
Iteration :  20   Loss :  2.1886681091
Iteration :  21   Loss :  2.0537882379
Iteration :  22   Loss :  1.92722116671
Iteration :  23   Loss :  1.8084543825
Iteration :  24   Loss :  1.69700712047
Iteration :  25   Loss :  1.59242834333
Iteration :  26   Loss :  1.49429485278
Iteration :  27   Loss :  1.40220953653
Iteration :  28   Loss :  1.31579974208
Iteration :  29   Loss :  1.23471576383
Iteration :  30   Loss :  1.15862943104
Iteration :  31   Loss :  1.08723278643
Iteration :  32   Loss :  1.0202368479
Iteration :  33   Loss :  0.957370447158
Iteration :  34   Loss :  0.898379140435
Iteration :  35   Loss :  0.843024186876
Iteration :  36   Loss :  0.791081590419
Iteration :  37   Loss :  0.742341200946
Iteration :  38   Loss :  0.696605870064
Iteration :  39   Loss :  0.6536906561
Iteration :  40   Loss :  0.613422071851
Iteration :  41   Loss :  0.575637367517
Iteration :  42   Loss :  0.540183841132
Iteration :  43   Loss :  0.506918171134
Iteration :  44   Loss :  0.475705772617
Iteration :  45   Loss :  0.446420190477
Iteration :  46   Loss :  0.418942554865
Iteration :  47   Loss :  0.393161126257
Iteration :  48   Loss :  0.368970938231
Iteration :  49   Loss :  0.346273509336
Iteration :  50   Loss :  0.324976566341
Iteration :  51   Loss :  0.304993727432
Iteration :  52   Loss :  0.286244133824
Iteration :  53   Loss :  0.268652058658
Iteration :  54   Loss :  0.252146532491
Iteration :  55   Loss :  0.236661005175
Iteration :  56   Loss :  0.222133037039
Iteration :  57   Loss :  0.208504000096
Iteration :  58   Loss :  0.195718779173
Iteration :  59   Loss :  0.183725484772
Iteration :  60   Loss :  0.172475208395
Iteration :  61   Loss :  0.161921853634
Iteration :  62   Loss :  0.152022057503
Iteration :  63   Loss :  0.142735182337
Iteration :  64   Loss :  0.134023326115
Iteration :  65   Loss :  0.125851287923
Iteration :  66   Loss :  0.118186443019
Iteration :  67   Loss :  0.110998516846
Iteration :  68   Loss :  0.104259279941
Iteration :  69   Loss :  0.0979422074495
Iteration :  70   Loss :  0.0920221616534
Iteration :  71   Loss :  0.0864751620827
Iteration :  72   Loss :  0.081278287574
Iteration :  73   Loss :  0.0764096959563
Iteration :  74   Loss :  0.0718486807548
Iteration :  75   Loss :  0.0675756730205
Iteration :  76   Loss :  0.0635721616826
Iteration :  77   Loss :  0.0598205918113
Iteration :  78   Loss :  0.0563043325832
Iteration :  79   Loss :  0.0530077622913
Iteration :  80   Loss :  0.0499164319247
Iteration :  81   Loss :  0.0470172066753
Iteration :  82   Loss :  0.0442982941111
Iteration :  83   Loss :  0.0417491351993
Iteration :  84   Loss :  0.0393601988422
Iteration :  85   Loss :  0.0371227398425
Iteration :  86   Loss :  0.0350285639164
Iteration :  87   Loss :  0.0330698229574
Iteration :  88   Loss :  0.0312388543793
Iteration :  89   Loss :  0.0295280751151
Iteration :  90   Loss :  0.0279299360871
Iteration :  91   Loss :  0.0264369373586
Iteration :  92   Loss :  0.0250417011865
Iteration :  93   Loss :  0.0237370955006
Iteration :  94   Loss :  0.0225163848069
Iteration :  95   Loss :  0.021373362434
Iteration :  96   Loss :  0.0203024095846
Iteration :  97   Loss :  0.0192984533701
Iteration :  98   Loss :  0.0183568513333
Iteration :  99   Loss :  0.0174732792865
[ -5.24113399e-04  -2.41364522e-04  -4.20278747e-04 ...,   4.76112141e-04
   4.09398972e-05   1.71129853e-04]
CROSS VALIDATION 19
Iteration :  0   Loss :  121.781367777
Iteration :  1   Loss :  19.1489279408
Iteration :  2   Loss :  7.93121044199
Iteration :  3   Loss :  7.44234938997
Iteration :  4   Loss :  6.9836205769
Iteration :  5   Loss :  6.55316672284
Iteration :  6   Loss :  6.14924502619
Iteration :  7   Loss :  5.77022010753
Iteration :  8   Loss :  5.41455738835
Iteration :  9   Loss :  5.08081687794
Iteration :  10   Loss :  4.7676473432
Iteration :  11   Loss :  4.47378083784
Iteration :  12   Loss :  4.19802756879
Iteration :  13   Loss :  3.939271079
Iteration :  14   Loss :  3.69646372722
Iteration :  15   Loss :  3.46862244629
Iteration :  16   Loss :  3.254824763
Iteration :  17   Loss :  3.05420506322
Iteration :  18   Loss :  2.86595108722
Iteration :  19   Loss :  2.68930064103
Iteration :  20   Loss :  2.52353851052
Iteration :  21   Loss :  2.36799356571
Iteration :  22   Loss :  2.22203604345
Iteration :  23   Loss :  2.08507499776
Iteration :  24   Loss :  1.95655590722
Iteration :  25   Loss :  1.83595842989
Iteration :  26   Loss :  1.72279429663
Iteration :  27   Loss :  1.61660533431
Iteration :  28   Loss :  1.51696161095
Iteration :  29   Loss :  1.42345969524
Iteration :  30   Loss :  1.33572102349
Iteration :  31   Loss :  1.2533903675
Iteration :  32   Loss :  1.17613439702
Iteration :  33   Loss :  1.10364033134
Iteration :  34   Loss :  1.03561467442
Iteration :  35   Loss :  0.971782028744
Iteration :  36   Loss :  0.911883983169
Iteration :  37   Loss :  0.855678070389
Iteration :  38   Loss :  0.802936789979
Iteration :  39   Loss :  0.753446693008
Iteration :  40   Loss :  0.707007524258
Iteration :  41   Loss :  0.663431417895
Iteration :  42   Loss :  0.622542141879
Iteration :  43   Loss :  0.584174385492
Iteration :  44   Loss :  0.548173083255
Iteration :  45   Loss :  0.514392767635
Iteration :  46   Loss :  0.48269694356
Iteration :  47   Loss :  0.45295748178
Iteration :  48   Loss :  0.42505403705
Iteration :  49   Loss :  0.398873509567
Iteration :  50   Loss :  0.374309576513
Iteration :  51   Loss :  0.351262312606
Iteration :  52   Loss :  0.329637888409
Iteration :  53   Loss :  0.309348297977
Iteration :  54   Loss :  0.290311055632
Iteration :  55   Loss :  0.27244883402
Iteration :  56   Loss :  0.255689072098
Iteration :  57   Loss :  0.239963619092
Iteration :  58   Loss :  0.225208470503
Iteration :  59   Loss :  0.211363601606
Iteration :  60   Loss :  0.198372847962
Iteration :  61   Loss :  0.186183760553
Iteration :  62   Loss :  0.17474738795
Iteration :  63   Loss :  0.164017984161
Iteration :  64   Loss :  0.15395267417
Iteration :  65   Loss :  0.144511118047
Iteration :  66   Loss :  0.135655210216
Iteration :  67   Loss :  0.127348843965
Iteration :  68   Loss :  0.119557762393
Iteration :  69   Loss :  0.112249503046
Iteration :  70   Loss :  0.105393427388
Iteration :  71   Loss :  0.0989608103487
Iteration :  72   Loss :  0.092924945744
Iteration :  73   Loss :  0.087261206088
Iteration :  74   Loss :  0.0819470032984
Iteration :  75   Loss :  0.0769616394732
Iteration :  76   Loss :  0.0722860846883
Iteration :  77   Loss :  0.0679027354998
Iteration :  78   Loss :  0.0637951918181
Iteration :  79   Loss :  0.0599480647945
Iteration :  80   Loss :  0.0563468110982
Iteration :  81   Loss :  0.0529775805089
Iteration :  82   Loss :  0.0498270631724
Iteration :  83   Loss :  0.0468823355479
Iteration :  84   Loss :  0.0441307298409
Iteration :  85   Loss :  0.0415597706852
Iteration :  86   Loss :  0.0391572104269
Iteration :  87   Loss :  0.0369111555924
Iteration :  88   Loss :  0.034810246279
Iteration :  89   Loss :  0.0328438467985
Iteration :  90   Loss :  0.0310022134221
Iteration :  91   Loss :  0.0292766053504
Iteration :  92   Loss :  0.027659308483
Iteration :  93   Loss :  0.0261435652336
Iteration :  94   Loss :  0.0247234370094
Iteration :  95   Loss :  0.0233936404734
Iteration :  96   Loss :  0.0221493849932
Iteration :  97   Loss :  0.0209862171689
Iteration :  98   Loss :  0.0198998727071
Iteration :  99   Loss :  0.018886148655
[ -5.36148909e-04  -7.87668444e-05  -3.49968941e-04 ...,   6.08472837e-04
   1.22357854e-05   9.14094665e-05]
Accuracy (Logistic Loss):	0.9
Hinge Loss
CROSS VALIDATION 0
Iteration :  0   Loss :  99.3498466167
Iteration :  1   Loss :  8.00476822682
Iteration :  2   Loss :  7.51137324693
Iteration :  3   Loss :  7.04838996657
Iteration :  4   Loss :  6.61394388053
Iteration :  5   Loss :  6.20627602364
Iteration :  6   Loss :  5.82373584919
Iteration :  7   Loss :  5.46477454628
Iteration :  8   Loss :  5.12793876903
Iteration :  9   Loss :  4.81186475238
Iteration :  10   Loss :  4.5152727905
Iteration :  11   Loss :  4.23696205563
Iteration :  12   Loss :  3.97580573617
Iteration :  13   Loss :  3.73074647453
Iteration :  14   Loss :  3.50079208614
Iteration :  15   Loss :  3.28501154235
Iteration :  16   Loss :  3.08253120089
Iteration :  17   Loss :  2.89253126875
Iteration :  18   Loss :  2.71424248302
Iteration :  19   Loss :  2.54694299634
Iteration :  20   Loss :  2.38995545431
Iteration :  21   Loss :  2.24264425305
Iteration :  22   Loss :  2.10441296579
Iteration :  23   Loss :  1.97470192813
Iteration :  24   Loss :  1.85298597203
Iteration :  25   Loss :  1.73877229956
Iteration :  26   Loss :  1.63159848772
Iteration :  27   Loss :  1.53103061614
Iteration :  28   Loss :  1.4366615103
Iteration :  29   Loss :  1.34810909293
Iteration :  30   Loss :  1.26501483711
Iteration :  31   Loss :  1.18704231468
Iteration :  32   Loss :  1.1138758341
Iteration :  33   Loss :  1.04521916233
Iteration :  34   Loss :  0.980794325411
Iteration :  35   Loss :  0.920340483057
Iteration :  36   Loss :  0.863612872554
Iteration :  37   Loss :  0.810381817786
Iteration :  38   Loss :  0.760431799327
Iteration :  39   Loss :  0.713560581859
Iteration :  40   Loss :  0.669578395372
Iteration :  41   Loss :  0.628307166829
Iteration :  42   Loss :  0.589579799195
Iteration :  43   Loss :  0.553239494901
Iteration :  44   Loss :  0.519139121008
Iteration :  45   Loss :  0.487140613504
Iteration :  46   Loss :  0.457114418317
Iteration :  47   Loss :  0.428938966781
Iteration :  48   Loss :  0.402500183434
Iteration :  49   Loss :  0.377691024157
Iteration :  50   Loss :  0.354411042777
Iteration :  51   Loss :  0.332565984386
Iteration :  52   Loss :  0.312067403724
Iteration :  53   Loss :  0.292832307089
Iteration :  54   Loss :  0.274782816313
Iteration :  55   Loss :  0.257845853457
Iteration :  56   Loss :  0.241952844931
Iteration :  57   Loss :  0.227039443858
Iteration :  58   Loss :  0.213045269553
Iteration :  59   Loss :  0.199913663052
Iteration :  60   Loss :  0.187591457715
Iteration :  61   Loss :  0.17602876397
Iteration :  62   Loss :  0.165178767319
Iteration :  63   Loss :  0.154997538797
Iteration :  64   Loss :  0.14544385712
Iteration :  65   Loss :  0.136479041784
Iteration :  66   Loss :  0.128066796461
Iteration :  67   Loss :  0.120173062043
Iteration :  68   Loss :  0.112765878743
Iteration :  69   Loss :  0.105815256701
Iteration :  70   Loss :  0.0992930545618
Iteration :  71   Loss :  0.0931728655356
Iteration :  72   Loss :  0.087429910485
Iteration :  73   Loss :  0.0820409375999
Iteration :  74   Loss :  0.076984128257
Iteration :  75   Loss :  0.0722390086811
Iteration :  76   Loss :  0.0677863670523
Iteration :  77   Loss :  0.0636081757217
Iteration :  78   Loss :  0.0596875182221
Iteration :  79   Loss :  0.0560085207772
Iteration :  80   Loss :  0.052556288033
Iteration :  81   Loss :  0.0493168427495
Iteration :  82   Loss :  0.0462770692112
Iteration :  83   Loss :  0.0434246601237
Iteration :  84   Loss :  0.0407480667857
Iteration :  85   Loss :  0.0382364523301
Iteration :  86   Loss :  0.0358796478488
Iteration :  87   Loss :  0.0336681112212
Iteration :  88   Loss :  0.0315928884804
Iteration :  89   Loss :  0.0296455775609
Iteration :  90   Loss :  0.0278182942804
Iteration :  91   Loss :  0.0261036404194
Iteration :  92   Loss :  0.0244946737666
Iteration :  93   Loss :  0.0229848800127
Iteration :  94   Loss :  0.0215681463746
Iteration :  95   Loss :  0.0202387368471
Iteration :  96   Loss :  0.0189912689783
Iteration :  97   Loss :  0.0178206920784
Iteration :  98   Loss :  0.0167222667698
Iteration :  99   Loss :  0.0156915457991
[ -8.79738834e-04   6.08834804e-05   9.94929958e-05 ...,   2.72399193e-04
   3.58926132e-04  -4.36746913e-05]
CROSS VALIDATION 1
Iteration :  0   Loss :  95.1280001171
Iteration :  1   Loss :  6.97146156356
Iteration :  2   Loss :  6.54175716232
Iteration :  3   Loss :  6.13853872399
Iteration :  4   Loss :  5.76017371648
Iteration :  5   Loss :  5.40513023308
Iteration :  6   Loss :  5.07197079022
Iteration :  7   Loss :  4.7593465074
Iteration :  8   Loss :  4.46599164593
Iteration :  9   Loss :  4.19071848424
Iteration :  10   Loss :  3.93241250914
Iteration :  11   Loss :  3.69002790338
Iteration :  12   Loss :  3.46258331141
Iteration :  13   Loss :  3.24915786611
Iteration :  14   Loss :  3.04888746045
Iteration :  15   Loss :  2.86096124889
Iteration :  16   Loss :  2.68461836452
Iteration :  17   Loss :  2.51914483843
Iteration :  18   Loss :  2.3638707091
Iteration :  19   Loss :  2.21816730984
Iteration :  20   Loss :  2.08144472347
Iteration :  21   Loss :  1.95314939394
Iteration :  22   Loss :  1.83276188506
Iteration :  23   Loss :  1.71979477748
Iteration :  24   Loss :  1.61379069522
Iteration :  25   Loss :  1.51432045385
Iteration :  26   Loss :  1.42098132288
Iteration :  27   Loss :  1.33339539517
Iteration :  28   Loss :  1.25120805688
Iteration :  29   Loss :  1.17408655173
Iteration :  30   Loss :  1.10171863374
Iteration :  31   Loss :  1.03381130304
Iteration :  32   Loss :  0.970089619584
Iteration :  33   Loss :  0.910295589978
Iteration :  34   Loss :  0.854187122926
Iteration :  35   Loss :  0.80153704907
Iteration :  36   Loss :  0.752132201234
Iteration :  37   Loss :  0.705772551362
Iteration :  38   Loss :  0.662270400653
Iteration :  39   Loss :  0.621449619619
Iteration :  40   Loss :  0.583144934975
Iteration :  41   Loss :  0.547201260491
Iteration :  42   Loss :  0.513473069085
Iteration :  43   Loss :  0.48182380362
Iteration :  44   Loss :  0.452125324018
Iteration :  45   Loss :  0.424257388454
Iteration :  46   Loss :  0.398107166522
Iteration :  47   Loss :  0.373568782417
Iteration :  48   Loss :  0.350542886268
Iteration :  49   Loss :  0.328936251894
Iteration :  50   Loss :  0.308661399357
Iteration :  51   Loss :  0.289636240775
Iteration :  52   Loss :  0.271783747968
Iteration :  53   Loss :  0.255031640591
Iteration :  54   Loss :  0.239312093489
Iteration :  55   Loss :  0.224561462089
Iteration :  56   Loss :  0.210720024719
Iteration :  57   Loss :  0.19773174081
Iteration :  58   Loss :  0.185544023999
Iteration :  59   Loss :  0.174107529226
Iteration :  60   Loss :  99.8450422946
Iteration :  61   Loss :  81.959978095
Iteration :  62   Loss :  3.37269609311
Iteration :  63   Loss :  3.16481108334
Iteration :  64   Loss :  2.96973961386
Iteration :  65   Loss :  2.78669188836
Iteration :  66   Loss :  2.6149267917
Iteration :  67   Loss :  2.45374888932
Iteration :  68   Loss :  2.30250561162
Iteration :  69   Loss :  2.16058461181
Iteration :  70   Loss :  2.02741128675
Iteration :  71   Loss :  1.90244645046
Iteration :  72   Loss :  1.78518415109
Iteration :  73   Loss :  1.6751496225
Iteration :  74   Loss :  1.57189736198
Iteration :  75   Loss :  1.47500932659
Iteration :  76   Loss :  1.38409324052
Iteration :  77   Loss :  1.29878100696
Iteration :  78   Loss :  1.21872721769
Iteration :  79   Loss :  1.14360775463
Iteration :  80   Loss :  1.0731184776
Iteration :  81   Loss :  1.00697399288
Iteration :  82   Loss :  0.944906497746
Iteration :  83   Loss :  0.886664696204
Iteration :  84   Loss :  0.83201278155
Iteration :  85   Loss :  0.780729481648
Iteration :  86   Loss :  0.732607163052
Iteration :  87   Loss :  0.687450990351
Iteration :  88   Loss :  0.645078137327
Iteration :  89   Loss :  0.60531704674
Iteration :  90   Loss :  0.56800673573
Iteration :  91   Loss :  0.532996144041
Iteration :  92   Loss :  0.500143522414
Iteration :  93   Loss :  0.469315858677
Iteration :  94   Loss :  0.440388339217
Iteration :  95   Loss :  0.413243843634
Iteration :  96   Loss :  0.387772470555
Iteration :  97   Loss :  0.363871092665
Iteration :  98   Loss :  0.341442939174
Iteration :  99   Loss :  0.320397204015
[-0.00618497 -0.00133888 -0.003112   ...,  0.00201005 -0.00047023
  0.00038856]
CROSS VALIDATION 2
Iteration :  0   Loss :  184.355437182
Iteration :  1   Loss :  7.89430323352
Iteration :  2   Loss :  7.40771705453
Iteration :  3   Loss :  6.9511228967
Iteration :  4   Loss :  6.52267212278
Iteration :  5   Loss :  6.12063004115
Iteration :  6   Loss :  5.74336888249
Iteration :  7   Loss :  5.38936120931
Iteration :  8   Loss :  5.05717373178
Iteration :  9   Loss :  4.74546150464
Iteration :  10   Loss :  4.45296248189
Iteration :  11   Loss :  4.17849240706
Iteration :  12   Loss :  3.92094001844
Iteration :  13   Loss :  3.67926254987
Iteration :  14   Loss :  3.45248150882
Iteration :  15   Loss :  3.23967871474
Iteration :  16   Loss :  3.03999258155
Iteration :  17   Loss :  2.85261462929
Iteration :  18   Loss :  2.67678621081
Iteration :  19   Loss :  2.51179544015
Iteration :  20   Loss :  2.35697431035
Iteration :  21   Loss :  2.21169598879
Iteration :  22   Loss :  2.07537227935
Iteration :  23   Loss :  1.94745124091
Iteration :  24   Loss :  1.82741495271
Iteration :  25   Loss :  1.71477741739
Iteration :  26   Loss :  1.60908259333
Iteration :  27   Loss :  1.50990254823
Iteration :  28   Loss :  1.41683572651
Iteration :  29   Loss :  1.32950532356
Iteration :  30   Loss :  1.24755776008
Iteration :  31   Loss :  1.17066125059
Iteration :  32   Loss :  1.09850446006
Iteration :  33   Loss :  1.03079524342
Iteration :  34   Loss :  0.967259462749
Iteration :  35   Loss :  0.918399328458
Iteration :  36   Loss :  29.90332226
Iteration :  37   Loss :  5.47422966033
Iteration :  38   Loss :  3.66099713487
Iteration :  39   Loss :  3.4353419308
Iteration :  40   Loss :  3.22359557977
Iteration :  41   Loss :  3.02490077297
Iteration :  42   Loss :  2.8384530441
Iteration :  43   Loss :  2.66349751223
Iteration :  44   Loss :  2.49932582552
Iteration :  45   Loss :  2.34527329327
Iteration :  46   Loss :  2.20071619473
Iteration :  47   Loss :  2.06506925382
Iteration :  48   Loss :  1.9377832695
Iteration :  49   Loss :  1.81834289219
Iteration :  50   Loss :  1.70626453723
Iteration :  51   Loss :  1.60109442697
Iteration :  52   Loss :  1.50240675354
Iteration :  53   Loss :  1.40980195489
Iteration :  54   Loss :  1.32290509698
Iteration :  55   Loss :  1.24136435586
Iteration :  56   Loss :  1.16484959315
Iteration :  57   Loss :  1.09305101944
Iteration :  58   Loss :  1.02567793998
Iteration :  59   Loss :  0.962457577789
Iteration :  60   Loss :  0.903133969187
Iteration :  61   Loss :  0.847466927501
Iteration :  62   Loss :  0.795231070596
Iteration :  63   Loss :  0.746214908358
Iteration :  64   Loss :  0.700219986422
Iteration :  65   Loss :  0.657060082683
Iteration :  66   Loss :  0.616560453325
Iteration :  67   Loss :  0.578557125327
Iteration :  68   Loss :  0.542896232578
Iteration :  69   Loss :  0.50943339291
Iteration :  70   Loss :  0.478033123529
Iteration :  71   Loss :  0.448568292482
Iteration :  72   Loss :  0.420919603928
Iteration :  73   Loss :  0.39497511514
Iteration :  74   Loss :  0.370629783274
Iteration :  75   Loss :  0.34778504008
Iteration :  76   Loss :  0.326348392822
Iteration :  77   Loss :  0.306233049798
Iteration :  78   Loss :  0.287357568939
Iteration :  79   Loss :  0.269645528074
Iteration :  80   Loss :  0.253025215513
Iteration :  81   Loss :  0.237429339708
Iteration :  82   Loss :  0.222794756798
Iteration :  83   Loss :  0.209062214963
Iteration :  84   Loss :  0.196176114526
Iteration :  85   Loss :  0.18408428284
Iteration :  86   Loss :  0.172737763059
Iteration :  87   Loss :  0.162090615919
Iteration :  88   Loss :  0.152099733745
Iteration :  89   Loss :  0.142724665917
Iteration :  90   Loss :  0.133927455096
Iteration :  91   Loss :  0.125672483541
Iteration :  92   Loss :  0.117926328907
Iteration :  93   Loss :  0.110657628924
Iteration :  94   Loss :  0.103836954414
Iteration :  95   Loss :  0.0974366901489
Iteration :  96   Loss :  0.0914309230343
Iteration :  97   Loss :  0.0857953371992
Iteration :  98   Loss :  0.0805071155452
Iteration :  99   Loss :  0.0755448473658
[ -3.21326601e-03  -1.45362797e-03  -1.15010825e-03 ...,   2.30571619e-03
   1.05429960e-03   8.48641570e-05]
CROSS VALIDATION 3
Iteration :  0   Loss :  180.285142839
Iteration :  1   Loss :  7.93231530947
Iteration :  2   Loss :  7.44338615603
Iteration :  3   Loss :  6.98459343915
Iteration :  4   Loss :  6.55407962016
Iteration :  5   Loss :  6.15010165468
Iteration :  6   Loss :  5.77102393547
Iteration :  7   Loss :  5.41531167024
Iteration :  8   Loss :  5.08152466767
Iteration :  9   Loss :  4.76831150644
Iteration :  10   Loss :  4.47440406363
Iteration :  11   Loss :  4.19861238042
Iteration :  12   Loss :  3.93981984423
Iteration :  13   Loss :  3.69697866785
Iteration :  14   Loss :  3.4691056472
Iteration :  15   Loss :  3.25527818055
Iteration :  16   Loss :  3.05463053318
Iteration :  17   Loss :  2.86635033221
Iteration :  18   Loss :  2.68967527748
Iteration :  19   Loss :  2.52389005525
Iteration :  20   Loss :  2.36832344198
Iteration :  21   Loss :  2.22234558679
Iteration :  22   Loss :  2.08536546131
Iteration :  23   Loss :  1.95682846677
Iteration :  24   Loss :  1.8441671871
Iteration :  25   Loss :  3.23331571908
Iteration :  26   Loss :  3.03402178589
Iteration :  27   Loss :  2.84701185935
Iteration :  28   Loss :  2.67152878235
Iteration :  29   Loss :  2.50686206715
Iteration :  30   Loss :  2.35234501879
Iteration :  31   Loss :  2.20735203581
Iteration :  32   Loss :  2.07129607736
Iteration :  33   Loss :  1.94362628637
Iteration :  34   Loss :  1.82382575932
Iteration :  35   Loss :  1.71140945339
Iteration :  36   Loss :  1.60592222267
Iteration :  37   Loss :  1.50693697534
Iteration :  38   Loss :  1.41405294452
Iteration :  39   Loss :  1.32689406567
Iteration :  40   Loss :  1.24510745395
Iteration :  41   Loss :  1.16836197553
Iteration :  42   Loss :  1.09634690688
Iteration :  43   Loss :  1.02877067672
Iteration :  44   Loss :  0.965359685545
Iteration :  45   Loss :  0.905857197885
Iteration :  46   Loss :  0.850022302824
Iteration :  47   Loss :  0.797628938628
Iteration :  48   Loss :  0.748464977476
Iteration :  49   Loss :  0.702331366602
Iteration :  50   Loss :  0.659041322383
Iteration :  51   Loss :  0.618419574096
Iteration :  52   Loss :  0.580301654291
Iteration :  53   Loss :  0.544533232903
Iteration :  54   Loss :  0.510969492406
Iteration :  55   Loss :  0.479474541486
Iteration :  56   Loss :  0.449920864846
Iteration :  57   Loss :  0.422188806931
Iteration :  58   Loss :  0.396166087471
Iteration :  59   Loss :  0.371747346887
Iteration :  60   Loss :  0.348833719716
Iteration :  61   Loss :  0.327332434326
Iteration :  62   Loss :  0.307156437311
Iteration :  63   Loss :  0.288224041029
Iteration :  64   Loss :  0.270458592874
Iteration :  65   Loss :  0.253788164923
Iteration :  66   Loss :  0.23814526272
Iteration :  67   Loss :  0.223466552009
Iteration :  68   Loss :  0.209692602306
Iteration :  69   Loss :  0.196767646284
Iteration :  70   Loss :  0.18463935398
Iteration :  71   Loss :  0.173258620927
Iteration :  72   Loss :  0.16257936934
Iteration :  73   Loss :  0.152558361561
Iteration :  74   Loss :  0.143155025
Iteration :  75   Loss :  0.134331287862
Iteration :  76   Loss :  0.126051425011
Iteration :  77   Loss :  0.118281913321
Iteration :  78   Loss :  0.110991295955
Iteration :  79   Loss :  0.104150055
Iteration :  80   Loss :  0.0977304919546
Iteration :  81   Loss :  0.0917066155916
Iteration :  82   Loss :  0.0860540367194
Iteration :  83   Loss :  0.0807498694389
Iteration :  84   Loss :  0.0757726384836
Iteration :  85   Loss :  0.0711021922718
Iteration :  86   Loss :  0.0667196213176
Iteration :  87   Loss :  0.0626071816709
Iteration :  88   Loss :  0.0587482230769
Iteration :  89   Loss :  0.0551271215631
Iteration :  90   Loss :  0.0517292161817
Iteration :  91   Loss :  0.0485407496509
Iteration :  92   Loss :  0.0455488126554
Iteration :  93   Loss :  0.0427412915796
Iteration :  94   Loss :  0.0401068194622
Iteration :  95   Loss :  0.0376347299749
Iteration :  96   Loss :  0.0353150142363
Iteration :  97   Loss :  0.0331382802891
Iteration :  98   Loss :  0.0310957150737
Iteration :  99   Loss :  0.029179048747
[ -2.23982546e-03  -7.91278410e-04  -6.39450738e-04 ...,   1.13820391e-03
   1.16603344e-03   9.08473754e-05]
CROSS VALIDATION 4
Iteration :  0   Loss :  180.285142839
Iteration :  1   Loss :  7.93231530947
Iteration :  2   Loss :  7.44338615603
Iteration :  3   Loss :  6.98459343915
Iteration :  4   Loss :  6.55407962016
Iteration :  5   Loss :  6.15010165468
Iteration :  6   Loss :  5.77102393547
Iteration :  7   Loss :  5.41531167024
Iteration :  8   Loss :  5.08152466767
Iteration :  9   Loss :  4.76831150644
Iteration :  10   Loss :  4.47440406363
Iteration :  11   Loss :  4.19861238042
Iteration :  12   Loss :  3.93981984423
Iteration :  13   Loss :  3.69697866785
Iteration :  14   Loss :  3.4691056472
Iteration :  15   Loss :  3.25527818055
Iteration :  16   Loss :  3.05463053318
Iteration :  17   Loss :  2.86635033221
Iteration :  18   Loss :  2.68967527748
Iteration :  19   Loss :  2.52389005525
Iteration :  20   Loss :  2.36832344198
Iteration :  21   Loss :  2.22234558679
Iteration :  22   Loss :  2.08536546131
Iteration :  23   Loss :  1.95682846677
Iteration :  24   Loss :  1.8441671871
Iteration :  25   Loss :  3.23331571908
Iteration :  26   Loss :  3.03402178589
Iteration :  27   Loss :  2.84701185935
Iteration :  28   Loss :  2.67152878235
Iteration :  29   Loss :  2.50686206715
Iteration :  30   Loss :  2.35234501879
Iteration :  31   Loss :  2.20735203581
Iteration :  32   Loss :  2.07129607736
Iteration :  33   Loss :  1.94362628637
Iteration :  34   Loss :  1.82382575932
Iteration :  35   Loss :  1.71140945339
Iteration :  36   Loss :  1.60592222267
Iteration :  37   Loss :  1.50693697534
Iteration :  38   Loss :  1.41405294452
Iteration :  39   Loss :  1.32689406567
Iteration :  40   Loss :  1.24510745395
Iteration :  41   Loss :  1.16836197553
Iteration :  42   Loss :  1.09634690688
Iteration :  43   Loss :  1.02877067672
Iteration :  44   Loss :  0.965359685545
Iteration :  45   Loss :  0.905857197885
Iteration :  46   Loss :  0.850022302824
Iteration :  47   Loss :  0.797628938628
Iteration :  48   Loss :  0.748464977476
Iteration :  49   Loss :  0.702331366602
Iteration :  50   Loss :  0.659041322383
Iteration :  51   Loss :  0.618419574096
Iteration :  52   Loss :  0.580301654291
Iteration :  53   Loss :  0.544533232903
Iteration :  54   Loss :  0.510969492406
Iteration :  55   Loss :  0.479474541486
Iteration :  56   Loss :  0.449920864846
Iteration :  57   Loss :  0.422188806931
Iteration :  58   Loss :  0.396166087471
Iteration :  59   Loss :  0.371747346887
Iteration :  60   Loss :  0.348833719716
Iteration :  61   Loss :  0.327332434326
Iteration :  62   Loss :  0.307156437311
Iteration :  63   Loss :  0.288224041029
Iteration :  64   Loss :  0.270458592874
Iteration :  65   Loss :  0.253788164923
Iteration :  66   Loss :  0.23814526272
Iteration :  67   Loss :  0.223466552009
Iteration :  68   Loss :  0.209692602306
Iteration :  69   Loss :  0.196767646284
Iteration :  70   Loss :  0.18463935398
Iteration :  71   Loss :  0.173258620927
Iteration :  72   Loss :  0.16257936934
Iteration :  73   Loss :  0.152558361561
Iteration :  74   Loss :  0.143155025
Iteration :  75   Loss :  0.134331287862
Iteration :  76   Loss :  0.126051425011
Iteration :  77   Loss :  0.118281913321
Iteration :  78   Loss :  0.110991295955
Iteration :  79   Loss :  0.104150055
Iteration :  80   Loss :  0.0977304919546
Iteration :  81   Loss :  0.0917066155916
Iteration :  82   Loss :  0.0860540367194
Iteration :  83   Loss :  0.0807498694389
Iteration :  84   Loss :  0.0757726384836
Iteration :  85   Loss :  0.0711021922718
Iteration :  86   Loss :  0.0667196213176
Iteration :  87   Loss :  0.0626071816709
Iteration :  88   Loss :  0.0587482230769
Iteration :  89   Loss :  0.0551271215631
Iteration :  90   Loss :  0.0517292161817
Iteration :  91   Loss :  0.0485407496509
Iteration :  92   Loss :  0.0455488126554
Iteration :  93   Loss :  0.0427412915796
Iteration :  94   Loss :  0.0401068194622
Iteration :  95   Loss :  0.0376347299749
Iteration :  96   Loss :  0.0353150142363
Iteration :  97   Loss :  0.0331382802891
Iteration :  98   Loss :  0.0310957150737
Iteration :  99   Loss :  0.029179048747
[ -2.23982546e-03  -7.91278410e-04  -6.39450738e-04 ...,   1.13820391e-03
   1.16603344e-03   9.08473754e-05]
CROSS VALIDATION 5
Iteration :  0   Loss :  161.732581699
Iteration :  1   Loss :  7.79523820743
Iteration :  2   Loss :  7.31475816233
Iteration :  3   Loss :  6.86389377074
Iteration :  4   Loss :  6.44081959382
Iteration :  5   Loss :  6.04382270847
Iteration :  6   Loss :  5.67129577212
Iteration :  7   Loss :  5.32173051499
Iteration :  8   Loss :  4.99371163349
Iteration :  9   Loss :  4.68591105998
Iteration :  10   Loss :  4.39708258579
Iteration :  11   Loss :  4.12605681558
Iteration :  12   Loss :  3.8717364328
Iteration :  13   Loss :  3.63309175686
Iteration :  14   Loss :  3.40915657429
Iteration :  15   Loss :  3.1990242267
Iteration :  16   Loss :  3.00184393999
Iteration :  17   Loss :  2.81681737977
Iteration :  18   Loss :  2.64319541907
Iteration :  19   Loss :  2.48027510537
Iteration :  20   Loss :  2.32739681444
Iteration :  21   Loss :  2.18394157977
Iteration :  22   Loss :  2.04932858645
Iteration :  23   Loss :  1.92301281964
Iteration :  24   Loss :  1.80448285793
Iteration :  25   Loss :  1.69325880269
Iteration :  26   Loss :  1.58889033514
Iteration :  27   Loss :  1.49095489307
Iteration :  28   Loss :  1.39905596001
Iteration :  29   Loss :  1.31282145981
Iteration :  30   Loss :  1.23190225023
Iteration :  31   Loss :  1.15597070933
Iteration :  32   Loss :  1.08471940902
Iteration :  33   Loss :  1.01785987032
Iteration :  34   Loss :  0.955121395451
Iteration :  35   Loss :  0.896249971776
Iteration :  36   Loss :  0.841007243408
Iteration :  37   Loss :  0.789169546152
Iteration :  38   Loss :  0.740527001943
Iteration :  39   Loss :  0.694882669105
Iteration :  40   Loss :  0.652051744981
Iteration :  41   Loss :  0.611860817713
Iteration :  42   Loss :  0.574147164138
Iteration :  43   Loss :  0.538758090966
Iteration :  44   Loss :  0.505550316559
Iteration :  45   Loss :  0.47438939082
Iteration :  46   Loss :  0.445149150839
Iteration :  47   Loss :  0.417711210088
Iteration :  48   Loss :  0.391964479107
Iteration :  49   Loss :  0.367804715725
Iteration :  50   Loss :  0.345134103013
Iteration :  51   Loss :  0.323860853246
Iteration :  52   Loss :  0.303898836277
Iteration :  53   Loss :  0.285167230818
Iteration :  54   Loss :  0.267590197214
Iteration :  55   Loss :  0.251096570387
Iteration :  56   Loss :  0.235619571705
Iteration :  57   Loss :  0.221096538614
Iteration :  58   Loss :  0.207468670931
Iteration :  59   Loss :  0.194680792779
Iteration :  60   Loss :  0.182681129189
Iteration :  61   Loss :  0.17142109648
Iteration :  62   Loss :  0.160855105553
Iteration :  63   Loss :  0.150940377316
Iteration :  64   Loss :  0.141636769477
Iteration :  65   Loss :  0.132906614018
Iteration :  66   Loss :  0.124714564693
Iteration :  67   Loss :  0.117027453912
Iteration :  68   Loss :  0.109814158456
Iteration :  69   Loss :  0.103045473472
Iteration :  70   Loss :  0.0966939942194
Iteration :  71   Loss :  0.0907340051252
Iteration :  72   Loss :  0.0851413756617
Iteration :  73   Loss :  0.07989346265
Iteration :  74   Loss :  0.0749690185836
Iteration :  75   Loss :  0.0703481056017
Iteration :  76   Loss :  0.0660120147661
Iteration :  77   Loss :  0.0619431903136
Iteration :  78   Loss :  0.0581251585763
Iteration :  79   Loss :  0.0545424612846
Iteration :  80   Loss :  0.0511805929799
Iteration :  81   Loss :  0.0480259422856
Iteration :  82   Loss :  0.0450657367984
Iteration :  83   Loss :  0.0422879913758
Iteration :  84   Loss :  0.0396814596108
Iteration :  85   Loss :  0.0372355882986
Iteration :  86   Loss :  0.0349404747089
Iteration :  87   Loss :  0.0327868264922
Iteration :  88   Loss :  0.0307659240576
Iteration :  89   Loss :  0.0288695852691
Iteration :  90   Loss :  0.0270901323181
Iteration :  91   Loss :  0.0254203606381
Iteration :  92   Loss :  0.0238535097349
Iteration :  93   Loss :  0.0223832358153
Iteration :  94   Loss :  0.0210035861025
Iteration :  95   Loss :  0.0197089747348
Iteration :  96   Loss :  0.0184941601496
Iteration :  97   Loss :  0.017354223862
Iteration :  98   Loss :  0.016284550551
Iteration :  99   Loss :  0.0152808093727
[ -2.47828082e-04   1.32083593e-04  -5.37218753e-04 ...,   1.30754411e-04
   4.82883477e-05   2.08821910e-04]
CROSS VALIDATION 6
Iteration :  0   Loss :  180.196086185
Iteration :  1   Loss :  7.93274194411
Iteration :  2   Loss :  7.44378649392
Iteration :  3   Loss :  6.98496910116
Iteration :  4   Loss :  6.55443212725
Iteration :  5   Loss :  6.15043243406
Iteration :  6   Loss :  5.77133432639
Iteration :  7   Loss :  5.4156029294
Iteration :  8   Loss :  5.08179797431
Iteration :  9   Loss :  4.76856796711
Iteration :  10   Loss :  4.47464471667
Iteration :  11   Loss :  4.19883820017
Iteration :  12   Loss :  3.94003174499
Iteration :  13   Loss :  3.69717750755
Iteration :  14   Loss :  3.46929223088
Iteration :  15   Loss :  3.25545326366
Iteration :  16   Loss :  3.05479482458
Iteration :  17   Loss :  2.86650449707
Iteration :  18   Loss :  2.68981993999
Iteration :  19   Loss :  2.5240258011
Iteration :  20   Loss :  2.36845082078
Iteration :  21   Loss :  2.22246511426
Iteration :  22   Loss :  2.08547762139
Iteration :  23   Loss :  1.95693371357
Iteration :  24   Loss :  1.84201549741
Iteration :  25   Loss :  3.23340831478
Iteration :  26   Loss :  3.03410867421
Iteration :  27   Loss :  2.84709339208
Iteration :  28   Loss :  2.6716052896
Iteration :  29   Loss :  2.50693385867
Iteration :  30   Loss :  2.35241238525
Iteration :  31   Loss :  2.20741524997
Iteration :  32   Loss :  2.07135539514
Iteration :  33   Loss :  1.94368194794
Iteration :  34   Loss :  1.82387799004
Iteration :  35   Loss :  1.71145846474
Iteration :  36   Loss :  1.60596821307
Iteration :  37   Loss :  1.506980131
Iteration :  38   Loss :  1.41409344017
Iteration :  39   Loss :  1.32693206526
Iteration :  40   Loss :  1.24514311134
Iteration :  41   Loss :  1.16839543508
Iteration :  42   Loss :  1.09637830406
Iteration :  43   Loss :  1.02880013865
Iteration :  44   Loss :  0.965387331519
Iteration :  45   Loss :  0.905883139826
Iteration :  46   Loss :  0.850046645765
Iteration :  47   Loss :  0.797651781128
Iteration :  48   Loss :  0.748486412018
Iteration :  49   Loss :  0.702351479969
Iteration :  50   Loss :  0.65906019601
Iteration :  51   Loss :  0.618437284397
Iteration :  52   Loss :  0.580318272971
Iteration :  53   Loss :  0.544548827247
Iteration :  54   Loss :  0.510984125552
Iteration :  55   Loss :  0.479488272679
Iteration :  56   Loss :  0.449933749681
Iteration :  57   Loss :  0.422200897575
Iteration :  58   Loss :  0.396177432876
Iteration :  59   Loss :  0.371757992989
Iteration :  60   Loss :  0.348843709617
Iteration :  61   Loss :  0.327341808473
Iteration :  62   Loss :  0.307165233658
Iteration :  63   Loss :  0.28823229519
Iteration :  64   Loss :  0.270466338268
Iteration :  65   Loss :  0.253795432909
Iteration :  66   Loss :  0.238152082724
Iteration :  67   Loss :  0.223472951644
Iteration :  68   Loss :  0.209698607484
Iteration :  69   Loss :  0.196773281317
Iteration :  70   Loss :  0.184644641682
Iteration :  71   Loss :  0.173263582707
Iteration :  72   Loss :  0.162584025288
Iteration :  73   Loss :  0.152562730528
Iteration :  74   Loss :  0.143159124674
Iteration :  75   Loss :  0.134335134842
Iteration :  76   Loss :  0.126055034872
Iteration :  77   Loss :  0.118285300679
Iteration :  78   Loss :  0.110994474525
Iteration :  79   Loss :  0.104153037649
Iteration :  80   Loss :  0.0977332907607
Iteration :  81   Loss :  0.0917092418859
Iteration :  82   Loss :  0.0860565011351
Iteration :  83   Loss :  0.0807521819538
Iteration :  84   Loss :  0.0757748084606
Iteration :  85   Loss :  0.0711042284966
Iteration :  86   Loss :  0.0667215320343
Iteration :  87   Loss :  0.0626089746155
Iteration :  88   Loss :  0.0587499055087
Iteration :  89   Loss :  0.0551287002938
Iteration :  90   Loss :  0.0517306976031
Iteration :  91   Loss :  0.048542139761
Iteration :  92   Loss :  0.0455501170825
Iteration :  93   Loss :  0.0427425156048
Iteration :  94   Loss :  0.0401079680414
Iteration :  95   Loss :  0.0376358077584
Iteration :  96   Loss :  0.0353160255878
Iteration :  97   Loss :  0.0331392293032
Iteration :  98   Loss :  0.0310966055929
Iteration :  99   Loss :  0.0291798843767
[ -2.23993323e-03  -7.91281063e-04  -6.39461212e-04 ...,   1.13830972e-03
   1.16616540e-03   9.08223579e-05]
CROSS VALIDATION 7
Iteration :  0   Loss :  180.196086185
Iteration :  1   Loss :  7.93274194411
Iteration :  2   Loss :  7.44378649392
Iteration :  3   Loss :  6.98496910116
Iteration :  4   Loss :  6.55443212725
Iteration :  5   Loss :  6.15043243406
Iteration :  6   Loss :  5.77133432639
Iteration :  7   Loss :  5.4156029294
Iteration :  8   Loss :  5.08179797431
Iteration :  9   Loss :  4.76856796711
Iteration :  10   Loss :  4.47464471667
Iteration :  11   Loss :  4.19883820017
Iteration :  12   Loss :  3.94003174499
Iteration :  13   Loss :  3.69717750755
Iteration :  14   Loss :  3.46929223088
Iteration :  15   Loss :  3.25545326366
Iteration :  16   Loss :  3.05479482458
Iteration :  17   Loss :  2.86650449707
Iteration :  18   Loss :  2.68981993999
Iteration :  19   Loss :  2.5240258011
Iteration :  20   Loss :  2.36845082078
Iteration :  21   Loss :  2.22246511426
Iteration :  22   Loss :  2.08547762139
Iteration :  23   Loss :  1.95693371357
Iteration :  24   Loss :  1.84201549741
Iteration :  25   Loss :  3.23340831478
Iteration :  26   Loss :  3.03410867421
Iteration :  27   Loss :  2.84709339208
Iteration :  28   Loss :  2.6716052896
Iteration :  29   Loss :  2.50693385867
Iteration :  30   Loss :  2.35241238525
Iteration :  31   Loss :  2.20741524997
Iteration :  32   Loss :  2.07135539514
Iteration :  33   Loss :  1.94368194794
Iteration :  34   Loss :  1.82387799004
Iteration :  35   Loss :  1.71145846474
Iteration :  36   Loss :  1.60596821307
Iteration :  37   Loss :  1.506980131
Iteration :  38   Loss :  1.41409344017
Iteration :  39   Loss :  1.32693206526
Iteration :  40   Loss :  1.24514311134
Iteration :  41   Loss :  1.16839543508
Iteration :  42   Loss :  1.09637830406
Iteration :  43   Loss :  1.02880013865
Iteration :  44   Loss :  0.965387331519
Iteration :  45   Loss :  0.905883139826
Iteration :  46   Loss :  0.850046645765
Iteration :  47   Loss :  0.797651781128
Iteration :  48   Loss :  0.748486412018
Iteration :  49   Loss :  0.702351479969
Iteration :  50   Loss :  0.65906019601
Iteration :  51   Loss :  0.618437284397
Iteration :  52   Loss :  0.580318272971
Iteration :  53   Loss :  0.544548827247
Iteration :  54   Loss :  0.510984125552
Iteration :  55   Loss :  0.479488272679
Iteration :  56   Loss :  0.449933749681
Iteration :  57   Loss :  0.422200897575
Iteration :  58   Loss :  0.396177432876
Iteration :  59   Loss :  0.371757992989
Iteration :  60   Loss :  0.348843709617
Iteration :  61   Loss :  0.327341808473
Iteration :  62   Loss :  0.307165233658
Iteration :  63   Loss :  0.28823229519
Iteration :  64   Loss :  0.270466338268
Iteration :  65   Loss :  0.253795432909
Iteration :  66   Loss :  0.238152082724
Iteration :  67   Loss :  0.223472951644
Iteration :  68   Loss :  0.209698607484
Iteration :  69   Loss :  0.196773281317
Iteration :  70   Loss :  0.184644641682
Iteration :  71   Loss :  0.173263582707
Iteration :  72   Loss :  0.162584025288
Iteration :  73   Loss :  0.152562730528
Iteration :  74   Loss :  0.143159124674
Iteration :  75   Loss :  0.134335134842
Iteration :  76   Loss :  0.126055034872
Iteration :  77   Loss :  0.118285300679
Iteration :  78   Loss :  0.110994474525
Iteration :  79   Loss :  0.104153037649
Iteration :  80   Loss :  0.0977332907607
Iteration :  81   Loss :  0.0917092418859
Iteration :  82   Loss :  0.0860565011351
Iteration :  83   Loss :  0.0807521819538
Iteration :  84   Loss :  0.0757748084606
Iteration :  85   Loss :  0.0711042284966
Iteration :  86   Loss :  0.0667215320343
Iteration :  87   Loss :  0.0626089746155
Iteration :  88   Loss :  0.0587499055087
Iteration :  89   Loss :  0.0551287002938
Iteration :  90   Loss :  0.0517306976031
Iteration :  91   Loss :  0.048542139761
Iteration :  92   Loss :  0.0455501170825
Iteration :  93   Loss :  0.0427425156048
Iteration :  94   Loss :  0.0401079680414
Iteration :  95   Loss :  0.0376358077584
Iteration :  96   Loss :  0.0353160255878
Iteration :  97   Loss :  0.0331392293032
Iteration :  98   Loss :  0.0310966055929
Iteration :  99   Loss :  0.0291798843767
[ -2.23993323e-03  -7.91281063e-04  -6.39461212e-04 ...,   1.13830972e-03
   1.16616540e-03   9.08223579e-05]
CROSS VALIDATION 8
Iteration :  0   Loss :  173.468722793
Iteration :  1   Loss :  7.2340940818
Iteration :  2   Loss :  6.78820163334
Iteration :  3   Loss :  6.36979294073
Iteration :  4   Loss :  5.97717397028
Iteration :  5   Loss :  5.60875510451
Iteration :  6   Loss :  5.26304470621
Iteration :  7   Loss :  4.93864307915
Iteration :  8   Loss :  4.63423680108
Iteration :  9   Loss :  4.34859340598
Iteration :  10   Loss :  4.08055639413
Iteration :  11   Loss :  3.8290405497
Iteration :  12   Loss :  3.59302754701
Iteration :  13   Loss :  3.37156182757
Iteration :  14   Loss :  3.16374673124
Iteration :  15   Loss :  2.96874086591
Iteration :  16   Loss :  2.78575470088
Iteration :  17   Loss :  2.61404737023
Iteration :  18   Loss :  2.45292367332
Iteration :  19   Loss :  2.30173125997
Iteration :  20   Loss :  2.15985798937
Iteration :  21   Loss :  2.0267294516
Iteration :  22   Loss :  1.90180664201
Iteration :  23   Loss :  1.78458377892
Iteration :  24   Loss :  1.67458625585
Iteration :  25   Loss :  1.57136871992
Iteration :  26   Loss :  1.47451326877
Iteration :  27   Loss :  1.38362775854
Iteration :  28   Loss :  1.29834421619
Iteration :  29   Loss :  1.21831734966
Iteration :  30   Loss :  1.1432231499
Iteration :  31   Loss :  1.072757579
Iteration :  32   Loss :  1.00663533921
Iteration :  33   Loss :  0.944588717891
Iteration :  34   Loss :  0.886366503548
Iteration :  35   Loss :  0.831732968784
Iteration :  36   Loss :  0.780466915879
Iteration :  37   Loss :  0.732360781216
Iteration :  38   Loss :  0.687219794908
Iteration :  39   Loss :  0.644861192225
Iteration :  40   Loss :  0.605113473621
Iteration :  41   Loss :  0.567815710376
Iteration :  42   Loss :  0.532816893038
Iteration :  43   Loss :  0.499975320019
Iteration :  44   Loss :  0.46915802388
Iteration :  45   Loss :  0.440240232983
Iteration :  46   Loss :  0.413104866319
Iteration :  47   Loss :  0.387642059472
Iteration :  48   Loss :  0.363748719813
Iteration :  49   Loss :  0.341328109095
Iteration :  50   Loss :  0.320289451791
Iteration :  51   Loss :  0.30054756756
Iteration :  52   Loss :  0.282022526378
Iteration :  53   Loss :  0.264639324918
Iteration :  54   Loss :  0.248327582879
Iteration :  55   Loss :  0.233021258037
Iteration :  56   Loss :  0.218658378855
Iteration :  57   Loss :  0.205180793573
Iteration :  58   Loss :  0.19253393477
Iteration :  59   Loss :  0.180666598429
Iteration :  60   Loss :  0.169530736631
Iteration :  61   Loss :  0.159081263015
Iteration :  62   Loss :  0.149275870236
Iteration :  63   Loss :  0.140074858676
Iteration :  64   Loss :  0.131440975705
Iteration :  65   Loss :  0.123339264859
Iteration :  66   Loss :  0.115736924306
Iteration :  67   Loss :  0.108603174043
Iteration :  68   Loss :  0.101909131273
Iteration :  69   Loss :  0.0956276934665
Iteration :  70   Loss :  0.0897334286285
Iteration :  71   Loss :  0.0842024723336
Iteration :  72   Loss :  0.0790124311023
Iteration :  73   Loss :  0.0741422917366
Iteration :  74   Loss :  0.0695723362421
Iteration :  75   Loss :  0.0652840619949
Iteration :  76   Loss :  0.0612601068294
Iteration :  77   Loss :  0.0574841787424
Iteration :  78   Loss :  0.0539409899316
Iteration :  79   Loss :  0.0506161948983
Iteration :  80   Loss :  0.0474963323668
Iteration :  81   Loss :  0.0445687707824
Iteration :  82   Loss :  0.0418216571695
Iteration :  83   Loss :  0.039243869142
Iteration :  84   Loss :  0.0368249698712
Iteration :  85   Loss :  0.0345551658302
Iteration :  86   Loss :  0.0324252671414
Iteration :  87   Loss :  0.03042665037
Iteration :  88   Loss :  0.0285512236091
Iteration :  89   Loss :  0.0267913937178
Iteration :  90   Loss :  0.0251400355785
Iteration :  91   Loss :  0.0235904632489
Iteration :  92   Loss :  0.0221364028926
Iteration :  93   Loss :  0.0207719673773
Iteration :  94   Loss :  0.0194916324399
Iteration :  95   Loss :  0.0182902143197
Iteration :  96   Loss :  0.0171628487708
Iteration :  97   Loss :  0.0161049713677
Iteration :  98   Loss :  0.0151122990255
Iteration :  99   Loss :  0.0141808126586
[-0.00029124 -0.00026776 -0.00036437 ...,  0.00012043  0.00010874
  0.00012624]
CROSS VALIDATION 9
Iteration :  0   Loss :  160.691016427
Iteration :  1   Loss :  7.93324882885
Iteration :  2   Loss :  7.44426213549
Iteration :  3   Loss :  6.9854154253
Iteration :  4   Loss :  6.55485094103
Iteration :  5   Loss :  6.15082543316
Iteration :  6   Loss :  5.77170310195
Iteration :  7   Loss :  5.41594897451
Iteration :  8   Loss :  5.08212269001
Iteration :  9   Loss :  4.7688726681
Iteration :  10   Loss :  4.47493063661
Iteration :  11   Loss :  4.19910649669
Iteration :  12   Loss :  3.94028350434
Iteration :  13   Loss :  3.69741374905
Iteration :  14   Loss :  3.46951391102
Iteration :  15   Loss :  3.25566127996
Iteration :  16   Loss :  3.05499001925
Iteration :  17   Loss :  2.8666876604
Iteration :  18   Loss :  2.68999181357
Iteration :  19   Loss :  2.52418708079
Iteration :  20   Loss :  2.36860215957
Iteration :  21   Loss :  2.22260712489
Iteration :  22   Loss :  2.08561087882
Iteration :  23   Loss :  1.95705875733
Iteration :  24   Loss :  1.83643028454
Iteration :  25   Loss :  3.23354161657
Iteration :  26   Loss :  3.03423375959
Iteration :  27   Loss :  2.84721076749
Iteration :  28   Loss :  2.67171543027
Iteration :  29   Loss :  2.50703721053
Iteration :  30   Loss :  2.35250936675
Iteration :  31   Loss :  2.20750625375
Iteration :  32   Loss :  2.07144078967
Iteration :  33   Loss :  1.94376207895
Iteration :  34   Loss :  1.82395318197
Iteration :  35   Loss :  1.71152902201
Iteration :  36   Loss :  1.60603442136
Iteration :  37   Loss :  1.50704225837
Iteration :  38   Loss :  1.41415173815
Iteration :  39   Loss :  1.3269867699
Iteration :  40   Loss :  1.24519444411
Iteration :  41   Loss :  1.16844360382
Iteration :  42   Loss :  1.09642350379
Iteration :  43   Loss :  1.02884255238
Iteration :  44   Loss :  0.965427130963
Iteration :  45   Loss :  0.905920486127
Iteration :  46   Loss :  0.850081690128
Iteration :  47   Loss :  0.797684665439
Iteration :  48   Loss :  0.748517269418
Iteration :  49   Loss :  0.702380435392
Iteration :  50   Loss :  0.659087366689
Iteration :  51   Loss :  0.61846278034
Iteration :  52   Loss :  0.580342197404
Iteration :  53   Loss :  0.544571277035
Iteration :  54   Loss :  0.511005191588
Iteration :  55   Loss :  0.479508040255
Iteration :  56   Loss :  0.44995229883
Iteration :  57   Loss :  0.422218303398
Iteration :  58   Loss :  0.396193765846
Iteration :  59   Loss :  0.371773319233
Iteration :  60   Loss :  0.348858091187
Iteration :  61   Loss :  0.327355303598
Iteration :  62   Loss :  0.307177896974
Iteration :  63   Loss :  0.28824417797
Iteration :  64   Loss :  0.270477488622
Iteration :  65   Loss :  0.253805895981
Iteration :  66   Loss :  0.238161900878
Iteration :  67   Loss :  0.22348216463
Iteration :  68   Loss :  0.209707252602
Iteration :  69   Loss :  0.196781393571
Iteration :  70   Loss :  0.184652253916
Iteration :  71   Loss :  0.173270725741
Iteration :  72   Loss :  0.162590728042
Iteration :  73   Loss :  0.15256902014
Iteration :  74   Loss :  0.143165026609
Iteration :  75   Loss :  0.134340672996
Iteration :  76   Loss :  0.126060231667
Iteration :  77   Loss :  0.118290177156
Iteration :  78   Loss :  0.110999050427
Iteration :  79   Loss :  0.104157331504
Iteration :  80   Loss :  0.0977373199524
Iteration :  81   Loss :  0.0917130227278
Iteration :  82   Loss :  0.0860600489348
Iteration :  83   Loss :  0.0807555110756
Iteration :  84   Loss :  0.0757779323832
Iteration :  85   Loss :  0.071107159868
Iteration :  86   Loss :  0.0667242827229
Iteration :  87   Loss :  0.0626115557582
Iteration :  88   Loss :  0.0587523275558
Iteration :  89   Loss :  0.0551309730516
Iteration :  90   Loss :  0.0517328302735
Iteration :  91   Loss :  0.0485441409787
Iteration :  92   Loss :  0.0455519949498
Iteration :  93   Loss :  0.0427442777248
Iteration :  94   Loss :  0.0401096215485
Iteration :  95   Loss :  0.0376373593472
Iteration :  96   Loss :  0.0353174815404
Iteration :  97   Loss :  0.0331405955144
Iteration :  98   Loss :  0.031097887594
Iteration :  99   Loss :  0.0291810873583
[ -2.23999724e-03  -7.91299681e-04  -6.39571688e-04 ...,   1.13831264e-03
   1.16624990e-03   9.08474205e-05]
CROSS VALIDATION 10
Iteration :  0   Loss :  81.5239684209
Iteration :  1   Loss :  7.92204691286
Iteration :  2   Loss :  7.4337506791
Iteration :  3   Loss :  6.97555187023
Iteration :  4   Loss :  6.54559535216
Iteration :  5   Loss :  6.14214033689
Iteration :  6   Loss :  5.76355333448
Iteration :  7   Loss :  5.40830153942
Iteration :  8   Loss :  5.07494662474
Iteration :  9   Loss :  4.76213891852
Iteration :  10   Loss :  4.46861193943
Iteration :  11   Loss :  4.19317726905
Iteration :  12   Loss :  3.93471974027
Iteration :  13   Loss :  3.69219292224
Iteration :  14   Loss :  3.46461488362
Iteration :  15   Loss :  3.25106421702
Iteration :  16   Loss :  3.0506763084
Iteration :  17   Loss :  2.86263983649
Iteration :  18   Loss :  2.68619348794
Iteration :  19   Loss :  2.52062287497
Iteration :  20   Loss :  2.36525764297
Iteration :  21   Loss :  2.21946875638
Iteration :  22   Loss :  2.08266595193
Iteration :  23   Loss :  1.95429534876
Iteration :  24   Loss :  1.83383720594
Iteration :  25   Loss :  1.72080381813
Iteration :  26   Loss :  1.61473754098
Iteration :  27   Loss :  1.51520893828
Iteration :  28   Loss :  1.42181504323
Iteration :  29   Loss :  1.33417772697
Iteration :  30   Loss :  1.2519421676
Iteration :  31   Loss :  1.1747754136
Iteration :  32   Loss :  1.10236503579
Iteration :  33   Loss :  1.0344178624
Iteration :  34   Loss :  0.970658792061
Iteration :  35   Loss :  0.910829680009
Iteration :  36   Loss :  0.854688292911
Iteration :  37   Loss :  0.802007328122
Iteration :  38   Loss :  0.752573493398
Iteration :  39   Loss :  0.70618664332
Iteration :  40   Loss :  0.662658968963
Iteration :  41   Loss :  0.621814237498
Iteration :  42   Loss :  0.58348707867
Iteration :  43   Loss :  0.547522315258
Iteration :  44   Loss :  0.513774334795
Iteration :  45   Loss :  0.482106500024
Iteration :  46   Loss :  0.452390595685
Iteration :  47   Loss :  0.424506309403
Iteration :  48   Loss :  0.398340744572
Iteration :  49   Loss :  0.373787963269
Iteration :  50   Loss :  0.35074855733
Iteration :  51   Loss :  0.329129245879
Iteration :  52   Loss :  0.308842497649
Iteration :  53   Loss :  0.289806176597
Iteration :  54   Loss :  0.271943209347
Iteration :  55   Loss :  0.255181273148
Iteration :  56   Loss :  0.239452503049
Iteration :  57   Loss :  0.224693217136
Iteration :  58   Loss :  0.210843658697
Iteration :  59   Loss :  0.197847754281
Iteration :  60   Loss :  0.185652886675
Iteration :  61   Loss :  0.174209681864
Iteration :  62   Loss :  0.16347180913
Iteration :  63   Loss :  0.153395793474
Iteration :  64   Loss :  0.14394083959
Iteration :  65   Loss :  0.135068666699
Iteration :  66   Loss :  0.126743353561
Iteration :  67   Loss :  0.118931193032
Iteration :  68   Loss :  0.111600555601
Iteration :  69   Loss :  0.104721761321
Iteration :  70   Loss :  0.0982669596509
Iteration :  71   Loss :  0.092210016688
Iteration :  72   Loss :  0.0865264093629
Iteration :  73   Loss :  0.08119312615
Iteration :  74   Loss :  0.0761885738995
Iteration :  75   Loss :  0.0714924904125
Iteration :  76   Loss :  0.067085862404
Iteration :  77   Loss :  0.062950848523
Iteration :  78   Loss :  0.0590707071172
Iteration :  79   Loss :  0.0554297284499
Iteration :  80   Loss :  0.0520131710957
Iteration :  81   Loss :  0.0488072022557
Iteration :  82   Loss :  0.0457988417519
Iteration :  83   Loss :  0.0429759094739
Iteration :  84   Loss :  0.0403269760645
Iteration :  85   Loss :  0.0378413166449
Iteration :  86   Loss :  0.0355088673926
Iteration :  87   Loss :  0.0333201847954
Iteration :  88   Loss :  0.0312664074165
Iteration :  89   Loss :  0.0293392200175
Iteration :  90   Loss :  0.0275308198914
Iteration :  91   Loss :  0.0258338852717
Iteration :  92   Loss :  0.0242415456883
Iteration :  93   Loss :  0.0227473541504
Iteration :  94   Loss :  0.0213452610446
Iteration :  95   Loss :  0.0200295896416
Iteration :  96   Loss :  0.0187950131119
Iteration :  97   Loss :  0.0176365329594
Iteration :  98   Loss :  0.0165494587833
Iteration :  99   Loss :  0.0155293892882
[ -8.12036992e-04   1.14969965e-04  -1.04359880e-04 ...,   7.37183868e-04
   3.04889475e-04   8.29004169e-05]
CROSS VALIDATION 11
Iteration :  0   Loss :  180.257487373
Iteration :  1   Loss :  7.93379370587
Iteration :  2   Loss :  7.44477342757
Iteration :  3   Loss :  6.98589520255
Iteration :  4   Loss :  6.55530114595
Iteration :  5   Loss :  6.15124788851
Iteration :  6   Loss :  5.77209951815
Iteration :  7   Loss :  5.41632095655
Iteration :  8   Loss :  5.08247174396
Iteration :  9   Loss :  4.76920020719
Iteration :  10   Loss :  4.47523798697
Iteration :  11   Loss :  4.19939490269
Iteration :  12   Loss :  3.94055413368
Iteration :  13   Loss :  3.69766769744
Iteration :  14   Loss :  3.46975220663
Iteration :  15   Loss :  3.25588488759
Iteration :  16   Loss :  3.05519984424
Iteration :  17   Loss :  2.86688455227
Iteration :  18   Loss :  2.69017656949
Iteration :  19   Loss :  2.5243604488
Iteration :  20   Loss :  2.36876484158
Iteration :  21   Loss :  2.22275977956
Iteration :  22   Loss :  2.08575412423
Iteration :  23   Loss :  1.95719317343
Iteration :  24   Loss :  1.83655641555
Iteration :  25   Loss :  3.23365687849
Iteration :  26   Loss :  3.03434191704
Iteration :  27   Loss :  2.84731225837
Iteration :  28   Loss :  2.67181066549
Iteration :  29   Loss :  2.50712657568
Iteration :  30   Loss :  2.35259322364
Iteration :  31   Loss :  2.2075849419
Iteration :  32   Loss :  2.07151462766
Iteration :  33   Loss :  1.94383136575
Iteration :  34   Loss :  1.82401819809
Iteration :  35   Loss :  1.7115900307
Iteration :  36   Loss :  1.60609166961
Iteration :  37   Loss :  1.50709597797
Iteration :  38   Loss :  1.41420214661
Iteration :  39   Loss :  1.32703407129
Iteration :  40   Loss :  1.24523882996
Iteration :  41   Loss :  1.16848525384
Iteration :  42   Loss :  1.0964625866
Iteration :  43   Loss :  1.02887922621
Iteration :  44   Loss :  0.965461544308
Iteration :  45   Loss :  0.905952778315
Iteration :  46   Loss :  0.850111991902
Iteration :  47   Loss :  0.797713099484
Iteration :  48   Loss :  0.748543950855
Iteration :  49   Loss :  0.702405472249
Iteration :  50   Loss :  0.659110860333
Iteration :  51   Loss :  0.618484825891
Iteration :  52   Loss :  0.58036288412
Iteration :  53   Loss :  0.54459068867
Iteration :  54   Loss :  0.511023406736
Iteration :  55   Loss :  0.479525132664
Iteration :  56   Loss :  0.449968337703
Iteration :  57   Loss :  0.422233353673
Iteration :  58   Loss :  0.396207888458
Iteration :  59   Loss :  0.37178657136
Iteration :  60   Loss :  0.348870526485
Iteration :  61   Loss :  0.327366972413
Iteration :  62   Loss :  0.307188846552
Iteration :  63   Loss :  0.288254452642
Iteration :  64   Loss :  0.270487129986
Iteration :  65   Loss :  0.253814943075
Iteration :  66   Loss :  0.23817039033
Iteration :  67   Loss :  0.223490130812
Iteration :  68   Loss :  0.209714727768
Iteration :  69   Loss :  0.196788407985
Iteration :  70   Loss :  0.184658835979
Iteration :  71   Loss :  0.173276902101
Iteration :  72   Loss :  0.162596523706
Iteration :  73   Loss :  0.152574458573
Iteration :  74   Loss :  0.143170129829
Iteration :  75   Loss :  0.134345461666
Iteration :  76   Loss :  0.126064725175
Iteration :  77   Loss :  0.118294393695
Iteration :  78   Loss :  0.111003007068
Iteration :  79   Loss :  0.104161044267
Iteration :  80   Loss :  0.0977408038696
Iteration :  81   Loss :  0.0917162919045
Iteration :  82   Loss :  0.0860631166073
Iteration :  83   Loss :  0.0807583896639
Iteration :  84   Loss :  0.0757806335422
Iteration :  85   Loss :  0.0711096945339
Iteration :  86   Loss :  0.066726661158
Iteration :  87   Loss :  0.0626137875922
Iteration :  88   Loss :  0.0587544218248
Iteration :  89   Loss :  0.0551329382349
Iteration :  90   Loss :  0.0517346743275
Iteration :  91   Loss :  0.0485458713696
Iteration :  92   Loss :  0.0455536186835
Iteration :  93   Loss :  0.0427458013754
Iteration :  94   Loss :  0.0401110512849
Iteration :  95   Loss :  0.037638700958
Iteration :  96   Loss :  0.0353187404574
Iteration :  97   Loss :  0.0331417768348
Iteration :  98   Loss :  0.0310989961006
Iteration :  99   Loss :  0.0291821275392
[ -2.24007402e-03  -7.91322695e-04  -6.39549107e-04 ...,   1.13832946e-03
   1.16629474e-03   9.08223278e-05]
CROSS VALIDATION 12
Iteration :  0   Loss :  125.411025264
Iteration :  1   Loss :  43.6979839075
Iteration :  2   Loss :  25.2566580589
Iteration :  3   Loss :  8.17077980657
Iteration :  4   Loss :  7.66715226557
Iteration :  5   Loss :  7.19456713497
Iteration :  6   Loss :  6.75111103402
Iteration :  7   Loss :  6.33498851822
Iteration :  8   Loss :  5.94451481005
Iteration :  9   Loss :  5.57810897767
Iteration :  10   Loss :  5.23428753414
Iteration :  11   Loss :  4.91165843115
Iteration :  12   Loss :  4.60891542296
Iteration :  13   Loss :  4.32483277771
Iteration :  14   Loss :  4.05826031478
Iteration :  15   Loss :  3.80811874795
Iteration :  16   Loss :  3.57339531563
Iteration :  17   Loss :  3.3531396805
Iteration :  18   Loss :  3.14646008175
Iteration :  19   Loss :  2.9525197246
Iteration :  20   Loss :  2.77053339235
Iteration :  21   Loss :  2.59976426716
Iteration :  22   Loss :  2.43952094693
Iteration :  23   Loss :  2.28915464594
Iteration :  24   Loss :  2.14805656808
Iteration :  25   Loss :  2.01565544201
Iteration :  26   Loss :  1.89141520817
Iteration :  27   Loss :  1.77483284849
Iteration :  28   Loss :  1.6654363497
Iteration :  29   Loss :  1.56278279235
Iteration :  30   Loss :  1.46645655747
Iteration :  31   Loss :  1.37606764386
Iteration :  32   Loss :  1.29125008909
Iteration :  33   Loss :  1.21166048779
Iteration :  34   Loss :  1.13697660125
Iteration :  35   Loss :  1.06689605283
Iteration :  36   Loss :  1.00113510365
Iteration :  37   Loss :  0.939427503839
Iteration :  38   Loss :  0.881523414522
Iteration :  39   Loss :  0.82718839631
Iteration :  40   Loss :  0.776202460103
Iteration :  41   Loss :  0.728359176407
Iteration :  42   Loss :  0.683464839555
Iteration :  43   Loss :  0.641337683438
Iteration :  44   Loss :  0.601807145582
Iteration :  45   Loss :  0.564713176577
Iteration :  46   Loss :  0.529905592084
Iteration :  47   Loss :  0.497243464769
Iteration :  48   Loss :  0.466594553726
Iteration :  49   Loss :  0.437834769067
Iteration :  50   Loss :  0.410847669508
Iteration :  51   Loss :  0.385523990934
Iteration :  52   Loss :  0.361761204009
Iteration :  53   Loss :  0.339463099065
Iteration :  54   Loss :  0.318539396568
Iteration :  55   Loss :  0.298905381602
Iteration :  56   Loss :  0.280481560878
Iteration :  57   Loss :  0.263193340886
Iteration :  58   Loss :  0.246970725883
Iteration :  59   Loss :  0.231748034497
Iteration :  60   Loss :  0.217463633802
Iteration :  61   Loss :  0.20405968978
Iteration :  62   Loss :  0.191481933162
Iteration :  63   Loss :  0.179679439713
Iteration :  64   Loss :  0.168604424043
Iteration :  65   Loss :  0.158212046143
Iteration :  66   Loss :  0.148460229837
Iteration :  67   Loss :  0.139309492421
Iteration :  68   Loss :  0.130722784816
Iteration :  69   Loss :  0.122665341557
Iteration :  70   Loss :  0.115104540043
Iteration :  71   Loss :  0.108009768451
Iteration :  72   Loss :  0.101352301798
Iteration :  73   Loss :  0.0951051856427
Iteration :  74   Loss :  0.0892431269512
Iteration :  75   Loss :  0.0837423916919
Iteration :  76   Loss :  0.0785807087432
Iteration :  77   Loss :  0.0737371797225
Iteration :  78   Loss :  0.0691921943743
Iteration :  79   Loss :  0.0649273511728
Iteration :  80   Loss :  0.0609253828186
Iteration :  81   Loss :  0.0571700863279
Iteration :  82   Loss :  0.0536462574306
Iteration :  83   Loss :  0.0503396290116
Iteration :  84   Loss :  0.0472368133472
Iteration :  85   Loss :  0.0443252479013
Iteration :  86   Loss :  0.0415931444628
Iteration :  87   Loss :  0.039029441418
Iteration :  88   Loss :  0.036623758965
Iteration :  89   Loss :  0.0343663570882
Iteration :  90   Loss :  0.0322480961238
Iteration :  91   Loss :  0.0302603997548
Iteration :  92   Loss :  0.0442502543304
Iteration :  93   Loss :  56.8667367035
Iteration :  94   Loss :  90.9099645097
Iteration :  95   Loss :  13.2743009445
Iteration :  96   Loss :  47.7968417885
Iteration :  97   Loss :  4.73406423086
Iteration :  98   Loss :  4.44226770911
Iteration :  99   Loss :  4.16845683478
[-0.00331318 -0.00209452 -0.00761631 ...,  0.00780259 -0.00725185
  0.00066301]
CROSS VALIDATION 13
Iteration :  0   Loss :  180.455760425
Iteration :  1   Loss :  7.93426373171
Iteration :  2   Loss :  7.44521448213
Iteration :  3   Loss :  6.98630907155
Iteration :  4   Loss :  6.55568950504
Iteration :  5   Loss :  6.15161231006
Iteration :  6   Loss :  5.77244147763
Iteration :  7   Loss :  5.41664183845
Iteration :  8   Loss :  5.08277284746
Iteration :  9   Loss :  4.76948275138
Iteration :  10   Loss :  4.47550311581
Iteration :  11   Loss :  4.19964368962
Iteration :  12   Loss :  3.94078758597
Iteration :  13   Loss :  3.69788676028
Iteration :  14   Loss :  3.46995776696
Iteration :  15   Loss :  3.25607777766
Iteration :  16   Loss :  3.05538084502
Iteration :  17   Loss :  2.8670543966
Iteration :  18   Loss :  2.69033594501
Iteration :  19   Loss :  2.52451000079
Iteration :  20   Loss :  2.36890517554
Iteration :  21   Loss :  2.22289146367
Iteration :  22   Loss :  2.08587769164
Iteration :  23   Loss :  1.95730912443
Iteration :  24   Loss :  1.83666521961
Iteration :  25   Loss :  1.7330522043
Iteration :  26   Loss :  3.12641194994
Iteration :  27   Loss :  2.93370731222
Iteration :  28   Loss :  2.75288053257
Iteration :  29   Loss :  2.58319948791
Iteration :  30   Loss :  2.42397718149
Iteration :  31   Loss :  2.27456896144
Iteration :  32   Loss :  2.13436991067
Iteration :  33   Loss :  2.00281239778
Iteration :  34   Loss :  1.87936377882
Iteration :  35   Loss :  1.76352424074
Iteration :  36   Loss :  1.65482477779
Iteration :  37   Loss :  1.55282529263
Iteration :  38   Loss :  1.45711281447
Iteration :  39   Loss :  1.36729982708
Iteration :  40   Loss :  1.2830226998
Iteration :  41   Loss :  1.20394021531
Iteration :  42   Loss :  1.12973218811
Iteration :  43   Loss :  1.06009816818
Iteration :  44   Loss :  0.994756224534
Iteration :  45   Loss :  0.933441803742
Iteration :  46   Loss :  0.875906658821
Iteration :  47   Loss :  0.821917844146
Iteration :  48   Loss :  0.771256772308
Iteration :  49   Loss :  0.723718329109
Iteration :  50   Loss :  0.679110043108
Iteration :  51   Loss :  0.637251306344
Iteration :  52   Loss :  0.597972643106
Iteration :  53   Loss :  0.561115023764
Iteration :  54   Loss :  0.5265292209
Iteration :  55   Loss :  0.494075205118
Iteration :  56   Loss :  0.463621578107
Iteration :  57   Loss :  0.435045040632
Iteration :  58   Loss :  0.408229893336
Iteration :  59   Loss :  0.383067568293
Iteration :  60   Loss :  0.35945618945
Iteration :  61   Loss :  0.337300160151
Iteration :  62   Loss :  0.316509776092
Iteration :  63   Loss :  0.297000862132
Iteration :  64   Loss :  0.278694431484
Iteration :  65   Loss :  0.26151636592
Iteration :  66   Loss :  0.245397115687
Iteration :  67   Loss :  0.230271417912
Iteration :  68   Loss :  0.216078032371
Iteration :  69   Loss :  0.202759493543
Iteration :  70   Loss :  0.190261877946
Iteration :  71   Loss :  0.178534585815
Iteration :  72   Loss :  0.167530136231
Iteration :  73   Loss :  0.157203974892
Iteration :  74   Loss :  0.147514293712
Iteration :  75   Loss :  0.138421861562
Iteration :  76   Loss :  0.129889865424
Iteration :  77   Loss :  0.121883761346
Iteration :  78   Loss :  0.114371134587
Iteration :  79   Loss :  0.107321568373
Iteration :  80   Loss :  0.100706520745
Iteration :  81   Loss :  0.0944992090063
Iteration :  82   Loss :  0.0886745012814
Iteration :  83   Loss :  0.0832088147634
Iteration :  84   Loss :  0.0780800202345
Iteration :  85   Loss :  0.0732673524692
Iteration :  86   Loss :  0.0687513261616
Iteration :  87   Loss :  0.0645136570339
Iteration :  88   Loss :  0.060537187808
Iteration :  89   Loss :  0.0568058187396
Iteration :  90   Loss :  0.0533044424348
Iteration :  91   Loss :  0.0500188826836
Iteration :  92   Loss :  0.0469358370642
Iteration :  93   Loss :  0.0440428230845
Iteration :  94   Loss :  0.0413281276437
Iteration :  95   Loss :  0.0387807596088
Iteration :  96   Loss :  0.0363904053143
Iteration :  97   Loss :  0.0341473868046
Iteration :  98   Loss :  0.0320426226505
Iteration :  99   Loss :  0.0300675911805
[ -2.27605327e-03  -8.15482719e-04  -6.57212556e-04 ...,   1.15227437e-03
   1.19345185e-03   9.38523307e-05]
CROSS VALIDATION 14
Iteration :  0   Loss :  180.455760425
Iteration :  1   Loss :  7.93426373171
Iteration :  2   Loss :  7.44521448213
Iteration :  3   Loss :  6.98630907155
Iteration :  4   Loss :  6.55568950504
Iteration :  5   Loss :  6.15161231006
Iteration :  6   Loss :  5.77244147763
Iteration :  7   Loss :  5.41664183845
Iteration :  8   Loss :  5.08277284746
Iteration :  9   Loss :  4.76948275138
Iteration :  10   Loss :  4.47550311581
Iteration :  11   Loss :  4.19964368962
Iteration :  12   Loss :  3.94078758597
Iteration :  13   Loss :  3.69788676028
Iteration :  14   Loss :  3.46995776696
Iteration :  15   Loss :  3.25607777766
Iteration :  16   Loss :  3.05538084502
Iteration :  17   Loss :  2.8670543966
Iteration :  18   Loss :  2.69033594501
Iteration :  19   Loss :  2.52451000079
Iteration :  20   Loss :  2.36890517554
Iteration :  21   Loss :  2.22289146367
Iteration :  22   Loss :  2.08587769164
Iteration :  23   Loss :  1.95730912443
Iteration :  24   Loss :  1.83666521961
Iteration :  25   Loss :  1.72345751972
Iteration :  26   Loss :  1.61722767468
Iteration :  27   Loss :  1.51754558603
Iteration :  28   Loss :  1.42400766554
Iteration :  29   Loss :  1.33623520123
Iteration :  30   Loss :  1.25387282401
Iteration :  31   Loss :  1.17658706892
Iteration :  32   Loss :  1.10406502496
Iteration :  33   Loss :  1.03601306826
Iteration :  34   Loss :  0.972155673202
Iteration :  35   Loss :  0.912234296937
Iteration :  36   Loss :  0.856006332574
Iteration :  37   Loss :  0.803244126938
Iteration :  38   Loss :  0.75373405886
Iteration :  39   Loss :  0.707275674273
Iteration :  40   Loss :  0.663680874623
Iteration :  41   Loss :  0.62277315531
Iteration :  42   Loss :  0.584386891057
Iteration :  43   Loss :  0.548366665337
Iteration :  44   Loss :  0.514566641133
Iteration :  45   Loss :  0.482849970473
Iteration :  46   Loss :  0.453088240374
Iteration :  47   Loss :  0.425160952923
Iteration :  48   Loss :  0.398955037414
Iteration :  49   Loss :  0.374364392553
Iteration :  50   Loss :  0.351289456877
Iteration :  51   Loss :  0.329636805657
Iteration :  52   Loss :  0.309318772643
Iteration :  53   Loss :  0.290253095127
Iteration :  54   Loss :  0.272362580876
Iteration :  55   Loss :  0.255574795608
Iteration :  56   Loss :  0.239821769715
Iteration :  57   Loss :  0.225039723078
Iteration :  58   Loss :  0.211168806832
Iteration :  59   Loss :  0.198152861055
Iteration :  60   Loss :  0.185939187389
Iteration :  61   Loss :  0.174478335678
Iteration :  62   Loss :  0.163723903759
Iteration :  63   Loss :  0.153632349586
Iteration :  64   Loss :  0.144162814943
Iteration :  65   Loss :  0.135276960017
Iteration :  66   Loss :  0.126938808171
Iteration :  67   Loss :  0.119114600283
Iteration :  68   Loss :  0.111772658062
Iteration :  69   Loss :  0.104883255794
Iteration :  70   Loss :  0.0984184999861
Iteration :  71   Loss :  0.0923522164354
Iteration :  72   Loss :  0.0866598442542
Iteration :  73   Loss :  0.0866138463573
[ -2.56642633e-03  -9.54678760e-05  -2.12659259e-04 ...,   1.61631296e-03
   7.44833208e-04  -4.85392541e-06]
CROSS VALIDATION 15
Iteration :  0   Loss :  180.455760425
Iteration :  1   Loss :  7.93426373171
Iteration :  2   Loss :  7.44521448213
Iteration :  3   Loss :  6.98630907155
Iteration :  4   Loss :  6.55568950504
Iteration :  5   Loss :  6.15161231006
Iteration :  6   Loss :  5.77244147763
Iteration :  7   Loss :  5.41664183845
Iteration :  8   Loss :  5.08277284746
Iteration :  9   Loss :  4.76948275138
Iteration :  10   Loss :  4.47550311581
Iteration :  11   Loss :  4.19964368962
Iteration :  12   Loss :  3.94078758597
Iteration :  13   Loss :  3.69788676028
Iteration :  14   Loss :  3.46995776696
Iteration :  15   Loss :  3.25607777766
Iteration :  16   Loss :  3.05538084502
Iteration :  17   Loss :  2.8670543966
Iteration :  18   Loss :  2.69033594501
Iteration :  19   Loss :  2.52451000079
Iteration :  20   Loss :  2.36890517554
Iteration :  21   Loss :  2.22289146367
Iteration :  22   Loss :  2.08587769164
Iteration :  23   Loss :  1.95730912443
Iteration :  24   Loss :  1.83666521961
Iteration :  25   Loss :  1.7330522043
Iteration :  26   Loss :  3.1269498548
Iteration :  27   Loss :  2.9342120619
Iteration :  28   Loss :  2.75335417067
Iteration :  29   Loss :  2.58364393208
Iteration :  30   Loss :  2.42439423117
Iteration :  31   Loss :  2.27496030516
Iteration :  32   Loss :  2.1347371329
Iteration :  33   Loss :  2.0031569853
Iteration :  34   Loss :  1.87968712677
Iteration :  35   Loss :  1.76382765829
Iteration :  36   Loss :  1.6551094934
Iteration :  37   Loss :  1.55309245904
Iteration :  38   Loss :  1.45736351338
Iteration :  39   Loss :  1.36753507351
Iteration :  40   Loss :  1.28324344619
Iteration :  41   Loss :  1.20414735541
Iteration :  42   Loss :  1.12992656059
Iteration :  43   Loss :  1.06028056
Iteration :  44   Loss :  0.994927374151
Iteration :  45   Loss :  0.933602404101
Iteration :  46   Loss :  0.876057360154
Iteration :  47   Loss :  0.822059256605
Iteration :  48   Loss :  0.771389468438
Iteration :  49   Loss :  0.723842846164
Iteration :  50   Loss :  0.679226885226
Iteration :  51   Loss :  0.63736094659
Iteration :  52   Loss :  0.598075525386
Iteration :  53   Loss :  0.561211564624
Iteration :  54   Loss :  0.52661981121
Iteration :  55   Loss :  0.494160211656
Iteration :  56   Loss :  0.463701345042
Iteration :  57   Loss :  0.435119890923
Iteration :  58   Loss :  0.408300130031
Iteration :  59   Loss :  0.383133475765
Iteration :  60   Loss :  0.359518034541
Iteration :  61   Loss :  0.337358193257
Iteration :  62   Loss :  0.316564232175
Iteration :  63   Loss :  0.29705196167
Iteration :  64   Loss :  0.278742381367
Iteration :  65   Loss :  0.261561360287
Iteration :  66   Loss :  0.245439336707
Iteration :  67   Loss :  0.230311036528
Iteration :  68   Loss :  0.216115208989
Iteration :  69   Loss :  0.202794378683
Iteration :  70   Loss :  0.190294612848
Iteration :  71   Loss :  0.178565303015
Iteration :  72   Loss :  0.167558960096
Iteration :  73   Loss :  0.157231022121
Iteration :  74   Loss :  0.147539673815
Iteration :  75   Loss :  0.138445677295
Iteration :  76   Loss :  0.129912213211
Iteration :  77   Loss :  0.121904731669
Iteration :  78   Loss :  0.114390812349
Iteration :  79   Loss :  0.107340033243
Iteration :  80   Loss :  0.100723847485
Iteration :  81   Loss :  0.0945154677671
Iteration :  82   Loss :  0.0886897578905
Iteration :  83   Loss :  0.0832231309913
Iteration :  84   Loss :  0.0780934540439
Iteration :  85   Loss :  0.0732799582504
Iteration :  86   Loss :  0.0687631549523
Iteration :  87   Loss :  0.0645247567259
Iteration :  88   Loss :  0.0605476033412
Iteration :  89   Loss :  0.056815592284
Iteration :  90   Loss :  0.053313613561
Iteration :  91   Loss :  0.0500274885233
Iteration :  92   Loss :  0.0469439124603
Iteration :  93   Loss :  0.0440504007323
Iteration :  94   Loss :  0.0413352382232
Iteration :  95   Loss :  0.038787431909
Iteration :  96   Loss :  0.0363966663497
Iteration :  97   Loss :  0.0341532619246
Iteration :  98   Loss :  0.032048135642
Iteration :  99   Loss :  0.0300727643642
[ -2.27641841e-03  -8.15659567e-04  -6.57372817e-04 ...,   1.15252169e-03
   1.19369885e-03   9.39098446e-05]
CROSS VALIDATION 16
Iteration :  0   Loss :  180.455760425
Iteration :  1   Loss :  7.25031851024
Iteration :  2   Loss :  6.80342602639
Iteration :  3   Loss :  6.3840789382
Iteration :  4   Loss :  5.99057941266
Iteration :  5   Loss :  5.62133426713
Iteration :  6   Loss :  5.274848519
Iteration :  7   Loss :  4.94971933284
Iteration :  8   Loss :  4.64463034069
Iteration :  9   Loss :  4.35834631239
Iteration :  10   Loss :  4.08970815446
Iteration :  11   Loss :  3.83762821717
Iteration :  12   Loss :  3.60108589097
Iteration :  13   Loss :  3.37912347427
Iteration :  14   Loss :  3.17084229592
Iteration :  15   Loss :  2.97539907677
Iteration :  16   Loss :  2.79200251536
Iteration :  17   Loss :  2.61991008421
Iteration :  18   Loss :  2.45842502347
Iteration :  19   Loss :  2.30689351992
Iteration :  20   Loss :  2.16470205983
Iteration :  21   Loss :  2.03127494501
Iteration :  22   Loss :  1.90607196196
Iteration :  23   Loss :  1.78858619464
Iteration :  24   Loss :  1.67834197213
Iteration :  25   Loss :  1.57489294273
Iteration :  26   Loss :  1.47782026682
Iteration :  27   Loss :  1.38673092105
Iteration :  28   Loss :  1.3012561071
Iteration :  29   Loss :  1.22104975851
Iteration :  30   Loss :  1.14578713953
Iteration :  31   Loss :  1.07516353037
Iteration :  32   Loss :  1.00889299343
Iteration :  33   Loss :  0.946707215644
Iteration :  34   Loss :  0.88835442211
Iteration :  35   Loss :  0.833598356748
Iteration :  36   Loss :  0.782217325741
Iteration :  37   Loss :  0.734003299955
Iteration :  38   Loss :  0.688761072677
Iteration :  39   Loss :  0.646307469278
Iteration :  40   Loss :  0.606470605577
Iteration :  41   Loss :  0.569089191929
Iteration :  42   Loss :  0.5340118802
Iteration :  43   Loss :  0.501096650998
Iteration :  44   Loss :  0.47021023867
Iteration :  45   Loss :  0.441227591743
Iteration :  46   Loss :  0.414031366619
Iteration :  47   Loss :  0.388511452486
Iteration :  48   Loss :  0.364564525497
Iteration :  49   Loss :  0.342093630447
Iteration :  50   Loss :  0.321007788217
Iteration :  51   Loss :  0.301221627428
Iteration :  52   Loss :  0.282655038791
Iteration :  53   Loss :  0.265232850761
Iteration :  54   Loss :  0.248884525193
Iteration :  55   Loss :  0.233543871745
Iteration :  56   Loss :  0.21914877989
Iteration :  57   Loss :  0.205640967448
Iteration :  58   Loss :  0.192965744615
Iteration :  59   Loss :  0.181071792537
Iteration :  60   Loss :  0.169910955533
Iteration :  61   Loss :  0.159438046123
Iteration :  62   Loss :  0.149610662077
Iteration :  63   Loss :  0.140389014739
Iteration :  64   Loss :  0.131735767932
Iteration :  65   Loss :  0.12361588679
Iteration :  66   Loss :  0.115996495916
Iteration :  67   Loss :  0.108846746273
Iteration :  68   Loss :  0.102137690287
Iteration :  69   Loss :  0.0958421646431
Iteration :  70   Loss :  0.0899346803092
Iteration :  71   Loss :  0.0843913193366
Iteration :  72   Loss :  0.0791896380228
Iteration :  73   Loss :  0.0743085760416
Iteration :  74   Loss :  0.0697283711758
Iteration :  75   Loss :  626.59819964
Iteration :  76   Loss :  50.3613960538
Iteration :  77   Loss :  46.3203830846
Iteration :  78   Loss :  8.7877429208
Iteration :  79   Loss :  5.89094468177
Iteration :  80   Loss :  5.52784078539
Iteration :  81   Loss :  5.18711775433
Iteration :  82   Loss :  4.867396085
Iteration :  83   Loss :  4.56738130313
Iteration :  84   Loss :  4.28585872279
Iteration :  85   Loss :  4.02168852842
Iteration :  86   Loss :  3.77380115999
Iteration :  87   Loss :  3.54119298263
Iteration :  88   Loss :  3.32292222313
Iteration :  89   Loss :  3.11810515698
Iteration :  90   Loss :  2.92591253033
Iteration :  91   Loss :  2.74556620259
Iteration :  92   Loss :  2.57633599593
Iteration :  93   Loss :  2.41753673893
Iteration :  94   Loss :  2.26852549252
Iteration :  95   Loss :  2.12869894688
Iteration :  96   Loss :  1.99749097879
Iteration :  97   Loss :  1.8743703595
Iteration :  98   Loss :  1.75883860398
Iteration :  99   Loss :  1.65042795259
[-0.00272775 -0.00383705 -0.0042776  ...,  0.0058749   0.00103704
  0.00296929]
CROSS VALIDATION 17
Iteration :  0   Loss :  180.455760425
Iteration :  1   Loss :  7.93480721202
Iteration :  2   Loss :  7.4457244636
Iteration :  3   Loss :  6.98678761897
Iteration :  4   Loss :  6.55613855593
Iteration :  5   Loss :  6.15203368252
Iteration :  6   Loss :  5.77283687768
Iteration :  7   Loss :  5.41701286698
Iteration :  8   Loss :  5.08312100667
Iteration :  9   Loss :  4.76980945088
Iteration :  10   Loss :  4.47580967832
Iteration :  11   Loss :  4.19993135634
Iteration :  12   Loss :  3.9410575216
Iteration :  13   Loss :  3.69814005771
Iteration :  14   Loss :  3.47019545173
Iteration :  15   Loss :  3.25630081211
Iteration :  16   Loss :  3.05559013215
Iteration :  17   Loss :  2.86725078377
Iteration :  18   Loss :  2.69052022734
Iteration :  19   Loss :  2.52468292439
Iteration :  20   Loss :  2.36906744054
Iteration :  21   Loss :  2.22304372704
Iteration :  22   Loss :  2.08602056985
Iteration :  23   Loss :  1.95744319597
Iteration :  24   Loss :  1.8367910273
Iteration :  25   Loss :  1.74171401442
Iteration :  26   Loss :  3.12705463293
Iteration :  27   Loss :  2.93431038175
Iteration :  28   Loss :  2.75344643032
Iteration :  29   Loss :  2.58373050506
Iteration :  30   Loss :  2.424475468
Iteration :  31   Loss :  2.27503653475
Iteration :  32   Loss :  2.13480866387
Iteration :  33   Loss :  2.00322410727
Iteration :  34   Loss :  1.87975011151
Iteration :  35   Loss :  1.76388676079
Iteration :  36   Loss :  1.65516495296
Iteration :  37   Loss :  1.55314450021
Iteration :  38   Loss :  1.45741234685
Iteration :  39   Loss :  1.367580897
Iteration :  40   Loss :  1.28328644524
Iteration :  41   Loss :  1.2041877041
Iteration :  42   Loss :  1.12996442227
Iteration :  43   Loss :  1.06031608798
Iteration :  44   Loss :  0.994960712271
Iteration :  45   Loss :  0.933633687339
Iteration :  46   Loss :  0.876086715167
Iteration :  47   Loss :  0.822086802244
Iteration :  48   Loss :  0.771415316229
Iteration :  49   Loss :  0.723867100758
Iteration :  50   Loss :  0.679249644824
Iteration :  51   Loss :  0.63738230334
Iteration :  52   Loss :  0.598095565757
Iteration :  53   Loss :  0.561230369754
Iteration :  54   Loss :  0.526637457236
Iteration :  55   Loss :  0.494176770023
Iteration :  56   Loss :  0.46371688279
Iteration :  57   Loss :  0.435134470961
Iteration :  58   Loss :  0.408313811391
Iteration :  59   Loss :  0.383146313838
Iteration :  60   Loss :  0.359530081305
Iteration :  61   Loss :  0.337369497487
Iteration :  62   Loss :  0.316574839639
Iteration :  63   Loss :  0.297061915315
Iteration :  64   Loss :  0.278751721493
Iteration :  65   Loss :  0.261570124709
Iteration :  66   Loss :  0.245447560912
Iteration :  67   Loss :  0.230318753812
Iteration :  68   Loss :  0.216122450598
Iteration :  69   Loss :  0.202801173936
Iteration :  70   Loss :  0.190300989258
Iteration :  71   Loss :  0.178571286398
Iteration :  72   Loss :  0.167564574677
Iteration :  73   Loss :  0.157236290633
Iteration :  74   Loss :  0.147544617588
Iteration :  75   Loss :  0.138450316346
Iteration :  76   Loss :  0.129916566322
Iteration :  77   Loss :  0.121908816464
Iteration :  78   Loss :  0.114394645367
Iteration :  79   Loss :  0.107343630003
Iteration :  80   Loss :  0.100727222549
Iteration :  81   Loss :  0.0945186348003
Iteration :  82   Loss :  0.0886927297153
Iteration :  83   Loss :  0.0832259196398
Iteration :  84   Loss :  0.0780960708067
Iteration :  85   Loss :  0.0732824137222
Iteration :  86   Loss :  0.0687654590746
Iteration :  87   Loss :  0.0645269188276
Iteration :  88   Loss :  0.060549632176
Iteration :  89   Loss :  0.0568174960662
Iteration :  90   Loss :  0.0533153999986
Iteration :  91   Loss :  0.0500291648491
Iteration :  92   Loss :  0.0469454854614
Iteration :  93   Loss :  0.0440518767773
Iteration :  94   Loss :  0.0413366232882
Iteration :  95   Loss :  0.0387887316019
Iteration :  96   Loss :  0.0363978859326
Iteration :  97   Loss :  0.0341544063353
Iteration :  98   Loss :  0.0320492095139
Iteration :  99   Loss :  0.0300737720452
[ -2.27652630e-03  -8.15665860e-04  -6.57407141e-04 ...,   1.15252555e-03
   1.19369774e-03   9.38838474e-05]
CROSS VALIDATION 18
Iteration :  0   Loss :  18.6098235245
Iteration :  1   Loss :  12.2942383157
Iteration :  2   Loss :  8.16021955443
Iteration :  3   Loss :  7.65724292239
Iteration :  4   Loss :  7.18526858026
Iteration :  5   Loss :  6.74238562023
Iteration :  6   Loss :  6.3268009183
Iteration :  7   Loss :  5.93683187443
Iteration :  8   Loss :  5.57089960002
Iteration :  9   Loss :  5.22752252548
Iteration :  10   Loss :  4.90531040162
Iteration :  11   Loss :  4.60295867095
Iteration :  12   Loss :  4.31924318581
Iteration :  13   Loss :  4.05301525211
Iteration :  14   Loss :  3.80319697853
Iteration :  15   Loss :  3.56877691244
Iteration :  16   Loss :  3.34880594476
Iteration :  17   Loss :  3.14239346722
Iteration :  18   Loss :  2.94870376658
Iteration :  19   Loss :  2.76695264095
Iteration :  20   Loss :  2.59640422481
Iteration :  21   Loss :  2.43636800965
Iteration :  22   Loss :  2.28619604825
Iteration :  23   Loss :  2.14528033136
Iteration :  24   Loss :  2.01305032594
Iteration :  25   Loss :  1.8889706653
Iteration :  26   Loss :  1.77253898146
Iteration :  27   Loss :  1.66328387121
Iteration :  28   Loss :  1.56076298754
Iteration :  29   Loss :  1.46456124865
Iteration :  30   Loss :  1.3742891574
Iteration :  31   Loss :  1.28958122433
Iteration :  32   Loss :  1.2100944879
Iteration :  33   Loss :  1.13550712589
Iteration :  34   Loss :  1.06551715245
Iteration :  35   Loss :  0.999841195435
Iteration :  36   Loss :  0.938213349064
Iteration :  37   Loss :  0.880384097376
Iteration :  38   Loss :  0.826119303978
Iteration :  39   Loss :  0.775199264094
Iteration :  40   Loss :  0.727417815027
Iteration :  41   Loss :  0.682581501464
Iteration :  42   Loss :  0.641344191852
Iteration :  43   Loss :  13.0492058719
Iteration :  44   Loss :  2.81799737137
Iteration :  45   Loss :  2.64430267879
Iteration :  46   Loss :  2.48131411621
Iteration :  47   Loss :  2.32837178312
Iteration :  48   Loss :  2.18485645369
Iteration :  49   Loss :  2.0501870697
Iteration :  50   Loss :  1.92381838802
Iteration :  51   Loss :  1.80523877297
Iteration :  52   Loss :  1.69396812492
Iteration :  53   Loss :  1.58955593643
Iteration :  54   Loss :  1.49157946827
Iteration :  55   Loss :  1.39964203786
Iteration :  56   Loss :  1.31337141321
Iteration :  57   Loss :  1.23241830581
Iteration :  58   Loss :  1.15645495647
Iteration :  59   Loss :  1.0851738083
Iteration :  60   Loss :  1.01828626151
Iteration :  61   Loss :  0.955521504894
Iteration :  62   Loss :  0.896625419419
Iteration :  63   Loss :  0.841359549347
Iteration :  64   Loss :  0.789500136785
Iteration :  65   Loss :  0.740837215752
Iteration :  66   Loss :  0.695173762068
Iteration :  67   Loss :  0.652324895663
Iteration :  68   Loss :  0.612117132033
Iteration :  69   Loss :  0.57438767985
Iteration :  70   Loss :  0.538983781858
Iteration :  71   Loss :  0.505762096398
Iteration :  72   Loss :  0.47458811705
Iteration :  73   Loss :  0.445335628055
Iteration :  74   Loss :  0.41788619329
Iteration :  75   Loss :  0.392128676758
Iteration :  76   Loss :  0.367958792621
Iteration :  77   Loss :  0.345278682974
Iteration :  78   Loss :  0.32399652164
Iteration :  79   Loss :  0.304026142392
Iteration :  80   Loss :  0.285286690085
Iteration :  81   Loss :  0.267702293295
Iteration :  82   Loss :  0.25120175713
Iteration :  83   Loss :  0.235718274986
Iteration :  84   Loss :  0.221189158058
Iteration :  85   Loss :  0.207555581532
Iteration :  86   Loss :  0.194762346416
Iteration :  87   Loss :  0.182757656052
Iteration :  88   Loss :  0.171492906408
Iteration :  89   Loss :  0.160922489288
Iteration :  90   Loss :  0.151003607677
Iteration :  91   Loss :  0.141696102467
Iteration :  92   Loss :  0.132962289864
Iteration :  93   Loss :  0.124766808811
Iteration :  94   Loss :  0.117076477826
Iteration :  95   Loss :  0.109860160653
Iteration :  96   Loss :  0.103088640201
Iteration :  97   Loss :  0.0967345002539
Iteration :  98   Loss :  0.0907720144635
Iteration :  99   Loss :  0.0851770421942
[-0.00277112 -0.00127236 -0.00200556 ...,  0.00192233  0.00125694
  0.00037468]
CROSS VALIDATION 19
Iteration :  0   Loss :  202.234461283
Iteration :  1   Loss :  60.1041041191
Iteration :  2   Loss :  7.2902942525
Iteration :  3   Loss :  6.84093775844
Iteration :  4   Loss :  6.41927853582
Iteration :  5   Loss :  6.02360939034
Iteration :  6   Loss :  5.65232835511
Iteration :  7   Loss :  5.3039322047
Iteration :  8   Loss :  4.97701036895
Iteration :  9   Loss :  4.67023922189
Iteration :  10   Loss :  4.38237672272
Iteration :  11   Loss :  4.11225738713
Iteration :  12   Loss :  3.85878756848
Iteration :  13   Loss :  3.62094102991
Iteration :  14   Loss :  3.39775478941
Iteration :  15   Loss :  3.18832522087
Iteration :  16   Loss :  2.99180439557
Iteration :  17   Loss :  2.80739664912
Iteration :  18   Loss :  2.63435535998
Iteration :  19   Loss :  2.47197992661
Iteration :  20   Loss :  2.31961293088
Iteration :  21   Loss :  2.17663747638
Iteration :  22   Loss :  2.04247469072
Iteration :  23   Loss :  1.91658138183
Iteration :  24   Loss :  1.79844783873
Iteration :  25   Loss :  1.6875957678
Iteration :  26   Loss :  1.58357635633
Iteration :  27   Loss :  1.48596845535
Iteration :  28   Loss :  1.39437687451
Iteration :  29   Loss :  1.30843078207
Iteration :  30   Loss :  1.22778220348
Iteration :  31   Loss :  1.15210461252
Iteration :  32   Loss :  1.08109160927
Iteration :  33   Loss :  1.01445567957
Iteration :  34   Loss :  0.951927030965
Iteration :  35   Loss :  0.893252500361
Iteration :  36   Loss :  0.838194529041
Iteration :  37   Loss :  0.786530200846
Iteration :  38   Loss :  0.738050339638
Iteration :  39   Loss :  0.692558662407
Iteration :  40   Loss :  0.649870984559
Iteration :  41   Loss :  0.609814474205
Iteration :  42   Loss :  0.572226952404
Iteration :  43   Loss :  0.536956236541
Iteration :  44   Loss :  0.503859524179
Iteration :  45   Loss :  0.472802814883
Iteration :  46   Loss :  0.44366036769
Iteration :  47   Loss :  0.416314192012
Iteration :  48   Loss :  0.39065356992
Iteration :  49   Loss :  0.36657460788
Iteration :  50   Loss :  0.343979816106
Iteration :  51   Loss :  0.322777713854
Iteration :  52   Loss :  0.302882459035
Iteration :  53   Loss :  0.284213500665
Iteration :  54   Loss :  0.266695252732
Iteration :  55   Loss :  0.250256788166
Iteration :  56   Loss :  0.234831551673
Iteration :  57   Loss :  0.220357090273
Iteration :  58   Loss :  0.206774800437
Iteration :  59   Loss :  0.194029690821
Iteration :  60   Loss :  0.182070159616
Iteration :  61   Loss :  0.170847785626
Iteration :  62   Loss :  0.160317132225
Iteration :  63   Loss :  0.150435563391
Iteration :  64   Loss :  0.141163071088
Iteration :  65   Loss :  0.132462113277
Iteration :  66   Loss :  0.124297461927
Iteration :  67   Loss :  0.116636060373
Iteration :  68   Loss :  0.10944688949
Iteration :  69   Loss :  0.102700842096
Iteration :  70   Loss :  0.0963706051062
Iteration :  71   Loss :  0.090430548952
Iteration :  72   Loss :  0.0848566238092
Iteration :  73   Loss :  0.0796262622282
Iteration :  74   Loss :  0.0747182877638
Iteration :  75   Loss :  0.0701128292365
Iteration :  76   Loss :  0.0657912402796
Iteration :  77   Loss :  0.0617360238442
Iteration :  78   Loss :  0.0579307613582
Iteration :  79   Loss :  0.0543600462512
Iteration :  80   Loss :  0.0510094215776
Iteration :  81   Loss :  0.0478653214837
Iteration :  82   Loss :  0.0449150162829
Iteration :  83   Loss :  0.0421465609163
Iteration :  84   Loss :  0.0395487465903
Iteration :  85   Loss :  0.0371110553948
Iteration :  86   Loss :  0.0348236177187
Iteration :  87   Loss :  0.0326771722905
Iteration :  88   Loss :  0.0306630286816
Iteration :  89   Loss :  0.0287730321206
Iteration :  90   Loss :  0.0269995304773
Iteration :  91   Loss :  0.0253353432804
Iteration :  92   Loss :  0.0237737326461
Iteration :  93   Loss :  0.0223083759976
Iteration :  94   Loss :  0.0209333404669
Iteration :  95   Loss :  0.019643058874
Iteration :  96   Loss :  0.0184323071865
Iteration :  97   Loss :  0.0172961833693
Iteration :  98   Loss :  0.016230087537
Iteration :  99   Loss :  0.0152297033301
[ -8.69212989e-04  -2.60428215e-04  -6.66787620e-04 ...,   4.71330713e-04
   1.52171106e-04   8.46404320e-05]
Accuracy (Hinge Loss):	0.8
lmda : 0.2  eta : 0.001
Logistic Loss
CROSS VALIDATION 0
Iteration :  0   Loss :  127.394656355
Iteration :  1   Loss :  46.1453711233
Iteration :  2   Loss :  5.13318541871
Iteration :  3   Loss :  4.92004624222
Iteration :  4   Loss :  4.7157570033
Iteration :  5   Loss :  4.51995024021
Iteration :  6   Loss :  4.3322737501
Iteration :  7   Loss :  4.15238995569
Iteration :  8   Loss :  3.97997529844
Iteration :  9   Loss :  3.81471965684
Iteration :  10   Loss :  3.65632578913
Iteration :  11   Loss :  3.50450879905
Iteration :  12   Loss :  3.35899562399
Iteration :  13   Loss :  3.21952454444
Iteration :  14   Loss :  3.08584471379
Iteration :  15   Loss :  2.95771570773
Iteration :  16   Loss :  2.83490709218
Iteration :  17   Loss :  2.71719800899
Iteration :  18   Loss :  2.60437677844
Iteration :  19   Loss :  2.49624051748
Iteration :  20   Loss :  2.39259477302
Iteration :  21   Loss :  2.29325316907
Iteration :  22   Loss :  2.19803706695
Iteration :  23   Loss :  2.10677523796
Iteration :  24   Loss :  2.01930354795
Iteration :  25   Loss :  1.9354646541
Iteration :  26   Loss :  1.85510771426
Iteration :  27   Loss :  1.77808811023
Iteration :  28   Loss :  1.70426718626
Iteration :  29   Loss :  1.63351200375
Iteration :  30   Loss :  1.56569511226
Iteration :  31   Loss :  1.50069433535
Iteration :  32   Loss :  1.43839256831
Iteration :  33   Loss :  1.37867758402
Iteration :  34   Loss :  1.32144184349
Iteration :  35   Loss :  1.26658230862
Iteration :  36   Loss :  1.2140002569
Iteration :  37   Loss :  1.16360109878
Iteration :  38   Loss :  1.11529419953
Iteration :  39   Loss :  1.0689927073
Iteration :  40   Loss :  1.02461338872
Iteration :  41   Loss :  0.982076472415
Iteration :  42   Loss :  0.941305500728
Iteration :  43   Loss :  0.902227189029
Iteration :  44   Loss :  0.86477129216
Iteration :  45   Loss :  0.828870477367
Iteration :  46   Loss :  0.794460203146
Iteration :  47   Loss :  0.761478603529
Iteration :  48   Loss :  0.729866377395
Iteration :  49   Loss :  0.699566682478
Iteration :  50   Loss :  0.670525033811
Iteration :  51   Loss :  0.642689206367
Iteration :  52   Loss :  0.616009141706
Iteration :  53   Loss :  0.590436858469
Iteration :  54   Loss :  0.565926366541
Iteration :  55   Loss :  0.542433584757
Iteration :  56   Loss :  0.519916262013
Iteration :  57   Loss :  0.498333901647
Iteration :  58   Loss :  0.477647688973
Iteration :  59   Loss :  0.457820421863
Iteration :  60   Loss :  0.438816444249
Iteration :  61   Loss :  0.420601582459
Iteration :  62   Loss :  0.40314308427
Iteration :  63   Loss :  0.386409560605
Iteration :  64   Loss :  0.37037092976
Iteration :  65   Loss :  0.354998364093
Iteration :  66   Loss :  0.34026423907
Iteration :  67   Loss :  0.326142084594
Iteration :  68   Loss :  0.31260653852
Iteration :  69   Loss :  0.299633302248
Iteration :  70   Loss :  0.287199098277
Iteration :  71   Loss :  0.275281629592
Iteration :  72   Loss :  0.263859540694
Iteration :  73   Loss :  0.252912380064
Iteration :  74   Loss :  0.242420563808
Iteration :  75   Loss :  0.232365340147
Iteration :  76   Loss :  0.222728754402
Iteration :  77   Loss :  0.21349361408
Iteration :  78   Loss :  0.204643453685
Iteration :  79   Loss :  0.196162498989
Iteration :  80   Loss :  0.188035630682
Iteration :  81   Loss :  0.180248347668
Iteration :  82   Loss :  0.172786730661
Iteration :  83   Loss :  0.165637407218
Iteration :  84   Loss :  0.158787519679
Iteration :  85   Loss :  0.152224697479
Iteration :  86   Loss :  0.145937034929
Iteration :  87   Loss :  0.139913074612
Iteration :  88   Loss :  0.134141795387
Iteration :  89   Loss :  0.128612602973
Iteration :  90   Loss :  0.123315320737
Iteration :  91   Loss :  0.118240178865
Iteration :  92   Loss :  0.113377801398
Iteration :  93   Loss :  0.10871919209
Iteration :  94   Loss :  0.104255720874
Iteration :  95   Loss :  0.099979112695
Iteration :  96   Loss :  0.0958814397058
Iteration :  97   Loss :  0.0919551166888
Iteration :  98   Loss :  0.0881928986609
Iteration :  99   Loss :  0.084587878808
[-0.00162268  0.00076448  0.00041549 ...,  0.00063679  0.00018577
  0.0001945 ]
CROSS VALIDATION 1
Iteration :  0   Loss :  64.2479949468
Iteration :  1   Loss :  5.03176728197
Iteration :  2   Loss :  4.82283914289
Iteration :  3   Loss :  4.62258608055
Iteration :  4   Loss :  4.43064788997
Iteration :  5   Loss :  4.24667932253
Iteration :  6   Loss :  4.07034946497
Iteration :  7   Loss :  3.90134114414
Iteration :  8   Loss :  3.73935035652
Iteration :  9   Loss :  3.58408572134
Iteration :  10   Loss :  3.43526795651
Iteration :  11   Loss :  3.29262937623
Iteration :  12   Loss :  3.15591340952
Iteration :  13   Loss :  3.02487413866
Iteration :  14   Loss :  2.89927585692
Iteration :  15   Loss :  2.77889264452
Iteration :  16   Loss :  2.66350796228
Iteration :  17   Loss :  2.55291426214
Iteration :  18   Loss :  2.44691261379
Iteration :  19   Loss :  2.34531234688
Iteration :  20   Loss :  2.24793070804
Iteration :  21   Loss :  2.15459253215
Iteration :  22   Loss :  2.06512992726
Iteration :  23   Loss :  1.97938197262
Iteration :  24   Loss :  1.89719442918
Iteration :  25   Loss :  1.81841946219
Iteration :  26   Loss :  1.74291537525
Iteration :  27   Loss :  1.67054635548
Iteration :  28   Loss :  1.60118222918
Iteration :  29   Loss :  1.53469822769
Iteration :  30   Loss :  1.47097476299
Iteration :  31   Loss :  1.40989721257
Iteration :  32   Loss :  1.35135571326
Iteration :  33   Loss :  1.29524496358
Iteration :  34   Loss :  1.2414640344
Iteration :  35   Loss :  1.18991618733
Iteration :  36   Loss :  1.14050870073
Iteration :  37   Loss :  1.09315270295
Iteration :  38   Loss :  1.04776301245
Iteration :  39   Loss :  1.00425798458
Iteration :  40   Loss :  0.962559364743
Iteration :  41   Loss :  0.922592147625
Iteration :  42   Loss :  0.884284442284
Iteration :  43   Loss :  0.84756734285
Iteration :  44   Loss :  0.812374804589
Iteration :  45   Loss :  0.778643525115
Iteration :  46   Loss :  0.746312830556
Iteration :  47   Loss :  0.715324566433
Iteration :  48   Loss :  0.685622993105
Iteration :  49   Loss :  0.657154685552
Iteration :  50   Loss :  0.629868437348
Iteration :  51   Loss :  0.603715168649
Iteration :  52   Loss :  0.578647838029
Iteration :  53   Loss :  0.554621358019
Iteration :  54   Loss :  0.531592514216
Iteration :  55   Loss :  0.509519887804
Iteration :  56   Loss :  0.488363781382
Iteration :  57   Loss :  0.468086147966
Iteration :  58   Loss :  0.448650523064
Iteration :  59   Loss :  0.430021959705
Iteration :  60   Loss :  0.412166966333
Iteration :  61   Loss :  0.395053447462
Iteration :  62   Loss :  0.378650646994
Iteration :  63   Loss :  0.362929094107
Iteration :  64   Loss :  0.347860551582
Iteration :  65   Loss :  0.333417966457
Iteration :  66   Loss :  0.319575422821
Iteration :  67   Loss :  0.306308096547
Iteration :  68   Loss :  0.293592211665
Iteration :  69   Loss :  0.281404998027
Iteration :  70   Loss :  0.269724649778
Iteration :  71   Loss :  0.258530284096
Iteration :  72   Loss :  0.247801899576
Iteration :  73   Loss :  0.237520333688
Iteration :  74   Loss :  0.227667218871
Iteration :  75   Loss :  0.218224937234
Iteration :  76   Loss :  0.209176574379
Iteration :  77   Loss :  0.200505873633
Iteration :  78   Loss :  0.192197192729
Iteration :  79   Loss :  0.184235465508
Iteration :  80   Loss :  0.176606171245
Iteration :  81   Loss :  0.169295313604
Iteration :  82   Loss :  0.162289410055
Iteration :  83   Loss :  0.15557549099
Iteration :  84   Loss :  0.149141106247
Iteration :  85   Loss :  0.142974335395
Iteration :  86   Loss :  0.137063797479
Iteration :  87   Loss :  0.131398656063
Iteration :  88   Loss :  0.125968616484
Iteration :  89   Loss :  0.120763914031
Iteration :  90   Loss :  0.11577529373
Iteration :  91   Loss :  0.110993983869
Iteration :  92   Loss :  0.106411666043
Iteration :  93   Loss :  0.102020444142
Iteration :  94   Loss :  0.0978128137722
Iteration :  95   Loss :  0.0937816324399
Iteration :  96   Loss :  0.0899200898597
Iteration :  97   Loss :  0.0862216772032
Iteration :  98   Loss :  0.0826801541036
Iteration :  99   Loss :  0.0792895127492
[ -7.27029030e-04   9.87886267e-04  -4.10643302e-04 ...,   2.51653613e-04
  -7.81858933e-05   6.15926969e-04]
CROSS VALIDATION 2
Iteration :  0   Loss :  318.342962588
Iteration :  1   Loss :  65.4898953403
Iteration :  2   Loss :  7.05368571302
Iteration :  3   Loss :  5.50578263976
Iteration :  4   Loss :  5.23245477028
Iteration :  5   Loss :  5.01519372129
Iteration :  6   Loss :  4.80695374663
Iteration :  7   Loss :  4.60736027487
Iteration :  8   Loss :  4.41605428746
Iteration :  9   Loss :  4.23269167298
Iteration :  10   Loss :  4.0569426081
Iteration :  11   Loss :  3.88849096439
Iteration :  12   Loss :  3.72703373964
Iteration :  13   Loss :  3.57228051285
Iteration :  14   Loss :  3.42395292182
Iteration :  15   Loss :  3.28178416246
Iteration :  16   Loss :  3.14551850888
Iteration :  17   Loss :  3.01491085341
Iteration :  18   Loss :  2.88972626569
Iteration :  19   Loss :  2.76973957013
Iteration :  20   Loss :  2.65473494087
Iteration :  21   Loss :  2.54450551353
Iteration :  22   Loss :  2.43885301319
Iteration :  23   Loss :  2.33758739774
Iteration :  24   Loss :  2.24052651603
Iteration :  25   Loss :  2.14749578029
Iteration :  26   Loss :  2.05832785212
Iteration :  27   Loss :  1.97286234154
Iteration :  28   Loss :  1.89094551857
Iteration :  29   Loss :  1.81243003675
Iteration :  30   Loss :  1.73717466826
Iteration :  31   Loss :  1.66504404999
Iteration :  32   Loss :  1.59590844025
Iteration :  33   Loss :  1.52964348559
Iteration :  34   Loss :  1.46612999742
Iteration :  35   Loss :  1.40525373789
Iteration :  36   Loss :  1.34690521482
Iteration :  37   Loss :  1.29097948525
Iteration :  38   Loss :  1.23737596718
Iteration :  39   Loss :  1.18599825931
Iteration :  40   Loss :  1.13675396839
Iteration :  41   Loss :  1.08955454388
Iteration :  42   Loss :  1.04431511958
Iteration :  43   Loss :  1.000954362
Iteration :  44   Loss :  0.959394325103
Iteration :  45   Loss :  0.919560311062
Iteration :  46   Loss :  0.881380736813
Iteration :  47   Loss :  0.844787005872
Iteration :  48   Loss :  0.809713385084
Iteration :  49   Loss :  0.776096885795
Iteration :  50   Loss :  0.743877148918
Iteration :  51   Loss :  0.71299633335
Iteration :  52   Loss :  0.683399007145
Iteration :  53   Loss :  0.655032040994
Iteration :  54   Loss :  0.627844503691
Iteration :  55   Loss :  0.601787559621
Iteration :  56   Loss :  0.576814368809
Iteration :  57   Loss :  0.552879990642
Iteration :  58   Loss :  0.529941293001
Iteration :  59   Loss :  0.507956868892
Iteration :  60   Loss :  0.486886962586
Iteration :  61   Loss :  0.466693406513
Iteration :  62   Loss :  0.447339568724
Iteration :  63   Loss :  0.428790308928
Iteration :  64   Loss :  0.41101193953
Iteration :  65   Loss :  0.393972187257
Iteration :  66   Loss :  0.377640151416
Iteration :  67   Loss :  0.361986256338
Iteration :  68   Loss :  0.346982197706
Iteration :  69   Loss :  0.332600884521
Iteration :  70   Loss :  0.318816379723
Iteration :  71   Loss :  0.305603842752
Iteration :  72   Loss :  0.292939476621
Iteration :  73   Loss :  0.280800480801
Iteration :  74   Loss :  0.269165009718
Iteration :  75   Loss :  0.25801213561
Iteration :  76   Loss :  0.247321813926
Iteration :  77   Loss :  0.237074849619
Iteration :  78   Loss :  0.227252863283
Iteration :  79   Loss :  0.217838256867
Iteration :  80   Loss :  0.208814179262
Iteration :  81   Loss :  0.200164492433
Iteration :  82   Loss :  0.191873738708
Iteration :  83   Loss :  0.18392710969
Iteration :  84   Loss :  0.176310416935
Iteration :  85   Loss :  0.169010064327
Iteration :  86   Loss :  0.162013021895
Iteration :  87   Loss :  0.155306800722
Iteration :  88   Loss :  0.148879428613
Iteration :  89   Loss :  0.142719426277
Iteration :  90   Loss :  0.136815783899
Iteration :  91   Loss :  0.131157938202
Iteration :  92   Loss :  0.125735750299
Iteration :  93   Loss :  0.120539484801
Iteration :  94   Loss :  0.115559790741
Iteration :  95   Loss :  0.110787684708
Iteration :  96   Loss :  0.106214536315
Iteration :  97   Loss :  0.101832055654
Iteration :  98   Loss :  0.0976322820577
Iteration :  99   Loss :  0.0936075733982
[-0.00259574 -0.00056826 -0.00093783 ...,  0.00265254  0.00053455  0.000223  ]
CROSS VALIDATION 3
Iteration :  0   Loss :  64.2735797519
Iteration :  1   Loss :  5.03224710505
Iteration :  2   Loss :  4.82329904285
Iteration :  3   Loss :  4.62302688462
Iteration :  4   Loss :  4.43107039105
Iteration :  5   Loss :  4.2470842806
Iteration :  6   Loss :  4.07073760844
Iteration :  7   Loss :  3.90171317119
Iteration :  8   Loss :  3.73970693633
Iteration :  9   Loss :  3.5844274953
Iteration :  10   Loss :  3.4355955394
Iteration :  11   Loss :  3.29294335728
Iteration :  12   Loss :  3.1562143535
Iteration :  13   Loss :  3.0251625869
Iteration :  14   Loss :  2.89955232827
Iteration :  15   Loss :  2.77915763627
Iteration :  16   Loss :  2.6637619511
Iteration :  17   Loss :  2.55315770488
Iteration :  18   Loss :  2.44714594834
Iteration :  19   Loss :  2.34553599295
Iteration :  20   Loss :  2.24814506792
Iteration :  21   Loss :  2.15479799142
Iteration :  22   Loss :  2.06532685549
Iteration :  23   Loss :  1.97957072403
Iteration :  24   Loss :  1.89737534329
Iteration :  25   Loss :  1.81859286441
Iteration :  26   Loss :  1.7430815775
Iteration :  27   Loss :  1.67070565671
Iteration :  28   Loss :  1.60133491593
Iteration :  29   Loss :  1.53484457461
Iteration :  30   Loss :  1.47111503332
Iteration :  31   Loss :  1.41003165862
Iteration :  32   Loss :  1.35148457686
Iteration :  33   Loss :  1.29536847654
Iteration :  34   Loss :  1.24158241887
Iteration :  35   Loss :  1.19002965626
Iteration :  36   Loss :  1.14061745822
Iteration :  37   Loss :  1.09325694463
Iteration :  38   Loss :  1.04786292582
Iteration :  39   Loss :  1.00435374937
Iteration :  40   Loss :  0.962651153196
Iteration :  41   Loss :  0.922680124846
Iteration :  42   Loss :  0.88436876652
Iteration :  43   Loss :  0.847648165773
Iteration :  44   Loss :  0.812452271571
Iteration :  45   Loss :  0.77871777549
Iteration :  46   Loss :  0.746383997864
Iteration :  47   Loss :  0.715392778667
Iteration :  48   Loss :  0.685688372929
Iteration :  49   Loss :  0.657217350526
Iteration :  50   Loss :  0.629928500134
Iteration :  51   Loss :  0.603772737206
Iteration :  52   Loss :  0.578703015803
Iteration :  53   Loss :  0.554674244124
Iteration :  54   Loss :  0.531643203599
Iteration :  55   Loss :  0.509568471406
Iteration :  56   Loss :  0.488410346291
Iteration :  57   Loss :  0.46813077756
Iteration :  58   Loss :  0.448693297144
Iteration :  59   Loss :  0.430062954628
Iteration :  60   Loss :  0.412206255135
Iteration :  61   Loss :  0.395091099983
Iteration :  62   Loss :  0.378686730008
Iteration :  63   Loss :  0.362963671459
Iteration :  64   Loss :  0.34789368435
Iteration :  65   Loss :  0.333449713145
Iteration :  66   Loss :  0.319605839609
Iteration :  67   Loss :  0.306337237617
Iteration :  68   Loss :  0.293620129646
Iteration :  69   Loss :  0.281431744598
Iteration :  70   Loss :  0.269750276486
Iteration :  71   Loss :  0.258554843451
Iteration :  72   Loss :  0.247825446499
Iteration :  73   Loss :  0.237542927366
Iteration :  74   Loss :  0.227688925093
Iteration :  75   Loss :  0.21824583121
Iteration :  76   Loss :  0.209196744056
Iteration :  77   Loss :  0.200525423438
Iteration :  78   Loss :  0.19221624769
Iteration :  79   Loss :  0.184254175679
Iteration :  80   Loss :  0.176624716443
Iteration :  81   Loss :  0.169313908504
Iteration :  82   Loss :  0.162308309744
Iteration :  83   Loss :  0.155594997019
Iteration :  84   Loss :  0.149161573
Iteration :  85   Loss :  0.142996176252
Iteration :  86   Loss :  0.137087489737
Iteration :  87   Loss :  0.131424742991
Iteration :  88   Loss :  0.125997704381
Iteration :  89   Loss :  0.120796661843
Iteration :  90   Loss :  0.115812392872
Iteration :  91   Loss :  0.111036126699
Iteration :  92   Loss :  0.106459503038
Iteration :  93   Loss :  0.102074532284
Iteration :  94   Loss :  0.0978735615031
Iteration :  95   Loss :  0.0938492489992
Iteration :  96   Loss :  0.0899945477162
Iteration :  97   Loss :  0.0863026948715
Iteration :  98   Loss :  0.0827672028655
Iteration :  99   Loss :  0.0793818458636
[ -7.30191019e-04   9.87947833e-04  -4.12392737e-04 ...,   2.55862299e-04
  -7.80267647e-05   6.15243900e-04]
CROSS VALIDATION 4
Iteration :  0   Loss :  64.2735797519
Iteration :  1   Loss :  5.03224710505
Iteration :  2   Loss :  4.82329904285
Iteration :  3   Loss :  4.62302688462
Iteration :  4   Loss :  4.43107039105
Iteration :  5   Loss :  4.2470842806
Iteration :  6   Loss :  4.07073760844
Iteration :  7   Loss :  3.90171317119
Iteration :  8   Loss :  3.73970693633
Iteration :  9   Loss :  3.5844274953
Iteration :  10   Loss :  3.4355955394
Iteration :  11   Loss :  3.29294335728
Iteration :  12   Loss :  3.1562143535
Iteration :  13   Loss :  3.0251625869
Iteration :  14   Loss :  2.89955232827
Iteration :  15   Loss :  2.77915763627
Iteration :  16   Loss :  2.6637619511
Iteration :  17   Loss :  2.55315770488
Iteration :  18   Loss :  2.44714594834
Iteration :  19   Loss :  2.34553599295
Iteration :  20   Loss :  2.24814506792
Iteration :  21   Loss :  2.15479799142
Iteration :  22   Loss :  2.06532685549
Iteration :  23   Loss :  1.97957072403
Iteration :  24   Loss :  1.89737534329
Iteration :  25   Loss :  1.81859286441
Iteration :  26   Loss :  1.7430815775
Iteration :  27   Loss :  1.67070565671
Iteration :  28   Loss :  1.60133491593
Iteration :  29   Loss :  1.53484457461
Iteration :  30   Loss :  1.47111503332
Iteration :  31   Loss :  1.41003165862
Iteration :  32   Loss :  1.35148457686
Iteration :  33   Loss :  1.29536847654
Iteration :  34   Loss :  1.24158241887
Iteration :  35   Loss :  1.19002965626
Iteration :  36   Loss :  1.14061745822
Iteration :  37   Loss :  1.09325694463
Iteration :  38   Loss :  1.04786292582
Iteration :  39   Loss :  1.00435374937
Iteration :  40   Loss :  0.962651153196
Iteration :  41   Loss :  0.922680124846
Iteration :  42   Loss :  0.88436876652
Iteration :  43   Loss :  0.847648165773
Iteration :  44   Loss :  0.812452271571
Iteration :  45   Loss :  0.77871777549
Iteration :  46   Loss :  0.746383997864
Iteration :  47   Loss :  0.715392778667
Iteration :  48   Loss :  0.685688372929
Iteration :  49   Loss :  0.657217350526
Iteration :  50   Loss :  0.629928500134
Iteration :  51   Loss :  0.603772737206
Iteration :  52   Loss :  0.578703015803
Iteration :  53   Loss :  0.554674244124
Iteration :  54   Loss :  0.531643203599
Iteration :  55   Loss :  0.509568471406
Iteration :  56   Loss :  0.488410346291
Iteration :  57   Loss :  0.46813077756
Iteration :  58   Loss :  0.448693297145
Iteration :  59   Loss :  0.430062954629
Iteration :  60   Loss :  0.412206255137
Iteration :  61   Loss :  0.395091099986
Iteration :  62   Loss :  0.378686730013
Iteration :  63   Loss :  0.362963671467
Iteration :  64   Loss :  0.347893684364
Iteration :  65   Loss :  0.33344971317
Iteration :  66   Loss :  0.31960583965
Iteration :  67   Loss :  0.306337237684
Iteration :  68   Loss :  0.293620129757
Iteration :  69   Loss :  0.281431744777
Iteration :  70   Loss :  0.269750276771
Iteration :  71   Loss :  0.258554843902
Iteration :  72   Loss :  0.247825447205
Iteration :  73   Loss :  0.237542928463
Iteration :  74   Loss :  0.227688926777
Iteration :  75   Loss :  0.218245833772
Iteration :  76   Loss :  0.209196747914
Iteration :  77   Loss :  0.200525429194
Iteration :  78   Loss :  0.192216256194
Iteration :  79   Loss :  0.184254188129
Iteration :  80   Loss :  0.176624734497
Iteration :  81   Loss :  0.16931393445
Iteration :  82   Loss :  0.1623083467
Iteration :  83   Loss :  0.155595049198
Iteration :  84   Loss :  0.149161646048
Iteration :  85   Loss :  0.142996277673
Iteration :  86   Loss :  0.137087629423
Iteration :  87   Loss :  0.131424933878
Iteration :  88   Loss :  0.125997963264
Iteration :  89   Loss :  0.120797010355
Iteration :  90   Loss :  0.115812858668
Iteration :  91   Loss :  0.111036744862
Iteration :  92   Loss :  0.10646031771
Iteration :  93   Loss :  0.102075598537
Iteration :  94   Loss :  0.0978749474259
Iteration :  95   Loss :  0.0938510379421
Iteration :  96   Loss :  0.0899968406261
Iteration :  97   Loss :  0.0863056125715
Iteration :  98   Loss :  0.0827708880989
Iteration :  99   Loss :  0.0793864648572
[ -7.30184968e-04   9.87948958e-04  -4.12344536e-04 ...,   2.55828973e-04
  -7.80273892e-05   6.15229485e-04]
CROSS VALIDATION 5
Iteration :  0   Loss :  64.1987662093
Iteration :  1   Loss :  5.03209725576
Iteration :  2   Loss :  4.82315541557
Iteration :  3   Loss :  4.62288922102
Iteration :  4   Loss :  4.43093844349
Iteration :  5   Loss :  4.24695781174
Iteration :  6   Loss :  4.0706163908
Iteration :  7   Loss :  3.90159698673
Iteration :  8   Loss :  3.73959557605
Iteration :  9   Loss :  3.58432075891
Iteration :  10   Loss :  3.43549323489
Iteration :  11   Loss :  3.29284530065
Iteration :  12   Loss :  3.15612036836
Iteration :  13   Loss :  3.02507250419
Iteration :  14   Loss :  2.89946598596
Iteration :  15   Loss :  2.77907487905
Iteration :  16   Loss :  2.66368263011
Iteration :  17   Loss :  2.55308167744
Iteration :  18   Loss :  2.4470730777
Iteration :  19   Loss :  2.34546614803
Iteration :  20   Loss :  2.24807812309
Iteration :  21   Loss :  2.15473382625
Iteration :  22   Loss :  2.06526535458
Iteration :  23   Loss :  1.97951177675
Iteration :  24   Loss :  1.89731884361
Iteration :  25   Loss :  1.8185387107
Iteration :  26   Loss :  1.74302967235
Iteration :  27   Loss :  1.67065590676
Iteration :  28   Loss :  1.60128723168
Iteration :  29   Loss :  1.5347988703
Iteration :  30   Loss :  1.47107122674
Iteration :  31   Loss :  1.40998967097
Iteration :  32   Loss :  1.35144433261
Iteration :  33   Loss :  1.2953299033
Iteration :  34   Loss :  1.24154544727
Iteration :  35   Loss :  1.18999421979
Iteration :  36   Loss :  1.14058349314
Iteration :  37   Loss :  1.09322438984
Iteration :  38   Loss :  1.04783172277
Iteration :  39   Loss :  1.00432384192
Iteration :  40   Loss :  0.962622487572
Iteration :  41   Loss :  0.922652649481
Iteration :  42   Loss :  0.884342431997
Iteration :  43   Loss :  0.847622924731
Iteration :  44   Loss :  0.812428078617
Iteration :  45   Loss :  0.778694587121
Iteration :  46   Loss :  0.746361772391
Iteration :  47   Loss :  0.715371476141
Iteration :  48   Loss :  0.685667955074
Iteration :  49   Loss :  0.657197780671
Iteration :  50   Loss :  0.629909743155
Iteration :  51   Loss :  0.60375475947
Iteration :  52   Loss :  0.578685785115
Iteration :  53   Loss :  0.55465772968
Iteration :  54   Loss :  0.531627375946
Iteration :  55   Loss :  0.509553302407
Iteration :  56   Loss :  0.488395809092
Iteration :  57   Loss :  0.46811684657
Iteration :  58   Loss :  0.44867994802
Iteration :  59   Loss :  0.430050164263
Iteration :  60   Loss :  0.412194001659
Iteration :  61   Loss :  0.395079362769
Iteration :  62   Loss :  0.378675489681
Iteration :  63   Loss :  0.36295290991
Iteration :  64   Loss :  0.347883384745
Iteration :  65   Loss :  0.333439859928
Iteration :  66   Loss :  0.31959641848
Iteration :  67   Loss :  0.306328235476
Iteration :  68   Loss :  0.293611534477
Iteration :  69   Loss :  0.281423545261
Iteration :  70   Loss :  0.26974246239
Iteration :  71   Loss :  0.258547404062
Iteration :  72   Loss :  0.247818370645
Iteration :  73   Loss :  0.237536202328
Iteration :  74   Loss :  0.22768253547
Iteration :  75   Loss :  0.218239757641
Iteration :  76   Loss :  0.209190961894
Iteration :  77   Loss :  0.200519901598
Iteration :  78   Loss :  0.192210947897
Iteration :  79   Loss :  0.184249052361
Iteration :  80   Loss :  0.176619717423
Iteration :  81   Loss :  0.169308976457
Iteration :  82   Loss :  0.162303384153
Iteration :  83   Loss :  0.155590016136
Iteration :  84   Loss :  0.149156475234
Iteration :  85   Loss :  0.142990900494
Iteration :  86   Loss :  0.137081974445
Iteration :  87   Loss :  0.131418924289
Iteration :  88   Loss :  0.125991513765
Iteration :  89   Loss :  0.120790024173
Iteration :  90   Loss :  0.115805225103
Iteration :  91   Loss :  0.111028337278
Iteration :  92   Loss :  0.106450991286
Iteration :  93   Loss :  0.102065186578
Iteration :  94   Loss :  0.0978632549887
Iteration :  95   Loss :  0.0938378319882
Iteration :  96   Loss :  0.089981837013
Iteration :  97   Loss :  0.0862884616431
Iteration :  98   Loss :  0.082751161962
Iteration :  99   Loss :  0.0793636501025
[ -7.28900058e-04   9.90725829e-04  -4.10045714e-04 ...,   2.54795048e-04
  -7.88095258e-05   6.16464022e-04]
CROSS VALIDATION 6
Iteration :  0   Loss :  64.2737596538
Iteration :  1   Loss :  5.03224453385
Iteration :  2   Loss :  4.8232965784
Iteration :  3   Loss :  4.62302452251
Iteration :  4   Loss :  4.43106812702
Iteration :  5   Loss :  4.24708211057
Iteration :  6   Loss :  4.07073552852
Iteration :  7   Loss :  3.90171117763
Iteration :  8   Loss :  3.73970502554
Iteration :  9   Loss :  3.58442566386
Iteration :  10   Loss :  3.435593784
Iteration :  11   Loss :  3.29294167477
Iteration :  12   Loss :  3.15621274085
Iteration :  13   Loss :  3.02516104121
Iteration :  14   Loss :  2.89955084676
Iteration :  15   Loss :  2.77915621628
Iteration :  16   Loss :  2.66376059006
Iteration :  17   Loss :  2.55315640035
Iteration :  18   Loss :  2.44714469798
Iteration :  19   Loss :  2.34553479451
Iteration :  20   Loss :  2.24814391924
Iteration :  21   Loss :  2.15479689043
Iteration :  22   Loss :  2.06532580022
Iteration :  23   Loss :  1.97956971258
Iteration :  24   Loss :  1.89737437383
Iteration :  25   Loss :  1.81859193521
Iteration :  26   Loss :  1.74308068688
Iteration :  27   Loss :  1.67070480307
Iteration :  28   Loss :  1.60133409773
Iteration :  29   Loss :  1.53484379038
Iteration :  30   Loss :  1.47111428166
Iteration :  31   Loss :  1.41003093817
Iteration :  32   Loss :  1.35148388633
Iteration :  33   Loss :  1.29536781467
Iteration :  34   Loss :  1.24158178449
Iteration :  35   Loss :  1.19002904822
Iteration :  36   Loss :  1.14061687543
Iteration :  37   Loss :  1.09325638604
Iteration :  38   Loss :  1.04786239042
Iteration :  39   Loss :  1.0043532362
Iteration :  40   Loss :  0.962650661335
Iteration :  41   Loss :  0.922679653408
Iteration :  42   Loss :  0.884368314656
Iteration :  43   Loss :  0.847647732672
Iteration :  44   Loss :  0.812451856453
Iteration :  45   Loss :  0.778717377608
Iteration :  46   Loss :  0.746383616503
Iteration :  47   Loss :  0.71539241314
Iteration :  48   Loss :  0.68568802258
Iteration :  49   Loss :  0.657217014724
Iteration :  50   Loss :  0.629928178275
Iteration :  51   Loss :  0.603772428711
Iteration :  52   Loss :  0.578702720118
Iteration :  53   Loss :  0.554673960716
Iteration :  54   Loss :  0.531642931958
Iteration :  55   Loss :  0.509568211044
Iteration :  56   Loss :  0.48841009674
Iteration :  57   Loss :  0.468130538371
Iteration :  58   Loss :  0.448693067887
Iteration :  59   Loss :  0.430062734891
Iteration :  60   Loss :  0.412206044524
Iteration :  61   Loss :  0.395090898119
Iteration :  62   Loss :  0.378686536529
Iteration :  63   Loss :  0.362963486019
Iteration :  64   Loss :  0.34789350662
Iteration :  65   Loss :  0.33344954281
Iteration :  66   Loss :  0.319605676372
Iteration :  67   Loss :  0.306337081196
Iteration :  68   Loss :  0.29361997978
Iteration :  69   Loss :  0.281431601047
Iteration :  70   Loss :  0.269750139035
Iteration :  71   Loss :  0.25855471192
Iteration :  72   Loss :  0.247825320749
Iteration :  73   Loss :  0.237542807317
Iteration :  74   Loss :  0.22768881074
Iteration :  75   Loss :  0.218245722655
Iteration :  76   Loss :  0.209196641544
Iteration :  77   Loss :  0.200525327414
Iteration :  78   Loss :  0.192216158867
Iteration :  79   Loss :  0.184254095135
Iteration :  80   Loss :  0.17662464574
Iteration :  81   Loss :  0.16931384985
Iteration :  82   Loss :  0.162308266195
Iteration :  83   Loss :  0.155594972739
Iteration :  84   Loss :  0.149161573593
Iteration :  85   Loss :  0.142996209187
Iteration :  86   Loss :  0.137087564875
Iteration :  87   Loss :  0.131424873249
Iteration :  88   Loss :  0.125997906546
Iteration :  89   Loss :  0.120796957564
Iteration :  90   Loss :  0.115812809845
Iteration :  91   Loss :  0.11103670008
Iteration :  92   Loss :  0.106460277089
Iteration :  93   Loss :  0.102075562261
Iteration :  94   Loss :  0.0978749157795
Iteration :  95   Loss :  0.0938510113684
Iteration :  96   Loss :  0.0899968198032
Iteration :  97   Loss :  0.0863055985115
Iteration :  98   Loss :  0.0827708822637
Iteration :  99   Loss :  0.0793864692909
[ -7.30186605e-04   9.87937976e-04  -4.12352326e-04 ...,   2.55832384e-04
  -7.80248961e-05   6.15225404e-04]
CROSS VALIDATION 7
Iteration :  0   Loss :  64.2737596538
Iteration :  1   Loss :  5.03224453385
Iteration :  2   Loss :  4.8232965784
Iteration :  3   Loss :  4.62302452251
Iteration :  4   Loss :  4.43106812702
Iteration :  5   Loss :  4.24708211057
Iteration :  6   Loss :  4.07073552852
Iteration :  7   Loss :  3.90171117763
Iteration :  8   Loss :  3.73970502554
Iteration :  9   Loss :  3.58442566386
Iteration :  10   Loss :  3.435593784
Iteration :  11   Loss :  3.29294167477
Iteration :  12   Loss :  3.15621274085
Iteration :  13   Loss :  3.02516104121
Iteration :  14   Loss :  2.89955084676
Iteration :  15   Loss :  2.77915621628
Iteration :  16   Loss :  2.66376059006
Iteration :  17   Loss :  2.55315640035
Iteration :  18   Loss :  2.44714469798
Iteration :  19   Loss :  2.34553479451
Iteration :  20   Loss :  2.24814391924
Iteration :  21   Loss :  2.15479689043
Iteration :  22   Loss :  2.06532580022
Iteration :  23   Loss :  1.97956971258
Iteration :  24   Loss :  1.89737437383
Iteration :  25   Loss :  1.81859193521
Iteration :  26   Loss :  1.74308068688
Iteration :  27   Loss :  1.67070480307
Iteration :  28   Loss :  1.60133409773
Iteration :  29   Loss :  1.53484379038
Iteration :  30   Loss :  1.47111428166
Iteration :  31   Loss :  1.41003093817
Iteration :  32   Loss :  1.35148388633
Iteration :  33   Loss :  1.29536781467
Iteration :  34   Loss :  1.24158178449
Iteration :  35   Loss :  1.19002904822
Iteration :  36   Loss :  1.14061687543
Iteration :  37   Loss :  1.09325638604
Iteration :  38   Loss :  1.04786239042
Iteration :  39   Loss :  1.0043532362
Iteration :  40   Loss :  0.962650661335
Iteration :  41   Loss :  0.922679653408
Iteration :  42   Loss :  0.884368314656
Iteration :  43   Loss :  0.847647732672
Iteration :  44   Loss :  0.812451856453
Iteration :  45   Loss :  0.778717377608
Iteration :  46   Loss :  0.746383616503
Iteration :  47   Loss :  0.71539241314
Iteration :  48   Loss :  0.68568802258
Iteration :  49   Loss :  0.657217014724
Iteration :  50   Loss :  0.629928178275
Iteration :  51   Loss :  0.603772428711
Iteration :  52   Loss :  0.578702720117
Iteration :  53   Loss :  0.554673960715
Iteration :  54   Loss :  0.531642931956
Iteration :  55   Loss :  0.509568211041
Iteration :  56   Loss :  0.488410096735
Iteration :  57   Loss :  0.468130538362
Iteration :  58   Loss :  0.448693067872
Iteration :  59   Loss :  0.430062734865
Iteration :  60   Loss :  0.412206044479
Iteration :  61   Loss :  0.395090898046
Iteration :  62   Loss :  0.37868653641
Iteration :  63   Loss :  0.362963485827
Iteration :  64   Loss :  0.347893506312
Iteration :  65   Loss :  0.333449542322
Iteration :  66   Loss :  0.319605675606
Iteration :  67   Loss :  0.306337080006
Iteration :  68   Loss :  0.29361997795
Iteration :  69   Loss :  0.281431598256
Iteration :  70   Loss :  0.269750134819
Iteration :  71   Loss :  0.258554705605
Iteration :  72   Loss :  0.247825311375
Iteration :  73   Loss :  0.237542793519
Iteration :  74   Loss :  0.227688790603
Iteration :  75   Loss :  0.218245693514
Iteration :  76   Loss :  0.209196599718
Iteration :  77   Loss :  0.200525267868
Iteration :  78   Loss :  0.192216074773
Iteration :  79   Loss :  0.184253977309
Iteration :  80   Loss :  0.176624481932
Iteration :  81   Loss :  0.169313623859
Iteration :  82   Loss :  0.162307956759
Iteration :  83   Loss :  0.15559455218
Iteration :  84   Loss :  0.149161006167
Iteration :  85   Loss :  0.142995449097
Iteration :  86   Loss :  0.137086553918
Iteration :  87   Loss :  0.131423538063
Iteration :  88   Loss :  0.12599615546
Iteration :  89   Loss :  0.120794677056
Iteration :  90   Loss :  0.115809860692
Iteration :  91   Loss :  0.111032913317
Iteration :  92   Loss :  0.106455449997
Iteration :  93   Loss :  0.102069454714
Iteration :  94   Loss :  0.0978672474082
Iteration :  95   Loss :  0.0938414601722
Iteration :  96   Loss :  0.0899850229791
Iteration :  97   Loss :  0.0862911563918
Iteration :  98   Loss :  0.0827533662573
Iteration :  99   Loss :  0.0793654345469
[ -7.30503763e-04   9.87814878e-04  -4.12641862e-04 ...,   2.55678403e-04
  -7.80681970e-05   6.15305571e-04]
CROSS VALIDATION 8
Iteration :  0   Loss :  64.2737596538
Iteration :  1   Loss :  5.03224453385
Iteration :  2   Loss :  4.8232965784
Iteration :  3   Loss :  4.62302452251
Iteration :  4   Loss :  4.43106812702
Iteration :  5   Loss :  4.24708211057
Iteration :  6   Loss :  4.07073552852
Iteration :  7   Loss :  3.90171117763
Iteration :  8   Loss :  3.73970502554
Iteration :  9   Loss :  3.58442566386
Iteration :  10   Loss :  3.435593784
Iteration :  11   Loss :  3.29294167477
Iteration :  12   Loss :  3.15621274085
Iteration :  13   Loss :  3.02516104121
Iteration :  14   Loss :  2.89955084676
Iteration :  15   Loss :  2.77915621628
Iteration :  16   Loss :  2.66376059006
Iteration :  17   Loss :  2.55315640035
Iteration :  18   Loss :  2.44714469798
Iteration :  19   Loss :  2.34553479451
Iteration :  20   Loss :  2.24814391924
Iteration :  21   Loss :  2.15479689043
Iteration :  22   Loss :  2.06532580022
Iteration :  23   Loss :  1.97956971258
Iteration :  24   Loss :  1.89737437383
Iteration :  25   Loss :  1.81859193521
Iteration :  26   Loss :  1.74308068688
Iteration :  27   Loss :  1.67070480307
Iteration :  28   Loss :  1.60133409773
Iteration :  29   Loss :  1.53484379038
Iteration :  30   Loss :  1.47111428166
Iteration :  31   Loss :  1.41003093817
Iteration :  32   Loss :  1.35148388633
Iteration :  33   Loss :  1.29536781467
Iteration :  34   Loss :  1.24158178449
Iteration :  35   Loss :  1.19002904822
Iteration :  36   Loss :  1.14061687543
Iteration :  37   Loss :  1.09325638604
Iteration :  38   Loss :  1.04786239042
Iteration :  39   Loss :  1.0043532362
Iteration :  40   Loss :  0.962650661335
Iteration :  41   Loss :  0.922679653408
Iteration :  42   Loss :  0.884368314656
Iteration :  43   Loss :  0.847647732672
Iteration :  44   Loss :  0.812451856453
Iteration :  45   Loss :  0.778717377608
Iteration :  46   Loss :  0.746383616503
Iteration :  47   Loss :  0.71539241314
Iteration :  48   Loss :  0.68568802258
Iteration :  49   Loss :  0.657217014724
Iteration :  50   Loss :  0.629928178275
Iteration :  51   Loss :  0.603772428711
Iteration :  52   Loss :  0.578702720118
Iteration :  53   Loss :  0.554673960716
Iteration :  54   Loss :  0.531642931958
Iteration :  55   Loss :  0.509568211044
Iteration :  56   Loss :  0.48841009674
Iteration :  57   Loss :  0.468130538371
Iteration :  58   Loss :  0.448693067887
Iteration :  59   Loss :  0.430062734891
Iteration :  60   Loss :  0.412206044524
Iteration :  61   Loss :  0.395090898119
Iteration :  62   Loss :  0.378686536529
Iteration :  63   Loss :  0.362963486019
Iteration :  64   Loss :  0.34789350662
Iteration :  65   Loss :  0.333449542811
Iteration :  66   Loss :  0.319605676372
Iteration :  67   Loss :  0.306337081196
Iteration :  68   Loss :  0.29361997978
Iteration :  69   Loss :  0.281431601047
Iteration :  70   Loss :  0.269750139036
Iteration :  71   Loss :  0.258554711921
Iteration :  72   Loss :  0.24782532075
Iteration :  73   Loss :  0.237542807319
Iteration :  74   Loss :  0.227688810742
Iteration :  75   Loss :  0.218245722658
Iteration :  76   Loss :  0.209196641548
Iteration :  77   Loss :  0.200525327418
Iteration :  78   Loss :  0.192216158872
Iteration :  79   Loss :  0.184254095141
Iteration :  80   Loss :  0.176624645747
Iteration :  81   Loss :  0.169313849857
Iteration :  82   Loss :  0.162308266202
Iteration :  83   Loss :  0.155594972744
Iteration :  84   Loss :  0.149161573593
Iteration :  85   Loss :  0.142996209177
Iteration :  86   Loss :  0.137087564849
Iteration :  87   Loss :  0.131424873194
Iteration :  88   Loss :  0.125997906443
Iteration :  89   Loss :  0.120796957382
Iteration :  90   Loss :  0.115812809536
Iteration :  91   Loss :  0.111036699568
Iteration :  92   Loss :  0.106460276259
Iteration :  93   Loss :  0.102075560939
Iteration :  94   Loss :  0.0978749137076
Iteration :  95   Loss :  0.0938510081731
Iteration :  96   Loss :  0.089996814953
Iteration :  97   Loss :  0.0863055912682
Iteration :  98   Loss :  0.0827708716287
Iteration :  99   Loss :  0.0793864539523
[ -7.30185093e-04   9.87938563e-04  -4.12350947e-04 ...,   2.55833114e-04
  -7.80246900e-05   6.15225024e-04]
CROSS VALIDATION 9
Iteration :  0   Loss :  160.602717626
Iteration :  1   Loss :  72.9123171051
Iteration :  2   Loss :  5.72753823072
Iteration :  3   Loss :  5.48972049947
Iteration :  4   Loss :  5.26177742305
Iteration :  5   Loss :  5.04329899716
Iteration :  6   Loss :  4.83389224413
Iteration :  7   Loss :  4.63318050663
Iteration :  8   Loss :  4.44080277056
Iteration :  9   Loss :  4.25641301628
Iteration :  10   Loss :  4.07967959678
Iteration :  11   Loss :  3.91028464162
Iteration :  12   Loss :  3.74792348558
Iteration :  13   Loss :  3.59230412076
Iteration :  14   Loss :  3.44314667115
Iteration :  15   Loss :  3.30018288831
Iteration :  16   Loss :  3.16315566714
Iteration :  17   Loss :  3.0318185806
Iteration :  18   Loss :  2.90593543227
Iteration :  19   Loss :  2.78527982597
Iteration :  20   Loss :  2.66963475177
Iteration :  21   Loss :  2.55879218846
Iteration :  22   Loss :  2.45255272263
Iteration :  23   Loss :  2.35072518556
Iteration :  24   Loss :  2.25312630857
Iteration :  25   Loss :  2.15958039779
Iteration :  26   Loss :  2.06991902791
Iteration :  27   Loss :  1.98398075319
Iteration :  28   Loss :  1.90161083279
Iteration :  29   Loss :  1.82266096694
Iteration :  30   Loss :  1.74698904054
Iteration :  31   Loss :  1.67445887248
Iteration :  32   Loss :  1.60493997002
Iteration :  33   Loss :  1.53830728942
Iteration :  34   Loss :  1.47444100396
Iteration :  35   Loss :  1.41322628094
Iteration :  36   Loss :  1.35455306853
Iteration :  37   Loss :  1.29831589263
Iteration :  38   Loss :  1.24441366378
Iteration :  39   Loss :  1.19274949343
Iteration :  40   Loss :  1.14323051894
Iteration :  41   Loss :  1.09576773672
Iteration :  42   Loss :  1.0502758427
Iteration :  43   Loss :  1.00667307961
Iteration :  44   Loss :  0.964881090499
Iteration :  45   Loss :  0.924824777782
Iteration :  46   Loss :  0.88643216753
Iteration :  47   Loss :  0.849634278289
Iteration :  48   Loss :  0.814364994134
Iteration :  49   Loss :  0.780560941634
Iteration :  50   Loss :  0.748161370715
Iteration :  51   Loss :  0.717108039736
Iteration :  52   Loss :  0.687345105433
Iteration :  53   Loss :  0.658819018737
Iteration :  54   Loss :  0.631478427455
Iteration :  55   Loss :  0.605274086496
Iteration :  56   Loss :  0.580158775432
Iteration :  57   Loss :  0.556087222193
Iteration :  58   Loss :  0.533016030714
Iteration :  59   Loss :  0.510903610137
Iteration :  60   Loss :  0.489710103803
Iteration :  61   Loss :  0.469397317661
Iteration :  62   Loss :  0.44992864932
Iteration :  63   Loss :  0.431269019943
Iteration :  64   Loss :  0.413384811163
Iteration :  65   Loss :  0.396243807976
Iteration :  66   Loss :  0.379815146804
Iteration :  67   Loss :  0.364069266292
Iteration :  68   Loss :  0.348977857744
Iteration :  69   Loss :  0.334513812598
Iteration :  70   Loss :  0.32065116583
Iteration :  71   Loss :  0.307365035903
Iteration :  72   Loss :  0.294631563417
Iteration :  73   Loss :  0.282427851455
Iteration :  74   Loss :  0.270731910825
Iteration :  75   Loss :  0.259522612993
Iteration :  76   Loss :  0.248779652489
Iteration :  77   Loss :  0.238483519168
Iteration :  78   Loss :  0.228615479076
Iteration :  79   Loss :  0.219157561195
Iteration :  80   Loss :  0.210092546519
Iteration :  81   Loss :  0.201403955943
Iteration :  82   Loss :  0.193076034326
Iteration :  83   Loss :  0.185093729435
Iteration :  84   Loss :  0.177442665755
Iteration :  85   Loss :  0.17010911413
Iteration :  86   Loss :  0.163079958553
Iteration :  87   Loss :  0.15634266152
Iteration :  88   Loss :  0.14988522907
Iteration :  89   Loss :  0.143696176437
Iteration :  90   Loss :  0.13776449503
Iteration :  91   Loss :  0.13207962142
Iteration :  92   Loss :  0.126631409073
Iteration :  93   Loss :  0.121410103603
Iteration :  94   Loss :  0.116406322295
Iteration :  95   Loss :  0.111611038332
Iteration :  96   Loss :  0.107015569632
Iteration :  97   Loss :  0.102611571496
Iteration :  98   Loss :  0.0983910316994
Iteration :  99   Loss :  0.0943462663514
[-0.00190938  0.00088129 -0.00031003 ...,  0.00250779  0.00099586
  0.00047247]
CROSS VALIDATION 10
Iteration :  0   Loss :  64.2718943181
Iteration :  1   Loss :  5.03219231905
Iteration :  2   Loss :  4.82324653166
Iteration :  3   Loss :  4.6229765538
Iteration :  4   Loss :  4.43102215005
Iteration :  5   Loss :  4.24703804265
Iteration :  6   Loss :  4.07069329038
Iteration :  7   Loss :  3.9016706933
Iteration :  8   Loss :  3.73966622219
Iteration :  9   Loss :  3.58438847169
Iteration :  10   Loss :  3.43555813612
Iteration :  11   Loss :  3.29290750705
Iteration :  12   Loss :  3.15617999184
Iteration :  13   Loss :  3.025129652
Iteration :  14   Loss :  2.89952076088
Iteration :  15   Loss :  2.77912737962
Iteration :  16   Loss :  2.66373295076
Iteration :  17   Loss :  2.55312990868
Iteration :  18   Loss :  2.44711930629
Iteration :  19   Loss :  2.34551045713
Iteration :  20   Loss :  2.24812059239
Iteration :  21   Loss :  2.15477453216
Iteration :  22   Loss :  2.06530437031
Iteration :  23   Loss :  1.97954917247
Iteration :  24   Loss :  1.89735468659
Iteration :  25   Loss :  1.81857306542
Iteration :  26   Loss :  1.7430626006
Iteration :  27   Loss :  1.67068746776
Iteration :  28   Loss :  1.60131748222
Iteration :  29   Loss :  1.53482786477
Iteration :  30   Loss :  1.47109901731
Iteration :  31   Loss :  1.41001630763
Iteration :  32   Loss :  1.35146986327
Iteration :  33   Loss :  1.29535437388
Iteration :  34   Loss :  1.24156890178
Iteration :  35   Loss :  1.19001670042
Iteration :  36   Loss :  1.14060504034
Iteration :  37   Loss :  1.09324504236
Iteration :  38   Loss :  1.04785151776
Iteration :  39   Loss :  1.00434281498
Iteration :  40   Loss :  0.962640672828
Iteration :  41   Loss :  0.922670079642
Iteration :  42   Loss :  0.884359138412
Iteration :  43   Loss :  0.847638937443
Iteration :  44   Loss :  0.81244342642
Iteration :  45   Loss :  0.778709297608
Iteration :  46   Loss :  0.746375872003
Iteration :  47   Loss :  0.715384990211
Iteration :  48   Loss :  0.685680907872
Iteration :  49   Loss :  0.657210195442
Iteration :  50   Loss :  0.629921642158
Iteration :  51   Loss :  0.603766164007
Iteration :  52   Loss :  0.578696715565
Iteration :  53   Loss :  0.554668205525
Iteration :  54   Loss :  0.53163741579
Iteration :  55   Loss :  0.509562923996
Iteration :  56   Loss :  0.488405029326
Iteration :  57   Loss :  0.468125681509
Iteration :  58   Loss :  0.448688412885
Iteration :  59   Loss :  0.430058273431
Iteration :  60   Loss :  0.412201768654
Iteration :  61   Loss :  0.395086800243
Iteration :  62   Loss :  0.378682609395
Iteration :  63   Loss :  0.362959722715
Iteration :  64   Loss :  0.347889900564
Iteration :  65   Loss :  0.333446087748
Iteration :  66   Loss :  0.319602366371
Iteration :  67   Loss :  0.306333910641
Iteration :  68   Loss :  0.293616943365
Iteration :  69   Loss :  0.281428693766
Iteration :  70   Loss :  0.26974735617
Iteration :  71   Loss :  0.258552049024
Iteration :  72   Loss :  0.247822773622
Iteration :  73   Loss :  0.237540371985
Iteration :  74   Loss :  0.227686483433
Iteration :  75   Loss :  0.218243499798
Iteration :  76   Loss :  0.209194519774
Iteration :  77   Loss :  0.200523303625
Iteration :  78   Loss :  0.192214230295
Iteration :  79   Loss :  0.184252259465
Iteration :  80   Loss :  0.176622901211
Iteration :  81   Loss :  0.169312195317
Iteration :  82   Loss :  0.162306701112
Iteration :  83   Loss :  0.155593497058
Iteration :  84   Loss :  0.149160187603
Iteration :  85   Loss :  0.142994913334
Iteration :  86   Loss :  0.137086359619
Iteration :  87   Loss :  0.131423758954
Iteration :  88   Loss :  0.125996883372
Iteration :  89   Loss :  0.120796025274
Iteration :  90   Loss :  0.115811967492
Iteration :  91   Loss :  0.111035945545
Iteration :  92   Loss :  0.106459606549
Iteration :  93   Loss :  0.102074969695
Iteration :  94   Loss :  0.0978743926352
Iteration :  95   Loss :  0.093850546467
Iteration :  96   Loss :  0.0899963994397
Iteration :  97   Loss :  0.0863052065793
Iteration :  98   Loss :  0.0827705001363
Iteration :  99   Loss :  0.0793860751787
[ -7.30129484e-04   9.87345067e-04  -4.12807678e-04 ...,   2.55874339e-04
  -7.80436249e-05   6.15085380e-04]
CROSS VALIDATION 11
Iteration :  0   Loss :  64.2718641865
Iteration :  1   Loss :  5.03219287758
Iteration :  2   Loss :  4.823247067
Iteration :  3   Loss :  4.62297706691
Iteration :  4   Loss :  4.43102264186
Iteration :  5   Loss :  4.24703851404
Iteration :  6   Loss :  4.07069374219
Iteration :  7   Loss :  3.90167112635
Iteration :  8   Loss :  3.73966663727
Iteration :  9   Loss :  3.58438886953
Iteration :  10   Loss :  3.43555851744
Iteration :  11   Loss :  3.29290787254
Iteration :  12   Loss :  3.15618034215
Iteration :  13   Loss :  3.02512998777
Iteration :  14   Loss :  2.89952108271
Iteration :  15   Loss :  2.77912768809
Iteration :  16   Loss :  2.66373324641
Iteration :  17   Loss :  2.55313019206
Iteration :  18   Loss :  2.44711957791
Iteration :  19   Loss :  2.34551071747
Iteration :  20   Loss :  2.24812084192
Iteration :  21   Loss :  2.15477477132
Iteration :  22   Loss :  2.06530459954
Iteration :  23   Loss :  1.97954939219
Iteration :  24   Loss :  1.89735489718
Iteration :  25   Loss :  1.81857326727
Iteration :  26   Loss :  1.74306279406
Iteration :  27   Loss :  1.67068765319
Iteration :  28   Loss :  1.60131765995
Iteration :  29   Loss :  1.53482803513
Iteration :  30   Loss :  1.47109918059
Iteration :  31   Loss :  1.41001646413
Iteration :  32   Loss :  1.35147001327
Iteration :  33   Loss :  1.29535451765
Iteration :  34   Loss :  1.24156903959
Iteration :  35   Loss :  1.19001683251
Iteration :  36   Loss :  1.14060516694
Iteration :  37   Loss :  1.0932451637
Iteration :  38   Loss :  1.04785163406
Iteration :  39   Loss :  1.00434292646
Iteration :  40   Loss :  0.962640779674
Iteration :  41   Loss :  0.922670182052
Iteration :  42   Loss :  0.884359236569
Iteration :  43   Loss :  0.847639031524
Iteration :  44   Loss :  0.812443516594
Iteration :  45   Loss :  0.778709384038
Iteration :  46   Loss :  0.746375954845
Iteration :  47   Loss :  0.715385069613
Iteration :  48   Loss :  0.685680983977
Iteration :  49   Loss :  0.657210268388
Iteration :  50   Loss :  0.629921712074
Iteration :  51   Loss :  0.60376623102
Iteration :  52   Loss :  0.578696779796
Iteration :  53   Loss :  0.554668267089
Iteration :  54   Loss :  0.531637474798
Iteration :  55   Loss :  0.509562980554
Iteration :  56   Loss :  0.488405083535
Iteration :  57   Loss :  0.468125733468
Iteration :  58   Loss :  0.448688462686
Iteration :  59   Loss :  0.430058321165
Iteration :  60   Loss :  0.412201814406
Iteration :  61   Loss :  0.395086844095
Iteration :  62   Loss :  0.378682651427
Iteration :  63   Loss :  0.362959763001
Iteration :  64   Loss :  0.347889939177
Iteration :  65   Loss :  0.333446124759
Iteration :  66   Loss :  0.319602401845
Iteration :  67   Loss :  0.306333944642
Iteration :  68   Loss :  0.293616975954
Iteration :  69   Loss :  0.281428725001
Iteration :  70   Loss :  0.269747386108
Iteration :  71   Loss :  0.258552077717
Iteration :  72   Loss :  0.247822801123
Iteration :  73   Loss :  0.237540398341
Iteration :  74   Loss :  0.227686508691
Iteration :  75   Loss :  0.218243524004
Iteration :  76   Loss :  0.209194542969
Iteration :  77   Loss :  0.200523325849
Iteration :  78   Loss :  0.192214251589
Iteration :  79   Loss :  0.184252279867
Iteration :  80   Loss :  0.176622920761
Iteration :  81   Loss :  0.169312214059
Iteration :  82   Loss :  0.162306719099
Iteration :  83   Loss :  0.155593514358
Iteration :  84   Loss :  0.14916020431
Iteration :  85   Loss :  0.14299492958
Iteration :  86   Loss :  0.137086375597
Iteration :  87   Loss :  0.13142377494
Iteration :  88   Loss :  0.125996899766
Iteration :  89   Loss :  0.120796042654
Iteration :  90   Loss :  0.115811986683
Iteration :  91   Loss :  0.111035967727
Iteration :  92   Loss :  0.106459633392
Iteration :  93   Loss :  0.102075003549
Iteration :  94   Loss :  0.0978744367813
Iteration :  95   Loss :  0.0938506054526
Iteration :  96   Loss :  0.0899964795214
Iteration :  97   Loss :  0.086305316307
Iteration :  98   Loss :  0.0827706511159
Iteration :  99   Loss :  0.0793862830641
[ -7.30131427e-04   9.87346227e-04  -4.12806089e-04 ...,   2.55874560e-04
  -7.80426294e-05   6.15085341e-04]
CROSS VALIDATION 12
Iteration :  0   Loss :  162.528425618
Iteration :  1   Loss :  43.2336373337
Iteration :  2   Loss :  5.50786846083
Iteration :  3   Loss :  5.27917975466
Iteration :  4   Loss :  5.05998635521
Iteration :  5   Loss :  4.8498939465
Iteration :  6   Loss :  4.64852461466
Iteration :  7   Loss :  4.45551616154
Iteration :  8   Loss :  4.27052144554
Iteration :  9   Loss :  4.09320775008
Iteration :  10   Loss :  3.92325617925
Iteration :  11   Loss :  3.76036107983
Iteration :  12   Loss :  3.60422948876
Iteration :  13   Loss :  3.45458060452
Iteration :  14   Loss :  3.31114528136
Iteration :  15   Loss :  3.17366554509
Iteration :  16   Loss :  3.04189412938
Iteration :  17   Loss :  2.91559403137
Iteration :  18   Loss :  2.79453808603
Iteration :  19   Loss :  2.67850855803
Iteration :  20   Loss :  2.56729675063
Iteration :  21   Loss :  2.46070263076
Iteration :  22   Loss :  2.35853446962
Iteration :  23   Loss :  2.26060849814
Iteration :  24   Loss :  2.16674857668
Iteration :  25   Loss :  2.0767858785
Iteration :  26   Loss :  1.9905585862
Iteration :  27   Loss :  1.90791160086
Iteration :  28   Loss :  1.82869626314
Iteration :  29   Loss :  1.752770086
Iteration :  30   Loss :  1.67999649851
Iteration :  31   Loss :  1.61024460018
Iteration :  32   Loss :  1.54338892566
Iteration :  33   Loss :  1.47930921902
Iteration :  34   Loss :  1.41789021755
Iteration :  35   Loss :  1.35902144439
Iteration :  36   Loss :  1.30259700996
Iteration :  37   Loss :  1.2485154214
Iteration :  38   Loss :  1.19667940018
Iteration :  39   Loss :  1.14699570707
Iteration :  40   Loss :  1.09937497454
Iteration :  41   Loss :  1.0537315461
Iteration :  42   Loss :  1.00998332226
Iteration :  43   Loss :  0.96805161299
Iteration :  44   Loss :  0.92786099635
Iteration :  45   Loss :  0.889339182938
Iteration :  46   Loss :  0.852416886097
Iteration :  47   Loss :  0.817027697521
Iteration :  48   Loss :  0.783107968099
Iteration :  49   Loss :  0.750596693775
Iteration :  50   Loss :  0.719435406216
Iteration :  51   Loss :  0.689568068103
Iteration :  52   Loss :  0.660940972837
Iteration :  53   Loss :  0.633502648483
Iteration :  54   Loss :  0.607203765737
Iteration :  55   Loss :  0.581997049731
Iteration :  56   Loss :  0.557837195434
Iteration :  57   Loss :  0.534680786431
Iteration :  58   Loss :  0.51248621681
Iteration :  59   Loss :  0.491213615888
Iteration :  60   Loss :  0.47082477549
Iteration :  61   Loss :  0.451283079506
Iteration :  62   Loss :  0.432553435525
Iteration :  63   Loss :  0.414602208424
Iteration :  64   Loss :  0.397397156003
Iteration :  65   Loss :  0.380907366914
Iteration :  66   Loss :  0.3651032014
Iteration :  67   Loss :  0.349956235419
Iteration :  68   Loss :  0.335439208672
Iteration :  69   Loss :  0.321525976687
Iteration :  70   Loss :  0.308191466504
Iteration :  71   Loss :  0.295411634899
Iteration :  72   Loss :  0.28316342768
Iteration :  73   Loss :  0.271424738731
Iteration :  74   Loss :  0.260174368279
Iteration :  75   Loss :  0.249391981035
Iteration :  76   Loss :  0.239058065925
Iteration :  77   Loss :  0.229153899637
Iteration :  78   Loss :  0.219661515642
Iteration :  79   Loss :  0.210563678998
Iteration :  80   Loss :  0.201843865569
Iteration :  81   Loss :  0.193486243125
Iteration :  82   Loss :  0.185475651667
Iteration :  83   Loss :  0.17779758125
Iteration :  84   Loss :  0.170438147045
Iteration :  85   Loss :  0.163384062624
Iteration :  86   Loss :  0.156622612994
Iteration :  87   Loss :  0.150141628715
Iteration :  88   Loss :  0.143929461803
Iteration :  89   Loss :  0.137974963426
Iteration :  90   Loss :  0.132267462977
Iteration :  91   Loss :  0.126796748019
Iteration :  92   Loss :  0.121553044646
Iteration :  93   Loss :  0.116526998099
Iteration :  94   Loss :  0.111709653581
Iteration :  95   Loss :  0.107092437344
Iteration :  96   Loss :  0.102667138095
Iteration :  97   Loss :  0.0984258887496
Iteration :  98   Loss :  0.0943611484747
Iteration :  99   Loss :  0.090465685008
[-0.00491995 -0.00096498 -0.0017627  ...,  0.00157219  0.0010228
  0.00011345]
CROSS VALIDATION 13
Iteration :  0   Loss :  229.034583244
Iteration :  1   Loss :  70.3431452734
Iteration :  2   Loss :  5.47029339434
Iteration :  3   Loss :  5.24315685264
Iteration :  4   Loss :  5.0254514337
Iteration :  5   Loss :  4.81678554014
Iteration :  6   Loss :  4.61678383441
Iteration :  7   Loss :  4.42508656365
Iteration :  8   Loss :  4.24134891261
Iteration :  9   Loss :  4.06524038337
Iteration :  10   Loss :  3.89644420092
Iteration :  11   Loss :  3.73465674329
Iteration :  12   Loss :  3.57958699548
Iteration :  13   Loss :  3.43095602594
Iteration :  14   Loss :  3.28849648487
Iteration :  15   Loss :  3.15195212333
Iteration :  16   Loss :  3.02107733229
Iteration :  17   Loss :  2.89563670086
Iteration :  18   Loss :  2.77540459285
Iteration :  19   Loss :  2.66016474087
Iteration :  20   Loss :  2.54970985737
Iteration :  21   Loss :  2.44384126174
Iteration :  22   Loss :  2.34236852297
Iteration :  23   Loss :  2.24510911707
Iteration :  24   Loss :  2.15188809879
Iteration :  25   Loss :  2.06253778694
Iteration :  26   Loss :  1.97689746273
Iteration :  27   Loss :  1.89481308075
Iteration :  28   Loss :  1.81613699181
Iteration :  29   Loss :  1.7407276774
Iteration :  30   Loss :  1.66844949514
Iteration :  31   Loss :  1.59917243477
Iteration :  32   Loss :  1.53277188433
Iteration :  33   Loss :  1.469128406
Iteration :  34   Loss :  1.4081275213
Iteration :  35   Loss :  1.34965950517
Iteration :  36   Loss :  1.29361918863
Iteration :  37   Loss :  1.23990576964
Iteration :  38   Loss :  1.18842263184
Iteration :  39   Loss :  1.13907717077
Iteration :  40   Loss :  1.09178062744
Iteration :  41   Loss :  1.0464479287
Iteration :  42   Loss :  1.00299753437
Iteration :  43   Loss :  0.961351290697
Iteration :  44   Loss :  0.921434289968
Iteration :  45   Loss :  0.883174735991
Iteration :  46   Loss :  0.846503815229
Iteration :  47   Loss :  0.811355573344
Iteration :  48   Loss :  0.777666796955
Iteration :  49   Loss :  0.74537690038
Iteration :  50   Loss :  0.714427817159
Iteration :  51   Loss :  0.684763896155
Iteration :  52   Loss :  0.656331802011
Iteration :  53   Loss :  0.629080419729
Iteration :  54   Loss :  0.602960763138
Iteration :  55   Loss :  0.577925886954
Iteration :  56   Loss :  0.55393080212
Iteration :  57   Loss :  0.530932394076
Iteration :  58   Loss :  0.508889343577
Iteration :  59   Loss :  0.487762049686
Iteration :  60   Loss :  0.467512554676
Iteration :  61   Loss :  0.448104470724
Iteration :  62   Loss :  0.429502908626
Iteration :  63   Loss :  0.411674409187
Iteration :  64   Loss :  0.394586878366
Iteration :  65   Loss :  0.37820952757
Iteration :  66   Loss :  0.362512820353
Iteration :  67   Loss :  0.347468426123
Iteration :  68   Loss :  0.333049180235
Iteration :  69   Loss :  0.319229048508
Iteration :  70   Loss :  0.305983093232
Iteration :  71   Loss :  0.293287437636
Iteration :  72   Loss :  0.281119226611
Iteration :  73   Loss :  0.269456582936
Iteration :  74   Loss :  0.258278559634
Iteration :  75   Loss :  0.2475650901
Iteration :  76   Loss :  0.237296938084
Iteration :  77   Loss :  0.227455649659
Iteration :  78   Loss :  0.218023509222
Iteration :  79   Loss :  0.208983501294
Iteration :  80   Loss :  0.200319279462
Iteration :  81   Loss :  0.192015142933
Iteration :  82   Loss :  0.184056020019
Iteration :  83   Loss :  0.176427456529
Iteration :  84   Loss :  0.169115606155
Iteration :  85   Loss :  0.162107219763
Iteration :  86   Loss :  0.155389631288
Iteration :  87   Loss :  0.148950739283
Iteration :  88   Loss :  0.142778984623
Iteration :  89   Loss :  0.136863325804
Iteration :  90   Loss :  0.131193213635
Iteration :  91   Loss :  0.125758566823
Iteration :  92   Loss :  0.120549749437
Iteration :  93   Loss :  0.115557550578
Iteration :  94   Loss :  0.110773166101
Iteration :  95   Loss :  0.10618818184
Iteration :  96   Loss :  0.101794557601
Iteration :  97   Loss :  0.0975846110684
Iteration :  98   Loss :  0.0935510007671
Iteration :  99   Loss :  0.0896867073673
[-0.00307366 -0.00116748 -0.00169976 ...,  0.00133383 -0.00034004
  0.00036582]
CROSS VALIDATION 14
Iteration :  0   Loss :  64.2612384827
Iteration :  1   Loss :  5.03292818282
Iteration :  2   Loss :  4.82395184103
Iteration :  3   Loss :  4.62365257744
Iteration :  4   Loss :  4.43167010396
Iteration :  5   Loss :  4.24765909234
Iteration :  6   Loss :  4.07128855295
Iteration :  7   Loss :  3.90224123948
Iteration :  8   Loss :  3.74021307826
Iteration :  9   Loss :  3.5849126213
Iteration :  10   Loss :  3.43606052208
Iteration :  11   Loss :  3.29338903304
Iteration :  12   Loss :  3.15664152399
Iteration :  13   Loss :  3.02557202049
Iteration :  14   Loss :  2.89994476143
Iteration :  15   Loss :  2.77953377489
Iteration :  16   Loss :  2.66412247176
Iteration :  17   Loss :  2.55350325606
Iteration :  18   Loss :  2.44747715161
Iteration :  19   Loss :  2.34585344406
Iteration :  20   Loss :  2.24844933788
Iteration :  21   Loss :  2.15508962753
Iteration :  22   Loss :  2.06560638235
Iteration :  23   Loss :  1.97983864442
Iteration :  24   Loss :  1.89763213914
Iteration :  25   Loss :  1.81883899763
Iteration :  26   Loss :  1.74331749082
Iteration :  27   Loss :  1.67093177448
Iteration :  28   Loss :  1.60155164487
Iteration :  29   Loss :  1.53505230457
Iteration :  30   Loss :  1.47131413796
Iteration :  31   Loss :  1.41022249607
Iteration :  32   Loss :  1.35166749039
Iteration :  33   Loss :  1.29554379517
Iteration :  34   Loss :  1.24175045795
Iteration :  35   Loss :  1.19019071805
Iteration :  36   Loss :  1.14077183243
Iteration :  37   Loss :  1.09340490893
Iteration :  38   Loss :  1.04800474636
Iteration :  39   Loss :  1.00448968123
Iteration :  40   Loss :  0.962781440881
Iteration :  41   Loss :  0.922805002686
Iteration :  42   Loss :  0.884488459104
Iteration :  43   Loss :  0.847762888346
Iteration :  44   Loss :  0.81256223041
Iteration :  45   Loss :  0.778823168263
Iteration :  46   Loss :  0.746485013966
Iteration :  47   Loss :  0.715489599533
Iteration :  48   Loss :  0.685781172327
Iteration :  49   Loss :  0.65730629481
Iteration :  50   Loss :  0.630013748477
Iteration :  51   Loss :  0.603854441783
Iteration :  52   Loss :  0.578781321931
Iteration :  53   Loss :  0.554749290341
Iteration :  54   Loss :  0.53171512167
Iteration :  55   Loss :  0.509637386243
Iteration :  56   Loss :  0.488476375753
Iteration :  57   Loss :  0.468194032121
Iteration :  58   Loss :  0.448753879387
Iteration :  59   Loss :  0.430120958527
Iteration :  60   Loss :  0.412261765082
Iteration :  61   Loss :  0.395144189507
Iteration :  62   Loss :  0.378737460125
Iteration :  63   Loss :  0.363012088581
Iteration :  64   Loss :  0.347939817699
Iteration :  65   Loss :  0.333493571591
Iteration :  66   Loss :  0.31964740788
Iteration :  67   Loss :  0.306376471849
Iteration :  68   Loss :  0.29365695228
Iteration :  69   Loss :  0.281466038685
Iteration :  70   Loss :  0.269781879595
Iteration :  71   Loss :  0.258583541493
Iteration :  72   Loss :  0.247850967998
Iteration :  73   Loss :  0.237564938955
Iteration :  74   Loss :  0.227707029298
Iteration :  75   Loss :  0.218259567873
Iteration :  76   Loss :  0.209205596917
Iteration :  77   Loss :  0.200528833403
Iteration :  78   Loss :  0.192213633886
Iteration :  79   Loss :  0.184244964552
Iteration :  80   Loss :  0.176608377695
Iteration :  81   Loss :  0.16928999492
Iteration :  82   Loss :  0.162276496187
Iteration :  83   Loss :  0.155555112836
Iteration :  84   Loss :  0.149113622297
Iteration :  85   Loss :  0.142940342249
Iteration :  86   Loss :  0.137024122387
Iteration :  87   Loss :  0.131354332333
Iteration :  88   Loss :  0.125920844667
Iteration :  89   Loss :  0.120714012555
Iteration :  90   Loss :  0.115724642242
Iteration :  91   Loss :  0.110943961698
Iteration :  92   Loss :  0.106363587755
Iteration :  93   Loss :  0.101975494809
Iteration :  94   Loss :  0.0977719882345
Iteration :  95   Loss :  0.0937456847488
Iteration :  96   Loss :  0.0898894999184
Iteration :  97   Loss :  0.0861966403694
Iteration :  98   Loss :  0.0826605959447
Iteration :  99   Loss :  0.0792751263337
[ -7.19581065e-04   9.93643790e-04  -4.07624628e-04 ...,   2.49074626e-04
  -8.47419359e-05   6.13919340e-04]
CROSS VALIDATION 15
Iteration :  0   Loss :  64.2612384827
Iteration :  1   Loss :  5.03292818282
Iteration :  2   Loss :  4.82395184103
Iteration :  3   Loss :  4.62365257744
Iteration :  4   Loss :  4.43167010396
Iteration :  5   Loss :  4.24765909234
Iteration :  6   Loss :  4.07128855295
Iteration :  7   Loss :  3.90224123948
Iteration :  8   Loss :  3.74021307826
Iteration :  9   Loss :  3.5849126213
Iteration :  10   Loss :  3.43606052208
Iteration :  11   Loss :  3.29338903304
Iteration :  12   Loss :  3.15664152399
Iteration :  13   Loss :  3.02557202049
Iteration :  14   Loss :  2.89994476143
Iteration :  15   Loss :  2.77953377489
Iteration :  16   Loss :  2.66412247176
Iteration :  17   Loss :  2.55350325606
Iteration :  18   Loss :  2.44747715161
Iteration :  19   Loss :  2.34585344406
Iteration :  20   Loss :  2.24844933788
Iteration :  21   Loss :  2.15508962753
Iteration :  22   Loss :  2.06560638235
Iteration :  23   Loss :  1.97983864442
Iteration :  24   Loss :  1.89763213914
Iteration :  25   Loss :  1.81883899763
Iteration :  26   Loss :  1.74331749082
Iteration :  27   Loss :  1.67093177448
Iteration :  28   Loss :  1.60155164487
Iteration :  29   Loss :  1.53505230457
Iteration :  30   Loss :  1.47131413796
Iteration :  31   Loss :  1.41022249608
Iteration :  32   Loss :  1.3516674904
Iteration :  33   Loss :  1.29554379517
Iteration :  34   Loss :  1.24175045796
Iteration :  35   Loss :  1.19019071805
Iteration :  36   Loss :  1.14077183244
Iteration :  37   Loss :  1.09340490895
Iteration :  38   Loss :  1.04800474639
Iteration :  39   Loss :  1.00448968129
Iteration :  40   Loss :  0.962781440977
Iteration :  41   Loss :  0.922805002842
Iteration :  42   Loss :  0.884488459354
Iteration :  43   Loss :  0.847762888744
Iteration :  44   Loss :  0.812562231037
Iteration :  45   Loss :  0.778823169241
Iteration :  46   Loss :  0.746485015479
Iteration :  47   Loss :  0.71548960185
Iteration :  48   Loss :  0.685781175845
Iteration :  49   Loss :  0.657306300105
Iteration :  50   Loss :  0.630013756377
Iteration :  51   Loss :  0.603854453471
Iteration :  52   Loss :  0.578781339082
Iteration :  53   Loss :  0.554749315306
Iteration :  54   Loss :  0.531715157726
Iteration :  55   Loss :  0.509637437914
Iteration :  56   Loss :  0.488476449244
Iteration :  57   Loss :  0.468194135871
Iteration :  58   Loss :  0.448754024792
Iteration :  59   Loss :  0.430121160859
Iteration :  60   Loss :  0.412262044658
Iteration :  61   Loss :  0.395144573156
Iteration :  62   Loss :  0.378737983016
Iteration :  63   Loss :  0.363012796477
Iteration :  64   Loss :  0.347940769703
Iteration :  65   Loss :  0.333494843454
Iteration :  66   Loss :  0.319649095921
Iteration :  67   Loss :  0.306378697521
Iteration :  68   Loss :  0.293659867377
Iteration :  69   Loss :  0.281469831111
Iteration :  70   Loss :  0.269786779508
Iteration :  71   Loss :  0.258589827503
Iteration :  72   Loss :  0.247858972879
Iteration :  73   Loss :  0.237575054096
Iteration :  74   Loss :  0.227719706821
Iteration :  75   Loss :  0.218275319085
Iteration :  76   Loss :  0.209224985567
Iteration :  77   Loss :  0.200552462258
Iteration :  78   Loss :  0.192242123525
Iteration :  79   Loss :  0.184278924157
Iteration :  80   Loss :  0.176648369032
Iteration :  81   Loss :  0.169336492451
Iteration :  82   Loss :  0.162329847965
Iteration :  83   Loss :  0.155615507861
Iteration :  84   Loss :  0.149181069766
Iteration :  85   Loss :  0.143014666404
Iteration :  86   Loss :  0.137104973737
Iteration :  87   Loss :  0.131441212823
Iteration :  88   Loss :  0.126013141852
Iteration :  89   Loss :  0.120811036793
Iteration :  90   Loss :  0.115825661423
Iteration :  91   Loss :  0.111048229631
Iteration :  92   Loss :  0.106470364338
Iteration :  93   Loss :  0.102084057918
Iteration :  94   Loss :  0.0978816385075
Iteration :  95   Loss :  0.0938557451394
Iteration :  96   Loss :  0.0899993121427
Iteration :  97   Loss :  0.0863055604128
Iteration :  98   Loss :  0.0827679907125
Iteration :  99   Loss :  0.0793803733396
[ -7.29852049e-04   9.88838783e-04  -4.12192411e-04 ...,   2.55870585e-04
  -7.79763473e-05   6.15450285e-04]
CROSS VALIDATION 16
Iteration :  0   Loss :  119.082692177
Iteration :  1   Loss :  4.5825826104
Iteration :  2   Loss :  4.38170142463
Iteration :  3   Loss :  4.19976520369
Iteration :  4   Loss :  4.02538330599
Iteration :  5   Loss :  3.85824206231
Iteration :  6   Loss :  3.69804082758
Iteration :  7   Loss :  3.54449144007
Iteration :  8   Loss :  3.39731770311
Iteration :  9   Loss :  3.25625488826
Iteration :  10   Loss :  3.12104925917
Iteration :  11   Loss :  2.9914576152
Iteration :  12   Loss :  2.86724685398
Iteration :  13   Loss :  2.74819355216
Iteration :  14   Loss :  2.63408356362
Iteration :  15   Loss :  2.52471163431
Iteration :  16   Loss :  2.41988103317
Iteration :  17   Loss :  2.31940319836
Iteration :  18   Loss :  2.22309739827
Iteration :  19   Loss :  2.1307904066
Iteration :  20   Loss :  2.04231619099
Iteration :  21   Loss :  1.95751561465
Iteration :  22   Loss :  1.87623615051
Iteration :  23   Loss :  1.79833160718
Iteration :  24   Loss :  1.7236618665
Iteration :  25   Loss :  1.65209263202
Iteration :  26   Loss :  1.58349518799
Iteration :  27   Loss :  1.51774616846
Iteration :  28   Loss :  1.45472733595
Iteration :  29   Loss :  1.39432536925
Iteration :  30   Loss :  1.33643165994
Iteration :  31   Loss :  1.28094211702
Iteration :  32   Loss :  1.22775697911
Iteration :  33   Loss :  1.17678063371
Iteration :  34   Loss :  1.12792144287
Iteration :  35   Loss :  1.08109157468
Iteration :  36   Loss :  1.03620684011
Iteration :  37   Loss :  0.993186534984
Iteration :  38   Loss :  0.951953287185
Iteration :  39   Loss :  0.912432909646
Iteration :  40   Loss :  0.874554260382
Iteration :  41   Loss :  0.83824911102
Iteration :  42   Loss :  0.803452025332
Iteration :  43   Loss :  0.770100248586
Iteration :  44   Loss :  0.738133607269
Iteration :  45   Loss :  0.707494417152
Iteration :  46   Loss :  0.678127396528
Iteration :  47   Loss :  0.649979581073
Iteration :  48   Loss :  0.623000237657
Iteration :  49   Loss :  0.59714077591
Iteration :  50   Loss :  0.572354658027
Iteration :  51   Loss :  0.548597308392
Iteration :  52   Loss :  0.525826024942
Iteration :  53   Loss :  0.503999893933
Iteration :  54   Loss :  0.483079709121
Iteration :  55   Loss :  0.463027895751
Iteration :  56   Loss :  0.443808439265
Iteration :  57   Loss :  0.425386818373
Iteration :  58   Loss :  0.407729941999
Iteration :  59   Loss :  0.39080608967
Iteration :  60   Loss :  0.374584854932
Iteration :  61   Loss :  0.359037091498
Iteration :  62   Loss :  0.344134861878
Iteration :  63   Loss :  0.329851388327
Iteration :  64   Loss :  0.316161005983
Iteration :  65   Loss :  0.303039118093
Iteration :  66   Loss :  0.290462153265
Iteration :  67   Loss :  0.278407524681
Iteration :  68   Loss :  0.266853591201
Iteration :  69   Loss :  0.255779620303
Iteration :  70   Loss :  0.245165752753
Iteration :  71   Loss :  0.23499296889
Iteration :  72   Loss :  0.225243056348
Iteration :  73   Loss :  0.215898578947
Iteration :  74   Loss :  0.20694284642
Iteration :  75   Loss :  0.198359884511
Iteration :  76   Loss :  0.190134404859
Iteration :  77   Loss :  0.182251774042
Iteration :  78   Loss :  0.174697981122
Iteration :  79   Loss :  0.167459603244
Iteration :  80   Loss :  0.160523769218
Iteration :  81   Loss :  0.153878121703
Iteration :  82   Loss :  0.147510779447
Iteration :  83   Loss :  0.141410301891
Iteration :  84   Loss :  0.135565658851
Iteration :  85   Loss :  0.129966207645
Iteration :  86   Loss :  0.124601678753
Iteration :  87   Loss :  0.119462169209
Iteration :  88   Loss :  0.114538141253
Iteration :  89   Loss :  0.109820423151
Iteration :  90   Loss :  0.105300209882
Iteration :  91   Loss :  0.100969063063
Iteration :  92   Loss :  0.0968189108482
Iteration :  93   Loss :  0.0928420486509
Iteration :  94   Loss :  0.0890311402493
Iteration :  95   Loss :  0.0853792170219
Iteration :  96   Loss :  0.0818796719459
Iteration :  97   Loss :  0.0785262453586
Iteration :  98   Loss :  0.0753130012645
Iteration :  99   Loss :  0.0722342954604
[-0.00151073  0.00101061 -0.0015165  ...,  0.00111192 -0.00043326
  0.00052475]
CROSS VALIDATION 17
Iteration :  0   Loss :  64.2637606255
Iteration :  1   Loss :  5.03290195444
Iteration :  2   Loss :  4.8239267017
Iteration :  3   Loss :  4.62362848194
Iteration :  4   Loss :  4.43164700895
Iteration :  5   Loss :  4.24763695627
Iteration :  6   Loss :  4.07126733601
Iteration :  7   Loss :  3.90222090351
Iteration :  8   Loss :  3.74019358667
Iteration :  9   Loss :  3.58489393904
Iteration :  10   Loss :  3.43604261554
Iteration :  11   Loss :  3.29337187001
Iteration :  12   Loss :  3.1566250736
Iteration :  13   Loss :  3.02555625316
Iteration :  14   Loss :  2.89992964878
Iteration :  15   Loss :  2.77951928975
Iteration :  16   Loss :  2.66410858807
Iteration :  17   Loss :  2.55348994885
Iteration :  18   Loss :  2.44746439693
Iteration :  19   Loss :  2.34584121898
Iteration :  20   Loss :  2.24843762041
Iteration :  21   Loss :  2.15507839659
Iteration :  22   Loss :  2.06559561773
Iteration :  23   Loss :  1.97982832678
Iteration :  24   Loss :  1.8976222499
Iteration :  25   Loss :  1.81882951901
Iteration :  26   Loss :  1.74330840577
Iteration :  27   Loss :  1.67092306666
Iteration :  28   Loss :  1.60154329862
Iteration :  29   Loss :  1.53504430487
Iteration :  30   Loss :  1.47130647042
Iteration :  31   Loss :  1.4102151469
Iteration :  32   Loss :  1.35166044637
Iteration :  33   Loss :  1.29553704363
Iteration :  34   Loss :  1.24174398675
Iteration :  35   Loss :  1.19018451555
Iteration :  36   Loss :  1.14076588747
Iteration :  37   Loss :  1.09339921083
Iteration :  38   Loss :  1.04799928487
Iteration :  39   Loss :  1.00448444653
Iteration :  40   Loss :  0.962776423579
Iteration :  41   Loss :  0.922800193775
Iteration :  42   Loss :  0.884483849969
Iteration :  43   Loss :  0.847758470749
Iteration :  44   Loss :  0.812557996484
Iteration :  45   Loss :  0.778819110513
Iteration :  46   Loss :  0.746481125275
Iteration :  47   Loss :  0.715485873172
Iteration :  48   Loss :  0.685777601985
Iteration :  49   Loss :  0.657302874634
Iteration :  50   Loss :  0.630010473131
Iteration :  51   Loss :  0.603851306542
Iteration :  52   Loss :  0.578778322807
Iteration :  53   Loss :  0.554746424255
Iteration :  54   Loss :  0.531712386693
Iteration :  55   Loss :  0.509634781908
Iteration :  56   Loss :  0.488473903478
Iteration :  57   Loss :  0.468191695755
Iteration :  58   Loss :  0.448751685923
Iteration :  59   Loss :  0.430118919013
Iteration :  60   Loss :  0.412259895782
Iteration :  61   Loss :  0.395142513361
Iteration :  62   Loss :  0.37873600857
Iteration :  63   Loss :  0.363010903803
Iteration :  64   Loss :  0.34793895537
Iteration :  65   Loss :  0.333493104177
Iteration :  66   Loss :  0.319647428564
Iteration :  67   Loss :  0.306377099104
Iteration :  68   Loss :  0.293658335085
Iteration :  69   Loss :  0.281468362316
Iteration :  70   Loss :  0.269785371804
Iteration :  71   Loss :  0.258588478753
Iteration :  72   Loss :  0.247857681286
Iteration :  73   Loss :  0.237573818302
Iteration :  74   Loss :  0.227718526035
Iteration :  75   Loss :  0.218274193258
Iteration :  76   Loss :  0.209223915614
Iteration :  77   Loss :  0.200551450336
Iteration :  78   Loss :  0.192241173373
Iteration :  79   Loss :  0.184278041509
Iteration :  80   Loss :  0.176647562112
Iteration :  81   Loss :  0.169335772567
Iteration :  82   Loss :  0.162329230235
Iteration :  83   Loss :  0.155615012096
Iteration :  84   Loss :  0.149180721576
Iteration :  85   Loss :  0.143014498569
Iteration :  86   Loss :  0.137105027882
Iteration :  87   Loss :  0.131441541432
Iteration :  88   Loss :  0.126013810617
Iteration :  89   Loss :  0.120812127283
Iteration :  90   Loss :  0.115827274034
Iteration :  91   Loss :  0.111050486765
Iteration :  92   Loss :  0.106473413722
Iteration :  93   Loss :  0.102088075909
Iteration :  94   Loss :  0.0978868331694
Iteration :  95   Loss :  0.0938623587061
Iteration :  96   Loss :  0.0900076223603
Iteration :  97   Loss :  0.0863158800859
Iteration :  98   Loss :  0.0827806646835
Iteration :  99   Loss :  0.0793957721464
[ -7.29827242e-04   9.88396802e-04  -4.12227722e-04 ...,   2.55874670e-04
  -7.79722295e-05   6.15417702e-04]
CROSS VALIDATION 18
Iteration :  0   Loss :  132.182331667
Iteration :  1   Loss :  140.296627035
Iteration :  2   Loss :  4.7848569441
Iteration :  3   Loss :  4.58615989077
Iteration :  4   Loss :  4.39572725241
Iteration :  5   Loss :  4.21320843002
Iteration :  6   Loss :  4.03827203664
Iteration :  7   Loss :  3.87060191207
Iteration :  8   Loss :  3.70989559972
Iteration :  9   Loss :  3.55586345593
Iteration :  10   Loss :  3.40822798558
Iteration :  11   Loss :  3.266723282
Iteration :  12   Loss :  3.131094525
Iteration :  13   Loss :  3.00109751593
Iteration :  14   Loss :  2.87649823785
Iteration :  15   Loss :  2.75707243415
Iteration :  16   Loss :  2.64260520185
Iteration :  17   Loss :  2.53289059881
Iteration :  18   Loss :  2.42773126472
Iteration :  19   Loss :  2.326938057
Iteration :  20   Loss :  2.23032970205
Iteration :  21   Loss :  2.13773246217
Iteration :  22   Loss :  2.04897981766
Iteration :  23   Loss :  1.96391216377
Iteration :  24   Loss :  1.88237652139
Iteration :  25   Loss :  1.8042262607
Iteration :  26   Loss :  1.72932083706
Iteration :  27   Loss :  1.65752553825
Iteration :  28   Loss :  1.58871124246
Iteration :  29   Loss :  1.52275418652
Iteration :  30   Loss :  1.45953574374
Iteration :  31   Loss :  1.39894221105
Iteration :  32   Loss :  1.34086460489
Iteration :  33   Loss :  1.28519846559
Iteration :  34   Loss :  1.23184366983
Iteration :  35   Loss :  1.18070425089
Iteration :  36   Loss :  1.13168822622
Iteration :  37   Loss :  1.08470743236
Iteration :  38   Loss :  1.03967736648
Iteration :  39   Loss :  0.996517034712
Iteration :  40   Loss :  0.95514880665
Iteration :  41   Loss :  0.915498275986
Iteration :  42   Loss :  0.877494126961
Iteration :  43   Loss :  0.841068006405
Iteration :  44   Loss :  0.806154401156
Iteration :  45   Loss :  0.772690520644
Iteration :  46   Loss :  0.740616184437
Iteration :  47   Loss :  0.709873714546
Iteration :  48   Loss :  0.680407832306
Iteration :  49   Loss :  0.65216555964
Iteration :  50   Loss :  0.625096124496
Iteration :  51   Loss :  0.599150870292
Iteration :  52   Loss :  0.574283169115
Iteration :  53   Loss :  0.550448338474
Iteration :  54   Loss :  0.52760356133
Iteration :  55   Loss :  0.505707809113
Iteration :  56   Loss :  0.484721767397
Iteration :  57   Loss :  0.464607763876
Iteration :  58   Loss :  0.445329698256
Iteration :  59   Loss :  0.426852973705
Iteration :  60   Loss :  0.409144429579
Iteration :  61   Loss :  0.392172275306
Iteration :  62   Loss :  0.375906025531
Iteration :  63   Loss :  0.360316437025
Iteration :  64   Loss :  0.345375448246
Iteration :  65   Loss :  0.331056122806
Iteration :  66   Loss :  0.317332598357
Iteration :  67   Loss :  0.304180042239
Iteration :  68   Loss :  0.291574614659
Iteration :  69   Loss :  0.279493439094
Iteration :  70   Loss :  0.267914578226
Iteration :  71   Loss :  0.256817012605
Iteration :  72   Loss :  0.246180618685
Iteration :  73   Loss :  0.235986143399
Iteration :  74   Loss :  0.226215173797
Iteration :  75   Loss :  0.216850101947
Iteration :  76   Loss :  0.20787408668
Iteration :  77   Loss :  0.199271014373
Iteration :  78   Loss :  0.191025460745
Iteration :  79   Loss :  0.183122654933
Iteration :  80   Loss :  0.175548446271
Iteration :  81   Loss :  0.168289273557
Iteration :  82   Loss :  0.161332136364
Iteration :  83   Loss :  0.154664567995
Iteration :  84   Loss :  0.148274609991
Iteration :  85   Loss :  0.142150788337
Iteration :  86   Loss :  0.136282091674
Iteration :  87   Loss :  0.130657951741
Iteration :  88   Loss :  0.12526822608
Iteration :  89   Loss :  0.120103182846
Iteration :  90   Loss :  0.115153487484
Iteration :  91   Loss :  0.110410191109
Iteration :  92   Loss :  0.105864720549
Iteration :  93   Loss :  0.101508869944
Iteration :  94   Loss :  0.097334793506
Iteration :  95   Loss :  0.0933349984218
Iteration :  96   Loss :  0.0895023363151
Iteration :  97   Loss :  0.0858299913936
Iteration :  98   Loss :  0.082311463742
Iteration :  99   Loss :  0.0789405472228
[ -1.25516989e-03  -5.38934477e-04  -1.14658904e-03 ...,   1.18633349e-03
  -6.86680255e-05   4.92214482e-04]
CROSS VALIDATION 19
Iteration :  0   Loss :  119.019668412
Iteration :  1   Loss :  4.95771160963
Iteration :  2   Loss :  4.75185839691
Iteration :  3   Loss :  4.55455258439
Iteration :  4   Loss :  4.36543926845
Iteration :  5   Loss :  4.18417828172
Iteration :  6   Loss :  4.0104435812
Iteration :  7   Loss :  3.84392266179
Iteration :  8   Loss :  3.68431599417
Iteration :  9   Loss :  3.53133648603
Iteration :  10   Loss :  3.38470896568
Iteration :  11   Loss :  3.24416968704
Iteration :  12   Loss :  3.10946585529
Iteration :  13   Loss :  2.98035517212
Iteration :  14   Loss :  2.85660539994
Iteration :  15   Loss :  2.73799394412
Iteration :  16   Loss :  2.62430745264
Iteration :  17   Loss :  2.51534143234
Iteration :  18   Loss :  2.41089988113
Iteration :  19   Loss :  2.31079493547
Iteration :  20   Loss :  2.21484653249
Iteration :  21   Loss :  2.1228820862
Iteration :  22   Loss :  2.03473617716
Iteration :  23   Loss :  1.95025025501
Iteration :  24   Loss :  1.8692723535
Iteration :  25   Loss :  1.7916568173
Iteration :  26   Loss :  1.71726404027
Iteration :  27   Loss :  1.64596021465
Iteration :  28   Loss :  1.57761709078
Iteration :  29   Loss :  1.51211174685
Iteration :  30   Loss :  1.44932636837
Iteration :  31   Loss :  1.3891480369
Iteration :  32   Loss :  1.33146852768
Iteration :  33   Loss :  1.27618411583
Iteration :  34   Loss :  1.22319539066
Iteration :  35   Loss :  1.17240707781
Iteration :  36   Loss :  1.12372786878
Iteration :  37   Loss :  1.07707025746
Iteration :  38   Loss :  1.03235038309
Iteration :  39   Loss :  0.989487879358
Iteration :  40   Loss :  0.94840572888
Iteration :  41   Loss :  0.909030122578
Iteration :  42   Loss :  0.871290323368
Iteration :  43   Loss :  0.835118533674
Iteration :  44   Loss :  0.800449766519
Iteration :  45   Loss :  0.767221720265
Iteration :  46   Loss :  0.735374657578
Iteration :  47   Loss :  0.704851289606
Iteration :  48   Loss :  0.675596666669
Iteration :  49   Loss :  0.647558076587
Iteration :  50   Loss :  0.620684951189
Iteration :  51   Loss :  0.59492878053
Iteration :  52   Loss :  0.570243033535
Iteration :  53   Loss :  0.546583083491
Iteration :  54   Loss :  0.52390613735
Iteration :  55   Loss :  0.502171168766
Iteration :  56   Loss :  0.481338855435
Iteration :  57   Loss :  0.461371521009
Iteration :  58   Loss :  0.442233080719
Iteration :  59   Loss :  0.423888988543
Iteration :  60   Loss :  0.406306183354
Iteration :  61   Loss :  0.389453032137
Iteration :  62   Loss :  0.373299269914
Iteration :  63   Loss :  0.357815937458
Iteration :  64   Loss :  0.342975318674
Iteration :  65   Loss :  0.328750879441
Iteration :  66   Loss :  0.315117209066
Iteration :  67   Loss :  0.302049964663
Iteration :  68   Loss :  0.28952581813
Iteration :  69   Loss :  0.277522405113
Iteration :  70   Loss :  0.26601827544
Iteration :  71   Loss :  0.254992844938
Iteration :  72   Loss :  0.244426349185
Iteration :  73   Loss :  0.234299800394
Iteration :  74   Loss :  0.224594948915
Iteration :  75   Loss :  0.215294250681
Iteration :  76   Loss :  0.20638084103
Iteration :  77   Loss :  0.197838514185
Iteration :  78   Loss :  0.189651706653
Iteration :  79   Loss :  0.181805482553
Iteration :  80   Loss :  0.174285519526
Iteration :  81   Loss :  0.167078094881
Iteration :  82   Loss :  0.160170072333
Iteration :  83   Loss :  0.153548889475
Iteration :  84   Loss :  0.147202545339
Iteration :  85   Loss :  0.141119586569
Iteration :  86   Loss :  0.135289090561
Iteration :  87   Loss :  0.129700644568
Iteration :  88   Loss :  0.12434432076
Iteration :  89   Loss :  0.119210648058
Iteration :  90   Loss :  0.114290581885
Iteration :  91   Loss :  0.109575472764
Iteration :  92   Loss :  0.105057034359
Iteration :  93   Loss :  0.100727311309
Iteration :  94   Loss :  0.0965786474553
Iteration :  95   Loss :  0.0926036556228
Iteration :  96   Loss :  0.0887951910101
Iteration :  97   Loss :  0.0851463308638
Iteration :  98   Loss :  0.0816503631464
Iteration :  99   Loss :  0.0783007859446
[ -1.56115274e-03  -8.89297733e-05  -9.95959898e-04 ...,   4.04578450e-04
   9.71739599e-05   5.61358746e-04]
Accuracy (Logistic Loss):	0.9
Hinge Loss
CROSS VALIDATION 0
Iteration :  0   Loss :  194.144612804
Iteration :  1   Loss :  5.49193566105
Iteration :  2   Loss :  5.26390049303
Iteration :  3   Loss :  5.04533376037
Iteration :  4   Loss :  4.83584231641
Iteration :  5   Loss :  4.63504933863
Iteration :  6   Loss :  4.44259365088
Iteration :  7   Loss :  4.2581290737
Iteration :  8   Loss :  4.08132380162
Iteration :  9   Loss :  3.91185980636
Iteration :  10   Loss :  3.74943226473
Iteration :  11   Loss :  3.59374901036
Iteration :  12   Loss :  3.44453000818
Iteration :  13   Loss :  3.30150685065
Iteration :  14   Loss :  3.16442227503
Iteration :  15   Loss :  3.03302970059
Iteration :  16   Loss :  2.90709278507
Iteration :  17   Loss :  2.78638499958
Iteration :  18   Loss :  2.67068922112
Iteration :  19   Loss :  2.55979734203
Iteration :  20   Loss :  2.45350989567
Iteration :  21   Loss :  2.35163569761
Iteration :  22   Loss :  2.25399150173
Iteration :  23   Loss :  2.16040167065
Iteration :  24   Loss :  2.07069785976
Iteration :  25   Loss :  1.98471871442
Iteration :  26   Loss :  1.90230957975
Iteration :  27   Loss :  1.82332222239
Iteration :  28   Loss :  1.74761456392
Iteration :  29   Loss :  1.67505042528
Iteration :  30   Loss :  1.6054992818
Iteration :  31   Loss :  1.53883602842
Iteration :  32   Loss :  1.47494075471
Iteration :  33   Loss :  1.41369852909
Iteration :  34   Loss :  1.35499919219
Iteration :  35   Loss :  1.29873715863
Iteration :  36   Loss :  1.24481122714
Iteration :  37   Loss :  1.19312439852
Iteration :  38   Loss :  1.14358370112
Iteration :  39   Loss :  1.09610002369
Iteration :  40   Loss :  1.05058795499
Iteration :  41   Loss :  1.00696563026
Iteration :  42   Loss :  0.965154583867
Iteration :  43   Loss :  0.925079608251
Iteration :  44   Loss :  0.886668618589
Iteration :  45   Loss :  0.849852523154
Iteration :  46   Loss :  0.814565099034
Iteration :  47   Loss :  0.780742873013
Iteration :  48   Loss :  0.748325007397
Iteration :  49   Loss :  0.717253190586
Iteration :  50   Loss :  0.687471532183
Iteration :  51   Loss :  0.658926462461
Iteration :  52   Loss :  0.631566636007
Iteration :  53   Loss :  0.605342839363
Iteration :  54   Loss :  0.580207902502
Iteration :  55   Loss :  0.556116613984
Iteration :  56   Loss :  0.533025639628
Iteration :  57   Loss :  0.510893444571
Iteration :  58   Loss :  0.489680218549
Iteration :  59   Loss :  0.469347804296
Iteration :  60   Loss :  0.449859628902
Iteration :  61   Loss :  0.431180638034
Iteration :  62   Loss :  0.413277232876
Iteration :  63   Loss :  0.396117209697
Iteration :  64   Loss :  0.379669701925
Iteration :  65   Loss :  0.363905124622
Iteration :  66   Loss :  0.348795121272
Iteration :  67   Loss :  0.334312512772
Iteration :  68   Loss :  0.320431248546
Iteration :  69   Loss :  0.307126359685
Iteration :  70   Loss :  0.294373914035
Iteration :  71   Loss :  0.282150973146
Iteration :  72   Loss :  0.270435551018
Iteration :  73   Loss :  0.259206574548
Iteration :  74   Loss :  0.248443845625
Iteration :  75   Loss :  0.238128004804
Iteration :  76   Loss :  0.228240496475
Iteration :  77   Loss :  0.218763535496
Iteration :  78   Loss :  0.209680075191
Iteration :  79   Loss :  0.200973776696
Iteration :  80   Loss :  0.192628979567
Iteration :  81   Loss :  0.184630673609
Iteration :  82   Loss :  0.176964471877
Iteration :  83   Loss :  0.169616584798
Iteration :  84   Loss :  0.16257379537
Iteration :  85   Loss :  0.155823435382
Iteration :  86   Loss :  0.149353362631
Iteration :  87   Loss :  0.143151939082
Iteration :  88   Loss :  0.137208009929
Iteration :  89   Loss :  0.131510883537
Iteration :  90   Loss :  0.126050312205
Iteration :  91   Loss :  0.120816473738
Iteration :  92   Loss :  0.115799953773
Iteration :  93   Loss :  0.110991728851
Iteration :  94   Loss :  0.106383150183
Iteration :  95   Loss :  0.101965928091
Iteration :  96   Loss :  0.0977321171036
Iteration :  97   Loss :  0.0936741016569
Iteration :  98   Loss :  0.0897845824001
Iteration :  99   Loss :  0.0860565630646
[-0.00251296  0.00017662  0.00027864 ...,  0.00077763  0.00102608
 -0.00012404]
CROSS VALIDATION 1
Iteration :  0   Loss :  92.1731553371
Iteration :  1   Loss :  4.79920871669
Iteration :  2   Loss :  4.59993683267
Iteration :  3   Loss :  4.4089390801
Iteration :  4   Loss :  4.22587190197
Iteration :  5   Loss :  4.0504060064
Iteration :  6   Loss :  3.88222577428
Iteration :  7   Loss :  3.72102869162
Iteration :  8   Loss :  3.56652480533
Iteration :  9   Loss :  3.4184362017
Iteration :  10   Loss :  3.27649650653
Iteration :  11   Loss :  3.14045040594
Iteration :  12   Loss :  3.01005318715
Iteration :  13   Loss :  2.8850702983
Iteration :  14   Loss :  2.76527692656
Iteration :  15   Loss :  2.65045759372
Iteration :  16   Loss :  2.54040576864
Iteration :  17   Loss :  2.43492349571
Iteration :  18   Loss :  2.33382103881
Iteration :  19   Loss :  2.23691654001
Iteration :  20   Loss :  2.14403569243
Iteration :  21   Loss :  2.05501142675
Iteration :  22   Loss :  1.96968361067
Iteration :  23   Loss :  1.88789876088
Iteration :  24   Loss :  1.80950976696
Iteration :  25   Loss :  1.73437562679
Iteration :  26   Loss :  1.66236119293
Iteration :  27   Loss :  1.59333692947
Iteration :  28   Loss :  1.52717867911
Iteration :  29   Loss :  1.46376743976
Iteration :  30   Loss :  1.40298915053
Iteration :  31   Loss :  1.34473448652
Iteration :  32   Loss :  1.28889866223
Iteration :  33   Loss :  1.23538124303
Iteration :  34   Loss :  1.18408596451
Iteration :  35   Loss :  1.13492055936
Iteration :  36   Loss :  1.08779659134
Iteration :  37   Loss :  1.04262929627
Iteration :  38   Loss :  0.999337429538
Iteration :  39   Loss :  0.957843119932
Iteration :  40   Loss :  0.918071729611
Iteration :  41   Loss :  0.879951719829
Iteration :  42   Loss :  0.843414522261
Iteration :  43   Loss :  0.80839441566
Iteration :  44   Loss :  0.774828407647
Iteration :  45   Loss :  0.742656121402
Iteration :  46   Loss :  0.711819687059
Iteration :  47   Loss :  0.682263637615
Iteration :  48   Loss :  0.653934809158
Iteration :  49   Loss :  0.626782245238
Iteration :  50   Loss :  0.60075710521
Iteration :  51   Loss :  0.57581257638
Iteration :  52   Loss :  0.551903789805
Iteration :  53   Loss :  0.52898773958
Iteration :  54   Loss :  0.507023205485
Iteration :  55   Loss :  0.485970678837
Iteration :  56   Loss :  0.465792291426
Iteration :  57   Loss :  0.4464517474
Iteration :  58   Loss :  0.427914257977
Iteration :  59   Loss :  0.410146478866
Iteration :  60   Loss :  0.393116450294
Iteration :  61   Loss :  0.376793539516
Iteration :  62   Loss :  0.361148385713
Iteration :  63   Loss :  0.346152847182
Iteration :  64   Loss :  0.331779950713
Iteration :  65   Loss :  0.318003843075
Iteration :  66   Loss :  0.304799744509
Iteration :  67   Loss :  0.292143904156
Iteration :  68   Loss :  0.280013557338
Iteration :  69   Loss :  0.268386884606
Iteration :  70   Loss :  0.257242972495
Iteration :  71   Loss :  0.246561775905
Iteration :  72   Loss :  0.236324082045
Iteration :  73   Loss :  0.226511475875
Iteration :  74   Loss :  0.217106306978
Iteration :  75   Loss :  0.208091657819
Iteration :  76   Loss :  0.199451313306
Iteration :  77   Loss :  0.191169731629
Iteration :  78   Loss :  0.183232016302
Iteration :  79   Loss :  0.175623889367
Iteration :  80   Loss :  0.168331665716
Iteration :  81   Loss :  0.161342228467
Iteration :  82   Loss :  0.154643005379
Iteration :  83   Loss :  0.148221946232
Iteration :  84   Loss :  0.142067501151
Iteration :  85   Loss :  0.136168599835
Iteration :  86   Loss :  0.130514631643
Iteration :  87   Loss :  0.125095426504
Iteration :  88   Loss :  0.11990123663
Iteration :  89   Loss :  0.115253829447
Iteration :  90   Loss :  99.6829973818
Iteration :  91   Loss :  80.8229766759
Iteration :  92   Loss :  2.32883499131
Iteration :  93   Loss :  2.23213752227
Iteration :  94   Loss :  2.13945510821
Iteration :  95   Loss :  2.05062103673
Iteration :  96   Loss :  1.96547551764
Iteration :  97   Loss :  1.88386539553
Iteration :  98   Loss :  1.80564387428
Iteration :  99   Loss :  1.73067025302
[-0.01765209 -0.00384294 -0.00882714 ...,  0.00572305 -0.00130842
  0.00109699]
CROSS VALIDATION 2
Iteration :  0   Loss :  181.722444538
Iteration :  1   Loss :  5.43315954582
Iteration :  2   Loss :  5.20756486912
Iteration :  3   Loss :  4.99133729414
Iteration :  4   Loss :  4.78408788177
Iteration :  5   Loss :  4.58544384235
Iteration :  6   Loss :  4.39504786513
Iteration :  7   Loss :  4.21255747554
Iteration :  8   Loss :  4.03764441919
Iteration :  9   Loss :  3.86999407141
Iteration :  10   Loss :  3.70930487132
Iteration :  11   Loss :  3.55528777939
Iteration :  12   Loss :  3.40766575754
Iteration :  13   Loss :  3.26617327082
Iteration :  14   Loss :  3.13055580977
Iteration :  15   Loss :  3.00056943263
Iteration :  16   Loss :  2.87598032654
Iteration :  17   Loss :  2.75656438697
Iteration :  18   Loss :  2.64210681465
Iteration :  19   Loss :  2.53240172912
Iteration :  20   Loss :  2.42725179848
Iteration :  21   Loss :  2.32646788442
Iteration :  22   Loss :  2.22986870197
Iteration :  23   Loss :  2.13728049346
Iteration :  24   Loss :  2.04853671594
Iteration :  25   Loss :  1.96347774165
Iteration :  26   Loss :  1.88195057084
Iteration :  27   Loss :  1.8038085566
Iteration :  28   Loss :  1.72891114107
Iteration :  29   Loss :  1.65712360259
Iteration :  30   Loss :  1.58831681343
Iteration :  31   Loss :  1.52236700742
Iteration :  32   Loss :  1.45915555744
Iteration :  33   Loss :  1.39856876196
Iteration :  34   Loss :  1.34049764054
Iteration :  35   Loss :  1.28483773781
Iteration :  36   Loss :  1.23148893558
Iteration :  37   Loss :  1.18035527276
Iteration :  38   Loss :  1.13134477271
Iteration :  39   Loss :  1.08436927787
Iteration :  40   Loss :  1.0393442911
Iteration :  41   Loss :  0.996188823759
Iteration :  42   Loss :  0.954825249997
Iteration :  43   Loss :  0.915179167131
Iteration :  44   Loss :  0.87717926181
Iteration :  45   Loss :  0.840757181746
Iteration :  46   Loss :  0.805847412762
Iteration :  47   Loss :  0.772387160947
Iteration :  48   Loss :  0.74031623971
Iteration :  49   Loss :  0.709576961515
Iteration :  50   Loss :  0.680114034119
Iteration :  51   Loss :  0.651874461113
Iteration :  52   Loss :  0.632063983722
Iteration :  53   Loss :  28.7238216263
Iteration :  54   Loss :  4.26655532817
Iteration :  55   Loss :  2.5417563604
Iteration :  56   Loss :  2.43621800844
Iteration :  57   Loss :  2.33506180102
Iteration :  58   Loss :  2.2381057835
Iteration :  59   Loss :  2.14517555637
Iteration :  60   Loss :  2.05610396146
Iteration :  61   Loss :  1.97073078136
Iteration :  62   Loss :  1.88890245113
Iteration :  63   Loss :  1.81047178216
Iteration :  64   Loss :  1.73529769737
Iteration :  65   Loss :  1.66324497746
Iteration :  66   Loss :  1.59418401766
Iteration :  67   Loss :  1.52799059465
Iteration :  68   Loss :  1.46454564309
Iteration :  69   Loss :  1.40373504144
Iteration :  70   Loss :  1.34544940668
Iteration :  71   Loss :  1.28958389761
Iteration :  72   Loss :  1.23603802619
Iteration :  73   Loss :  1.18471547684
Iteration :  74   Loss :  1.13552393318
Iteration :  75   Loss :  1.08837491199
Iteration :  76   Loss :  1.04318360399
Iteration :  77   Loss :  0.999868721392
Iteration :  78   Loss :  0.958352351581
Iteration :  79   Loss :  0.918559817035
Iteration :  80   Loss :  0.880419540975
Iteration :  81   Loss :  0.843862918621
Iteration :  82   Loss :  0.808824193786
Iteration :  83   Loss :  0.775240340603
Iteration :  84   Loss :  0.743050950152
Iteration :  85   Loss :  0.712198121801
Iteration :  86   Loss :  0.682626359057
Iteration :  87   Loss :  0.654282469746
Iteration :  88   Loss :  0.627115470325
Iteration :  89   Loss :  0.601076494184
Iteration :  90   Loss :  0.576118703743
Iteration :  91   Loss :  0.552197206202
Iteration :  92   Loss :  0.529268972794
Iteration :  93   Loss :  0.507292761384
Iteration :  94   Loss :  0.486229042284
Iteration :  95   Loss :  0.466039927153
Iteration :  96   Loss :  0.446689100841
Iteration :  97   Loss :  0.42814175607
Iteration :  98   Loss :  0.410364530824
Iteration :  99   Loss :  0.393325448338
[-0.00902    -0.00407527 -0.00327709 ...,  0.00648843  0.00296027
  0.00024331]
CROSS VALIDATION 3
Iteration :  0   Loss :  177.68514741
Iteration :  1   Loss :  5.44926746977
Iteration :  2   Loss :  5.22300396274
Iteration :  3   Loss :  5.00613532848
Iteration :  4   Loss :  4.79827147479
Iteration :  5   Loss :  4.59903850676
Iteration :  6   Loss :  4.40807805431
Iteration :  7   Loss :  4.22504662754
Iteration :  8   Loss :  4.04961499887
Iteration :  9   Loss :  3.88146761082
Iteration :  10   Loss :  3.72030200849
Iteration :  11   Loss :  3.5658282954
Iteration :  12   Loss :  3.41776861214
Iteration :  13   Loss :  3.2758566365
Iteration :  14   Loss :  3.13983710448
Iteration :  15   Loss :  3.00946535108
Iteration :  16   Loss :  2.88450687025
Iteration :  17   Loss :  2.76473689307
Iteration :  18   Loss :  2.64993998341
Iteration :  19   Loss :  2.53990965045
Iteration :  20   Loss :  2.43444797725
Iteration :  21   Loss :  2.33336526475
Iteration :  22   Loss :  2.23647969051
Iteration :  23   Loss :  2.14361698171
Iteration :  24   Loss :  2.05461010167
Iteration :  25   Loss :  1.96929894934
Iteration :  26   Loss :  1.88753007139
Iteration :  27   Loss :  1.80915638613
Iteration :  28   Loss :  1.73403691897
Iteration :  29   Loss :  1.66203654887
Iteration :  30   Loss :  1.59302576523
Iteration :  31   Loss :  1.52688043497
Iteration :  32   Loss :  1.46348157927
Iteration :  33   Loss :  1.40271515948
Iteration :  34   Loss :  1.34447187208
Iteration :  35   Loss :  1.28864695202
Iteration :  36   Loss :  1.23513998428
Iteration :  37   Loss :  1.18385472327
Iteration :  38   Loss :  1.13469891967
Iteration :  39   Loss :  1.08758415454
Iteration :  40   Loss :  1.04242568023
Iteration :  41   Loss :  0.999142268007
Iteration :  42   Loss :  0.957656061863
Iteration :  43   Loss :  0.917892438534
Iteration :  44   Loss :  0.879779873245
Iteration :  45   Loss :  0.843249811059
Iteration :  46   Loss :  0.808236543567
Iteration :  47   Loss :  0.789015860203
Iteration :  48   Loss :  1.75069577168
Iteration :  49   Loss :  1.67800369568
Iteration :  50   Loss :  1.60832992703
Iteration :  51   Loss :  1.54154914011
Iteration :  52   Loss :  1.47754121306
Iteration :  53   Loss :  1.41619101168
Iteration :  54   Loss :  1.35738818236
Iteration :  55   Loss :  1.30102695358
Iteration :  56   Loss :  1.24700594564
Iteration :  57   Loss :  1.1952279883
Iteration :  58   Loss :  1.14559994603
Iteration :  59   Loss :  1.09803255044
Iteration :  60   Loss :  1.05244023971
Iteration :  61   Loss :  1.00874100474
Iteration :  62   Loss :  0.966856241572
Iteration :  63   Loss :  0.92671061003
Iteration :  64   Loss :  0.888231898204
Iteration :  65   Loss :  0.851350892553
Iteration :  66   Loss :  0.816001253408
Iteration :  67   Loss :  0.782119395642
Iteration :  68   Loss :  0.7496443743
Iteration :  69   Loss :  0.718517774973
Iteration :  70   Loss :  0.688683608724
Iteration :  71   Loss :  0.660088211377
Iteration :  72   Loss :  0.632680146993
Iteration :  73   Loss :  0.606410115345
Iteration :  74   Loss :  0.581230863242
Iteration :  75   Loss :  0.557097099532
Iteration :  76   Loss :  0.533965413631
Iteration :  77   Loss :  0.511794197446
Iteration :  78   Loss :  0.490543570525
Iteration :  79   Loss :  0.470175308326
Iteration :  80   Loss :  0.450652773458
Iteration :  81   Loss :  0.431940849783
Iteration :  82   Loss :  0.414005879249
Iteration :  83   Loss :  0.396815601347
Iteration :  84   Loss :  0.380339095081
Iteration :  85   Loss :  0.364546723356
Iteration :  86   Loss :  0.349410079658
Iteration :  87   Loss :  0.334901936967
Iteration :  88   Loss :  0.320996198775
Iteration :  89   Loss :  0.307667852152
Iteration :  90   Loss :  0.294892922748
Iteration :  91   Loss :  0.282648431673
Iteration :  92   Loss :  0.270912354161
Iteration :  93   Loss :  0.259663579956
Iteration :  94   Loss :  0.248881875337
Iteration :  95   Loss :  0.238547846724
Iteration :  96   Loss :  0.228642905795
Iteration :  97   Loss :  0.219149236048
Iteration :  98   Loss :  0.210049760754
Iteration :  99   Loss :  0.201328112242
[-0.00721941 -0.00279855 -0.00222925 ...,  0.00357248  0.00395441
  0.00033022]
CROSS VALIDATION 4
Iteration :  0   Loss :  177.68514741
Iteration :  1   Loss :  5.44926746977
Iteration :  2   Loss :  5.22300396274
Iteration :  3   Loss :  5.00613532848
Iteration :  4   Loss :  4.79827147479
Iteration :  5   Loss :  4.59903850676
Iteration :  6   Loss :  4.40807805431
Iteration :  7   Loss :  4.22504662754
Iteration :  8   Loss :  4.04961499887
Iteration :  9   Loss :  3.88146761082
Iteration :  10   Loss :  3.72030200849
Iteration :  11   Loss :  3.5658282954
Iteration :  12   Loss :  3.41776861214
Iteration :  13   Loss :  3.2758566365
Iteration :  14   Loss :  3.13983710448
Iteration :  15   Loss :  3.00946535108
Iteration :  16   Loss :  2.88450687025
Iteration :  17   Loss :  2.76473689307
Iteration :  18   Loss :  2.64993998341
Iteration :  19   Loss :  2.53990965045
Iteration :  20   Loss :  2.43444797725
Iteration :  21   Loss :  2.33336526475
Iteration :  22   Loss :  2.23647969051
Iteration :  23   Loss :  2.14361698171
Iteration :  24   Loss :  2.05461010167
Iteration :  25   Loss :  1.96929894934
Iteration :  26   Loss :  1.88753007139
Iteration :  27   Loss :  1.80915638613
Iteration :  28   Loss :  1.73403691897
Iteration :  29   Loss :  1.66203654887
Iteration :  30   Loss :  1.59302576523
Iteration :  31   Loss :  1.52688043497
Iteration :  32   Loss :  1.46348157927
Iteration :  33   Loss :  1.40271515948
Iteration :  34   Loss :  1.34447187208
Iteration :  35   Loss :  1.28864695202
Iteration :  36   Loss :  1.23513998428
Iteration :  37   Loss :  1.18385472327
Iteration :  38   Loss :  1.13469891967
Iteration :  39   Loss :  1.08758415454
Iteration :  40   Loss :  1.04242568023
Iteration :  41   Loss :  0.999142268007
Iteration :  42   Loss :  0.957656061863
Iteration :  43   Loss :  0.917892438534
Iteration :  44   Loss :  0.879779873245
Iteration :  45   Loss :  0.843249811059
Iteration :  46   Loss :  0.808236543567
Iteration :  47   Loss :  0.789015860203
Iteration :  48   Loss :  1.75069577168
Iteration :  49   Loss :  1.67800369568
Iteration :  50   Loss :  1.60832992703
Iteration :  51   Loss :  1.54154914011
Iteration :  52   Loss :  1.47754121306
Iteration :  53   Loss :  1.41619101168
Iteration :  54   Loss :  1.35738818236
Iteration :  55   Loss :  1.30102695358
Iteration :  56   Loss :  1.24700594564
Iteration :  57   Loss :  1.1952279883
Iteration :  58   Loss :  1.14559994603
Iteration :  59   Loss :  1.09803255044
Iteration :  60   Loss :  1.05244023971
Iteration :  61   Loss :  1.00874100474
Iteration :  62   Loss :  0.966856241572
Iteration :  63   Loss :  0.92671061003
Iteration :  64   Loss :  0.888231898204
Iteration :  65   Loss :  0.851350892553
Iteration :  66   Loss :  0.816001253408
Iteration :  67   Loss :  0.782119395642
Iteration :  68   Loss :  0.7496443743
Iteration :  69   Loss :  0.718517774973
Iteration :  70   Loss :  0.688683608724
Iteration :  71   Loss :  0.660088211377
Iteration :  72   Loss :  0.632680146993
Iteration :  73   Loss :  0.606410115345
Iteration :  74   Loss :  0.581230863242
Iteration :  75   Loss :  0.557097099532
Iteration :  76   Loss :  0.533965413631
Iteration :  77   Loss :  0.511794197446
Iteration :  78   Loss :  0.490543570525
Iteration :  79   Loss :  0.470175308326
Iteration :  80   Loss :  0.450652773458
Iteration :  81   Loss :  0.431940849783
Iteration :  82   Loss :  0.414005879249
Iteration :  83   Loss :  0.396815601347
Iteration :  84   Loss :  0.380339095081
Iteration :  85   Loss :  0.364546723356
Iteration :  86   Loss :  0.349410079658
Iteration :  87   Loss :  0.334901936967
Iteration :  88   Loss :  0.320996198775
Iteration :  89   Loss :  0.307667852152
Iteration :  90   Loss :  0.294892922748
Iteration :  91   Loss :  0.282648431673
Iteration :  92   Loss :  0.270912354161
Iteration :  93   Loss :  0.259663579956
Iteration :  94   Loss :  0.248881875337
Iteration :  95   Loss :  0.238547846724
Iteration :  96   Loss :  0.228642905795
Iteration :  97   Loss :  0.219149236048
Iteration :  98   Loss :  0.210049760754
Iteration :  99   Loss :  0.201328112242
[-0.00721941 -0.00279855 -0.00222925 ...,  0.00357248  0.00395441
  0.00033022]
CROSS VALIDATION 5
Iteration :  0   Loss :  158.778082056
Iteration :  1   Loss :  25.6317864508
Iteration :  2   Loss :  5.15730275508
Iteration :  3   Loss :  4.94316215459
Iteration :  4   Loss :  4.73791306173
Iteration :  5   Loss :  4.54118628491
Iteration :  6   Loss :  4.35262796205
Iteration :  7   Loss :  4.17189892407
Iteration :  8   Loss :  3.99867408481
Iteration :  9   Loss :  3.83264185627
Iteration :  10   Loss :  3.67350358816
Iteration :  11   Loss :  3.52097303069
Iteration :  12   Loss :  3.37477581969
Iteration :  13   Loss :  3.23464898308
Iteration :  14   Loss :  3.10034046785
Iteration :  15   Loss :  2.97160868672
Iteration :  16   Loss :  2.84822208352
Iteration :  17   Loss :  2.72995871675
Iteration :  18   Loss :  2.61660586029
Iteration :  19   Loss :  2.50795962082
Iteration :  20   Loss :  2.40382457103
Iteration :  21   Loss :  2.3040133981
Iteration :  22   Loss :  2.20834656681
Iteration :  23   Loss :  2.11665199653
Iteration :  24   Loss :  2.02876475177
Iteration :  25   Loss :  1.94452674543
Iteration :  26   Loss :  1.86378645448
Iteration :  27   Loss :  1.78639864743
Iteration :  28   Loss :  1.71222412302
Iteration :  29   Loss :  1.64112945992
Iteration :  30   Loss :  1.57298677668
Iteration :  31   Loss :  1.50767350172
Iteration :  32   Loss :  1.44507215285
Iteration :  33   Loss :  1.38507012596
Iteration :  34   Loss :  1.32755949248
Iteration :  35   Loss :  1.27243680522
Iteration :  36   Loss :  1.21960291229
Iteration :  37   Loss :  1.16896277879
Iteration :  38   Loss :  1.12042531584
Iteration :  39   Loss :  1.07390321673
Iteration :  40   Loss :  1.02931279987
Iteration :  41   Loss :  0.986573858315
Iteration :  42   Loss :  0.945609515425
Iteration :  43   Loss :  0.906346086637
Iteration :  44   Loss :  0.868712946902
Iteration :  45   Loss :  0.832642403648
Iteration :  46   Loss :  0.798069575025
Iteration :  47   Loss :  0.764932273194
Iteration :  48   Loss :  0.73317089247
Iteration :  49   Loss :  0.702728302102
Iteration :  50   Loss :  0.673549743515
Iteration :  51   Loss :  0.645582731807
Iteration :  52   Loss :  0.618776961345
Iteration :  53   Loss :  0.593084215279
Iteration :  54   Loss :  0.56845827881
Iteration :  55   Loss :  0.54485485606
Iteration :  56   Loss :  0.522231490399
Iteration :  57   Loss :  0.500547488071
Iteration :  58   Loss :  0.479763845
Iteration :  59   Loss :  0.459843176632
Iteration :  60   Loss :  0.440749650684
Iteration :  61   Loss :  0.422448922698
Iteration :  62   Loss :  0.404908074259
Iteration :  63   Loss :  0.388095553784
Iteration :  64   Loss :  0.371981119771
Iteration :  65   Loss :  0.356535786398
Iteration :  66   Loss :  0.341731771388
Iteration :  67   Loss :  0.327542446036
Iteration :  68   Loss :  0.313942287308
Iteration :  69   Loss :  0.300906831932
Iteration :  70   Loss :  0.288412632398
Iteration :  71   Loss :  0.276437214777
Iteration :  72   Loss :  0.2649590383
Iteration :  73   Loss :  0.253957456608
Iteration :  74   Loss :  0.24341268062
Iteration :  75   Loss :  0.23330574293
Iteration :  76   Loss :  0.223618463695
Iteration :  77   Loss :  0.214333417932
Iteration :  78   Loss :  0.205433904174
Iteration :  79   Loss :  0.196903914432
Iteration :  80   Loss :  0.188728105394
Iteration :  81   Loss :  0.180891770834
Iteration :  82   Loss :  0.173380815152
Iteration :  83   Loss :  0.166181728026
Iteration :  84   Loss :  0.159281560105
Iteration :  85   Loss :  0.152667899719
Iteration :  86   Loss :  0.146328850555
Iteration :  87   Loss :  0.140253010253
Iteration :  88   Loss :  0.134429449904
Iteration :  89   Loss :  0.128847694383
Iteration :  90   Loss :  0.123497703514
Iteration :  91   Loss :  0.118369854007
Iteration :  92   Loss :  0.113454922148
Iteration :  93   Loss :  0.108744067209
Iteration :  94   Loss :  0.104228815544
Iteration :  95   Loss :  0.0999010453495
Iteration :  96   Loss :  0.0957529720529
Iteration :  97   Loss :  0.0917771343122
Iteration :  98   Loss :  0.0879663805935
Iteration :  99   Loss :  0.0843138563075
[-0.00072278  0.00038771 -0.00156064 ...,  0.00037664  0.00014051
  0.00059937]
CROSS VALIDATION 6
Iteration :  0   Loss :  177.62511352
Iteration :  1   Loss :  5.44946512026
Iteration :  2   Loss :  5.22319340642
Iteration :  3   Loss :  5.00631690612
Iteration :  4   Loss :  4.79844551299
Iteration :  5   Loss :  4.59920531858
Iteration :  6   Loss :  4.4082379398
Iteration :  7   Loss :  4.22519987429
Iteration :  8   Loss :  4.04976188253
Iteration :  9   Loss :  3.88160839561
Iteration :  10   Loss :  3.72043694764
Iteration :  11   Loss :  3.56595763163
Iteration :  12   Loss :  3.41789257809
Iteration :  13   Loss :  3.27597545516
Iteration :  14   Loss :  3.13995098957
Iteration :  15   Loss :  3.00957450746
Iteration :  16   Loss :  2.88461149426
Iteration :  17   Loss :  2.7648371729
Iteration :  18   Loss :  2.65003609943
Iteration :  19   Loss :  2.54000177556
Iteration :  20   Loss :  2.43453627717
Iteration :  21   Loss :  2.33344989828
Iteration :  22   Loss :  2.23656080991
Iteration :  23   Loss :  2.14369473289
Iteration :  24   Loss :  2.05468462447
Iteration :  25   Loss :  1.96937037782
Iteration :  26   Loss :  1.88759853403
Iteration :  27   Loss :  1.80922200607
Iteration :  28   Loss :  1.73409981426
Iteration :  29   Loss :  1.66209683263
Iteration :  30   Loss :  1.5930835459
Iteration :  31   Loss :  1.52693581649
Iteration :  32   Loss :  1.46353466124
Iteration :  33   Loss :  1.40276603739
Iteration :  34   Loss :  1.34452063745
Iteration :  35   Loss :  1.28869369256
Iteration :  36   Loss :  1.23518478407
Iteration :  37   Loss :  1.18389766289
Iteration :  38   Loss :  1.13474007636
Iteration :  39   Loss :  1.08762360232
Iteration :  40   Loss :  1.04246349008
Iteration :  41   Loss :  0.999178507917
Iteration :  42   Loss :  0.957690797026
Iteration :  43   Loss :  0.917925731429
Iteration :  44   Loss :  0.879811783758
Iteration :  45   Loss :  0.84328039659
Iteration :  46   Loss :  0.808265859131
Iteration :  47   Loss :  0.78784446524
Iteration :  48   Loss :  1.75072267743
Iteration :  49   Loss :  1.67802948426
Iteration :  50   Loss :  1.60835464482
Iteration :  51   Loss :  1.54157283157
Iteration :  52   Loss :  1.47756392081
Iteration :  53   Loss :  1.41621277656
Iteration :  54   Loss :  1.35740904352
Iteration :  55   Loss :  1.30104694855
Iteration :  56   Loss :  1.24702511038
Iteration :  57   Loss :  1.19524635729
Iteration :  58   Loss :  1.14561755231
Iteration :  59   Loss :  1.09804942567
Iteration :  60   Loss :  1.05245641425
Iteration :  61   Loss :  1.00875650768
Iteration :  62   Loss :  0.966871100805
Iteration :  63   Loss :  0.92672485228
Iteration :  64   Loss :  0.888245549091
Iteration :  65   Loss :  0.85136397663
Iteration :  66   Loss :  0.81601379421
Iteration :  67   Loss :  0.782131415727
Iteration :  68   Loss :  0.749655895289
Iteration :  69   Loss :  0.71852881759
Iteration :  70   Loss :  0.688694192831
Iteration :  71   Loss :  0.660098356013
Iteration :  72   Loss :  0.632689870405
Iteration :  73   Loss :  0.606419435024
Iteration :  74   Loss :  0.581239795951
Iteration :  75   Loss :  0.557105661337
Iteration :  76   Loss :  0.533973619935
Iteration :  77   Loss :  0.51180206301
Iteration :  78   Loss :  0.490551109496
Iteration :  79   Loss :  0.470182534265
Iteration :  80   Loss :  0.450659699363
Iteration :  81   Loss :  0.431947488112
Iteration :  82   Loss :  0.414012241943
Iteration :  83   Loss :  0.396821699849
Iteration :  84   Loss :  0.380344940363
Iteration :  85   Loss :  0.364552325931
Iteration :  86   Loss :  0.349415449604
Iteration :  87   Loss :  0.334907083942
Iteration :  88   Loss :  0.321001132039
Iteration :  89   Loss :  0.307672580578
Iteration :  90   Loss :  0.294897454841
Iteration :  91   Loss :  0.282652775585
Iteration :  92   Loss :  0.270916517706
Iteration :  93   Loss :  0.259667570623
Iteration :  94   Loss :  0.248885700304
Iteration :  95   Loss :  0.238551512872
Iteration :  96   Loss :  0.228646419718
Iteration :  97   Loss :  0.219152604066
Iteration :  98   Loss :  0.210052988926
Iteration :  99   Loss :  0.201331206374
[-0.00721962 -0.00279856 -0.00222927 ...,  0.00357269  0.00395466
  0.00033017]
CROSS VALIDATION 7
Iteration :  0   Loss :  177.62511352
Iteration :  1   Loss :  5.44946512026
Iteration :  2   Loss :  5.22319340642
Iteration :  3   Loss :  5.00631690612
Iteration :  4   Loss :  4.79844551299
Iteration :  5   Loss :  4.59920531858
Iteration :  6   Loss :  4.4082379398
Iteration :  7   Loss :  4.22519987429
Iteration :  8   Loss :  4.04976188253
Iteration :  9   Loss :  3.88160839561
Iteration :  10   Loss :  3.72043694764
Iteration :  11   Loss :  3.56595763163
Iteration :  12   Loss :  3.41789257809
Iteration :  13   Loss :  3.27597545516
Iteration :  14   Loss :  3.13995098957
Iteration :  15   Loss :  3.00957450746
Iteration :  16   Loss :  2.88461149426
Iteration :  17   Loss :  2.7648371729
Iteration :  18   Loss :  2.65003609943
Iteration :  19   Loss :  2.54000177556
Iteration :  20   Loss :  2.43453627717
Iteration :  21   Loss :  2.33344989828
Iteration :  22   Loss :  2.23656080991
Iteration :  23   Loss :  2.14369473289
Iteration :  24   Loss :  2.05468462447
Iteration :  25   Loss :  1.96937037782
Iteration :  26   Loss :  1.88759853403
Iteration :  27   Loss :  1.80922200607
Iteration :  28   Loss :  1.73409981426
Iteration :  29   Loss :  1.66209683263
Iteration :  30   Loss :  1.5930835459
Iteration :  31   Loss :  1.52693581649
Iteration :  32   Loss :  1.46353466124
Iteration :  33   Loss :  1.40276603739
Iteration :  34   Loss :  1.34452063745
Iteration :  35   Loss :  1.28869369256
Iteration :  36   Loss :  1.23518478407
Iteration :  37   Loss :  1.18389766289
Iteration :  38   Loss :  1.13474007636
Iteration :  39   Loss :  1.08762360232
Iteration :  40   Loss :  1.04246349008
Iteration :  41   Loss :  0.999178507917
Iteration :  42   Loss :  0.957690797026
Iteration :  43   Loss :  0.917925731429
Iteration :  44   Loss :  0.879811783758
Iteration :  45   Loss :  0.84328039659
Iteration :  46   Loss :  0.808265859131
Iteration :  47   Loss :  0.78784446524
Iteration :  48   Loss :  1.75072267743
Iteration :  49   Loss :  1.67802948426
Iteration :  50   Loss :  1.60835464482
Iteration :  51   Loss :  1.54157283157
Iteration :  52   Loss :  1.47756392081
Iteration :  53   Loss :  1.41621277656
Iteration :  54   Loss :  1.35740904352
Iteration :  55   Loss :  1.30104694855
Iteration :  56   Loss :  1.24702511038
Iteration :  57   Loss :  1.19524635729
Iteration :  58   Loss :  1.14561755231
Iteration :  59   Loss :  1.09804942567
Iteration :  60   Loss :  1.05245641425
Iteration :  61   Loss :  1.00875650768
Iteration :  62   Loss :  0.966871100805
Iteration :  63   Loss :  0.92672485228
Iteration :  64   Loss :  0.888245549091
Iteration :  65   Loss :  0.85136397663
Iteration :  66   Loss :  0.81601379421
Iteration :  67   Loss :  0.782131415727
Iteration :  68   Loss :  0.749655895289
Iteration :  69   Loss :  0.71852881759
Iteration :  70   Loss :  0.688694192831
Iteration :  71   Loss :  0.660098356013
Iteration :  72   Loss :  0.632689870405
Iteration :  73   Loss :  0.606419435024
Iteration :  74   Loss :  0.581239795951
Iteration :  75   Loss :  0.557105661337
Iteration :  76   Loss :  0.533973619935
Iteration :  77   Loss :  0.51180206301
Iteration :  78   Loss :  0.490551109496
Iteration :  79   Loss :  0.470182534265
Iteration :  80   Loss :  0.450659699363
Iteration :  81   Loss :  0.431947488112
Iteration :  82   Loss :  0.414012241943
Iteration :  83   Loss :  0.396821699849
Iteration :  84   Loss :  0.380344940363
Iteration :  85   Loss :  0.364552325931
Iteration :  86   Loss :  0.349415449604
Iteration :  87   Loss :  0.334907083942
Iteration :  88   Loss :  0.321001132039
Iteration :  89   Loss :  0.307672580578
Iteration :  90   Loss :  0.294897454841
Iteration :  91   Loss :  0.282652775585
Iteration :  92   Loss :  0.270916517706
Iteration :  93   Loss :  0.259667570623
Iteration :  94   Loss :  0.248885700304
Iteration :  95   Loss :  0.238551512872
Iteration :  96   Loss :  0.228646419718
Iteration :  97   Loss :  0.219152604066
Iteration :  98   Loss :  0.210052988926
Iteration :  99   Loss :  0.201331206374
[-0.00721962 -0.00279856 -0.00222927 ...,  0.00357269  0.00395466
  0.00033017]
CROSS VALIDATION 8
Iteration :  0   Loss :  171.263071046
Iteration :  1   Loss :  4.97227259891
Iteration :  2   Loss :  4.7658147874
Iteration :  3   Loss :  4.56792948013
Iteration :  4   Loss :  4.37826073111
Iteration :  5   Loss :  4.19646737389
Iteration :  6   Loss :  4.02222240786
Iteration :  7   Loss :  3.85521241007
Iteration :  8   Loss :  3.69513697147
Iteration :  9   Loss :  3.54170815654
Iteration :  10   Loss :  3.39464998536
Iteration :  11   Loss :  3.2536979372
Iteration :  12   Loss :  3.11859847471
Iteration :  13   Loss :  2.98910858789
Iteration :  14   Loss :  2.86499535694
Iteration :  15   Loss :  2.74603553332
Iteration :  16   Loss :  2.63201513816
Iteration :  17   Loss :  2.52272907741
Iteration :  18   Loss :  2.41798077289
Iteration :  19   Loss :  2.31758180869
Iteration :  20   Loss :  2.22135159229
Iteration :  21   Loss :  2.12911702968
Iteration :  22   Loss :  2.04071221404
Iteration :  23   Loss :  1.95597812731
Iteration :  24   Loss :  1.87476235414
Iteration :  25   Loss :  1.79691880775
Iteration :  26   Loss :  1.72230746713
Iteration :  27   Loss :  1.65079412522
Iteration :  28   Loss :  1.58225014748
Iteration :  29   Loss :  1.5165522405
Iteration :  30   Loss :  1.45358223023
Iteration :  31   Loss :  1.39322684943
Iteration :  32   Loss :  1.33537753395
Iteration :  33   Loss :  1.27993022736
Iteration :  34   Loss :  1.22678519391
Iteration :  35   Loss :  1.175846839
Iteration :  36   Loss :  1.12702353734
Iteration :  37   Loss :  1.08022746805
Iteration :  38   Loss :  1.03537445677
Iteration :  39   Loss :  0.992383824176
Iteration :  40   Loss :  0.951178240926
Iteration :  41   Loss :  0.911683588516
Iteration :  42   Loss :  0.873828825984
Iteration :  43   Loss :  0.837545862116
Iteration :  44   Loss :  0.802769432969
Iteration :  45   Loss :  0.76943698448
Iteration :  46   Loss :  0.737488559941
Iteration :  47   Loss :  0.706866692159
Iteration :  48   Loss :  0.677516300081
Iteration :  49   Loss :  0.649384589722
Iteration :  50   Loss :  0.622420959198
Iteration :  51   Loss :  0.596576907707
Iteration :  52   Loss :  0.571805948289
Iteration :  53   Loss :  0.548063524207
Iteration :  54   Loss :  0.525306928802
Iteration :  55   Loss :  0.503495228672
Iteration :  56   Loss :  0.482589190044
Iteration :  57   Loss :  0.462551208205
Iteration :  58   Loss :  0.443345239856
Iteration :  59   Loss :  0.424936738282
Iteration :  60   Loss :  0.40729259121
Iteration :  61   Loss :  0.39038106125
Iteration :  62   Loss :  0.374171728805
Iteration :  63   Loss :  0.358635437356
Iteration :  64   Loss :  0.343744241016
Iteration :  65   Loss :  0.329471354261
Iteration :  66   Loss :  0.315791103751
Iteration :  67   Loss :  0.302678882151
Iteration :  68   Loss :  0.290111103865
Iteration :  69   Loss :  0.278065162616
Iteration :  70   Loss :  0.26651939078
Iteration :  71   Loss :  0.255453020413
Iteration :  72   Loss :  0.244846145893
Iteration :  73   Loss :  0.234679688115
Iteration :  74   Loss :  0.224935360174
Iteration :  75   Loss :  0.215595634471
Iteration :  76   Loss :  0.206643711185
Iteration :  77   Loss :  0.198063488052
Iteration :  78   Loss :  0.189839531407
Iteration :  79   Loss :  0.181957048415
Iteration :  80   Loss :  0.174401860468
Iteration :  81   Loss :  0.167160377682
Iteration :  82   Loss :  0.160219574445
Iteration :  83   Loss :  0.153566965996
Iteration :  84   Loss :  0.147190585963
Iteration :  85   Loss :  0.141078964838
Iteration :  86   Loss :  0.13522110935
Iteration :  87   Loss :  0.129606482688
Iteration :  88   Loss :  0.124224985547
Iteration :  89   Loss :  0.119066937966
Iteration :  90   Loss :  0.114123061912
Iteration :  91   Loss :  0.109384464593
Iteration :  92   Loss :  0.104842622463
Iteration :  93   Loss :  0.100489365887
Iteration :  94   Loss :  0.0963168644509
Iteration :  95   Loss :  0.0923176128712
Iteration :  96   Loss :  0.0884844174986
Iteration :  97   Loss :  0.0848103833771
Iteration :  98   Loss :  0.0812889018418
Iteration :  99   Loss :  0.0779136386315
[-0.00083153 -0.00075302 -0.00104412 ...,  0.00034392  0.00031783
  0.00036303]
CROSS VALIDATION 9
Iteration :  0   Loss :  158.271428032
Iteration :  1   Loss :  5.44969941639
Iteration :  2   Loss :  5.22341797414
Iteration :  3   Loss :  5.00653214938
Iteration :  4   Loss :  4.79865181896
Iteration :  5   Loss :  4.59940305835
Iteration :  6   Loss :  4.40842746906
Iteration :  7   Loss :  4.22538153395
Iteration :  8   Loss :  4.04993599935
Iteration :  9   Loss :  3.88177528278
Iteration :  10   Loss :  3.72059690535
Iteration :  11   Loss :  3.5661109476
Iteration :  12   Loss :  3.41803952811
Iteration :  13   Loss :  3.27611630354
Iteration :  14   Loss :  3.14008598967
Iteration :  15   Loss :  3.00970390211
Iteration :  16   Loss :  2.88473551621
Iteration :  17   Loss :  2.76495604523
Iteration :  18   Loss :  2.65015003597
Iteration :  19   Loss :  2.54011098125
Iteration :  20   Loss :  2.43464094843
Iteration :  21   Loss :  2.33355022341
Iteration :  22   Loss :  2.23665696935
Iteration :  23   Loss :  2.14378689962
Iteration :  24   Loss :  2.05477296427
Iteration :  25   Loss :  1.96945504959
Iteration :  26   Loss :  1.88767969007
Iteration :  27   Loss :  1.80929979237
Iteration :  28   Loss :  1.73417437073
Iteration :  29   Loss :  1.66216829338
Iteration :  30   Loss :  1.59315203947
Iteration :  31   Loss :  1.52700146607
Iteration :  32   Loss :  1.46359758493
Iteration :  33   Loss :  1.40282634838
Iteration :  34   Loss :  1.34457844422
Iteration :  35   Loss :  1.28874909909
Iteration :  36   Loss :  1.23523789002
Iteration :  37   Loss :  1.18394856378
Iteration :  38   Loss :  1.13478886375
Iteration :  39   Loss :  1.08767036398
Iteration :  40   Loss :  1.0425083101
Iteration :  41   Loss :  0.999221466931
Iteration :  42   Loss :  0.957731972304
Iteration :  43   Loss :  0.917965197035
Iteration :  44   Loss :  0.87984961068
Iteration :  45   Loss :  0.843316652869
Iteration :  46   Loss :  0.808300609983
Iteration :  47   Loss :  0.780988344918
Iteration :  48   Loss :  1.7507629504
Iteration :  49   Loss :  1.67806808502
Iteration :  50   Loss :  1.60839164281
Iteration :  51   Loss :  1.54160829334
Iteration :  52   Loss :  1.47759791014
Iteration :  53   Loss :  1.41624535459
Iteration :  54   Loss :  1.35744026886
Iteration :  55   Loss :  1.30107687735
Iteration :  56   Loss :  1.24705379648
Iteration :  57   Loss :  1.19527385229
Iteration :  58   Loss :  1.14564390567
Iteration :  59   Loss :  1.09807468479
Iteration :  60   Loss :  1.05248062457
Iteration :  61   Loss :  1.00877971274
Iteration :  62   Loss :  0.966893342348
Iteration :  63   Loss :  0.926746170314
Iteration :  64   Loss :  0.888265981961
Iteration :  65   Loss :  0.85138356109
Iteration :  66   Loss :  0.816032565487
Iteration :  67   Loss :  0.782149407587
Iteration :  68   Loss :  0.749673140095
Iteration :  69   Loss :  0.71854534636
Iteration :  70   Loss :  0.688710035296
Iteration :  71   Loss :  0.66011354067
Iteration :  72   Loss :  0.632704424568
Iteration :  73   Loss :  0.606433384871
Iteration :  74   Loss :  0.581253166575
Iteration :  75   Loss :  0.557118476789
Iteration :  76   Loss :  0.533985903266
Iteration :  77   Loss :  0.511813836314
Iteration :  78   Loss :  0.490562393951
Iteration :  79   Loss :  0.470193350169
Iteration :  80   Loss :  0.450670066171
Iteration :  81   Loss :  0.431957424471
Iteration :  82   Loss :  0.414021765726
Iteration :  83   Loss :  0.396830828188
Iteration :  84   Loss :  0.380353689677
Iteration :  85   Loss :  0.364560711957
Iteration :  86   Loss :  0.349423487427
Iteration :  87   Loss :  0.33491478802
Iteration :  88   Loss :  0.32100851623
Iteration :  89   Loss :  0.307679658164
Iteration :  90   Loss :  0.294904238553
Iteration :  91   Loss :  0.282659277625
Iteration :  92   Loss :  0.270922749769
Iteration :  93   Loss :  0.259673543919
Iteration :  94   Loss :  0.248891425578
Iteration :  95   Loss :  0.238557000422
Iteration :  96   Loss :  0.228651679415
Iteration :  97   Loss :  0.219157645371
Iteration :  98   Loss :  0.210057820906
Iteration :  99   Loss :  0.201335837722
[-0.00721974 -0.00279859 -0.00222948 ...,  0.00357269  0.00395482
  0.00033022]
CROSS VALIDATION 10
Iteration :  0   Loss :  78.3103609542
Iteration :  1   Loss :  5.45119825144
Iteration :  2   Loss :  5.22485457484
Iteration :  3   Loss :  5.0079090998
Iteration :  4   Loss :  4.79997159589
Iteration :  5   Loss :  4.60066803573
Iteration :  6   Loss :  4.40963992228
Iteration :  7   Loss :  4.22654364391
Iteration :  8   Loss :  4.05104985638
Iteration :  9   Loss :  3.88284289045
Iteration :  10   Loss :  3.721620184
Iteration :  11   Loss :  3.56709173786
Iteration :  12   Loss :  3.41897959417
Iteration :  13   Loss :  3.27701733635
Iteration :  14   Loss :  3.14094960996
Iteration :  15   Loss :  3.01053166331
Iteration :  16   Loss :  2.88552890726
Iteration :  17   Loss :  2.76571649323
Iteration :  18   Loss :  2.65087890879
Iteration :  19   Loss :  2.54080958994
Iteration :  20   Loss :  2.43531054962
Iteration :  21   Loss :  2.33419202154
Iteration :  22   Loss :  2.23727211885
Iteration :  23   Loss :  2.14437650699
Iteration :  24   Loss :  2.05533809007
Iteration :  25   Loss :  1.96999671034
Iteration :  26   Loss :  1.88819886008
Iteration :  27   Loss :  1.80979740549
Iteration :  28   Loss :  1.73465132205
Iteration :  29   Loss :  1.66262544081
Iteration :  30   Loss :  1.59359020531
Iteration :  31   Loss :  1.52742143847
Iteration :  32   Loss :  1.46400011931
Iteration :  33   Loss :  1.4032121688
Iteration :  34   Loss :  1.34494824467
Iteration :  35   Loss :  1.28910354475
Iteration :  36   Loss :  1.23557761845
Iteration :  37   Loss :  1.18427418607
Iteration :  38   Loss :  1.13510096561
Iteration :  39   Loss :  1.0879695068
Iteration :  40   Loss :  1.04279503197
Iteration :  41   Loss :  0.999496283586
Iteration :  42   Loss :  0.957995378071
Iteration :  43   Loss :  0.918217665715
Iteration :  44   Loss :  0.8800915964
Iteration :  45   Loss :  0.843548590902
Iteration :  46   Loss :  0.808522917527
Iteration :  47   Loss :  0.774951573883
Iteration :  48   Loss :  0.742774173551
Iteration :  49   Loss :  0.711932837468
Iteration :  50   Loss :  0.682372089813
Iteration :  51   Loss :  0.654038758224
Iteration :  52   Loss :  0.626881878149
Iteration :  53   Loss :  0.600852601181
Iteration :  54   Loss :  0.575904107185
Iteration :  55   Loss :  0.551991520084
Iteration :  56   Loss :  0.529071827138
Iteration :  57   Loss :  0.507103801574
Iteration :  58   Loss :  0.48604792843
Iteration :  59   Loss :  0.465866333475
Iteration :  60   Loss :  0.446522715089
Iteration :  61   Loss :  0.427982278958
Iteration :  62   Loss :  0.410211675492
Iteration :  63   Loss :  0.393178939837
Iteration :  64   Loss :  0.37685343438
Iteration :  65   Loss :  0.361205793633
Iteration :  66   Loss :  0.34620787142
Iteration :  67   Loss :  0.331832690245
Iteration :  68   Loss :  0.318054392766
Iteration :  69   Loss :  0.304848195284
Iteration :  70   Loss :  0.292190343167
Iteration :  71   Loss :  0.280058068117
Iteration :  72   Loss :  0.268429547216
Iteration :  73   Loss :  0.257283863676
Iteration :  74   Loss :  0.24660096921
Iteration :  75   Loss :  0.236361647973
Iteration :  76   Loss :  0.226547481996
Iteration :  77   Loss :  0.21714081806
Iteration :  78   Loss :  0.208124735938
Iteration :  79   Loss :  0.199483017961
Iteration :  80   Loss :  0.191200119849
Iteration :  81   Loss :  0.183261142747
Iteration :  82   Loss :  0.17565180643
Iteration :  83   Loss :  0.168358423611
Iteration :  84   Loss :  0.161367875326
Iteration :  85   Loss :  0.154667587334
Iteration :  86   Loss :  0.148245507499
Iteration :  87   Loss :  0.142090084111
Iteration :  88   Loss :  0.13619024511
Iteration :  89   Loss :  0.130535378166
Iteration :  90   Loss :  0.125115311594
Iteration :  91   Loss :  0.119920296055
Iteration :  92   Loss :  0.114940987019
Iteration :  93   Loss :  0.110168427961
Iteration :  94   Loss :  0.105594034244
Iteration :  95   Loss :  0.101209577683
Iteration :  96   Loss :  0.0970071717415
Iteration :  97   Loss :  0.0929792573462
Iteration :  98   Loss :  0.0891185892903
Iteration :  99   Loss :  0.0854182232014
[-0.00233045  0.0003354  -0.00030573 ...,  0.00210878  0.00088238
  0.0002396 ]
CROSS VALIDATION 11
Iteration :  0   Loss :  177.666161444
Iteration :  1   Loss :  5.44995077616
Iteration :  2   Loss :  5.223658897
Iteration :  3   Loss :  5.00676306868
Iteration :  4   Loss :  4.79887315007
Iteration :  5   Loss :  4.59961519939
Iteration :  6   Loss :  4.40863080162
Iteration :  7   Loss :  4.22557642377
Iteration :  8   Loss :  4.05012279699
Iteration :  9   Loss :  3.88195432425
Iteration :  10   Loss :  3.72076851268
Iteration :  11   Loss :  3.56627542949
Iteration :  12   Loss :  3.41819718041
Iteration :  13   Loss :  3.27626740983
Iteration :  14   Loss :  3.14023082175
Iteration :  15   Loss :  3.0098427205
Iteration :  16   Loss :  2.88486857061
Iteration :  17   Loss :  2.76508357497
Iteration :  18   Loss :  2.65027227044
Iteration :  19   Loss :  2.54022814032
Iteration :  20   Loss :  2.43475324284
Iteration :  21   Loss :  2.33365785515
Iteration :  22   Loss :  2.23676013203
Iteration :  23   Loss :  2.14388577879
Iteration :  24   Loss :  2.0548677378
Iteration :  25   Loss :  1.96954588795
Iteration :  26   Loss :  1.88776675666
Iteration :  27   Loss :  1.80938324379
Iteration :  28   Loss :  1.7342543571
Iteration :  29   Loss :  1.66224495857
Iteration :  30   Loss :  1.59322552138
Iteration :  31   Loss :  1.52707189688
Iteration :  32   Loss :  1.46366509133
Iteration :  33   Loss :  1.40289105179
Iteration :  34   Loss :  1.34464046102
Iteration :  35   Loss :  1.28880854084
Iteration :  36   Loss :  1.23529486364
Iteration :  37   Loss :  1.18400317175
Iteration :  38   Loss :  1.1348412043
Iteration :  39   Loss :  1.08772053125
Iteration :  40   Loss :  1.04255639434
Iteration :  41   Loss :  0.999267554626
Iteration :  42   Loss :  0.957776146353
Iteration :  43   Loss :  0.918007536897
Iteration :  44   Loss :  0.879890192514
Iteration :  45   Loss :  0.843355549672
Iteration :  46   Loss :  0.80833789172
Iteration :  47   Loss :  0.782316060562
Iteration :  48   Loss :  1.75079608585
Iteration :  49   Loss :  1.67809984463
Iteration :  50   Loss :  1.6084220837
Iteration :  51   Loss :  1.54163747027
Iteration :  52   Loss :  1.47762587558
Iteration :  53   Loss :  1.41627215886
Iteration :  54   Loss :  1.35746596016
Iteration :  55   Loss :  1.30110150191
Iteration :  56   Loss :  1.24707739858
Iteration :  57   Loss :  1.19529647439
Iteration :  58   Loss :  1.14566558846
Iteration :  59   Loss :  1.09809546727
Iteration :  60   Loss :  1.05250054412
Iteration :  61   Loss :  1.0087988052
Iteration :  62   Loss :  0.966911642051
Iteration :  63   Loss :  0.926763710181
Iteration :  64   Loss :  0.88828279354
Iteration :  65   Loss :  0.851399674622
Iteration :  66   Loss :  0.816048009956
Iteration :  67   Loss :  0.782164210773
Iteration :  68   Loss :  0.749687328626
Iteration :  69   Loss :  0.718558945757
Iteration :  70   Loss :  0.688723070022
Iteration :  71   Loss :  0.66012603417
Iteration :  72   Loss :  0.632716399315
Iteration :  73   Loss :  0.606444862405
Iteration :  74   Loss :  0.581264167541
Iteration :  75   Loss :  0.557129020975
Iteration :  76   Loss :  0.533996009638
Iteration :  77   Loss :  0.511823523051
Iteration :  78   Loss :  0.490571678477
Iteration :  79   Loss :  0.470202249184
Iteration :  80   Loss :  0.450678595683
Iteration :  81   Loss :  0.431965599823
Iteration :  82   Loss :  0.414029601622
Iteration :  83   Loss :  0.396838338723
Iteration :  84   Loss :  0.380360888361
Iteration :  85   Loss :  0.364567611739
Iteration :  86   Loss :  0.349430100717
Iteration :  87   Loss :  0.334921126715
Iteration :  88   Loss :  0.32101459173
Iteration :  89   Loss :  0.307685481398
Iteration :  90   Loss :  0.294909819996
Iteration :  91   Loss :  0.282664627316
Iteration :  92   Loss :  0.270927877332
Iteration :  93   Loss :  0.259678458576
Iteration :  94   Loss :  0.24889613617
Iteration :  95   Loss :  0.238561515421
Iteration :  96   Loss :  0.228656006943
Iteration :  97   Loss :  0.219161793212
Iteration :  98   Loss :  0.210061796522
Iteration :  99   Loss :  0.201339648263
[-0.00721989 -0.00279864 -0.00222944 ...,  0.00357272  0.00395491
  0.00033017]
CROSS VALIDATION 12
Iteration :  0   Loss :  123.745919039
Iteration :  1   Loss :  40.5211488357
Iteration :  2   Loss :  20.6641845046
Iteration :  3   Loss :  5.80041749504
Iteration :  4   Loss :  5.55957359961
Iteration :  5   Loss :  5.32872998122
Iteration :  6   Loss :  5.10747141018
Iteration :  7   Loss :  4.89539989786
Iteration :  8   Loss :  4.69213398086
Iteration :  9   Loss :  4.49730803483
Iteration :  10   Loss :  4.31057161681
Iteration :  11   Loss :  4.13158883486
Iteration :  12   Loss :  3.96003774389
Iteration :  13   Loss :  3.79560976658
Iteration :  14   Loss :  3.63800913826
Iteration :  15   Loss :  3.48695237498
Iteration :  16   Loss :  3.34216776354
Iteration :  17   Loss :  3.20339487279
Iteration :  18   Loss :  3.07038408513
Iteration :  19   Loss :  2.94289614755
Iteration :  20   Loss :  2.82070174126
Iteration :  21   Loss :  2.7035810692
Iteration :  22   Loss :  2.5913234607
Iteration :  23   Loss :  2.48372699249
Iteration :  24   Loss :  2.38059812554
Iteration :  25   Loss :  2.28175135693
Iteration :  26   Loss :  2.18700888611
Iteration :  27   Loss :  2.09620029519
Iteration :  28   Loss :  2.00916224231
Iteration :  29   Loss :  1.92573816785
Iteration :  30   Loss :  1.84577801285
Iteration :  31   Loss :  1.76913794908
Iteration :  32   Loss :  1.6956801203
Iteration :  33   Loss :  1.62527239432
Iteration :  34   Loss :  1.55778812531
Iteration :  35   Loss :  1.49310592602
Iteration :  36   Loss :  1.43110944942
Iteration :  37   Loss :  1.3716871794
Iteration :  38   Loss :  1.31473223023
Iteration :  39   Loss :  1.26014215425
Iteration :  40   Loss :  1.20781875762
Iteration :  41   Loss :  1.15766792368
Iteration :  42   Loss :  1.10959944368
Iteration :  43   Loss :  1.06352685449
Iteration :  44   Loss :  1.01936728308
Iteration :  45   Loss :  0.97704129749
Iteration :  46   Loss :  0.936472763882
Iteration :  47   Loss :  0.897588709655
Iteration :  48   Loss :  0.860319192156
Iteration :  49   Loss :  0.824597172882
Iteration :  50   Loss :  0.790358396888
Iteration :  51   Loss :  0.757541277214
Iteration :  52   Loss :  0.7260867841
Iteration :  53   Loss :  0.69593833881
Iteration :  54   Loss :  0.667041711862
Iteration :  55   Loss :  0.639344925477
Iteration :  56   Loss :  0.612798160092
Iteration :  57   Loss :  0.58735366474
Iteration :  58   Loss :  0.562965671163
Iteration :  59   Loss :  0.539590311483
Iteration :  60   Loss :  0.517185539298
Iteration :  61   Loss :  0.495711054047
Iteration :  62   Loss :  0.475128228523
Iteration :  63   Loss :  0.455400039391
Iteration :  64   Loss :  0.436491000591
Iteration :  65   Loss :  0.418367099511
Iteration :  66   Loss :  0.400995735802
Iteration :  67   Loss :  0.384345662744
Iteration :  68   Loss :  0.368386931034
Iteration :  69   Loss :  0.353090834921
Iteration :  70   Loss :  0.338429860569
Iteration :  71   Loss :  0.324377636567
Iteration :  72   Loss :  0.31090888649
Iteration :  73   Loss :  0.297999383439
Iteration :  74   Loss :  0.28562590646
Iteration :  75   Loss :  0.273766198774
Iteration :  76   Loss :  0.262398927744
Iteration :  77   Loss :  0.251503646504
Iteration :  78   Loss :  0.241060757179
Iteration :  79   Loss :  0.231051475633
Iteration :  80   Loss :  0.22145779768
Iteration :  81   Loss :  0.212262466704
Iteration :  82   Loss :  0.203448942612
Iteration :  83   Loss :  0.195001372088
Iteration :  84   Loss :  0.186904560072
Iteration :  85   Loss :  0.179143942434
Iteration :  86   Loss :  0.17170555977
Iteration :  87   Loss :  0.164576032297
Iteration :  88   Loss :  0.157742535787
Iteration :  89   Loss :  0.151192778493
Iteration :  90   Loss :  0.144914979047
Iteration :  91   Loss :  0.138897845265
Iteration :  92   Loss :  0.133130553833
Iteration :  93   Loss :  0.127602730842
Iteration :  94   Loss :  0.122304433126
Iteration :  95   Loss :  0.117226130379
Iteration :  96   Loss :  0.112358688007
Iteration :  97   Loss :  0.107693350706
Iteration :  98   Loss :  0.103221726703
Iteration :  99   Loss :  0.0989357726705
[ -5.32689393e-03  -1.15967937e-03  -2.58882765e-03 ...,   2.68735762e-03
   1.31003573e-03   8.54550632e-06]
CROSS VALIDATION 13
Iteration :  0   Loss :  177.799291093
Iteration :  1   Loss :  5.4501675919
Iteration :  2   Loss :  5.22386671016
Iteration :  3   Loss :  5.00696225306
Iteration :  4   Loss :  4.79906406395
Iteration :  5   Loss :  4.59979818618
Iteration :  6   Loss :  4.40880619046
Iteration :  7   Loss :  4.22574453015
Iteration :  8   Loss :  4.05028392329
Iteration :  9   Loss :  3.88210876029
Iteration :  10   Loss :  3.72091653626
Iteration :  11   Loss :  3.56641730686
Iteration :  12   Loss :  3.41833316676
Iteration :  13   Loss :  3.27639774979
Iteration :  14   Loss :  3.14035574976
Iteration :  15   Loss :  3.00996246126
Iteration :  16   Loss :  2.88498333952
Iteration :  17   Loss :  2.76519357846
Iteration :  18   Loss :  2.65037770639
Iteration :  19   Loss :  2.54032919838
Iteration :  20   Loss :  2.43485010479
Iteration :  21   Loss :  2.33375069522
Iteration :  22   Loss :  2.2368491172
Iteration :  23   Loss :  2.14397106914
Iteration :  24   Loss :  2.05494948674
Iteration :  25   Loss :  1.96962424253
Iteration :  26   Loss :  1.88784185781
Iteration :  27   Loss :  1.8094552266
Iteration :  28   Loss :  1.73432335105
Iteration :  29   Loss :  1.66231108777
Iteration :  30   Loss :  1.59328890477
Iteration :  31   Loss :  1.52713264848
Iteration :  32   Loss :  1.46372332041
Iteration :  33   Loss :  1.40294686309
Iteration :  34   Loss :  1.34469395494
Iteration :  35   Loss :  1.28885981359
Iteration :  36   Loss :  1.23534400746
Iteration :  37   Loss :  1.18405027503
Iteration :  38   Loss :  1.13488635176
Iteration :  39   Loss :  1.08776380411
Iteration :  40   Loss :  1.04259787043
Iteration :  41   Loss :  0.999307308551
Iteration :  42   Loss :  0.957814249624
Iteration :  43   Loss :  0.91804405805
Iteration :  44   Loss :  0.879925197243
Iteration :  45   Loss :  0.84338910094
Iteration :  46   Loss :  0.808370049878
Iteration :  47   Loss :  0.774805053577
Iteration :  48   Loss :  1.7508188997
Iteration :  49   Loss :  1.67812171121
Iteration :  50   Loss :  1.60844304234
Iteration :  51   Loss :  1.54165755867
Iteration :  52   Loss :  1.47764512988
Iteration :  53   Loss :  1.41629061368
Iteration :  54   Loss :  1.3574836487
Iteration :  55   Loss :  1.30111845599
Iteration :  56   Loss :  1.2470936487
Iteration :  57   Loss :  1.19531204978
Iteration :  58   Loss :  1.14568051712
Iteration :  59   Loss :  1.09810977606
Iteration :  60   Loss :  1.05251425879
Iteration :  61   Loss :  1.00881195041
Iteration :  62   Loss :  0.966924241451
Iteration :  63   Loss :  0.92677578643
Iteration :  64   Loss :  0.888294368362
Iteration :  65   Loss :  0.851410768836
Iteration :  66   Loss :  0.816058643518
Iteration :  67   Loss :  0.78217440281
Iteration :  68   Loss :  0.749697097471
Iteration :  69   Loss :  0.718568308982
Iteration :  70   Loss :  0.688732044469
Iteration :  71   Loss :  0.660134635982
Iteration :  72   Loss :  0.632724643964
Iteration :  73   Loss :  0.606452764721
Iteration :  74   Loss :  0.581271741738
Iteration :  75   Loss :  0.557136280677
Iteration :  76   Loss :  0.534002967904
Iteration :  77   Loss :  0.511830192398
Iteration :  78   Loss :  0.490578070901
Iteration :  79   Loss :  0.470208376183
Iteration :  80   Loss :  0.450684468277
Iteration :  81   Loss :  0.431971228576
Iteration :  82   Loss :  0.414034996659
Iteration :  83   Loss :  0.396843509749
Iteration :  84   Loss :  0.380365844676
Iteration :  85   Loss :  0.364572362259
Iteration :  86   Loss :  0.349434653987
Iteration :  87   Loss :  0.334925490924
Iteration :  88   Loss :  0.32101877473
Iteration :  89   Loss :  0.307689490712
Iteration :  90   Loss :  0.294913662836
Iteration :  91   Loss :  0.282668310594
Iteration :  92   Loss :  0.270931407673
Iteration :  93   Loss :  0.259681842332
Iteration :  94   Loss :  0.248899379426
Iteration :  95   Loss :  0.238564624011
Iteration :  96   Loss :  0.228658986459
Iteration :  97   Loss :  0.219164649013
Iteration :  98   Loss :  0.210064533744
Iteration :  99   Loss :  0.201342271831
[-0.00721962 -0.00279851 -0.00222944 ...,  0.00357272  0.00395489
  0.00033022]
CROSS VALIDATION 14
Iteration :  0   Loss :  177.799291093
Iteration :  1   Loss :  5.4501675919
Iteration :  2   Loss :  5.22386671016
Iteration :  3   Loss :  5.00696225306
Iteration :  4   Loss :  4.79906406395
Iteration :  5   Loss :  4.59979818618
Iteration :  6   Loss :  4.40880619046
Iteration :  7   Loss :  4.22574453015
Iteration :  8   Loss :  4.05028392329
Iteration :  9   Loss :  3.88210876029
Iteration :  10   Loss :  3.72091653626
Iteration :  11   Loss :  3.56641730686
Iteration :  12   Loss :  3.41833316676
Iteration :  13   Loss :  3.27639774979
Iteration :  14   Loss :  3.14035574976
Iteration :  15   Loss :  3.00996246126
Iteration :  16   Loss :  2.88498333952
Iteration :  17   Loss :  2.76519357846
Iteration :  18   Loss :  2.65037770639
Iteration :  19   Loss :  2.54032919838
Iteration :  20   Loss :  2.43485010479
Iteration :  21   Loss :  2.33375069522
Iteration :  22   Loss :  2.2368491172
Iteration :  23   Loss :  2.14397106914
Iteration :  24   Loss :  2.05494948674
Iteration :  25   Loss :  1.96962424253
Iteration :  26   Loss :  1.88784185781
Iteration :  27   Loss :  1.8094552266
Iteration :  28   Loss :  1.73432335105
Iteration :  29   Loss :  1.66231108777
Iteration :  30   Loss :  1.59328890477
Iteration :  31   Loss :  1.52713264848
Iteration :  32   Loss :  1.46372332041
Iteration :  33   Loss :  1.40294686309
Iteration :  34   Loss :  1.34469395494
Iteration :  35   Loss :  1.28885981359
Iteration :  36   Loss :  1.23534400746
Iteration :  37   Loss :  1.18405027503
Iteration :  38   Loss :  1.13488635176
Iteration :  39   Loss :  1.08776380411
Iteration :  40   Loss :  1.04259787043
Iteration :  41   Loss :  0.999307308551
Iteration :  42   Loss :  0.957814249624
Iteration :  43   Loss :  0.91804405805
Iteration :  44   Loss :  0.879925197243
Iteration :  45   Loss :  0.84338910094
Iteration :  46   Loss :  0.808370049878
Iteration :  47   Loss :  0.774805053577
Iteration :  48   Loss :  0.742633737036
Iteration :  49   Loss :  0.711798232133
Iteration :  50   Loss :  0.682243073537
Iteration :  51   Loss :  0.653915098938
Iteration :  52   Loss :  0.626763353422
Iteration :  53   Loss :  0.600738997816
Iteration :  54   Loss :  0.575795220838
Iteration :  55   Loss :  0.551887154897
Iteration :  56   Loss :  0.528971795384
Iteration :  57   Loss :  0.50700792332
Iteration :  58   Loss :  0.485956031215
Iteration :  59   Loss :  0.465778252001
Iteration :  60   Loss :  0.446438290918
Iteration :  61   Loss :  0.427901360232
Iteration :  62   Loss :  0.410134116659
Iteration :  63   Loss :  0.393104601388
Iteration :  64   Loss :  0.376782182598
Iteration :  65   Loss :  0.361137500355
Iteration :  66   Loss :  0.346142413803
Iteration :  67   Loss :  0.331769950548
Iteration :  68   Loss :  0.317994258135
Iteration :  69   Loss :  0.304790557553
Iteration :  70   Loss :  0.29213509866
Iteration :  71   Loss :  0.280005117462
Iteration :  72   Loss :  0.268378795169
Iteration :  73   Loss :  0.257235218946
Iteration :  74   Loss :  0.246554344297
Iteration :  75   Loss :  0.236316959011
Iteration :  76   Loss :  0.226504648602
Iteration :  77   Loss :  0.217099763186
Iteration :  78   Loss :  0.208085385737
Iteration :  79   Loss :  0.199445301652
Iteration :  80   Loss :  0.19116396959
Iteration :  81   Loss :  0.183226493513
Iteration :  82   Loss :  0.175618595895
Iteration :  83   Loss :  0.168326592038
Iteration :  84   Loss :  0.161337365458
Iteration :  85   Loss :  0.154638344291
Iteration :  86   Loss :  0.14821747868
Iteration :  87   Loss :  0.1420632191
Iteration :  88   Loss :  0.136164495583
Iteration :  89   Loss :  0.130510697807
Iteration :  90   Loss :  0.125091656008
Iteration :  91   Loss :  0.119897622692
Iteration :  92   Loss :  0.114919255095
Iteration :  93   Loss :  0.110147598386
Iteration :  94   Loss :  0.105574069551
Iteration :  95   Loss :  0.101190441961
Iteration :  96   Loss :  0.0969888305691
Iteration :  97   Loss :  0.0929616777326
Iteration :  98   Loss :  0.0891017396143
Iteration :  99   Loss :  0.0854020731546
[ -3.21194753e-03  -1.13097595e-04  -2.70332849e-04 ...,   2.02131493e-03
   9.42597849e-04  -4.05209261e-06]
CROSS VALIDATION 15
Iteration :  0   Loss :  177.799291093
Iteration :  1   Loss :  5.4501675919
Iteration :  2   Loss :  5.22386671016
Iteration :  3   Loss :  5.00696225306
Iteration :  4   Loss :  4.79906406395
Iteration :  5   Loss :  4.59979818618
Iteration :  6   Loss :  4.40880619046
Iteration :  7   Loss :  4.22574453015
Iteration :  8   Loss :  4.05028392329
Iteration :  9   Loss :  3.88210876029
Iteration :  10   Loss :  3.72091653626
Iteration :  11   Loss :  3.56641730686
Iteration :  12   Loss :  3.41833316676
Iteration :  13   Loss :  3.27639774979
Iteration :  14   Loss :  3.14035574976
Iteration :  15   Loss :  3.00996246126
Iteration :  16   Loss :  2.88498333952
Iteration :  17   Loss :  2.76519357846
Iteration :  18   Loss :  2.65037770639
Iteration :  19   Loss :  2.54032919838
Iteration :  20   Loss :  2.43485010479
Iteration :  21   Loss :  2.33375069522
Iteration :  22   Loss :  2.2368491172
Iteration :  23   Loss :  2.14397106914
Iteration :  24   Loss :  2.05494948674
Iteration :  25   Loss :  1.96962424253
Iteration :  26   Loss :  1.88784185781
Iteration :  27   Loss :  1.8094552266
Iteration :  28   Loss :  1.73432335105
Iteration :  29   Loss :  1.66231108777
Iteration :  30   Loss :  1.59328890477
Iteration :  31   Loss :  1.52713264848
Iteration :  32   Loss :  1.46372332041
Iteration :  33   Loss :  1.40294686309
Iteration :  34   Loss :  1.34469395494
Iteration :  35   Loss :  1.28885981359
Iteration :  36   Loss :  1.23534400746
Iteration :  37   Loss :  1.18405027503
Iteration :  38   Loss :  1.13488635176
Iteration :  39   Loss :  1.08776380411
Iteration :  40   Loss :  1.04259787043
Iteration :  41   Loss :  0.999307308551
Iteration :  42   Loss :  0.957814249624
Iteration :  43   Loss :  0.91804405805
Iteration :  44   Loss :  0.879925197243
Iteration :  45   Loss :  0.84338910094
Iteration :  46   Loss :  0.808370049878
Iteration :  47   Loss :  0.774805053577
Iteration :  48   Loss :  1.75106171401
Iteration :  49   Loss :  1.67835444342
Iteration :  50   Loss :  1.60866611108
Iteration :  51   Loss :  1.54187136519
Iteration :  52   Loss :  1.47785005876
Iteration :  53   Loss :  1.41648703355
Iteration :  54   Loss :  1.35767191286
Iteration :  55   Loss :  1.30129890308
Iteration :  56   Loss :  1.2472666033
Iteration :  57   Loss :  1.19547782298
Iteration :  58   Loss :  1.14583940712
Iteration :  59   Loss :  1.09826206866
Iteration :  60   Loss :  1.05266022792
Iteration :  61   Loss :  1.00895185864
Iteration :  62   Loss :  0.967058340435
Iteration :  63   Loss :  0.92690431738
Iteration :  64   Loss :  0.888417562472
Iteration :  65   Loss :  0.851528847703
Iteration :  66   Loss :  0.816171819536
Iteration :  67   Loss :  0.782282879553
Iteration :  68   Loss :  0.749801070062
Iteration :  69   Loss :  0.718667964442
Iteration :  70   Loss :  0.688827562052
Iteration :  71   Loss :  0.660226187502
Iteration :  72   Loss :  0.632812394098
Iteration :  73   Loss :  0.606536871309
Iteration :  74   Loss :  0.581352356068
Iteration :  75   Loss :  0.557213547754
Iteration :  76   Loss :  0.534077026711
Iteration :  77   Loss :  0.511901176148
Iteration :  78   Loss :  0.490646107276
Iteration :  79   Loss :  0.470273587564
Iteration :  80   Loss :  0.450746971963
Iteration :  81   Loss :  0.432031136995
Iteration :  82   Loss :  0.414092417572
Iteration :  83   Loss :  0.396898546441
Iteration :  84   Loss :  0.380418596144
Iteration :  85   Loss :  0.36462292339
Iteration :  86   Loss :  0.349483115728
Iteration :  87   Loss :  0.334971940446
Iteration :  88   Loss :  0.321063295582
Iteration :  89   Loss :  0.307732162977
Iteration :  90   Loss :  0.294954563271
Iteration :  91   Loss :  0.282707512769
Iteration :  92   Loss :  0.270968982103
Iteration :  93   Loss :  0.259717856602
Iteration :  94   Loss :  0.248933898318
Iteration :  95   Loss :  0.238597709616
Iteration :  96   Loss :  0.228690698289
Iteration :  97   Loss :  0.21919504411
Iteration :  98   Loss :  0.210093666782
Iteration :  99   Loss :  0.201370195212
[-0.00722047 -0.00279893 -0.00222981 ...,  0.0035733   0.00395546
  0.00033035]
CROSS VALIDATION 16
Iteration :  0   Loss :  177.799291093
Iteration :  1   Loss :  4.98851315969
Iteration :  2   Loss :  4.78138101053
Iteration :  3   Loss :  4.58284936533
Iteration :  4   Loss :  4.39256111551
Iteration :  5   Loss :  4.21017398029
Iteration :  6   Loss :  4.03535989101
Iteration :  7   Loss :  3.867804401
Iteration :  8   Loss :  3.70720612001
Iteration :  9   Loss :  3.55327617206
Iteration :  10   Loss :  3.40573767581
Iteration :  11   Loss :  3.26432524656
Iteration :  12   Loss :  3.12878451884
Iteration :  13   Loss :  2.99887168892
Iteration :  14   Loss :  2.87435307624
Iteration :  15   Loss :  2.75500470307
Iteration :  16   Loss :  2.64061189165
Iteration :  17   Loss :  2.53096887805
Iteration :  18   Loss :  2.425878442
Iteration :  19   Loss :  2.3251515522
Iteration :  20   Loss :  2.22860702626
Iteration :  21   Loss :  2.13607120482
Iteration :  22   Loss :  2.04737763917
Iteration :  23   Loss :  1.96236679186
Iteration :  24   Loss :  1.8808857497
Iteration :  25   Loss :  1.80278794878
Iteration :  26   Loss :  1.72793291074
Iteration :  27   Loss :  1.65618599017
Iteration :  28   Loss :  1.58741813238
Iteration :  29   Loss :  1.52150564125
Iteration :  30   Loss :  1.45832995676
Iteration :  31   Loss :  1.39777744172
Iteration :  32   Loss :  1.33973917735
Iteration :  33   Loss :  1.2841107674
Iteration :  34   Loss :  1.2307921503
Iteration :  35   Loss :  1.17968741927
Iteration :  36   Loss :  1.13070464972
Iteration :  37   Loss :  1.08375573395
Iteration :  38   Loss :  1.03875622264
Iteration :  39   Loss :  0.995625172975
Iteration :  40   Loss :  0.954285003021
Iteration :  41   Loss :  0.914661352192
Iteration :  42   Loss :  0.876682947491
Iteration :  43   Loss :  0.840281475301
Iteration :  44   Loss :  0.805391458514
Iteration :  45   Loss :  0.77195013875
Iteration :  46   Loss :  0.73989736347
Iteration :  47   Loss :  0.70917547778
Iteration :  48   Loss :  0.67972922072
Iteration :  49   Loss :  0.651505625867
Iteration :  50   Loss :  0.62445392606
Iteration :  51   Loss :  0.598525462083
Iteration :  52   Loss :  0.573673595139
Iteration :  53   Loss :  0.54985362296
Iteration :  54   Loss :  0.527022699395
Iteration :  55   Loss :  0.505139757346
Iteration :  56   Loss :  0.484165434894
Iteration :  57   Loss :  0.464062004499
Iteration :  58   Loss :  0.444793305137
Iteration :  59   Loss :  0.426324677255
Iteration :  60   Loss :  0.40862290043
Iteration :  61   Loss :  0.391656133609
Iteration :  62   Loss :  0.375393857838
Iteration :  63   Loss :  0.359806821367
Iteration :  64   Loss :  0.34486698703
Iteration :  65   Loss :  0.330547481816
Iteration :  66   Loss :  0.316822548531
Iteration :  67   Loss :  0.303667499465
Iteration :  68   Loss :  0.291058671988
Iteration :  69   Loss :  0.278973385986
Iteration :  70   Loss :  0.267389903062
Iteration :  71   Loss :  0.25628738744
Iteration :  72   Loss :  0.245645868481
Iteration :  73   Loss :  0.235446204765
Iteration :  74   Loss :  0.225670049658
Iteration :  75   Loss :  0.21629981831
Iteration :  76   Loss :  0.207318656028
Iteration :  77   Loss :  0.198710407957
Iteration :  78   Loss :  0.190459590019
Iteration :  79   Loss :  0.182551361065
Iteration :  80   Loss :  0.174971496175
Iteration :  81   Loss :  0.167706361077
Iteration :  82   Loss :  0.160742887615
Iteration :  83   Loss :  0.154068550251
Iteration :  84   Loss :  0.147671343526
Iteration :  85   Loss :  0.141539760472
Iteration :  86   Loss :  0.13566277191
Iteration :  87   Loss :  0.130029806614
Iteration :  88   Loss :  0.124630732293
Iteration :  89   Loss :  0.119455837369
Iteration :  90   Loss :  0.114495813503
Iteration :  91   Loss :  0.10974173886
Iteration :  92   Loss :  0.105185062051
Iteration :  93   Loss :  0.100817586761
Iteration :  94   Loss :  0.096631456996
Iteration :  95   Loss :  0.0926191429609
Iteration :  96   Loss :  0.08877342751
Iteration :  97   Loss :  0.0850873931667
Iteration :  98   Loss :  0.0815544096805
Iteration :  99   Loss :  0.0781681221014
[ -2.02427247e-03   1.68482997e-04  -1.70365201e-05 ...,   2.00302427e-03
   9.47886387e-04   1.19286911e-04]
CROSS VALIDATION 17
Iteration :  0   Loss :  177.799291093
Iteration :  1   Loss :  5.45041272992
Iteration :  2   Loss :  5.2241016696
Iteration :  3   Loss :  5.00718745656
Iteration :  4   Loss :  4.79927991658
Iteration :  5   Loss :  4.60000507622
Iteration :  6   Loss :  4.40900449005
Iteration :  7   Loss :  4.22593459598
Iteration :  8   Loss :  4.05046609723
Iteration :  9   Loss :  3.88228337004
Iteration :  10   Loss :  3.7210838959
Iteration :  11   Loss :  3.56657771742
Iteration :  12   Loss :  3.41848691679
Iteration :  13   Loss :  3.27654511584
Iteration :  14   Loss :  3.1404969969
Iteration :  15   Loss :  3.01009784356
Iteration :  16   Loss :  2.8851131005
Iteration :  17   Loss :  2.76531795153
Iteration :  18   Loss :  2.65049691526
Iteration :  19   Loss :  2.54044345748
Iteration :  20   Loss :  2.43495961965
Iteration :  21   Loss :  2.33385566281
Iteration :  22   Loss :  2.23694972636
Iteration :  23   Loss :  2.14406750082
Iteration :  24   Loss :  2.0550419144
Iteration :  25   Loss :  1.96971283242
Iteration :  26   Loss :  1.88792676929
Iteration :  27   Loss :  1.80953661241
Iteration :  28   Loss :  1.73440135757
Iteration :  29   Loss :  1.66238585531
Iteration :  30   Loss :  1.59336056783
Iteration :  31   Loss :  1.52720133596
Iteration :  32   Loss :  1.46378915586
Iteration :  33   Loss :  1.40300996493
Iteration :  34   Loss :  1.34475443667
Iteration :  35   Loss :  1.28891778402
Iteration :  36   Loss :  1.23539957084
Iteration :  37   Loss :  1.18410353132
Iteration :  38   Loss :  1.13493739676
Iteration :  39   Loss :  1.08781272962
Iteration :  40   Loss :  1.04264476447
Iteration :  41   Loss :  0.999352255464
Iteration :  42   Loss :  0.957857330259
Iteration :  43   Loss :  0.918085349899
Iteration :  44   Loss :  0.879964774579
Iteration :  45   Loss :  0.843427034953
Iteration :  46   Loss :  0.808406408802
Iteration :  47   Loss :  0.777649085761
Iteration :  48   Loss :  1.75109290851
Iteration :  49   Loss :  1.67838434267
Iteration :  50   Loss :  1.60869476886
Iteration :  51   Loss :  1.54189883305
Iteration :  52   Loss :  1.47787638611
Iteration :  53   Loss :  1.41651226773
Iteration :  54   Loss :  1.35769609928
Iteration :  55   Loss :  1.30132208523
Iteration :  56   Loss :  1.24728882288
Iteration :  57   Loss :  1.19549911997
Iteration :  58   Loss :  1.14585981982
Iteration :  59   Loss :  1.09828163379
Iteration :  60   Loss :  1.05267898067
Iteration :  61   Loss :  1.00896983274
Iteration :  62   Loss :  0.967075568215
Iteration :  63   Loss :  0.926920829832
Iteration :  64   Loss :  0.888433389297
Iteration :  65   Loss :  0.851544017369
Iteration :  66   Loss :  0.816186359329
Iteration :  67   Loss :  0.782296815629
Iteration :  68   Loss :  0.749814427486
Iteration :  69   Loss :  0.718680767241
Iteration :  70   Loss :  0.688839833256
Iteration :  71   Loss :  0.660237949183
Iteration :  72   Loss :  0.632823667412
Iteration :  73   Loss :  0.606547676535
Iteration :  74   Loss :  0.581362712641
Iteration :  75   Loss :  0.557223474303
Iteration :  76   Loss :  0.534086541092
Iteration :  77   Loss :  0.511910295474
Iteration :  78   Loss :  0.490654847952
Iteration :  79   Loss :  0.470281965311
Iteration :  80   Loss :  0.450755001851
Iteration :  81   Loss :  0.432038833468
Iteration :  82   Loss :  0.414099794472
Iteration :  83   Loss :  0.396905617039
Iteration :  84   Loss :  0.380425373158
Iteration :  85   Loss :  0.36462941901
Iteration :  86   Loss :  0.349489341638
Iteration :  87   Loss :  0.334977907845
Iteration :  88   Loss :  0.321069015204
Iteration :  89   Loss :  0.30773764511
Iteration :  90   Loss :  0.294959817776
Iteration :  91   Loss :  0.282712549097
Iteration :  92   Loss :  0.270973809313
Iteration :  93   Loss :  0.259722483378
Iteration :  94   Loss :  0.248938332982
Iteration :  95   Loss :  0.238601960145
Iteration :  96   Loss :  0.228694772328
Iteration :  97   Loss :  0.219198948988
Iteration :  98   Loss :  0.210097409522
Iteration :  99   Loss :  0.201373782546
[-0.00722067 -0.00279894 -0.00222987 ...,  0.0035733   0.00395546
  0.0003303 ]
CROSS VALIDATION 18
Iteration :  0   Loss :  16.2342483272
Iteration :  1   Loss :  9.79586014061
Iteration :  2   Loss :  5.30372801025
Iteration :  3   Loss :  5.08350756657
Iteration :  4   Loss :  4.87243107667
Iteration :  5   Loss :  4.67011886695
Iteration :  6   Loss :  4.47620702853
Iteration :  7   Loss :  4.2903467627
Iteration :  8   Loss :  4.11220375352
Iteration :  9   Loss :  3.94145756643
Iteration :  10   Loss :  3.77780107191
Iteration :  11   Loss :  3.62093989302
Iteration :  12   Loss :  3.47059187588
Iteration :  13   Loss :  3.32648658215
Iteration :  14   Loss :  3.1883648026
Iteration :  15   Loss :  3.05597809082
Iteration :  16   Loss :  2.92908831635
Iteration :  17   Loss :  2.80746723636
Iteration :  18   Loss :  2.69089608504
Iteration :  19   Loss :  2.57916518017
Iteration :  20   Loss :  2.47207354591
Iteration :  21   Loss :  2.36942855128
Iteration :  22   Loss :  2.27104556373
Iteration :  23   Loss :  2.17674761694
Iteration :  24   Loss :  2.0863650926
Iteration :  25   Loss :  1.99973541523
Iteration :  26   Loss :  1.9167027598
Iteration :  27   Loss :  1.8371177714
Iteration :  28   Loss :  1.76083729662
Iteration :  29   Loss :  1.68772412604
Iteration :  30   Loss :  1.61764674742
Iteration :  31   Loss :  1.55047910916
Iteration :  32   Loss :  1.48610039353
Iteration :  33   Loss :  1.42439479939
Iteration :  34   Loss :  1.36525133387
Iteration :  35   Loss :  1.30856361272
Iteration :  36   Loss :  1.25422966897
Iteration :  37   Loss :  1.20215176949
Iteration :  38   Loss :  1.15223623922
Iteration :  39   Loss :  1.10439329265
Iteration :  40   Loss :  1.05853687233
Iteration :  41   Loss :  1.01458449407
Iteration :  42   Loss :  0.972457098583
Iteration :  43   Loss :  0.932078909261
Iteration :  44   Loss :  0.893377295877
Iteration :  45   Loss :  0.856282643946
Iteration :  46   Loss :  0.820728229503
Iteration :  47   Loss :  0.786650099083
Iteration :  48   Loss :  0.753986954685
Iteration :  49   Loss :  0.722680043514
Iteration :  50   Loss :  0.692673052297
Iteration :  51   Loss :  0.663912005989
Iteration :  52   Loss :  0.636345170691
Iteration :  53   Loss :  0.609922960586
Iteration :  54   Loss :  0.584597848752
Iteration :  55   Loss :  0.560324281673
Iteration :  56   Loss :  33.7136153906
Iteration :  57   Loss :  1.51379277538
Iteration :  58   Loss :  1.45093734313
Iteration :  59   Loss :  1.39069178286
Iteration :  60   Loss :  1.33294772795
Iteration :  61   Loss :  1.27760131134
Iteration :  62   Loss :  1.22455297872
Iteration :  63   Loss :  1.17370730945
Iteration :  64   Loss :  1.12497284495
Iteration :  65   Loss :  1.07826192414
Iteration :  66   Loss :  1.03349052582
Iteration :  67   Loss :  0.990578117474
Iteration :  68   Loss :  0.949447510454
Iteration :  69   Loss :  0.910024721125
Iteration :  70   Loss :  0.872238837787
Iteration :  71   Loss :  0.836021893124
Iteration :  72   Loss :  0.801308741944
Iteration :  73   Loss :  0.768036944006
Iteration :  74   Loss :  0.736146651697
Iteration :  75   Loss :  0.705580502388
Iteration :  76   Loss :  0.676283515252
Iteration :  77   Loss :  0.648202992363
Iteration :  78   Loss :  0.621288423913
Iteration :  79   Loss :  0.595491397348
Iteration :  80   Loss :  0.570765510297
Iteration :  81   Loss :  0.547066287095
Iteration :  82   Loss :  0.52435109879
Iteration :  83   Loss :  0.502579086463
Iteration :  84   Loss :  0.481711087728
Iteration :  85   Loss :  0.461709566296
Iteration :  86   Loss :  0.442538544451
Iteration :  87   Loss :  0.424163538339
Iteration :  88   Loss :  0.406551495936
Iteration :  89   Loss :  0.389670737601
Iteration :  90   Loss :  0.373490899088
Iteration :  91   Loss :  0.357982876929
Iteration :  92   Loss :  0.343118776086
Iteration :  93   Loss :  0.328871859773
Iteration :  94   Loss :  0.315216501366
Iteration :  95   Loss :  0.302128138303
Iteration :  96   Loss :  0.289583227904
Iteration :  97   Loss :  0.277559205026
Iteration :  98   Loss :  0.266034441471
Iteration :  99   Loss :  0.254988207082
[-0.00547225 -0.00059224 -0.00124889 ...,  0.00269613  0.00221669
  0.00039617]
CROSS VALIDATION 19
Iteration :  0   Loss :  201.202877299
Iteration :  1   Loss :  57.5056405295
Iteration :  2   Loss :  5.0995040489
Iteration :  3   Loss :  4.88776335593
Iteration :  4   Loss :  4.68481452206
Iteration :  5   Loss :  4.49029249328
Iteration :  6   Loss :  4.30384737331
Iteration :  7   Loss :  4.1251437942
Iteration :  8   Loss :  3.95386031306
Iteration :  9   Loss :  3.78968883391
Iteration :  10   Loss :  3.6323340535
Iteration :  11   Loss :  3.48151293007
Iteration :  12   Loss :  3.33695417429
Iteration :  13   Loss :  3.19839776126
Iteration :  14   Loss :  3.06559446276
Iteration :  15   Loss :  2.93830539903
Iteration :  16   Loss :  2.81630160897
Iteration :  17   Loss :  2.69936363842
Iteration :  18   Loss :  2.58728114532
Iteration :  19   Loss :  2.47985252141
Iteration :  20   Loss :  2.37688452956
Iteration :  21   Loss :  2.27819195621
Iteration :  22   Loss :  2.18359727819
Iteration :  23   Loss :  2.09293034344
Iteration :  24   Loss :  2.00602806491
Iteration :  25   Loss :  1.9227341272
Iteration :  26   Loss :  1.84289870544
Iteration :  27   Loss :  1.76637819576
Iteration :  28   Loss :  1.69303495696
Iteration :  29   Loss :  1.62273706297
Iteration :  30   Loss :  1.55535806554
Iteration :  31   Loss :  1.49077676676
Iteration :  32   Loss :  1.42887700109
Iteration :  33   Loss :  1.36954742639
Iteration :  34   Loss :  1.31268132366
Iteration :  35   Loss :  1.25817640506
Iteration :  36   Loss :  1.20593462991
Iteration :  37   Loss :  1.15586202838
Iteration :  38   Loss :  1.10786853244
Iteration :  39   Loss :  1.06186781384
Iteration :  40   Loss :  1.01777712882
Iteration :  41   Loss :  0.97551716933
Iteration :  42   Loss :  0.935011920299
Iteration :  43   Loss :  0.896188522956
Iteration :  44   Loss :  0.858977143759
Iteration :  45   Loss :  0.823310848777
Iteration :  46   Loss :  0.789125483303
Iteration :  47   Loss :  0.756359556446
Iteration :  48   Loss :  0.724954130529
Iteration :  49   Loss :  0.694852715078
Iteration :  50   Loss :  0.666001165204
Iteration :  51   Loss :  0.638347584211
Iteration :  52   Loss :  0.611842230251
Iteration :  53   Loss :  0.586437426846
Iteration :  54   Loss :  0.562087477135
Iteration :  55   Loss :  0.53874858167
Iteration :  56   Loss :  0.516378759639
Iteration :  57   Loss :  0.494937773349
Iteration :  58   Loss :  0.474387055847
Iteration :  59   Loss :  0.454689641554
Iteration :  60   Loss :  0.435810099766
Iteration :  61   Loss :  0.417714470928
Iteration :  62   Loss :  0.400370205546
Iteration :  63   Loss :  0.383746105643
Iteration :  64   Loss :  0.367812268635
Iteration :  65   Loss :  0.352540033552
Iteration :  66   Loss :  0.337901929476
Iteration :  67   Loss :  0.323871626133
Iteration :  68   Loss :  0.310423886531
Iteration :  69   Loss :  0.297534521561
Iteration :  70   Loss :  0.285180346494
Iteration :  71   Loss :  0.273339139269
Iteration :  72   Loss :  0.261989600527
Iteration :  73   Loss :  0.251111315299
Iteration :  74   Loss :  0.24068471628
Iteration :  75   Loss :  0.230691048636
Iteration :  76   Loss :  0.221112336269
Iteration :  77   Loss :  0.211931349479
Iteration :  78   Loss :  0.203131573978
Iteration :  79   Loss :  0.194697181177
Iteration :  80   Loss :  0.186612999722
Iteration :  81   Loss :  0.1788644882
Iteration :  82   Loss :  0.171437708984
Iteration :  83   Loss :  0.164319303164
Iteration :  84   Loss :  0.157496466514
Iteration :  85   Loss :  0.150956926465
Iteration :  86   Loss :  0.144688920025
Iteration :  87   Loss :  0.138681172625
Iteration :  88   Loss :  0.132922877836
Iteration :  89   Loss :  0.12740367793
Iteration :  90   Loss :  0.122113645254
Iteration :  91   Loss :  0.117043264366
Iteration :  92   Loss :  0.112183414924
Iteration :  93   Loss :  0.10752535528
Iteration :  94   Loss :  0.103060706753
Iteration :  95   Loss :  0.0987814385625
Iteration :  96   Loss :  0.0946798533782
Iteration :  97   Loss :  0.0907485734788
Iteration :  98   Loss :  0.0869805274786
Iteration :  99   Loss :  0.0833689376089
[-0.00252306 -0.00074842 -0.00192762 ...,  0.00134463  0.00044598
  0.00024289]
Accuracy (Hinge Loss):	0.75
lmda : 0.1  eta : 0.001
Logistic Loss
CROSS VALIDATION 0
Iteration :  0   Loss :  84.6771384237
Iteration :  1   Loss :  8.17347281839
Iteration :  2   Loss :  4.86884688993
Iteration :  3   Loss :  2.86775414818
Iteration :  4   Loss :  2.80759171903
Iteration :  5   Loss :  2.7486914336
Iteration :  6   Loss :  2.69102681346
Iteration :  7   Loss :  2.63457193567
Iteration :  8   Loss :  2.57930142111
Iteration :  9   Loss :  2.5251904231
Iteration :  10   Loss :  2.47221461623
Iteration :  11   Loss :  2.4203501854
Iteration :  12   Loss :  2.36957381511
Iteration :  13   Loss :  2.31986267901
Iteration :  14   Loss :  2.27119442963
Iteration :  15   Loss :  2.22354718831
Iteration :  16   Loss :  2.1768995354
Iteration :  17   Loss :  2.13123050058
Iteration :  18   Loss :  2.0865195535
Iteration :  19   Loss :  2.04274659449
Iteration :  20   Loss :  1.99989194556
Iteration :  21   Loss :  1.95793634155
Iteration :  22   Loss :  1.91686092146
Iteration :  23   Loss :  1.87664721995
Iteration :  24   Loss :  1.83727715909
Iteration :  25   Loss :  1.79873304021
Iteration :  26   Loss :  1.76099753592
Iteration :  27   Loss :  1.72405368234
Iteration :  28   Loss :  1.6878848715
Iteration :  29   Loss :  1.65247484382
Iteration :  30   Loss :  1.61780768085
Iteration :  31   Loss :  1.58386779807
Iteration :  32   Loss :  1.55063993793
Iteration :  33   Loss :  1.51810916296
Iteration :  34   Loss :  1.48626084906
Iteration :  35   Loss :  1.45508067893
Iteration :  36   Loss :  1.42455463565
Iteration :  37   Loss :  1.39466899634
Iteration :  38   Loss :  1.36541032602
Iteration :  39   Loss :  1.33676547157
Iteration :  40   Loss :  1.30872155582
Iteration :  41   Loss :  1.28126597173
Iteration :  42   Loss :  1.25438637678
Iteration :  43   Loss :  1.22807068736
Iteration :  44   Loss :  1.20230707338
Iteration :  45   Loss :  1.17708395295
Iteration :  46   Loss :  1.15238998716
Iteration :  47   Loss :  1.12821407498
Iteration :  48   Loss :  1.1045453483
Iteration :  49   Loss :  1.08137316702
Iteration :  50   Loss :  1.05868711427
Iteration :  51   Loss :  1.03647699176
Iteration :  52   Loss :  1.01473281513
Iteration :  53   Loss :  0.993444809569
Iteration :  54   Loss :  0.972603405318
Iteration :  55   Loss :  0.952199233442
Iteration :  56   Loss :  0.932223121598
Iteration :  57   Loss :  0.912666089916
Iteration :  58   Loss :  0.893519346975
Iteration :  59   Loss :  0.874774285855
Iteration :  60   Loss :  0.856422480272
Iteration :  61   Loss :  0.838455680805
Iteration :  62   Loss :  0.82086581119
Iteration :  63   Loss :  0.803644964706
Iteration :  64   Loss :  0.786785400627
Iteration :  65   Loss :  0.770279540756
Iteration :  66   Loss :  0.754119966035
Iteration :  67   Loss :  0.738299413222
Iteration :  68   Loss :  0.722810771643
Iteration :  69   Loss :  0.707647080014
Iteration :  70   Loss :  0.692801523331
Iteration :  71   Loss :  0.678267429827
Iteration :  72   Loss :  0.664038267996
Iteration :  73   Loss :  0.65010764368
Iteration :  74   Loss :  0.636469297222
Iteration :  75   Loss :  0.623117100678
Iteration :  76   Loss :  0.61004505509
Iteration :  77   Loss :  0.597247287818
Iteration :  78   Loss :  0.584718049933
Iteration :  79   Loss :  0.572451713658
Iteration :  80   Loss :  0.560442769873
Iteration :  81   Loss :  0.548685825661
Iteration :  82   Loss :  0.537175601917
Iteration :  83   Loss :  0.525906930994
Iteration :  84   Loss :  0.514874754403
Iteration :  85   Loss :  0.504074120547
Iteration :  86   Loss :  0.493500182509
Iteration :  87   Loss :  0.483148195861
Iteration :  88   Loss :  0.473013516524
Iteration :  89   Loss :  0.463091598642
Iteration :  90   Loss :  0.453377992497
Iteration :  91   Loss :  0.443868342437
Iteration :  92   Loss :  0.434558384827
Iteration :  93   Loss :  0.425443946014
Iteration :  94   Loss :  0.416520940303
Iteration :  95   Loss :  0.407785367944
Iteration :  96   Loss :  0.399233313117
Iteration :  97   Loss :  0.39086094193
Iteration :  98   Loss :  0.382664500413
Iteration :  99   Loss :  0.374640312527
[-0.00404072  0.00220376  0.00327653 ...,  0.00310454  0.00337861
  0.00062006]
CROSS VALIDATION 1
Iteration :  0   Loss :  61.7684463657
Iteration :  1   Loss :  2.59401399274
Iteration :  2   Loss :  2.53959434065
Iteration :  3   Loss :  2.48631635492
Iteration :  4   Loss :  2.43415608462
Iteration :  5   Loss :  2.38309008126
Iteration :  6   Loss :  2.33309538829
Iteration :  7   Loss :  2.28414953075
Iteration :  8   Loss :  2.2362305052
Iteration :  9   Loss :  2.18931676979
Iteration :  10   Loss :  2.14338723461
Iteration :  11   Loss :  2.09842125219
Iteration :  12   Loss :  2.0543986082
Iteration :  13   Loss :  2.01129951242
Iteration :  14   Loss :  1.9691045898
Iteration :  15   Loss :  1.92779487172
Iteration :  16   Loss :  1.88735178756
Iteration :  17   Loss :  1.84775715625
Iteration :  18   Loss :  1.80899317815
Iteration :  19   Loss :  1.77104242705
Iteration :  20   Loss :  1.7338878423
Iteration :  21   Loss :  1.69751272119
Iteration :  22   Loss :  1.6619007114
Iteration :  23   Loss :  1.62703580366
Iteration :  24   Loss :  1.59290232457
Iteration :  25   Loss :  1.55948492953
Iteration :  26   Loss :  1.52676859587
Iteration :  27   Loss :  1.49473861606
Iteration :  28   Loss :  1.46338059113
Iteration :  29   Loss :  1.43268042418
Iteration :  30   Loss :  1.40262431405
Iteration :  31   Loss :  1.37319874911
Iteration :  32   Loss :  1.3443905012
Iteration :  33   Loss :  1.31618661966
Iteration :  34   Loss :  1.28857442554
Iteration :  35   Loss :  1.26154150585
Iteration :  36   Loss :  1.23507570804
Iteration :  37   Loss :  1.20916513449
Iteration :  38   Loss :  1.18379813719
Iteration :  39   Loss :  1.15896331249
Iteration :  40   Loss :  1.13464949598
Iteration :  41   Loss :  1.11084575745
Iteration :  42   Loss :  1.08754139602
Iteration :  43   Loss :  1.06472593528
Iteration :  44   Loss :  1.04238911862
Iteration :  45   Loss :  1.02052090459
Iteration :  46   Loss :  0.999111462413
Iteration :  47   Loss :  0.978151167542
Iteration :  48   Loss :  0.957630597344
Iteration :  49   Loss :  0.937540526863
Iteration :  50   Loss :  0.917871924674
Iteration :  51   Loss :  0.89861594882
Iteration :  52   Loss :  0.879763942841
Iteration :  53   Loss :  0.861307431878
Iteration :  54   Loss :  0.843238118867
Iteration :  55   Loss :  0.825547880808
Iteration :  56   Loss :  0.80822876511
Iteration :  57   Loss :  0.791272986023
Iteration :  58   Loss :  0.774672921131
Iteration :  59   Loss :  0.758421107929
Iteration :  60   Loss :  0.742510240466
Iteration :  61   Loss :  0.726933166066
Iteration :  62   Loss :  0.711682882103
Iteration :  63   Loss :  0.696752532864
Iteration :  64   Loss :  0.682135406458
Iteration :  65   Loss :  0.667824931803
Iteration :  66   Loss :  0.653814675673
Iteration :  67   Loss :  0.640098339802
Iteration :  68   Loss :  0.626669758057
Iteration :  69   Loss :  0.613522893662
Iteration :  70   Loss :  0.600651836489
Iteration :  71   Loss :  0.588050800394
Iteration :  72   Loss :  0.575714120624
Iteration :  73   Loss :  0.563636251266
Iteration :  74   Loss :  0.551811762752
Iteration :  75   Loss :  0.540235339424
Iteration :  76   Loss :  0.52890177714
Iteration :  77   Loss :  0.517805980934
Iteration :  78   Loss :  0.506942962729
Iteration :  79   Loss :  0.496307839093
Iteration :  80   Loss :  0.485895829044
Iteration :  81   Loss :  0.475702251898
Iteration :  82   Loss :  0.465722525171
Iteration :  83   Loss :  0.455952162514
Iteration :  84   Loss :  0.446386771698
Iteration :  85   Loss :  0.43702205264
Iteration :  86   Loss :  0.427853795469
Iteration :  87   Loss :  0.418877878637
Iteration :  88   Loss :  0.410090267062
Iteration :  89   Loss :  0.401487010318
Iteration :  90   Loss :  0.393064240857
Iteration :  91   Loss :  0.384818172274
Iteration :  92   Loss :  0.376745097604
Iteration :  93   Loss :  0.368841387655
Iteration :  94   Loss :  0.361103489379
Iteration :  95   Loss :  0.353527924277
Iteration :  96   Loss :  0.346111286835
Iteration :  97   Loss :  0.338850242995
Iteration :  98   Loss :  0.331741528657
Iteration :  99   Loss :  0.324781948216
[-0.00199199  0.0029073  -0.0011481  ...,  0.00067383 -0.00022959
  0.0017726 ]
CROSS VALIDATION 2
Iteration :  0   Loss :  355.264228726
Iteration :  1   Loss :  65.162013481
Iteration :  2   Loss :  2.83081556598
Iteration :  3   Loss :  2.77142519396
Iteration :  4   Loss :  2.71328166014
Iteration :  5   Loss :  2.65635857454
Iteration :  6   Loss :  2.60063017446
Iteration :  7   Loss :  2.54607128427
Iteration :  8   Loss :  2.49265728723
Iteration :  9   Loss :  2.44036410393
Iteration :  10   Loss :  2.38916817474
Iteration :  11   Loss :  2.33904644476
Iteration :  12   Loss :  2.28997635047
Iteration :  13   Loss :  2.24193580741
Iteration :  14   Loss :  2.19490319879
Iteration :  15   Loss :  2.14885736466
Iteration :  16   Loss :  2.10377759157
Iteration :  17   Loss :  2.05964360271
Iteration :  18   Loss :  2.01643554828
Iteration :  19   Loss :  1.97413399624
Iteration :  20   Loss :  1.93271992328
Iteration :  21   Loss :  1.89217470606
Iteration :  22   Loss :  1.85248011269
Iteration :  23   Loss :  1.81361829435
Iteration :  24   Loss :  1.77557177718
Iteration :  25   Loss :  1.73832345435
Iteration :  26   Loss :  1.70185657825
Iteration :  27   Loss :  1.66615475294
Iteration :  28   Loss :  1.63120192671
Iteration :  29   Loss :  1.59698238482
Iteration :  30   Loss :  1.5634807424
Iteration :  31   Loss :  1.53068193752
Iteration :  32   Loss :  1.49857122436
Iteration :  33   Loss :  1.46713416658
Iteration :  34   Loss :  1.43635663082
Iteration :  35   Loss :  1.40622478031
Iteration :  36   Loss :  1.37672506865
Iteration :  37   Loss :  1.3478442337
Iteration :  38   Loss :  1.31956929159
Iteration :  39   Loss :  1.29188753093
Iteration :  40   Loss :  1.26478650703
Iteration :  41   Loss :  1.23825403632
Iteration :  42   Loss :  1.21227819086
Iteration :  43   Loss :  1.186847293
Iteration :  44   Loss :  1.16194991009
Iteration :  45   Loss :  1.13757484935
Iteration :  46   Loss :  1.11371115284
Iteration :  47   Loss :  1.09034809253
Iteration :  48   Loss :  1.06747516546
Iteration :  49   Loss :  1.04508208905
Iteration :  50   Loss :  1.02315879643
Iteration :  51   Loss :  1.00169543194
Iteration :  52   Loss :  0.980682346701
Iteration :  53   Loss :  0.960110094265
Iteration :  54   Loss :  0.939969426367
Iteration :  55   Loss :  0.920251288775
Iteration :  56   Loss :  0.900946817213
Iteration :  57   Loss :  0.882047333374
Iteration :  58   Loss :  0.863544341025
Iteration :  59   Loss :  0.84542952218
Iteration :  60   Loss :  0.827694733366
Iteration :  61   Loss :  0.81033200196
Iteration :  62   Loss :  0.793333522604
Iteration :  63   Loss :  0.7766916537
Iteration :  64   Loss :  0.760398913973
Iteration :  65   Loss :  0.744447979106
Iteration :  66   Loss :  0.728831678455
Iteration :  67   Loss :  0.713542991822
Iteration :  68   Loss :  0.698575046301
Iteration :  69   Loss :  0.683921113193
Iteration :  70   Loss :  0.669574604977
Iteration :  71   Loss :  0.65552907236
Iteration :  72   Loss :  0.641778201373
Iteration :  73   Loss :  0.628315810537
Iteration :  74   Loss :  0.615135848093
Iteration :  75   Loss :  0.602232389278
Iteration :  76   Loss :  0.589599633673
Iteration :  77   Loss :  0.577231902594
Iteration :  78   Loss :  0.565123636551
Iteration :  79   Loss :  0.553269392752
Iteration :  80   Loss :  0.541663842664
Iteration :  81   Loss :  0.530301769624
Iteration :  82   Loss :  0.519178066506
Iteration :  83   Loss :  0.508287733433
Iteration :  84   Loss :  0.497625875539
Iteration :  85   Loss :  0.487187700781
Iteration :  86   Loss :  0.476968517798
Iteration :  87   Loss :  0.466963733816
Iteration :  88   Loss :  0.457168852597
Iteration :  89   Loss :  0.447579472436
Iteration :  90   Loss :  0.438191284196
Iteration :  91   Loss :  0.429000069393
Iteration :  92   Loss :  0.420001698318
Iteration :  93   Loss :  0.4111921282
Iteration :  94   Loss :  0.402567401411
Iteration :  95   Loss :  0.394123643708
Iteration :  96   Loss :  0.385857062516
Iteration :  97   Loss :  0.377763945245
Iteration :  98   Loss :  0.369840657641
Iteration :  99   Loss :  0.362083642177
[-0.00639262 -0.00016253 -0.00330075 ...,  0.00635582  0.00034165
  0.00035797]
CROSS VALIDATION 3
Iteration :  0   Loss :  61.7792608817
Iteration :  1   Loss :  2.59409272201
Iteration :  2   Loss :  2.53967141826
Iteration :  3   Loss :  2.48639181553
Iteration :  4   Loss :  2.43422996214
Iteration :  5   Loss :  2.38316240891
Iteration :  6   Loss :  2.33316619858
Iteration :  7   Loss :  2.28421885552
Iteration :  8   Loss :  2.23629837561
Iteration :  9   Loss :  2.18938321635
Iteration :  10   Loss :  2.1434522872
Iteration :  11   Loss :  2.09848494004
Iteration :  12   Loss :  2.05446095995
Iteration :  13   Loss :  2.0113605561
Iteration :  14   Loss :  1.96916435284
Iteration :  15   Loss :  1.927853381
Iteration :  16   Loss :  1.88740906937
Iteration :  17   Loss :  1.84781323635
Iteration :  18   Loss :  1.80904808175
Iteration :  19   Loss :  1.77109617883
Iteration :  20   Loss :  1.73394046643
Iteration :  21   Loss :  1.69756424132
Iteration :  22   Loss :  1.66195115069
Iteration :  23   Loss :  1.62708518479
Iteration :  24   Loss :  1.59295066974
Iteration :  25   Loss :  1.55953226047
Iteration :  26   Loss :  1.52681493385
Iteration :  27   Loss :  1.49478398192
Iteration :  28   Loss :  1.46342500526
Iteration :  29   Loss :  1.43272390655
Iteration :  30   Loss :  1.40266688421
Iteration :  31   Loss :  1.37324042619
Iteration :  32   Loss :  1.34443130394
Iteration :  33   Loss :  1.31622656641
Iteration :  34   Loss :  1.28861353424
Iteration :  35   Loss :  1.26157979409
Iteration :  36   Loss :  1.23511319304
Iteration :  37   Loss :  1.20920183309
Iteration :  38   Loss :  1.1838340659
Iteration :  39   Loss :  1.15899848745
Iteration :  40   Loss :  1.13468393301
Iteration :  41   Loss :  1.11087947203
Iteration :  42   Loss :  1.0875744033
Iteration :  43   Loss :  1.0647582501
Iteration :  44   Loss :  1.04242075551
Iteration :  45   Loss :  1.02055187777
Iteration :  46   Loss :  0.99914178581
Iteration :  47   Loss :  0.978180854786
Iteration :  48   Loss :  0.957659661781
Iteration :  49   Loss :  0.937568981559
Iteration :  50   Loss :  0.91789978242
Iteration :  51   Loss :  0.898643222141
Iteration :  52   Loss :  0.879790643997
Iteration :  53   Loss :  0.861333572872
Iteration :  54   Loss :  0.843263711451
Iteration :  55   Loss :  0.825572936486
Iteration :  56   Loss :  0.808253295147
Iteration :  57   Loss :  0.791297001446
Iteration :  58   Loss :  0.774696432736
Iteration :  59   Loss :  0.758444126285
Iteration :  60   Loss :  0.742532775922
Iteration :  61   Loss :  0.726955228752
Iteration :  62   Loss :  0.711704481938
Iteration :  63   Loss :  0.696773679557
Iteration :  64   Loss :  0.682156109515
Iteration :  65   Loss :  0.667845200533
Iteration :  66   Loss :  0.653834519186
Iteration :  67   Loss :  0.64011776702
Iteration :  68   Loss :  0.626688777712
Iteration :  69   Loss :  0.613541514305
Iteration :  70   Loss :  0.60067006649
Iteration :  71   Loss :  0.588068647949
Iteration :  72   Loss :  0.575731593757
Iteration :  73   Loss :  0.56365335783
Iteration :  74   Loss :  0.551828510439
Iteration :  75   Loss :  0.540251735763
Iteration :  76   Loss :  0.5289178295
Iteration :  77   Loss :  0.517821696532
Iteration :  78   Loss :  0.506958348631
Iteration :  79   Loss :  0.496322902214
Iteration :  80   Loss :  0.485910576156
Iteration :  81   Loss :  0.47571668963
Iteration :  82   Loss :  0.465736660014
Iteration :  83   Loss :  0.455966000822
Iteration :  84   Loss :  0.446400319691
Iteration :  85   Loss :  0.437035316409
Iteration :  86   Loss :  0.427866780976
Iteration :  87   Loss :  0.418890591719
Iteration :  88   Loss :  0.410102713433
Iteration :  89   Loss :  0.401499195573
Iteration :  90   Loss :  0.393076170474
Iteration :  91   Loss :  0.384829851614
Iteration :  92   Loss :  0.376756531917
Iteration :  93   Loss :  0.368852582079
Iteration :  94   Loss :  0.361114448945
Iteration :  95   Loss :  0.353538653911
Iteration :  96   Loss :  0.346121791358
Iteration :  97   Loss :  0.338860527126
Iteration :  98   Loss :  0.331751597018
Iteration :  99   Loss :  0.324791805329
[-0.00199228  0.0029065  -0.00114889 ...,  0.00067359 -0.00022944
  0.00177238]
CROSS VALIDATION 4
Iteration :  0   Loss :  61.7792608817
Iteration :  1   Loss :  2.59409272201
Iteration :  2   Loss :  2.53967141826
Iteration :  3   Loss :  2.48639181553
Iteration :  4   Loss :  2.43422996214
Iteration :  5   Loss :  2.38316240891
Iteration :  6   Loss :  2.33316619858
Iteration :  7   Loss :  2.28421885552
Iteration :  8   Loss :  2.23629837561
Iteration :  9   Loss :  2.18938321635
Iteration :  10   Loss :  2.1434522872
Iteration :  11   Loss :  2.09848494004
Iteration :  12   Loss :  2.05446095995
Iteration :  13   Loss :  2.0113605561
Iteration :  14   Loss :  1.96916435284
Iteration :  15   Loss :  1.927853381
Iteration :  16   Loss :  1.88740906937
Iteration :  17   Loss :  1.84781323635
Iteration :  18   Loss :  1.80904808175
Iteration :  19   Loss :  1.77109617883
Iteration :  20   Loss :  1.73394046643
Iteration :  21   Loss :  1.69756424132
Iteration :  22   Loss :  1.66195115069
Iteration :  23   Loss :  1.62708518479
Iteration :  24   Loss :  1.59295066974
Iteration :  25   Loss :  1.55953226047
Iteration :  26   Loss :  1.52681493385
Iteration :  27   Loss :  1.49478398192
Iteration :  28   Loss :  1.46342500526
Iteration :  29   Loss :  1.43272390655
Iteration :  30   Loss :  1.40266688421
Iteration :  31   Loss :  1.37324042619
Iteration :  32   Loss :  1.34443130394
Iteration :  33   Loss :  1.31622656641
Iteration :  34   Loss :  1.28861353424
Iteration :  35   Loss :  1.26157979409
Iteration :  36   Loss :  1.23511319304
Iteration :  37   Loss :  1.20920183309
Iteration :  38   Loss :  1.1838340659
Iteration :  39   Loss :  1.15899848745
Iteration :  40   Loss :  1.13468393301
Iteration :  41   Loss :  1.11087947203
Iteration :  42   Loss :  1.0875744033
Iteration :  43   Loss :  1.0647582501
Iteration :  44   Loss :  1.04242075551
Iteration :  45   Loss :  1.02055187777
Iteration :  46   Loss :  0.99914178581
Iteration :  47   Loss :  0.978180854786
Iteration :  48   Loss :  0.957659661781
Iteration :  49   Loss :  0.937568981559
Iteration :  50   Loss :  0.91789978242
Iteration :  51   Loss :  0.898643222141
Iteration :  52   Loss :  0.879790643997
Iteration :  53   Loss :  0.861333572872
Iteration :  54   Loss :  0.843263711451
Iteration :  55   Loss :  0.825572936486
Iteration :  56   Loss :  0.808253295147
Iteration :  57   Loss :  0.791297001446
Iteration :  58   Loss :  0.774696432736
Iteration :  59   Loss :  0.758444126285
Iteration :  60   Loss :  0.742532775922
Iteration :  61   Loss :  0.726955228752
Iteration :  62   Loss :  0.711704481938
Iteration :  63   Loss :  0.696773679557
Iteration :  64   Loss :  0.682156109515
Iteration :  65   Loss :  0.667845200533
Iteration :  66   Loss :  0.653834519186
Iteration :  67   Loss :  0.64011776702
Iteration :  68   Loss :  0.626688777712
Iteration :  69   Loss :  0.613541514305
Iteration :  70   Loss :  0.60067006649
Iteration :  71   Loss :  0.588068647949
Iteration :  72   Loss :  0.575731593757
Iteration :  73   Loss :  0.56365335783
Iteration :  74   Loss :  0.551828510439
Iteration :  75   Loss :  0.540251735763
Iteration :  76   Loss :  0.5289178295
Iteration :  77   Loss :  0.517821696532
Iteration :  78   Loss :  0.506958348631
Iteration :  79   Loss :  0.496322902214
Iteration :  80   Loss :  0.485910576156
Iteration :  81   Loss :  0.47571668963
Iteration :  82   Loss :  0.465736660014
Iteration :  83   Loss :  0.455966000822
Iteration :  84   Loss :  0.446400319691
Iteration :  85   Loss :  0.437035316409
Iteration :  86   Loss :  0.427866780976
Iteration :  87   Loss :  0.418890591719
Iteration :  88   Loss :  0.410102713433
Iteration :  89   Loss :  0.401499195573
Iteration :  90   Loss :  0.393076170474
Iteration :  91   Loss :  0.384829851614
Iteration :  92   Loss :  0.376756531917
Iteration :  93   Loss :  0.368852582079
Iteration :  94   Loss :  0.361114448945
Iteration :  95   Loss :  0.353538653911
Iteration :  96   Loss :  0.346121791358
Iteration :  97   Loss :  0.338860527126
Iteration :  98   Loss :  0.331751597018
Iteration :  99   Loss :  0.324791805329
[-0.00199228  0.0029065  -0.00114889 ...,  0.00067359 -0.00022944
  0.00177238]
CROSS VALIDATION 5
Iteration :  0   Loss :  61.7276158214
Iteration :  1   Loss :  2.5946541916
Iteration :  2   Loss :  2.54022110882
Iteration :  3   Loss :  2.48692997416
Iteration :  4   Loss :  2.43475683078
Iteration :  5   Loss :  2.38367822441
Iteration :  6   Loss :  2.33367119282
Iteration :  7   Loss :  2.28471325552
Iteration :  8   Loss :  2.23678240362
Iteration :  9   Loss :  2.18985708997
Iteration :  10   Loss :  2.14391621945
Iteration :  11   Loss :  2.09893913949
Iteration :  12   Loss :  2.05490563078
Iteration :  13   Loss :  2.01179589821
Iteration :  14   Loss :  1.96959056193
Iteration :  15   Loss :  1.92827064868
Iteration :  16   Loss :  1.88781758322
Iteration :  17   Loss :  1.84821318002
Iteration :  18   Loss :  1.80943963502
Iteration :  19   Loss :  1.77147951773
Iteration :  20   Loss :  1.73431576329
Iteration :  21   Loss :  1.69793166485
Iteration :  22   Loss :  1.66231086607
Iteration :  23   Loss :  1.62743735372
Iteration :  24   Loss :  1.59329545054
Iteration :  25   Loss :  1.55986980814
Iteration :  26   Loss :  1.52714540013
Iteration :  27   Loss :  1.49510751537
Iteration :  28   Loss :  1.46374175132
Iteration :  29   Loss :  1.43303400761
Iteration :  30   Loss :  1.40297047968
Iteration :  31   Loss :  1.37353765256
Iteration :  32   Loss :  1.34472229481
Iteration :  33   Loss :  1.3165114526
Iteration :  34   Loss :  1.28889244382
Iteration :  35   Loss :  1.26185285245
Iteration :  36   Loss :  1.23538052292
Iteration :  37   Loss :  1.20946355468
Iteration :  38   Loss :  1.18409029684
Iteration :  39   Loss :  1.15924934295
Iteration :  40   Loss :  1.13492952582
Iteration :  41   Loss :  1.11111991256
Iteration :  42   Loss :  1.08780979965
Iteration :  43   Loss :  1.06498870808
Iteration :  44   Loss :  1.04264637873
Iteration :  45   Loss :  1.02077276766
Iteration :  46   Loss :  0.999358041662
Iteration :  47   Loss :  0.978392573821
Iteration :  48   Loss :  0.957866939176
Iteration :  49   Loss :  0.937771910495
Iteration :  50   Loss :  0.918098454123
Iteration :  51   Loss :  0.898837725923
Iteration :  52   Loss :  0.879981067296
Iteration :  53   Loss :  0.861520001294
Iteration :  54   Loss :  0.843446228803
Iteration :  55   Loss :  0.825751624818
Iteration :  56   Loss :  0.808428234789
Iteration :  57   Loss :  0.79146827104
Iteration :  58   Loss :  0.774864109276
Iteration :  59   Loss :  0.75860828515
Iteration :  60   Loss :  0.742693490909
Iteration :  61   Loss :  0.727112572109
Iteration :  62   Loss :  0.711858524399
Iteration :  63   Loss :  0.696924490371
Iteration :  64   Loss :  0.682303756479
Iteration :  65   Loss :  0.66798975002
Iteration :  66   Loss :  0.653976036179
Iteration :  67   Loss :  0.640256315137
Iteration :  68   Loss :  0.626824419237
Iteration :  69   Loss :  0.613674310216
Iteration :  70   Loss :  0.600800076484
Iteration :  71   Loss :  0.588195930472
Iteration :  72   Loss :  0.575856206027
Iteration :  73   Loss :  0.563775355868
Iteration :  74   Loss :  0.551947949088
Iteration :  75   Loss :  0.540368668715
Iteration :  76   Loss :  0.529032309323
Iteration :  77   Loss :  0.51793377469
Iteration :  78   Loss :  0.507068075508
Iteration :  79   Loss :  0.496430327138
Iteration :  80   Loss :  0.486015747418
Iteration :  81   Loss :  0.475819654511
Iteration :  82   Loss :  0.4658374648
Iteration :  83   Loss :  0.45606469083
Iteration :  84   Loss :  0.446496939286
Iteration :  85   Loss :  0.437129909026
Iteration :  86   Loss :  0.427959389139
Iteration :  87   Loss :  0.418981257058
Iteration :  88   Loss :  0.410191476707
Iteration :  89   Loss :  0.401586096684
Iteration :  90   Loss :  0.393161248487
Iteration :  91   Loss :  0.384913144776
Iteration :  92   Loss :  0.376838077669
Iteration :  93   Loss :  0.36893241708
Iteration :  94   Loss :  0.361192609083
Iteration :  95   Loss :  0.35361517432
Iteration :  96   Loss :  0.346196706435
Iteration :  97   Loss :  0.338933870547
Iteration :  98   Loss :  0.33182340175
Iteration :  99   Loss :  0.324862103648
[-0.00199345  0.00291651 -0.00114188 ...,  0.0006718  -0.00023105
  0.00177547]
CROSS VALIDATION 6
Iteration :  0   Loss :  61.779310341
Iteration :  1   Loss :  2.59409090944
Iteration :  2   Loss :  2.53966964372
Iteration :  3   Loss :  2.48639007821
Iteration :  4   Loss :  2.43422826127
Iteration :  5   Loss :  2.38316074372
Iteration :  6   Loss :  2.33316456833
Iteration :  7   Loss :  2.28421725947
Iteration :  8   Loss :  2.23629681304
Iteration :  9   Loss :  2.18938168657
Iteration :  10   Loss :  2.1434507895
Iteration :  11   Loss :  2.09848347376
Iteration :  12   Loss :  2.05445952444
Iteration :  13   Loss :  2.0113591507
Iteration :  14   Loss :  1.96916297692
Iteration :  15   Loss :  1.92785203395
Iteration :  16   Loss :  1.88740775059
Iteration :  17   Loss :  1.84781194523
Iteration :  18   Loss :  1.80904681772
Iteration :  19   Loss :  1.77109494131
Iteration :  20   Loss :  1.73393925488
Iteration :  21   Loss :  1.69756305518
Iteration :  22   Loss :  1.66194998944
Iteration :  23   Loss :  1.6270840479
Iteration :  24   Loss :  1.59294955669
Iteration :  25   Loss :  1.55953117078
Iteration :  26   Loss :  1.52681386702
Iteration :  27   Loss :  1.49478293747
Iteration :  28   Loss :  1.46342398272
Iteration :  29   Loss :  1.43272290546
Iteration :  30   Loss :  1.40266590412
Iteration :  31   Loss :  1.37323946667
Iteration :  32   Loss :  1.34443036455
Iteration :  33   Loss :  1.31622564672
Iteration :  34   Loss :  1.28861263385
Iteration :  35   Loss :  1.26157891259
Iteration :  36   Loss :  1.23511233003
Iteration :  37   Loss :  1.20920098819
Iteration :  38   Loss :  1.18383323872
Iteration :  39   Loss :  1.15899767763
Iteration :  40   Loss :  1.13468314017
Iteration :  41   Loss :  1.11087869582
Iteration :  42   Loss :  1.08757364338
Iteration :  43   Loss :  1.06475750612
Iteration :  44   Loss :  1.04242002714
Iteration :  45   Loss :  1.02055116468
Iteration :  46   Loss :  0.999141087681
Iteration :  47   Loss :  0.978180171303
Iteration :  48   Loss :  0.957658992636
Iteration :  49   Loss :  0.937568326452
Iteration :  50   Loss :  0.917899141057
Iteration :  51   Loss :  0.898642594233
Iteration :  52   Loss :  0.879790029261
Iteration :  53   Loss :  0.861332971033
Iteration :  54   Loss :  0.843263122238
Iteration :  55   Loss :  0.825572359635
Iteration :  56   Loss :  0.808252730398
Iteration :  57   Loss :  0.791296448544
Iteration :  58   Loss :  0.774695891433
Iteration :  59   Loss :  0.758443596339
Iteration :  60   Loss :  0.742532257093
Iteration :  61   Loss :  0.726954720807
Iteration :  62   Loss :  0.711703984649
Iteration :  63   Loss :  0.696773192701
Iteration :  64   Loss :  0.682155632873
Iteration :  65   Loss :  0.66784473389
Iteration :  66   Loss :  0.653834062333
Iteration :  67   Loss :  0.640117319751
Iteration :  68   Loss :  0.626688339826
Iteration :  69   Loss :  0.613541085606
Iteration :  70   Loss :  0.600669646785
Iteration :  71   Loss :  0.588068237049
Iteration :  72   Loss :  0.575731191476
Iteration :  73   Loss :  0.56365296399
Iteration :  74   Loss :  0.551828124861
Iteration :  75   Loss :  0.540251358273
Iteration :  76   Loss :  0.52891745993
Iteration :  77   Loss :  0.517821334715
Iteration :  78   Loss :  0.506957994404
Iteration :  79   Loss :  0.496322555419
Iteration :  80   Loss :  0.485910236636
Iteration :  81   Loss :  0.475716357234
Iteration :  82   Loss :  0.465736334591
Iteration :  83   Loss :  0.455965682225
Iteration :  84   Loss :  0.446400007778
Iteration :  85   Loss :  0.437035011039
Iteration :  86   Loss :  0.427866482013
Iteration :  87   Loss :  0.418890299028
Iteration :  88   Loss :  0.410102426883
Iteration :  89   Loss :  0.401498915034
Iteration :  90   Loss :  0.39307589582
Iteration :  91   Loss :  0.384829582723
Iteration :  92   Loss :  0.376756268666
Iteration :  93   Loss :  0.368852324351
Iteration :  94   Loss :  0.361114196625
Iteration :  95   Loss :  0.353538406884
Iteration :  96   Loss :  0.346121549513
Iteration :  97   Loss :  0.338860290355
Iteration :  98   Loss :  0.331751365214
Iteration :  99   Loss :  0.324791578389
[-0.00199227  0.00290648 -0.00114891 ...,  0.0006736  -0.00022944
  0.00177237]
CROSS VALIDATION 7
Iteration :  0   Loss :  61.779310341
Iteration :  1   Loss :  2.59409090944
Iteration :  2   Loss :  2.53966964372
Iteration :  3   Loss :  2.48639007821
Iteration :  4   Loss :  2.43422826127
Iteration :  5   Loss :  2.38316074372
Iteration :  6   Loss :  2.33316456833
Iteration :  7   Loss :  2.28421725947
Iteration :  8   Loss :  2.23629681304
Iteration :  9   Loss :  2.18938168657
Iteration :  10   Loss :  2.1434507895
Iteration :  11   Loss :  2.09848347376
Iteration :  12   Loss :  2.05445952444
Iteration :  13   Loss :  2.0113591507
Iteration :  14   Loss :  1.96916297692
Iteration :  15   Loss :  1.92785203395
Iteration :  16   Loss :  1.88740775059
Iteration :  17   Loss :  1.84781194523
Iteration :  18   Loss :  1.80904681772
Iteration :  19   Loss :  1.77109494131
Iteration :  20   Loss :  1.73393925488
Iteration :  21   Loss :  1.69756305518
Iteration :  22   Loss :  1.66194998944
Iteration :  23   Loss :  1.6270840479
Iteration :  24   Loss :  1.59294955669
Iteration :  25   Loss :  1.55953117078
Iteration :  26   Loss :  1.52681386702
Iteration :  27   Loss :  1.49478293747
Iteration :  28   Loss :  1.46342398272
Iteration :  29   Loss :  1.43272290546
Iteration :  30   Loss :  1.40266590412
Iteration :  31   Loss :  1.37323946667
Iteration :  32   Loss :  1.34443036455
Iteration :  33   Loss :  1.31622564672
Iteration :  34   Loss :  1.28861263385
Iteration :  35   Loss :  1.26157891259
Iteration :  36   Loss :  1.23511233003
Iteration :  37   Loss :  1.20920098819
Iteration :  38   Loss :  1.18383323872
Iteration :  39   Loss :  1.15899767763
Iteration :  40   Loss :  1.13468314017
Iteration :  41   Loss :  1.11087869582
Iteration :  42   Loss :  1.08757364338
Iteration :  43   Loss :  1.06475750612
Iteration :  44   Loss :  1.04242002714
Iteration :  45   Loss :  1.02055116468
Iteration :  46   Loss :  0.999141087681
Iteration :  47   Loss :  0.978180171303
Iteration :  48   Loss :  0.957658992636
Iteration :  49   Loss :  0.937568326452
Iteration :  50   Loss :  0.917899141057
Iteration :  51   Loss :  0.898642594233
Iteration :  52   Loss :  0.879790029261
Iteration :  53   Loss :  0.861332971033
Iteration :  54   Loss :  0.843263122238
Iteration :  55   Loss :  0.825572359635
Iteration :  56   Loss :  0.808252730398
Iteration :  57   Loss :  0.791296448544
Iteration :  58   Loss :  0.774695891433
Iteration :  59   Loss :  0.758443596339
Iteration :  60   Loss :  0.742532257093
Iteration :  61   Loss :  0.726954720807
Iteration :  62   Loss :  0.711703984649
Iteration :  63   Loss :  0.696773192701
Iteration :  64   Loss :  0.682155632873
Iteration :  65   Loss :  0.66784473389
Iteration :  66   Loss :  0.653834062333
Iteration :  67   Loss :  0.640117319751
Iteration :  68   Loss :  0.626688339826
Iteration :  69   Loss :  0.613541085606
Iteration :  70   Loss :  0.600669646785
Iteration :  71   Loss :  0.588068237049
Iteration :  72   Loss :  0.575731191476
Iteration :  73   Loss :  0.56365296399
Iteration :  74   Loss :  0.551828124861
Iteration :  75   Loss :  0.540251358273
Iteration :  76   Loss :  0.52891745993
Iteration :  77   Loss :  0.517821334715
Iteration :  78   Loss :  0.506957994404
Iteration :  79   Loss :  0.496322555419
Iteration :  80   Loss :  0.485910236636
Iteration :  81   Loss :  0.475716357234
Iteration :  82   Loss :  0.465736334591
Iteration :  83   Loss :  0.455965682225
Iteration :  84   Loss :  0.446400007778
Iteration :  85   Loss :  0.437035011039
Iteration :  86   Loss :  0.427866482013
Iteration :  87   Loss :  0.418890299028
Iteration :  88   Loss :  0.410102426883
Iteration :  89   Loss :  0.401498915034
Iteration :  90   Loss :  0.39307589582
Iteration :  91   Loss :  0.384829582723
Iteration :  92   Loss :  0.376756268666
Iteration :  93   Loss :  0.368852324351
Iteration :  94   Loss :  0.361114196625
Iteration :  95   Loss :  0.353538406884
Iteration :  96   Loss :  0.346121549513
Iteration :  97   Loss :  0.338860290355
Iteration :  98   Loss :  0.331751365214
Iteration :  99   Loss :  0.324791578389
[-0.00199227  0.00290648 -0.00114891 ...,  0.0006736  -0.00022944
  0.00177237]
CROSS VALIDATION 8
Iteration :  0   Loss :  61.779310341
Iteration :  1   Loss :  2.59409090944
Iteration :  2   Loss :  2.53966964372
Iteration :  3   Loss :  2.48639007821
Iteration :  4   Loss :  2.43422826127
Iteration :  5   Loss :  2.38316074372
Iteration :  6   Loss :  2.33316456833
Iteration :  7   Loss :  2.28421725947
Iteration :  8   Loss :  2.23629681304
Iteration :  9   Loss :  2.18938168657
Iteration :  10   Loss :  2.1434507895
Iteration :  11   Loss :  2.09848347376
Iteration :  12   Loss :  2.05445952444
Iteration :  13   Loss :  2.0113591507
Iteration :  14   Loss :  1.96916297692
Iteration :  15   Loss :  1.92785203395
Iteration :  16   Loss :  1.88740775059
Iteration :  17   Loss :  1.84781194523
Iteration :  18   Loss :  1.80904681772
Iteration :  19   Loss :  1.77109494131
Iteration :  20   Loss :  1.73393925488
Iteration :  21   Loss :  1.69756305518
Iteration :  22   Loss :  1.66194998944
Iteration :  23   Loss :  1.6270840479
Iteration :  24   Loss :  1.59294955669
Iteration :  25   Loss :  1.55953117078
Iteration :  26   Loss :  1.52681386702
Iteration :  27   Loss :  1.49478293747
Iteration :  28   Loss :  1.46342398272
Iteration :  29   Loss :  1.43272290546
Iteration :  30   Loss :  1.40266590412
Iteration :  31   Loss :  1.37323946667
Iteration :  32   Loss :  1.34443036455
Iteration :  33   Loss :  1.31622564672
Iteration :  34   Loss :  1.28861263385
Iteration :  35   Loss :  1.26157891259
Iteration :  36   Loss :  1.23511233003
Iteration :  37   Loss :  1.20920098819
Iteration :  38   Loss :  1.18383323872
Iteration :  39   Loss :  1.15899767763
Iteration :  40   Loss :  1.13468314017
Iteration :  41   Loss :  1.11087869582
Iteration :  42   Loss :  1.08757364338
Iteration :  43   Loss :  1.06475750612
Iteration :  44   Loss :  1.04242002714
Iteration :  45   Loss :  1.02055116468
Iteration :  46   Loss :  0.999141087681
Iteration :  47   Loss :  0.978180171303
Iteration :  48   Loss :  0.957658992636
Iteration :  49   Loss :  0.937568326452
Iteration :  50   Loss :  0.917899141057
Iteration :  51   Loss :  0.898642594233
Iteration :  52   Loss :  0.879790029261
Iteration :  53   Loss :  0.861332971033
Iteration :  54   Loss :  0.843263122238
Iteration :  55   Loss :  0.825572359635
Iteration :  56   Loss :  0.808252730398
Iteration :  57   Loss :  0.791296448544
Iteration :  58   Loss :  0.774695891433
Iteration :  59   Loss :  0.758443596339
Iteration :  60   Loss :  0.742532257093
Iteration :  61   Loss :  0.726954720807
Iteration :  62   Loss :  0.711703984649
Iteration :  63   Loss :  0.696773192701
Iteration :  64   Loss :  0.682155632873
Iteration :  65   Loss :  0.66784473389
Iteration :  66   Loss :  0.653834062333
Iteration :  67   Loss :  0.640117319751
Iteration :  68   Loss :  0.626688339826
Iteration :  69   Loss :  0.613541085606
Iteration :  70   Loss :  0.600669646785
Iteration :  71   Loss :  0.588068237049
Iteration :  72   Loss :  0.575731191476
Iteration :  73   Loss :  0.56365296399
Iteration :  74   Loss :  0.551828124861
Iteration :  75   Loss :  0.540251358273
Iteration :  76   Loss :  0.52891745993
Iteration :  77   Loss :  0.517821334715
Iteration :  78   Loss :  0.506957994404
Iteration :  79   Loss :  0.496322555419
Iteration :  80   Loss :  0.485910236636
Iteration :  81   Loss :  0.475716357234
Iteration :  82   Loss :  0.465736334591
Iteration :  83   Loss :  0.455965682225
Iteration :  84   Loss :  0.446400007778
Iteration :  85   Loss :  0.437035011039
Iteration :  86   Loss :  0.427866482013
Iteration :  87   Loss :  0.418890299028
Iteration :  88   Loss :  0.410102426883
Iteration :  89   Loss :  0.401498915034
Iteration :  90   Loss :  0.39307589582
Iteration :  91   Loss :  0.384829582723
Iteration :  92   Loss :  0.376756268666
Iteration :  93   Loss :  0.368852324351
Iteration :  94   Loss :  0.361114196625
Iteration :  95   Loss :  0.353538406884
Iteration :  96   Loss :  0.346121549513
Iteration :  97   Loss :  0.338860290355
Iteration :  98   Loss :  0.331751365214
Iteration :  99   Loss :  0.324791578389
[-0.00199227  0.00290648 -0.00114891 ...,  0.0006736  -0.00022944
  0.00177237]
CROSS VALIDATION 9
Iteration :  0   Loss :  158.461629111
Iteration :  1   Loss :  68.3467355739
Iteration :  2   Loss :  3.0027405059
Iteration :  3   Loss :  2.93974622876
Iteration :  4   Loss :  2.8780735083
Iteration :  5   Loss :  2.81769462025
Iteration :  6   Loss :  2.75858242207
Iteration :  7   Loss :  2.70071034073
Iteration :  8   Loss :  2.64405236078
Iteration :  9   Loss :  2.58858301267
Iteration :  10   Loss :  2.53427736129
Iteration :  11   Loss :  2.4811109948
Iteration :  12   Loss :  2.42906001365
Iteration :  13   Loss :  2.37810101984
Iteration :  14   Loss :  2.32821110646
Iteration :  15   Loss :  2.27936784733
Iteration :  16   Loss :  2.231549287
Iteration :  17   Loss :  2.18473393086
Iteration :  18   Loss :  2.13890073548
Iteration :  19   Loss :  2.0940290992
Iteration :  20   Loss :  2.05009885282
Iteration :  21   Loss :  2.00709025058
Iteration :  22   Loss :  1.96498396129
Iteration :  23   Loss :  1.92376105957
Iteration :  24   Loss :  1.88340301742
Iteration :  25   Loss :  1.84389169581
Iteration :  26   Loss :  1.80520933652
Iteration :  27   Loss :  1.76733855411
Iteration :  28   Loss :  1.73026232806
Iteration :  29   Loss :  1.69396399503
Iteration :  30   Loss :  1.65842724131
Iteration :  31   Loss :  1.62363609538
Iteration :  32   Loss :  1.58957492062
Iteration :  33   Loss :  1.55622840812
Iteration :  34   Loss :  1.52358156967
Iteration :  35   Loss :  1.49161973088
Iteration :  36   Loss :  1.4603285244
Iteration :  37   Loss :  1.4296938833
Iteration :  38   Loss :  1.3997020346
Iteration :  39   Loss :  1.37033949296
Iteration :  40   Loss :  1.34159305447
Iteration :  41   Loss :  1.31344979069
Iteration :  42   Loss :  1.28589704282
Iteration :  43   Loss :  1.25892241603
Iteration :  44   Loss :  1.23251377402
Iteration :  45   Loss :  1.20665923371
Iteration :  46   Loss :  1.18134716016
Iteration :  47   Loss :  1.15656616158
Iteration :  48   Loss :  1.13230508456
Iteration :  49   Loss :  1.10855300938
Iteration :  50   Loss :  1.08529924546
Iteration :  51   Loss :  1.06253332685
Iteration :  52   Loss :  1.04024500784
Iteration :  53   Loss :  1.01842425861
Iteration :  54   Loss :  0.997061260875
Iteration :  55   Loss :  0.976146403608
Iteration :  56   Loss :  0.95567027878
Iteration :  57   Loss :  0.935623677129
Iteration :  58   Loss :  0.915997583957
Iteration :  59   Loss :  0.896783174988
Iteration :  60   Loss :  0.877971812272
Iteration :  61   Loss :  0.859555040153
Iteration :  62   Loss :  0.841524581309
Iteration :  63   Loss :  0.823872332877
Iteration :  64   Loss :  0.806590362658
Iteration :  65   Loss :  0.789670905409
Iteration :  66   Loss :  0.773106359231
Iteration :  67   Loss :  0.756889282043
Iteration :  68   Loss :  0.741012388153
Iteration :  69   Loss :  0.725468544904
Iteration :  70   Loss :  0.710250769422
Iteration :  71   Loss :  0.695352225436
Iteration :  72   Loss :  0.680766220177
Iteration :  73   Loss :  0.66648620136
Iteration :  74   Loss :  0.652505754232
Iteration :  75   Loss :  0.638818598695
Iteration :  76   Loss :  0.62541858649
Iteration :  77   Loss :  0.61229969845
Iteration :  78   Loss :  0.599456041809
Iteration :  79   Loss :  0.58688184757
Iteration :  80   Loss :  0.574571467932
Iteration :  81   Loss :  0.562519373765
Iteration :  82   Loss :  0.550720152136
Iteration :  83   Loss :  0.539168503883
Iteration :  84   Loss :  0.527859241238
Iteration :  85   Loss :  0.516787285485
Iteration :  86   Loss :  0.505947664662
Iteration :  87   Loss :  0.495335511302
Iteration :  88   Loss :  0.484946060205
Iteration :  89   Loss :  0.474774646247
Iteration :  90   Loss :  0.464816702219
Iteration :  91   Loss :  0.455067756693
Iteration :  92   Loss :  0.445523431923
Iteration :  93   Loss :  0.436179441774
Iteration :  94   Loss :  0.427031589682
Iteration :  95   Loss :  0.418075766644
Iteration :  96   Loss :  0.409307949259
Iteration :  97   Loss :  0.400724197796
Iteration :  98   Loss :  0.392320654324
Iteration :  99   Loss :  0.384093540896
[-0.00526566  0.00260645 -0.00075184 ...,  0.00708569  0.00279972
  0.00138353]
CROSS VALIDATION 10
Iteration :  0   Loss :  61.7766995079
Iteration :  1   Loss :  2.59403139159
Iteration :  2   Loss :  2.53961137448
Iteration :  3   Loss :  2.4863330314
Iteration :  4   Loss :  2.43417241125
Iteration :  5   Loss :  2.38310606537
Iteration :  6   Loss :  2.33311103707
Iteration :  7   Loss :  2.28416485124
Iteration :  8   Loss :  2.23624550428
Iteration :  9   Loss :  2.18933145421
Iteration :  10   Loss :  2.14340161097
Iteration :  11   Loss :  2.09843532694
Iteration :  12   Loss :  2.05441238768
Iteration :  13   Loss :  2.01131300282
Iteration :  14   Loss :  1.96911779718
Iteration :  15   Loss :  1.92780780203
Iteration :  16   Loss :  1.8873644466
Iteration :  17   Loss :  1.84776954972
Iteration :  18   Loss :  1.80900531162
Iteration :  19   Loss :  1.77105430597
Iteration :  20   Loss :  1.73389947202
Iteration :  21   Loss :  1.69752410693
Iteration :  22   Loss :  1.66191185827
Iteration :  23   Loss :  1.62704671669
Iteration :  24   Loss :  1.59291300865
Iteration :  25   Loss :  1.55949538948
Iteration :  26   Loss :  1.52677883637
Iteration :  27   Loss :  1.49474864173
Iteration :  28   Loss :  1.46339040647
Iteration :  29   Loss :  1.4326900336
Iteration :  30   Loss :  1.40263372188
Iteration :  31   Loss :  1.37320795957
Iteration :  32   Loss :  1.34439951844
Iteration :  33   Loss :  1.31619544773
Iteration :  34   Loss :  1.2885830684
Iteration :  35   Loss :  1.26154996739
Iteration :  36   Loss :  1.23508399207
Iteration :  37   Loss :  1.20917324473
Iteration :  38   Loss :  1.18380607729
Iteration :  39   Loss :  1.15897108602
Iteration :  40   Loss :  1.13465710642
Iteration :  41   Loss :  1.11085320823
Iteration :  42   Loss :  1.08754869049
Iteration :  43   Loss :  1.06473307672
Iteration :  44   Loss :  1.04239611024
Iteration :  45   Loss :  1.02052774954
Iteration :  46   Loss :  0.999118163759
Iteration :  47   Loss :  0.9781577283
Iteration :  48   Loss :  0.957637020464
Iteration :  49   Loss :  0.937546815233
Iteration :  50   Loss :  0.917878081121
Iteration :  51   Loss :  0.898621976111
Iteration :  52   Loss :  0.879769843686
Iteration :  53   Loss :  0.86131320893
Iteration :  54   Loss :  0.843243774723
Iteration :  55   Loss :  0.825553418009
Iteration :  56   Loss :  0.808234186147
Iteration :  57   Loss :  0.791278293333
Iteration :  58   Loss :  0.774678117099
Iteration :  59   Loss :  0.758426194891
Iteration :  60   Loss :  0.742515220709
Iteration :  61   Loss :  0.726938041828
Iteration :  62   Loss :  0.711687655578
Iteration :  63   Loss :  0.696757206196
Iteration :  64   Loss :  0.682139981748
Iteration :  65   Loss :  0.667829411109
Iteration :  66   Loss :  0.653819061007
Iteration :  67   Loss :  0.640102633137
Iteration :  68   Loss :  0.626673961322
Iteration :  69   Loss :  0.613527008748
Iteration :  70   Loss :  0.600655865244
Iteration :  71   Loss :  0.58805474463
Iteration :  72   Loss :  0.575717982114
Iteration :  73   Loss :  0.563640031746
Iteration :  74   Loss :  0.551815463922
Iteration :  75   Loss :  0.540238962947
Iteration :  76   Loss :  0.528905324645
Iteration :  77   Loss :  0.517809454016
Iteration :  78   Loss :  0.506946362949
Iteration :  79   Loss :  0.49631116798
Iteration :  80   Loss :  0.485899088094
Iteration :  81   Loss :  0.475705442576
Iteration :  82   Loss :  0.465725648912
Iteration :  83   Loss :  0.455955220721
Iteration :  84   Loss :  0.446389765746
Iteration :  85   Loss :  0.437024983875
Iteration :  86   Loss :  0.427856665209
Iteration :  87   Loss :  0.418880688171
Iteration :  88   Loss :  0.410093017653
Iteration :  89   Loss :  0.401489703202
Iteration :  90   Loss :  0.393066877245
Iteration :  91   Loss :  0.384820753351
Iteration :  92   Loss :  0.376747624529
Iteration :  93   Loss :  0.368843861563
Iteration :  94   Loss :  0.361105911382
Iteration :  95   Loss :  0.353530295463
Iteration :  96   Loss :  0.346113608269
Iteration :  97   Loss :  0.338852515719
Iteration :  98   Loss :  0.331743753692
Iteration :  99   Loss :  0.324784126561
[-0.00199188  0.00290549 -0.00114959 ...,  0.00067362 -0.00022945
  0.0017722 ]
CROSS VALIDATION 11
Iteration :  0   Loss :  61.7766801365
Iteration :  1   Loss :  2.59403196836
Iteration :  2   Loss :  2.53961193915
Iteration :  3   Loss :  2.48633358423
Iteration :  4   Loss :  2.43417295247
Iteration :  5   Loss :  2.38310659524
Iteration :  6   Loss :  2.33311155583
Iteration :  7   Loss :  2.28416535911
Iteration :  8   Loss :  2.2362460015
Iteration :  9   Loss :  2.189331941
Iteration :  10   Loss :  2.14340208754
Iteration :  11   Loss :  2.09843579352
Iteration :  12   Loss :  2.05441284447
Iteration :  13   Loss :  2.01131345003
Iteration :  14   Loss :  1.96911823501
Iteration :  15   Loss :  1.92780823067
Iteration :  16   Loss :  1.88736486625
Iteration :  17   Loss :  1.84776996056
Iteration :  18   Loss :  1.80900571385
Iteration :  19   Loss :  1.77105469976
Iteration :  20   Loss :  1.73389985754
Iteration :  21   Loss :  1.69752448437
Iteration :  22   Loss :  1.66191222779
Iteration :  23   Loss :  1.62704707845
Iteration :  24   Loss :  1.59291336283
Iteration :  25   Loss :  1.55949573622
Iteration :  26   Loss :  1.52677917585
Iteration :  27   Loss :  1.49474897408
Iteration :  28   Loss :  1.46339073185
Iteration :  29   Loss :  1.43269035216
Iteration :  30   Loss :  1.40263403375
Iteration :  31   Loss :  1.3732082649
Iteration :  32   Loss :  1.34439981736
Iteration :  33   Loss :  1.31619574038
Iteration :  34   Loss :  1.28858335491
Iteration :  35   Loss :  1.2615502479
Iteration :  36   Loss :  1.23508426669
Iteration :  37   Loss :  1.20917351359
Iteration :  38   Loss :  1.1838063405
Iteration :  39   Loss :  1.15897134371
Iteration :  40   Loss :  1.13465735871
Iteration :  41   Loss :  1.11085345523
Iteration :  42   Loss :  1.0875489323
Iteration :  43   Loss :  1.06473331346
Iteration :  44   Loss :  1.04239634201
Iteration :  45   Loss :  1.02052797645
Iteration :  46   Loss :  0.999118385909
Iteration :  47   Loss :  0.97815794579
Iteration :  48   Loss :  0.957637233391
Iteration :  49   Loss :  0.937547023693
Iteration :  50   Loss :  0.917878285208
Iteration :  51   Loss :  0.898622175917
Iteration :  52   Loss :  0.8797700393
Iteration :  53   Loss :  0.86131340044
Iteration :  54   Loss :  0.843243962215
Iteration :  55   Loss :  0.825553601568
Iteration :  56   Loss :  0.808234365855
Iteration :  57   Loss :  0.791278469271
Iteration :  58   Loss :  0.774678289346
Iteration :  59   Loss :  0.758426363524
Iteration :  60   Loss :  0.742515385805
Iteration :  61   Loss :  0.72693820346
Iteration :  62   Loss :  0.711687813819
Iteration :  63   Loss :  0.696757361117
Iteration :  64   Loss :  0.682140133419
Iteration :  65   Loss :  0.667829559598
Iteration :  66   Loss :  0.653819206382
Iteration :  67   Loss :  0.640102775462
Iteration :  68   Loss :  0.626674100661
Iteration :  69   Loss :  0.613527145163
Iteration :  70   Loss :  0.600655998797
Iteration :  71   Loss :  0.588054875382
Iteration :  72   Loss :  0.575718110123
Iteration :  73   Loss :  0.563640157069
Iteration :  74   Loss :  0.551815586616
Iteration :  75   Loss :  0.540239083067
Iteration :  76   Loss :  0.528905442245
Iteration :  77   Loss :  0.517809569149
Iteration :  78   Loss :  0.506946475667
Iteration :  79   Loss :  0.496311278333
Iteration :  80   Loss :  0.485899196131
Iteration :  81   Loss :  0.475705548347
Iteration :  82   Loss :  0.465725752464
Iteration :  83   Loss :  0.455955322101
Iteration :  84   Loss :  0.446389864999
Iteration :  85   Loss :  0.437025081046
Iteration :  86   Loss :  0.427856760341
Iteration :  87   Loss :  0.418880781308
Iteration :  88   Loss :  0.410093108836
Iteration :  89   Loss :  0.401489792472
Iteration :  90   Loss :  0.393066964642
Iteration :  91   Loss :  0.384820838914
Iteration :  92   Loss :  0.376747708297
Iteration :  93   Loss :  0.368843943574
Iteration :  94   Loss :  0.361105991672
Iteration :  95   Loss :  0.353530374069
Iteration :  96   Loss :  0.346113685226
Iteration :  97   Loss :  0.338852591061
Iteration :  98   Loss :  0.331743827454
Iteration :  99   Loss :  0.324784198776
[-0.00199188  0.0029055  -0.00114959 ...,  0.00067362 -0.00022945
  0.0017722 ]
CROSS VALIDATION 12
Iteration :  0   Loss :  165.707031682
Iteration :  1   Loss :  36.4039249988
Iteration :  2   Loss :  2.9032708494
Iteration :  3   Loss :  2.84236502439
Iteration :  4   Loss :  2.78273690353
Iteration :  5   Loss :  2.72435968469
Iteration :  6   Loss :  2.66720712797
Iteration :  7   Loss :  2.6112535438
Iteration :  8   Loss :  2.55647378143
Iteration :  9   Loss :  2.50284321761
Iteration :  10   Loss :  2.45033774547
Iteration :  11   Loss :  2.39893376376
Iteration :  12   Loss :  2.34860816619
Iteration :  13   Loss :  2.29933833106
Iteration :  14   Loss :  2.25110211112
Iteration :  15   Loss :  2.2038778236
Iteration :  16   Loss :  2.15764424049
Iteration :  17   Loss :  2.11238057899
Iteration :  18   Loss :  2.06806649221
Iteration :  19   Loss :  2.02468206002
Iteration :  20   Loss :  1.98220778006
Iteration :  21   Loss :  1.94062455908
Iteration :  22   Loss :  1.89991370427
Iteration :  23   Loss :  1.86005691491
Iteration :  24   Loss :  1.82103627417
Iteration :  25   Loss :  1.78283424101
Iteration :  26   Loss :  1.74543364235
Iteration :  27   Loss :  1.70881766533
Iteration :  28   Loss :  1.67296984977
Iteration :  29   Loss :  1.63787408075
Iteration :  30   Loss :  1.60351458143
Iteration :  31   Loss :  1.56987590588
Iteration :  32   Loss :  1.53694293221
Iteration :  33   Loss :  1.50470085573
Iteration :  34   Loss :  1.47313518233
Iteration :  35   Loss :  1.44223172194
Iteration :  36   Loss :  1.41197658218
Iteration :  37   Loss :  1.38235616209
Iteration :  38   Loss :  1.35335714603
Iteration :  39   Loss :  1.3249664977
Iteration :  40   Loss :  1.29717145429
Iteration :  41   Loss :  1.2699595207
Iteration :  42   Loss :  1.24331846399
Iteration :  43   Loss :  1.21723630782
Iteration :  44   Loss :  1.19170132711
Iteration :  45   Loss :  1.16670204275
Iteration :  46   Loss :  1.14222721645
Iteration :  47   Loss :  1.11826584566
Iteration :  48   Loss :  1.09480715868
Iteration :  49   Loss :  1.07184060977
Iteration :  50   Loss :  1.04935587442
Iteration :  51   Loss :  1.02734284472
Iteration :  52   Loss :  1.00579162482
Iteration :  53   Loss :  0.984692526459
Iteration :  54   Loss :  0.96403606464
Iteration :  55   Loss :  0.943812953338
Iteration :  56   Loss :  0.924014101345
Iteration :  57   Loss :  0.904630608176
Iteration :  58   Loss :  0.885653760069
Iteration :  59   Loss :  0.867075026072
Iteration :  60   Loss :  0.848886054202
Iteration :  61   Loss :  0.831078667697
Iteration :  62   Loss :  0.813644861338
Iteration :  63   Loss :  0.796576797851
Iteration :  64   Loss :  0.779866804386
Iteration :  65   Loss :  0.763507369066
Iteration :  66   Loss :  0.747491137615
Iteration :  67   Loss :  0.731810910048
Iteration :  68   Loss :  0.716459637442
Iteration :  69   Loss :  0.70143041876
Iteration :  70   Loss :  0.686716497761
Iteration :  71   Loss :  0.672311259953
Iteration :  72   Loss :  0.658208229631
Iteration :  73   Loss :  0.644401066963
Iteration :  74   Loss :  0.630883565143
Iteration :  75   Loss :  0.617649647604
Iteration :  76   Loss :  0.604693365292
Iteration :  77   Loss :  0.592008893989
Iteration :  78   Loss :  0.579590531703
Iteration :  79   Loss :  0.567432696108
Iteration :  80   Loss :  0.555529922039
Iteration :  81   Loss :  0.543876859039
Iteration :  82   Loss :  0.53246826896
Iteration :  83   Loss :  0.521299023613
Iteration :  84   Loss :  0.510364102471
Iteration :  85   Loss :  0.499658590416
Iteration :  86   Loss :  0.489177675539
Iteration :  87   Loss :  0.478916646984
Iteration :  88   Loss :  0.468870892837
Iteration :  89   Loss :  0.459035898063
Iteration :  90   Loss :  0.449407242485
Iteration :  91   Loss :  0.439980598804
Iteration :  92   Loss :  0.430751730665
Iteration :  93   Loss :  0.421716490761
Iteration :  94   Loss :  0.412870818975
Iteration :  95   Loss :  0.404210740567
Iteration :  96   Loss :  0.395732364391
Iteration :  97   Loss :  0.387431881152
Iteration :  98   Loss :  0.379305561697
Iteration :  99   Loss :  0.371349755341
[-0.0139635  -0.00262841 -0.00501502 ...,  0.00452852  0.00294831
  0.00035248]
CROSS VALIDATION 13
Iteration :  0   Loss :  233.32084818
Iteration :  1   Loss :  65.4390968119
Iteration :  2   Loss :  2.87336074734
Iteration :  3   Loss :  2.81308069771
Iteration :  4   Loss :  2.75406525935
Iteration :  5   Loss :  2.69628790207
Iteration :  6   Loss :  2.63972265223
Iteration :  7   Loss :  2.58434408112
Iteration :  8   Loss :  2.53012729348
Iteration :  9   Loss :  2.47704791632
Iteration :  10   Loss :  2.425082088
Iteration :  11   Loss :  2.37420644742
Iteration :  12   Loss :  2.32439812363
Iteration :  13   Loss :  2.27563472545
Iteration :  14   Loss :  2.22789433145
Iteration :  15   Loss :  2.1811554801
Iteration :  16   Loss :  2.13539716009
Iteration :  17   Loss :  2.09059880092
Iteration :  18   Loss :  2.04674026363
Iteration :  19   Loss :  2.00380183177
Iteration :  20   Loss :  1.9617642025
Iteration :  21   Loss :  1.92060847794
Iteration :  22   Loss :  1.88031615667
Iteration :  23   Loss :  1.8408691254
Iteration :  24   Loss :  1.80224965086
Iteration :  25   Loss :  1.76444037178
Iteration :  26   Loss :  1.72742429113
Iteration :  27   Loss :  1.69118476844
Iteration :  28   Loss :  1.65570551236
Iteration :  29   Loss :  1.6209705733
Iteration :  30   Loss :  1.58696433629
Iteration :  31   Loss :  1.55367151392
Iteration :  32   Loss :  1.52107713953
Iteration :  33   Loss :  1.48916656042
Iteration :  34   Loss :  1.45792543128
Iteration :  35   Loss :  1.42733970778
Iteration :  36   Loss :  1.39739564019
Iteration :  37   Loss :  1.36807976727
Iteration :  38   Loss :  1.33937891015
Iteration :  39   Loss :  1.31128016646
Iteration :  40   Loss :  1.2837709045
Iteration :  41   Loss :  1.25683875757
Iteration :  42   Loss :  1.23047161842
Iteration :  43   Loss :  1.20465763377
Iteration :  44   Loss :  1.17938519905
Iteration :  45   Loss :  1.1546429531
Iteration :  46   Loss :  1.13041977314
Iteration :  47   Loss :  1.1067047697
Iteration :  48   Loss :  1.0834872818
Iteration :  49   Loss :  1.06075687208
Iteration :  50   Loss :  1.03850332216
Iteration :  51   Loss :  1.01671662804
Iteration :  52   Loss :  0.995386995579
Iteration :  53   Loss :  0.974504836107
Iteration :  54   Loss :  0.954060762124
Iteration :  55   Loss :  0.934045583065
Iteration :  56   Loss :  0.914450301176
Iteration :  57   Loss :  0.895266107464
Iteration :  58   Loss :  0.876484377743
Iteration :  59   Loss :  0.858096668755
Iteration :  60   Loss :  0.84009471437
Iteration :  61   Loss :  0.82247042188
Iteration :  62   Loss :  0.80521586835
Iteration :  63   Loss :  0.788323297066
Iteration :  64   Loss :  0.771785114042
Iteration :  65   Loss :  0.755593884611
Iteration :  66   Loss :  0.739742330079
Iteration :  67   Loss :  0.72422332446
Iteration :  68   Loss :  0.709029891264
Iteration :  69   Loss :  0.694155200371
Iteration :  70   Loss :  0.679592564955
Iteration :  71   Loss :  0.665335438483
Iteration :  72   Loss :  0.651377411769
Iteration :  73   Loss :  0.6377122101
Iteration :  74   Loss :  0.624333690412
Iteration :  75   Loss :  0.611235838532
Iteration :  76   Loss :  0.598412766477
Iteration :  77   Loss :  0.585858709809
Iteration :  78   Loss :  0.573568025048
Iteration :  79   Loss :  0.561535187133
Iteration :  80   Loss :  0.549754786949
Iteration :  81   Loss :  0.538221528894
Iteration :  82   Loss :  0.526930228504
Iteration :  83   Loss :  0.51587581013
Iteration :  84   Loss :  0.505053304656
Iteration :  85   Loss :  0.494457847278
Iteration :  86   Loss :  0.484084675318
Iteration :  87   Loss :  0.473929126094
Iteration :  88   Loss :  0.463986634832
Iteration :  89   Loss :  0.454252732617
Iteration :  90   Loss :  0.444723044403
Iteration :  91   Loss :  0.435393287047
Iteration :  92   Loss :  0.426259267403
Iteration :  93   Loss :  0.417316880441
Iteration :  94   Loss :  0.408562107417
Iteration :  95   Loss :  0.399991014081
Iteration :  96   Loss :  0.391599748916
Iteration :  97   Loss :  0.383384541424
Iteration :  98   Loss :  0.375341700439
Iteration :  99   Loss :  0.367467612482
[-0.00872243 -0.00331092 -0.00477011 ...,  0.00374864 -0.00105407
  0.00104626]
CROSS VALIDATION 14
Iteration :  0   Loss :  61.7746984389
Iteration :  1   Loss :  2.59431980626
Iteration :  2   Loss :  2.53989373853
Iteration :  3   Loss :  2.48660947175
Iteration :  4   Loss :  2.43444305217
Iteration :  5   Loss :  2.38337102853
Iteration :  6   Loss :  2.33337044159
Iteration :  7   Loss :  2.28441881373
Iteration :  8   Loss :  2.23649413891
Iteration :  9   Loss :  2.18957487274
Iteration :  10   Loss :  2.14363992284
Iteration :  11   Loss :  2.09866863928
Iteration :  12   Loss :  2.05464080538
Iteration :  13   Loss :  2.01153662856
Iteration :  14   Loss :  1.96933673149
Iteration :  15   Loss :  1.92802214333
Iteration :  16   Loss :  1.88757429125
Iteration :  17   Loss :  1.84797499205
Iteration :  18   Loss :  1.80920644399
Iteration :  19   Loss :  1.7712512188
Iteration :  20   Loss :  1.73409225382
Iteration :  21   Loss :  1.69771284438
Iteration :  22   Loss :  1.66209663621
Iteration :  23   Loss :  1.62722761818
Iteration :  24   Loss :  1.59309011502
Iteration :  25   Loss :  1.55966878035
Iteration :  26   Loss :  1.52694858969
Iteration :  27   Loss :  1.4949148338
Iteration :  28   Loss :  1.46355311201
Iteration :  29   Loss :  1.43284932575
Iteration :  30   Loss :  1.40278967224
Iteration :  31   Loss :  1.37336063827
Iteration :  32   Loss :  1.3445489941
Iteration :  33   Loss :  1.31634178755
Iteration :  34   Loss :  1.28872633817
Iteration :  35   Loss :  1.26169023151
Iteration :  36   Loss :  1.2352213136
Iteration :  37   Loss :  1.2093076854
Iteration :  38   Loss :  1.18393769753
Iteration :  39   Loss :  1.15909994501
Iteration :  40   Loss :  1.13478326209
Iteration :  41   Loss :  1.11097671729
Iteration :  42   Loss :  1.08766960846
Iteration :  43   Loss :  1.06485145796
Iteration :  44   Loss :  1.04251200796
Iteration :  45   Loss :  1.02064121585
Iteration :  46   Loss :  0.999229249671
Iteration :  47   Loss :  0.978266483748
Iteration :  48   Loss :  0.957743494338
Iteration :  49   Loss :  0.937651055399
Iteration :  50   Loss :  0.917980134438
Iteration :  51   Loss :  0.898721888459
Iteration :  52   Loss :  0.879867659979
Iteration :  53   Loss :  0.861408973141
Iteration :  54   Loss :  0.843337529903
Iteration :  55   Loss :  0.825645206306
Iteration :  56   Loss :  0.808324048823
Iteration :  57   Loss :  0.791366270785
Iteration :  58   Loss :  0.774764248878
Iteration :  59   Loss :  0.758510519716
Iteration :  60   Loss :  0.74259777649
Iteration :  61   Loss :  0.727018865676
Iteration :  62   Loss :  0.711766783827
Iteration :  63   Loss :  0.696834674419
Iteration :  64   Loss :  0.68221582477
Iteration :  65   Loss :  0.667903663025
Iteration :  66   Loss :  0.653891755197
Iteration :  67   Loss :  0.640173802279
Iteration :  68   Loss :  0.626743637411
Iteration :  69   Loss :  0.613595223105
Iteration :  70   Loss :  0.600722648535
Iteration :  71   Loss :  0.588120126877
Iteration :  72   Loss :  0.57578199271
Iteration :  73   Loss :  0.563702699464
Iteration :  74   Loss :  0.551876816935
Iteration :  75   Loss :  0.540299028835
Iteration :  76   Loss :  0.528964130408
Iteration :  77   Loss :  0.517867026089
Iteration :  78   Loss :  0.507002727212
Iteration :  79   Loss :  0.496366349769
Iteration :  80   Loss :  0.485953112211
Iteration :  81   Loss :  0.475758333305
Iteration :  82   Loss :  0.465777430024
Iteration :  83   Loss :  0.456005915489
Iteration :  84   Loss :  0.446439396952
Iteration :  85   Loss :  0.437073573821
Iteration :  86   Loss :  0.427904235727
Iteration :  87   Loss :  0.418927260631
Iteration :  88   Loss :  0.410138612974
Iteration :  89   Loss :  0.40153434186
Iteration :  90   Loss :  0.393110579279
Iteration :  91   Loss :  0.384863538375
Iteration :  92   Loss :  0.376789511736
Iteration :  93   Loss :  0.368884869735
Iteration :  94   Loss :  0.361146058894
Iteration :  95   Loss :  0.353569600291
Iteration :  96   Loss :  0.346152087995
Iteration :  97   Loss :  0.338890187535
Iteration :  98   Loss :  0.331780634406
Iteration :  99   Loss :  0.3248202326
[-0.0019921   0.00290748 -0.00114857 ...,  0.00067367 -0.00022937
  0.00177268]
CROSS VALIDATION 15
Iteration :  0   Loss :  61.7746984389
Iteration :  1   Loss :  2.59431980626
Iteration :  2   Loss :  2.53989373853
Iteration :  3   Loss :  2.48660947175
Iteration :  4   Loss :  2.43444305217
Iteration :  5   Loss :  2.38337102853
Iteration :  6   Loss :  2.33337044159
Iteration :  7   Loss :  2.28441881373
Iteration :  8   Loss :  2.23649413891
Iteration :  9   Loss :  2.18957487274
Iteration :  10   Loss :  2.14363992284
Iteration :  11   Loss :  2.09866863928
Iteration :  12   Loss :  2.05464080538
Iteration :  13   Loss :  2.01153662856
Iteration :  14   Loss :  1.96933673149
Iteration :  15   Loss :  1.92802214333
Iteration :  16   Loss :  1.88757429125
Iteration :  17   Loss :  1.84797499205
Iteration :  18   Loss :  1.80920644399
Iteration :  19   Loss :  1.7712512188
Iteration :  20   Loss :  1.73409225382
Iteration :  21   Loss :  1.69771284438
Iteration :  22   Loss :  1.66209663621
Iteration :  23   Loss :  1.62722761818
Iteration :  24   Loss :  1.59309011502
Iteration :  25   Loss :  1.55966878035
Iteration :  26   Loss :  1.52694858969
Iteration :  27   Loss :  1.4949148338
Iteration :  28   Loss :  1.46355311201
Iteration :  29   Loss :  1.43284932575
Iteration :  30   Loss :  1.40278967224
Iteration :  31   Loss :  1.37336063827
Iteration :  32   Loss :  1.3445489941
Iteration :  33   Loss :  1.31634178755
Iteration :  34   Loss :  1.28872633817
Iteration :  35   Loss :  1.26169023151
Iteration :  36   Loss :  1.2352213136
Iteration :  37   Loss :  1.2093076854
Iteration :  38   Loss :  1.18393769753
Iteration :  39   Loss :  1.15909994501
Iteration :  40   Loss :  1.13478326209
Iteration :  41   Loss :  1.11097671729
Iteration :  42   Loss :  1.08766960846
Iteration :  43   Loss :  1.06485145796
Iteration :  44   Loss :  1.04251200796
Iteration :  45   Loss :  1.02064121585
Iteration :  46   Loss :  0.999229249671
Iteration :  47   Loss :  0.978266483748
Iteration :  48   Loss :  0.957743494338
Iteration :  49   Loss :  0.937651055399
Iteration :  50   Loss :  0.917980134438
Iteration :  51   Loss :  0.898721888459
Iteration :  52   Loss :  0.879867659979
Iteration :  53   Loss :  0.861408973141
Iteration :  54   Loss :  0.843337529903
Iteration :  55   Loss :  0.825645206306
Iteration :  56   Loss :  0.808324048823
Iteration :  57   Loss :  0.791366270785
Iteration :  58   Loss :  0.774764248878
Iteration :  59   Loss :  0.758510519717
Iteration :  60   Loss :  0.74259777649
Iteration :  61   Loss :  0.727018865677
Iteration :  62   Loss :  0.711766783828
Iteration :  63   Loss :  0.69683467442
Iteration :  64   Loss :  0.682215824771
Iteration :  65   Loss :  0.667903663026
Iteration :  66   Loss :  0.653891755198
Iteration :  67   Loss :  0.640173802281
Iteration :  68   Loss :  0.626743637413
Iteration :  69   Loss :  0.613595223108
Iteration :  70   Loss :  0.60072264854
Iteration :  71   Loss :  0.588120126884
Iteration :  72   Loss :  0.575781992718
Iteration :  73   Loss :  0.563702699475
Iteration :  74   Loss :  0.551876816948
Iteration :  75   Loss :  0.540299028853
Iteration :  76   Loss :  0.528964130431
Iteration :  77   Loss :  0.517867026119
Iteration :  78   Loss :  0.507002727251
Iteration :  79   Loss :  0.496366349818
Iteration :  80   Loss :  0.485953112275
Iteration :  81   Loss :  0.475758333387
Iteration :  82   Loss :  0.465777430129
Iteration :  83   Loss :  0.456005915622
Iteration :  84   Loss :  0.446439397121
Iteration :  85   Loss :  0.437073574035
Iteration :  86   Loss :  0.427904235998
Iteration :  87   Loss :  0.418927260973
Iteration :  88   Loss :  0.410138613405
Iteration :  89   Loss :  0.4015343424
Iteration :  90   Loss :  0.393110579956
Iteration :  91   Loss :  0.38486353922
Iteration :  92   Loss :  0.37678951279
Iteration :  93   Loss :  0.368884871046
Iteration :  94   Loss :  0.361146060521
Iteration :  95   Loss :  0.353569602305
Iteration :  96   Loss :  0.346152090483
Iteration :  97   Loss :  0.338890190602
Iteration :  98   Loss :  0.331780638178
Iteration :  99   Loss :  0.324820237228
[-0.0019921   0.00290748 -0.00114857 ...,  0.00067367 -0.00022937
  0.00177268]
CROSS VALIDATION 16
Iteration :  0   Loss :  134.623771838
Iteration :  1   Loss :  2.52360585715
Iteration :  2   Loss :  2.46331618388
Iteration :  3   Loss :  2.41163844278
Iteration :  4   Loss :  2.36104484752
Iteration :  5   Loss :  2.31151265419
Iteration :  6   Loss :  2.26301959604
Iteration :  7   Loss :  2.21554387356
Iteration :  8   Loss :  2.16906414458
Iteration :  9   Loss :  2.12355951477
Iteration :  10   Loss :  2.07900952819
Iteration :  11   Loss :  2.03539415816
Iteration :  12   Loss :  1.9926937982
Iteration :  13   Loss :  1.9508892533
Iteration :  14   Loss :  1.90996173124
Iteration :  15   Loss :  1.86989283418
Iteration :  16   Loss :  1.83066455038
Iteration :  17   Loss :  1.79225924616
Iteration :  18   Loss :  1.75465965795
Iteration :  19   Loss :  1.71784888453
Iteration :  20   Loss :  1.68181037948
Iteration :  21   Loss :  1.64652794373
Iteration :  22   Loss :  1.61198571832
Iteration :  23   Loss :  1.57816817725
Iteration :  24   Loss :  1.54506012051
Iteration :  25   Loss :  1.5126466673
Iteration :  26   Loss :  1.48091324929
Iteration :  27   Loss :  1.44984560412
Iteration :  28   Loss :  1.41942976894
Iteration :  29   Loss :  1.38965207417
Iteration :  30   Loss :  1.36049913731
Iteration :  31   Loss :  1.33195785687
Iteration :  32   Loss :  1.30401540653
Iteration :  33   Loss :  1.27665922922
Iteration :  34   Loss :  1.24987703148
Iteration :  35   Loss :  1.22365677781
Iteration :  36   Loss :  1.19798668519
Iteration :  37   Loss :  1.17285521766
Iteration :  38   Loss :  1.14825108098
Iteration :  39   Loss :  1.12416321744
Iteration :  40   Loss :  1.10058080068
Iteration :  41   Loss :  1.07749323072
Iteration :  42   Loss :  1.05489012898
Iteration :  43   Loss :  1.03276133343
Iteration :  44   Loss :  1.01109689397
Iteration :  45   Loss :  0.989887067738
Iteration :  46   Loss :  0.969122314715
Iteration :  47   Loss :  0.948793293388
Iteration :  48   Loss :  0.928890856586
Iteration :  49   Loss :  0.909406047462
Iteration :  50   Loss :  0.890330095632
Iteration :  51   Loss :  0.871654413464
Iteration :  52   Loss :  0.853370592498
Iteration :  53   Loss :  0.835470400006
Iteration :  54   Loss :  0.817945775647
Iteration :  55   Loss :  0.800788828218
Iteration :  56   Loss :  0.783991832469
Iteration :  57   Loss :  0.767547225964
Iteration :  58   Loss :  0.751447605968
Iteration :  59   Loss :  0.735685726352
Iteration :  60   Loss :  0.7202544945
Iteration :  61   Loss :  0.705146968223
Iteration :  62   Loss :  0.690356352672
Iteration :  63   Loss :  0.675875997263
Iteration :  64   Loss :  0.661699392622
Iteration :  65   Loss :  0.647820167547
Iteration :  66   Loss :  0.634232086023
Iteration :  67   Loss :  0.620929044269
Iteration :  68   Loss :  0.607905067846
Iteration :  69   Loss :  0.595154308829
Iteration :  70   Loss :  0.582671043039
Iteration :  71   Loss :  0.570449667341
Iteration :  72   Loss :  0.558484697011
Iteration :  73   Loss :  0.546770763169
Iteration :  74   Loss :  0.535302610272
Iteration :  75   Loss :  0.52407509366
Iteration :  76   Loss :  0.513083177167
Iteration :  77   Loss :  0.502321930762
Iteration :  78   Loss :  0.491786528247
Iteration :  79   Loss :  0.481472244981
Iteration :  80   Loss :  0.471374455643
Iteration :  81   Loss :  0.461488632013
Iteration :  82   Loss :  0.451810340789
Iteration :  83   Loss :  0.442335241414
Iteration :  84   Loss :  0.433059083941
Iteration :  85   Loss :  0.423977706912
Iteration :  86   Loss :  0.415087035274
Iteration :  87   Loss :  0.406383078335
Iteration :  88   Loss :  0.397861927768
Iteration :  89   Loss :  0.389519755667
Iteration :  90   Loss :  0.381352812688
Iteration :  91   Loss :  0.373357426258
Iteration :  92   Loss :  0.365529998879
Iteration :  93   Loss :  0.357867006531
Iteration :  94   Loss :  0.350364997172
Iteration :  95   Loss :  0.343020589322
Iteration :  96   Loss :  0.335830470747
Iteration :  97   Loss :  0.328791397196
Iteration :  98   Loss :  0.321900191204
Iteration :  99   Loss :  0.315153740923
[-0.00539114  0.00194729 -0.00301885 ...,  0.00361488 -0.00202585
  0.00122987]
CROSS VALIDATION 17
Iteration :  0   Loss :  61.7755272035
Iteration :  1   Loss :  2.59429768622
Iteration :  2   Loss :  2.53987208254
Iteration :  3   Loss :  2.48658827008
Iteration :  4   Loss :  2.43442229529
Iteration :  5   Loss :  2.38335070711
Iteration :  6   Loss :  2.33335054649
Iteration :  7   Loss :  2.28439933601
Iteration :  8   Loss :  2.23647506981
Iteration :  9   Loss :  2.1895562037
Iteration :  10   Loss :  2.14362164545
Iteration :  11   Loss :  2.09865074533
Iteration :  12   Loss :  2.05462328683
Iteration :  13   Loss :  2.01151947753
Iteration :  14   Loss :  1.96931994027
Iteration :  15   Loss :  1.92800570437
Iteration :  16   Loss :  1.88755819716
Iteration :  17   Loss :  1.8479592356
Iteration :  18   Loss :  1.80919101809
Iteration :  19   Loss :  1.77123611652
Iteration :  20   Loss :  1.73407746837
Iteration :  21   Loss :  1.69769836911
Iteration :  22   Loss :  1.66208246462
Iteration :  23   Loss :  1.62721374389
Iteration :  24   Loss :  1.5930765318
Iteration :  25   Loss :  1.55965548209
Iteration :  26   Loss :  1.52693557041
Iteration :  27   Loss :  1.49490208765
Iteration :  28   Loss :  1.46354063326
Iteration :  29   Loss :  1.4328371088
Iteration :  30   Loss :  1.40277771159
Iteration :  31   Loss :  1.37334892854
Iteration :  32   Loss :  1.34453753002
Iteration :  33   Loss :  1.31633056398
Iteration :  34   Loss :  1.28871535005
Iteration :  35   Loss :  1.26167947392
Iteration :  36   Loss :  1.23521078168
Iteration :  37   Loss :  1.20929737444
Iteration :  38   Loss :  1.18392760288
Iteration :  39   Loss :  1.15909006213
Iteration :  40   Loss :  1.13477358655
Iteration :  41   Loss :  1.11096724473
Iteration :  42   Loss :  1.08766033462
Iteration :  43   Loss :  1.06484237868
Iteration :  44   Loss :  1.04250311916
Iteration :  45   Loss :  1.02063251352
Iteration :  46   Loss :  0.999220729907
Iteration :  47   Loss :  0.97825814272
Iteration :  48   Loss :  0.957735328296
Iteration :  49   Loss :  0.937643060672
Iteration :  50   Loss :  0.917972307432
Iteration :  51   Loss :  0.898714225655
Iteration :  52   Loss :  0.879860157933
Iteration :  53   Loss :  0.86140162848
Iteration :  54   Loss :  0.843330339324
Iteration :  55   Loss :  0.825638166578
Iteration :  56   Loss :  0.808317156781
Iteration :  57   Loss :  0.791359523331
Iteration :  58   Loss :  0.774757642978
Iteration :  59   Loss :  0.758504052402
Iteration :  60   Loss :  0.742591444853
Iteration :  61   Loss :  0.72701266687
Iteration :  62   Loss :  0.711760715066
Iteration :  63   Loss :  0.696828732974
Iteration :  64   Loss :  0.68221000797
Iteration :  65   Loss :  0.667897968255
Iteration :  66   Loss :  0.653886179898
Iteration :  67   Loss :  0.640168343945
Iteration :  68   Loss :  0.626738293587
Iteration :  69   Loss :  0.61358999139
Iteration :  70   Loss :  0.600717526577
Iteration :  71   Loss :  0.588115112374
Iteration :  72   Loss :  0.575777083408
Iteration :  73   Loss :  0.563697893157
Iteration :  74   Loss :  0.551872111462
Iteration :  75   Loss :  0.540294422082
Iteration :  76   Loss :  0.528959620306
Iteration :  77   Loss :  0.517862610611
Iteration :  78   Loss :  0.506998404376
Iteration :  79   Loss :  0.496362117633
Iteration :  80   Loss :  0.485948968876
Iteration :  81   Loss :  0.475754276912
Iteration :  82   Loss :  0.465773458755
Iteration :  83   Loss :  0.456002027564
Iteration :  84   Loss :  0.44643559063
Iteration :  85   Loss :  0.4370698474
Iteration :  86   Loss :  0.427900587543
Iteration :  87   Loss :  0.41892368906
Iteration :  88   Loss :  0.410135116427
Iteration :  89   Loss :  0.401530918785
Iteration :  90   Loss :  0.393107228165
Iteration :  91   Loss :  0.384860257747
Iteration :  92   Loss :  0.376786300159
Iteration :  93   Loss :  0.368881725813
Iteration :  94   Loss :  0.361142981273
Iteration :  95   Loss :  0.353566587658
Iteration :  96   Loss :  0.34614913908
Iteration :  97   Loss :  0.338887301118
Iteration :  98   Loss :  0.331777809314
Iteration :  99   Loss :  0.324817467713
[-0.00199199  0.00290725 -0.0011487  ...,  0.00067367 -0.00022937
  0.00177263]
CROSS VALIDATION 18
Iteration :  0   Loss :  129.72406955
Iteration :  1   Loss :  117.626725907
Iteration :  2   Loss :  2.84112844616
Iteration :  3   Loss :  2.78152471769
Iteration :  4   Loss :  2.72317142995
Iteration :  5   Loss :  2.66604235234
Iteration :  6   Loss :  2.61011180483
Iteration :  7   Loss :  2.55535464636
Iteration :  8   Loss :  2.50174626362
Iteration :  9   Loss :  2.44926255992
Iteration :  10   Loss :  2.3978799444
Iteration :  11   Loss :  2.3475753214
Iteration :  12   Loss :  2.2983260801
Iteration :  13   Loss :  2.25011008428
Iteration :  14   Loss :  2.20290566243
Iteration :  15   Loss :  2.15669159788
Iteration :  16   Loss :  2.11144711927
Iteration :  17   Loss :  2.06715189116
Iteration :  18   Loss :  2.02378600475
Iteration :  19   Loss :  1.9813299689
Iteration :  20   Loss :  1.9397647012
Iteration :  21   Loss :  1.89907151935
Iteration :  22   Loss :  1.85923213251
Iteration :  23   Loss :  1.82022863303
Iteration :  24   Loss :  1.78204348817
Iteration :  25   Loss :  1.7446595321
Iteration :  26   Loss :  1.70805995801
Iteration :  27   Loss :  1.67222831043
Iteration :  28   Loss :  1.63714847771
Iteration :  29   Loss :  1.60280468474
Iteration :  30   Loss :  1.56918148578
Iteration :  31   Loss :  1.5362637576
Iteration :  32   Loss :  1.50403669274
Iteration :  33   Loss :  1.472485793
Iteration :  34   Loss :  1.44159686316
Iteration :  35   Loss :  1.41135600486
Iteration :  36   Loss :  1.38174961062
Iteration :  37   Loss :  1.3527643581
Iteration :  38   Loss :  1.3243872044
Iteration :  39   Loss :  1.29660538055
Iteration :  40   Loss :  1.26940638604
Iteration :  41   Loss :  1.24277798346
Iteration :  42   Loss :  1.21670819315
Iteration :  43   Loss :  1.19118528801
Iteration :  44   Loss :  1.16619778823
Iteration :  45   Loss :  1.14173445616
Iteration :  46   Loss :  1.11778429119
Iteration :  47   Loss :  1.09433652475
Iteration :  48   Loss :  1.07138061532
Iteration :  49   Loss :  1.04890624353
Iteration :  50   Loss :  1.0269033074
Iteration :  51   Loss :  1.00536191761
Iteration :  52   Loss :  0.98427239291
Iteration :  53   Loss :  0.963625255617
Iteration :  54   Loss :  0.943411227233
Iteration :  55   Loss :  0.923621224152
Iteration :  56   Loss :  0.90424635348
Iteration :  57   Loss :  0.885277908958
Iteration :  58   Loss :  0.866707366979
Iteration :  59   Loss :  0.848526382709
Iteration :  60   Loss :  0.830726786297
Iteration :  61   Loss :  0.813300579174
Iteration :  62   Loss :  0.796239930442
Iteration :  63   Loss :  0.779537173342
Iteration :  64   Loss :  0.763184801804
Iteration :  65   Loss :  0.747175467075
Iteration :  66   Loss :  0.731501974417
Iteration :  67   Loss :  0.716157279882
Iteration :  68   Loss :  0.701134487155
Iteration :  69   Loss :  0.686426844464
Iteration :  70   Loss :  0.672027741553
Iteration :  71   Loss :  0.657930706725
Iteration :  72   Loss :  0.644129403942
Iteration :  73   Loss :  0.630617629988
Iteration :  74   Loss :  0.617389311693
Iteration :  75   Loss :  0.604438503207
Iteration :  76   Loss :  0.591759383345
Iteration :  77   Loss :  0.579346252971
Iteration :  78   Loss :  0.56719353245
Iteration :  79   Loss :  0.555295759142
Iteration :  80   Loss :  0.543647584959
Iteration :  81   Loss :  0.532243773961
Iteration :  82   Loss :  0.521079200014
Iteration :  83   Loss :  0.510148844485
Iteration :  84   Loss :  0.499447793995
Iteration :  85   Loss :  0.488971238212
Iteration :  86   Loss :  0.478714467693
Iteration :  87   Loss :  0.468672871772
Iteration :  88   Loss :  0.458841936486
Iteration :  89   Loss :  0.449217242555
Iteration :  90   Loss :  0.439794463392
Iteration :  91   Loss :  0.430569363163
Iteration :  92   Loss :  0.421537794885
Iteration :  93   Loss :  0.412695698563
Iteration :  94   Loss :  0.404039099366
Iteration :  95   Loss :  0.395564105843
Iteration :  96   Loss :  0.387266908176
Iteration :  97   Loss :  0.379143776463
Iteration :  98   Loss :  0.371191059049
Iteration :  99   Loss :  0.363405180885
[-0.00343158 -0.00136791 -0.00269135 ...,  0.00447703  0.00175876
  0.00135852]
CROSS VALIDATION 19
Iteration :  0   Loss :  115.663745727
Iteration :  1   Loss :  2.56791344916
Iteration :  2   Loss :  2.46123470703
Iteration :  3   Loss :  2.40960062534
Iteration :  4   Loss :  2.3590497722
Iteration :  5   Loss :  2.3095594227
Iteration :  6   Loss :  2.26110732872
Iteration :  7   Loss :  2.21367170888
Iteration :  8   Loss :  2.16723123877
Iteration :  9   Loss :  2.12176504138
Iteration :  10   Loss :  2.0772526777
Iteration :  11   Loss :  2.03367413753
Iteration :  12   Loss :  1.99100983051
Iteration :  13   Loss :  1.9492405773
Iteration :  14   Loss :  1.90834760096
Iteration :  15   Loss :  1.86831251854
Iteration :  16   Loss :  1.82911733279
Iteration :  17   Loss :  1.79074442408
Iteration :  18   Loss :  1.75317654252
Iteration :  19   Loss :  1.71639680017
Iteration :  20   Loss :  1.68038866349
Iteration :  21   Loss :  1.64513594588
Iteration :  22   Loss :  1.61062280045
Iteration :  23   Loss :  1.57683371289
Iteration :  24   Loss :  1.54375349451
Iteration :  25   Loss :  1.51136727542
Iteration :  26   Loss :  1.47966049785
Iteration :  27   Loss :  1.44861890964
Iteration :  28   Loss :  1.41822855784
Iteration :  29   Loss :  1.38847578243
Iteration :  30   Loss :  1.35934721021
Iteration :  31   Loss :  1.3308297488
Iteration :  32   Loss :  1.30291058077
Iteration :  33   Loss :  1.27557715787
Iteration :  34   Loss :  1.2488171954
Iteration :  35   Loss :  1.2226186667
Iteration :  36   Loss :  1.19696979773
Iteration :  37   Loss :  1.17185906177
Iteration :  38   Loss :  1.14727517422
Iteration :  39   Loss :  1.12320708752
Iteration :  40   Loss :  1.0996439861
Iteration :  41   Loss :  1.07657528152
Iteration :  42   Loss :  1.05399060763
Iteration :  43   Loss :  1.0318798158
Iteration :  44   Loss :  1.01023297031
Iteration :  45   Loss :  0.989040343727
Iteration :  46   Loss :  0.96829241244
Iteration :  47   Loss :  0.947979852196
Iteration :  48   Loss :  0.928093533776
Iteration :  49   Loss :  0.908624518716
Iteration :  50   Loss :  0.889564055122
Iteration :  51   Loss :  0.870903573574
Iteration :  52   Loss :  0.852634683129
Iteration :  53   Loss :  0.834749167428
Iteration :  54   Loss :  0.817238980921
Iteration :  55   Loss :  0.800096245213
Iteration :  56   Loss :  0.783313245544
Iteration :  57   Loss :  0.766882427403
Iteration :  58   Loss :  0.750796393283
Iteration :  59   Loss :  0.735047899566
Iteration :  60   Loss :  0.719629853534
Iteration :  61   Loss :  0.704535310493
Iteration :  62   Loss :  0.689757470995
Iteration :  63   Loss :  0.675289678129
Iteration :  64   Loss :  0.661125414877
Iteration :  65   Loss :  0.647258301492
Iteration :  66   Loss :  0.633682092907
Iteration :  67   Loss :  0.620390676138
Iteration :  68   Loss :  0.607378067686
Iteration :  69   Loss :  0.594638410933
Iteration :  70   Loss :  0.582165973528
Iteration :  71   Loss :  0.569955144772
Iteration :  72   Loss :  0.55800043301
Iteration :  73   Loss :  0.54629646304
Iteration :  74   Loss :  0.534837973548
Iteration :  75   Loss :  0.523619814578
Iteration :  76   Loss :  0.51263694505
Iteration :  77   Loss :  0.501884430329
Iteration :  78   Loss :  0.491357439847
Iteration :  79   Loss :  0.481051244797
Iteration :  80   Loss :  0.470961215877
Iteration :  81   Loss :  0.461082821104
Iteration :  82   Loss :  0.451411623686
Iteration :  83   Loss :  0.441943279958
Iteration :  84   Loss :  0.432673537372
Iteration :  85   Loss :  0.423598232547
Iteration :  86   Loss :  0.414713289365
Iteration :  87   Loss :  0.406014717125
Iteration :  88   Loss :  0.397498608734
Iteration :  89   Loss :  0.38916113896
Iteration :  90   Loss :  0.380998562704
Iteration :  91   Loss :  0.373007213338
Iteration :  92   Loss :  0.365183501061
Iteration :  93   Loss :  0.357523911309
Iteration :  94   Loss :  0.350025003186
Iteration :  95   Loss :  0.342683407945
Iteration :  96   Loss :  0.335495827488
Iteration :  97   Loss :  0.32845903291
Iteration :  98   Loss :  0.32156986307
Iteration :  99   Loss :  0.314825223189
[-0.00389047  0.00016309 -0.00236424 ...,  0.00057896  0.000639    0.00175967]
Accuracy (Logistic Loss):	0.85
Hinge Loss
CROSS VALIDATION 0
Iteration :  0   Loss :  190.88766221
Iteration :  1   Loss :  2.82621840961
Iteration :  2   Loss :  2.76692735604
Iteration :  3   Loss :  2.70888016566
Iteration :  4   Loss :  2.65205074354
Iteration :  5   Loss :  2.5964135422
Iteration :  6   Loss :  2.54194355011
Iteration :  7   Loss :  2.48861628047
Iteration :  8   Loss :  2.43640776018
Iteration :  9   Loss :  2.38529451906
Iteration :  10   Loss :  2.33525357933
Iteration :  11   Loss :  2.28626244524
Iteration :  12   Loss :  2.238299093
Iteration :  13   Loss :  2.19134196082
Iteration :  14   Loss :  2.14536993929
Iteration :  15   Loss :  2.10036236183
Iteration :  16   Loss :  2.05629899543
Iteration :  17   Loss :  2.01316003155
Iteration :  18   Loss :  1.97092607721
Iteration :  19   Loss :  1.92957814627
Iteration :  20   Loss :  1.88909765091
Iteration :  21   Loss :  1.84946639324
Iteration :  22   Loss :  1.81066655717
Iteration :  23   Loss :  1.77268070036
Iteration :  24   Loss :  1.73549174638
Iteration :  25   Loss :  1.69908297707
Iteration :  26   Loss :  1.66343802498
Iteration :  27   Loss :  1.62854086604
Iteration :  28   Loss :  1.59437581234
Iteration :  29   Loss :  1.5609275051
Iteration :  30   Loss :  1.52818090774
Iteration :  31   Loss :  1.49612129913
Iteration :  32   Loss :  1.46473426698
Iteration :  33   Loss :  1.43400570135
Iteration :  34   Loss :  1.40392178832
Iteration :  35   Loss :  1.37446900375
Iteration :  36   Loss :  1.34563410725
Iteration :  37   Loss :  1.31740413618
Iteration :  38   Loss :  1.28976639986
Iteration :  39   Loss :  1.26270847382
Iteration :  40   Loss :  1.23621819426
Iteration :  41   Loss :  1.21028365257
Iteration :  42   Loss :  1.18489318995
Iteration :  43   Loss :  1.16003539221
Iteration :  44   Loss :  1.1356990846
Iteration :  45   Loss :  1.11187332682
Iteration :  46   Loss :  1.08854740808
Iteration :  47   Loss :  1.06571084228
Iteration :  48   Loss :  1.04335336332
Iteration :  49   Loss :  1.02146492046
Iteration :  50   Loss :  1.00003567384
Iteration :  51   Loss :  0.979055989993
Iteration :  52   Loss :  0.958516437583
Iteration :  53   Loss :  0.938407783117
Iteration :  54   Loss :  0.918720986815
Iteration :  55   Loss :  0.899447198542
Iteration :  56   Loss :  0.880577753828
Iteration :  57   Loss :  0.862104169977
Iteration :  58   Loss :  0.844018142249
Iteration :  59   Loss :  0.826311540129
Iteration :  60   Loss :  0.808976403672
Iteration :  61   Loss :  0.792004939924
Iteration :  62   Loss :  0.775389519418
Iteration :  63   Loss :  0.759122672746
Iteration :  64   Loss :  0.7431970872
Iteration :  65   Loss :  0.727605603486
Iteration :  66   Loss :  0.712341212502
Iteration :  67   Loss :  0.69739705219
Iteration :  68   Loss :  0.682766404454
Iteration :  69   Loss :  0.668442692131
Iteration :  70   Loss :  0.654419476045
Iteration :  71   Loss :  0.640690452104
Iteration :  72   Loss :  0.62724944847
Iteration :  73   Loss :  0.614090422783
Iteration :  74   Loss :  0.601207459446
Iteration :  75   Loss :  0.588594766966
Iteration :  76   Loss :  0.576246675347
Iteration :  77   Loss :  0.564157633545
Iteration :  78   Loss :  0.552322206971
Iteration :  79   Loss :  0.540735075047
Iteration :  80   Loss :  0.529391028815
Iteration :  81   Loss :  0.518284968597
Iteration :  82   Loss :  0.507411901699
Iteration :  83   Loss :  0.49676694017
Iteration :  84   Loss :  0.486345298601
Iteration :  85   Loss :  0.476142291978
Iteration :  86   Loss :  0.466153333573
Iteration :  87   Loss :  0.456373932881
Iteration :  88   Loss :  0.446799693605
Iteration :  89   Loss :  0.437426311676
Iteration :  90   Loss :  0.428249573322
Iteration :  91   Loss :  0.419265353169
Iteration :  92   Loss :  0.410469612391
Iteration :  93   Loss :  0.401858396891
Iteration :  94   Loss :  0.393427835525
Iteration :  95   Loss :  0.385174138362
Iteration :  96   Loss :  0.377093594978
Iteration :  97   Loss :  0.369182572793
Iteration :  98   Loss :  0.361437515431
Iteration :  99   Loss :  0.353854941128
[-0.00719351  0.00052873  0.00080373 ...,  0.00221955  0.00293277
 -0.00035222]
CROSS VALIDATION 1
Iteration :  0   Loss :  89.1558470117
Iteration :  1   Loss :  2.47796723549
Iteration :  2   Loss :  2.42598212082
Iteration :  3   Loss :  2.37508759852
Iteration :  4   Loss :  2.32526078912
Iteration :  5   Loss :  2.27647929314
Iteration :  6   Loss :  2.228721181
Iteration :  7   Loss :  2.18196498323
Iteration :  8   Loss :  2.1361896807
Iteration :  9   Loss :  2.09137469529
Iteration :  10   Loss :  2.04749988057
Iteration :  11   Loss :  2.00454551276
Iteration :  12   Loss :  1.96249228185
Iteration :  13   Loss :  1.92132128296
Iteration :  14   Loss :  1.8810140078
Iteration :  15   Loss :  1.84155233636
Iteration :  16   Loss :  1.80291852878
Iteration :  17   Loss :  1.76509521735
Iteration :  18   Loss :  1.72806539873
Iteration :  19   Loss :  1.69181242629
Iteration :  20   Loss :  1.65632000262
Iteration :  21   Loss :  1.62157217221
Iteration :  22   Loss :  1.58755331429
Iteration :  23   Loss :  1.55424813579
Iteration :  24   Loss :  1.52164166449
Iteration :  25   Loss :  1.48971924224
Iteration :  26   Loss :  1.45846651843
Iteration :  27   Loss :  1.42786944349
Iteration :  28   Loss :  1.39791426261
Iteration :  29   Loss :  1.36858750954
Iteration :  30   Loss :  1.33987600053
Iteration :  31   Loss :  1.31176682841
Iteration :  32   Loss :  1.2842473568
Iteration :  33   Loss :  1.25730521441
Iteration :  34   Loss :  1.23092828948
Iteration :  35   Loss :  1.20510472436
Iteration :  36   Loss :  1.17982291014
Iteration :  37   Loss :  1.15507148148
Iteration :  38   Loss :  1.13083931143
Iteration :  39   Loss :  1.10711550652
Iteration :  40   Loss :  1.08388940178
Iteration :  41   Loss :  1.06115055599
Iteration :  42   Loss :  1.03888874698
Iteration :  43   Loss :  1.01709396703
Iteration :  44   Loss :  0.995756418351
Iteration :  45   Loss :  0.974866508733
Iteration :  46   Loss :  0.954414847179
Iteration :  47   Loss :  0.934392239713
Iteration :  48   Loss :  0.914789685237
Iteration :  49   Loss :  0.895598371486
Iteration :  50   Loss :  0.876809671067
Iteration :  51   Loss :  0.858415137581
Iteration :  52   Loss :  0.840406501825
Iteration :  53   Loss :  0.822775668077
Iteration :  54   Loss :  0.805514710451
Iteration :  55   Loss :  0.788615869341
Iteration :  56   Loss :  0.772071547928
Iteration :  57   Loss :  0.755874308766
Iteration :  58   Loss :  0.740016870439
Iteration :  59   Loss :  0.724492104286
Iteration :  60   Loss :  0.709293031201
Iteration :  61   Loss :  0.694412818488
Iteration :  62   Loss :  0.679844776798
Iteration :  63   Loss :  0.665582357113
Iteration :  64   Loss :  0.651619147811
Iteration :  65   Loss :  0.637948871775
Iteration :  66   Loss :  0.624565383577
Iteration :  67   Loss :  0.611462666715
Iteration :  68   Loss :  0.598634830903
Iteration :  69   Loss :  0.586076109431
Iteration :  70   Loss :  0.573780856566
Iteration :  71   Loss :  0.561743545017
Iteration :  72   Loss :  0.54995876345
Iteration :  73   Loss :  0.538421214055
Iteration :  74   Loss :  0.527125710164
Iteration :  75   Loss :  0.516067173919
Iteration :  76   Loss :  0.505240633993
Iteration :  77   Loss :  0.494641223348
Iteration :  78   Loss :  0.484264177056
Iteration :  79   Loss :  0.474104830148
Iteration :  80   Loss :  0.464158615522
Iteration :  81   Loss :  0.454421061891
Iteration :  82   Loss :  0.44488779177
Iteration :  83   Loss :  0.435554519506
Iteration :  84   Loss :  0.426417049359
Iteration :  85   Loss :  0.417471273608
Iteration :  86   Loss :  0.40871317071
Iteration :  87   Loss :  0.400138803486
Iteration :  88   Loss :  0.391744317359
Iteration :  89   Loss :  0.383525938615
Iteration :  90   Loss :  0.375479972708
Iteration :  91   Loss :  0.3676028026
Iteration :  92   Loss :  0.359890887135
Iteration :  93   Loss :  0.352340759447
Iteration :  94   Loss :  0.3449490254
Iteration :  95   Loss :  0.337712362065
Iteration :  96   Loss :  0.330627516222
Iteration :  97   Loss :  0.323691302903
Iteration :  98   Loss :  0.316900603956
Iteration :  99   Loss :  0.310252366643
[-0.00411286 -0.00083363 -0.0002304  ...,  0.00272512  0.0026645
  0.00069562]
CROSS VALIDATION 2
Iteration :  0   Loss :  179.024126893
Iteration :  1   Loss :  2.80459442683
Iteration :  2   Loss :  2.7457570214
Iteration :  3   Loss :  2.6881539621
Iteration :  4   Loss :  2.63175935367
Iteration :  5   Loss :  2.57654784408
Iteration :  6   Loss :  2.52249461319
Iteration :  7   Loss :  2.46957536155
Iteration :  8   Loss :  2.41776629946
Iteration :  9   Loss :  2.36704413635
Iteration :  10   Loss :  2.31738607022
Iteration :  11   Loss :  2.26876977745
Iteration :  12   Loss :  2.22117340275
Iteration :  13   Loss :  2.17457554933
Iteration :  14   Loss :  2.12895526926
Iteration :  15   Loss :  2.0842920541
Iteration :  16   Loss :  2.04056582565
Iteration :  17   Loss :  1.99775692693
Iteration :  18   Loss :  1.95584611332
Iteration :  19   Loss :  1.91481454397
Iteration :  20   Loss :  1.87464377327
Iteration :  21   Loss :  1.83531574257
Iteration :  22   Loss :  1.79681277209
Iteration :  23   Loss :  1.75911755294
Iteration :  24   Loss :  1.72221313935
Iteration :  25   Loss :  1.68608294108
Iteration :  26   Loss :  1.65071071591
Iteration :  27   Loss :  1.61608056236
Iteration :  28   Loss :  1.58217691257
Iteration :  29   Loss :  1.54898452526
Iteration :  30   Loss :  1.5164884789
Iteration :  31   Loss :  1.484674165
Iteration :  32   Loss :  1.45352728154
Iteration :  33   Loss :  1.42303382653
Iteration :  34   Loss :  1.39318009175
Iteration :  35   Loss :  1.36395265655
Iteration :  36   Loss :  1.33533838182
Iteration :  37   Loss :  1.30732440411
Iteration :  38   Loss :  1.27989812983
Iteration :  39   Loss :  1.25304722959
Iteration :  40   Loss :  1.22675963265
Iteration :  41   Loss :  1.20102352151
Iteration :  42   Loss :  1.17582732659
Iteration :  43   Loss :  1.15115972101
Iteration :  44   Loss :  1.12700961554
Iteration :  45   Loss :  1.10336615357
Iteration :  46   Loss :  1.08021870626
Iteration :  47   Loss :  1.05755686776
Iteration :  48   Loss :  1.0353704505
Iteration :  49   Loss :  1.01364948066
Iteration :  50   Loss :  0.992384193641
Iteration :  51   Loss :  0.971565029707
Iteration :  52   Loss :  0.951182629669
Iteration :  53   Loss :  0.931227830686
Iteration :  54   Loss :  0.911691662142
Iteration :  55   Loss :  0.892565341617
Iteration :  56   Loss :  0.873840270936
Iteration :  57   Loss :  0.855508032304
Iteration :  58   Loss :  0.837560384522
Iteration :  59   Loss :  0.819989259285
Iteration :  60   Loss :  0.802786757549
Iteration :  61   Loss :  0.785945145988
Iteration :  62   Loss :  0.769456853507
Iteration :  63   Loss :  0.753314467851
Iteration :  64   Loss :  0.737510732261
Iteration :  65   Loss :  0.722038542219
Iteration :  66   Loss :  0.706890942254
Iteration :  67   Loss :  0.69206112281
Iteration :  68   Loss :  0.677542417192
Iteration :  69   Loss :  0.663328298561
Iteration :  70   Loss :  0.649412377007
Iteration :  71   Loss :  0.635788396673
Iteration :  72   Loss :  0.622450232943
Iteration :  73   Loss :  0.609391889689
Iteration :  74   Loss :  0.596607496576
Iteration :  75   Loss :  0.584091306419
Iteration :  76   Loss :  0.571837692609
Iteration :  77   Loss :  0.559841146571
Iteration :  78   Loss :  0.548096275298
Iteration :  79   Loss :  0.536597798922
Iteration :  80   Loss :  0.525340548339
Iteration :  81   Loss :  0.514319462889
Iteration :  82   Loss :  0.503529588079
Iteration :  83   Loss :  0.492966073357
Iteration :  84   Loss :  0.482624169928
Iteration :  85   Loss :  0.472499228623
Iteration :  86   Loss :  0.462586697809
Iteration :  87   Loss :  0.452882121338
Iteration :  88   Loss :  0.443381136551
Iteration :  89   Loss :  0.43407947231
Iteration :  90   Loss :  0.424972947083
Iteration :  91   Loss :  0.41605746706
Iteration :  92   Loss :  0.407329024318
Iteration :  93   Loss :  0.398783695012
Iteration :  94   Loss :  0.390417637619
Iteration :  95   Loss :  0.382227091203
Iteration :  96   Loss :  0.374208373731
Iteration :  97   Loss :  0.366357880416
Iteration :  98   Loss :  0.358672082094
Iteration :  99   Loss :  0.351147523638
[-0.0048241  -0.00088933 -0.00495644 ...,  0.00836813  0.00197823
  0.00069442]
CROSS VALIDATION 3
Iteration :  0   Loss :  175.023419988
Iteration :  1   Loss :  2.80781800136
Iteration :  2   Loss :  2.74891296877
Iteration :  3   Loss :  2.69124370106
Iteration :  4   Loss :  2.63478427319
Iteration :  5   Loss :  2.57950930401
Iteration :  6   Loss :  2.52539394485
Iteration :  7   Loss :  2.47241386831
Iteration :  8   Loss :  2.42054525737
Iteration :  9   Loss :  2.36976479468
Iteration :  10   Loss :  2.32004965204
Iteration :  11   Loss :  2.27137748016
Iteration :  12   Loss :  2.22372639863
Iteration :  13   Loss :  2.17707498607
Iteration :  14   Loss :  2.13140227048
Iteration :  15   Loss :  2.08668771984
Iteration :  16   Loss :  2.04291123288
Iteration :  17   Loss :  2.00005313001
Iteration :  18   Loss :  1.95809414451
Iteration :  19   Loss :  1.91701541386
Iteration :  20   Loss :  1.87679847124
Iteration :  21   Loss :  1.83742523727
Iteration :  22   Loss :  1.79887801184
Iteration :  23   Loss :  1.76113946616
Iteration :  24   Loss :  1.72419263499
Iteration :  25   Loss :  1.688020909
Iteration :  26   Loss :  1.65260802733
Iteration :  27   Loss :  1.61793807021
Iteration :  28   Loss :  1.58399545188
Iteration :  29   Loss :  1.55076491355
Iteration :  30   Loss :  1.51823151654
Iteration :  31   Loss :  1.48638063557
Iteration :  32   Loss :  1.45519795217
Iteration :  33   Loss :  1.42466944827
Iteration :  34   Loss :  1.39478139989
Iteration :  35   Loss :  1.36552037094
Iteration :  36   Loss :  1.33687320723
Iteration :  37   Loss :  1.30882703052
Iteration :  38   Loss :  1.28136923274
Iteration :  39   Loss :  1.25448747032
Iteration :  40   Loss :  1.22816965867
Iteration :  41   Loss :  1.20240396669
Iteration :  42   Loss :  1.17717881149
Iteration :  43   Loss :  1.1524828532
Iteration :  44   Loss :  1.12830498982
Iteration :  45   Loss :  1.10463435228
Iteration :  46   Loss :  1.08146029951
Iteration :  47   Loss :  1.0587724137
Iteration :  48   Loss :  1.03656049559
Iteration :  49   Loss :  1.01481455987
Iteration :  50   Loss :  0.993524830727
Iteration :  51   Loss :  0.972681737441
Iteration :  52   Loss :  0.952275910064
Iteration :  53   Loss :  0.932298175222
Iteration :  54   Loss :  0.912739551989
Iteration :  55   Loss :  0.893591247851
Iteration :  56   Loss :  0.874844654749
Iteration :  57   Loss :  0.856491345213
Iteration :  58   Loss :  0.838523068574
Iteration :  59   Loss :  0.82093174725
Iteration :  60   Loss :  0.80370947312
Iteration :  61   Loss :  0.786848503968
Iteration :  62   Loss :  0.770341259999
Iteration :  63   Loss :  0.754180320436
Iteration :  64   Loss :  0.73835842018
Iteration :  65   Loss :  0.722868446549
Iteration :  66   Loss :  0.707703436074
Iteration :  67   Loss :  0.692856571375
Iteration :  68   Loss :  0.678321178092
Iteration :  69   Loss :  0.664090721887
Iteration :  70   Loss :  0.650158805503
Iteration :  71   Loss :  0.636519165894
Iteration :  72   Loss :  0.623165671404
Iteration :  73   Loss :  0.610092319013
Iteration :  74   Loss :  0.597293231637
Iteration :  75   Loss :  0.58476265549
Iteration :  76   Loss :  0.57249495749
Iteration :  77   Loss :  0.560484622735
Iteration :  78   Loss :  0.548726252017
Iteration :  79   Loss :  0.5372145594
Iteration :  80   Loss :  0.525944369838
Iteration :  81   Loss :  0.514910616856
Iteration :  82   Loss :  0.504108340266
Iteration :  83   Loss :  0.493532683939
Iteration :  84   Loss :  0.483178893623
Iteration :  85   Loss :  0.473042314806
Iteration :  86   Loss :  0.463118390621
Iteration :  87   Loss :  0.453402659801
Iteration :  88   Loss :  0.443890754671
Iteration :  89   Loss :  0.434578399185
Iteration :  90   Loss :  0.425461407003
Iteration :  91   Loss :  0.416535679611
Iteration :  92   Loss :  0.407797204479
Iteration :  93   Loss :  0.399242053252
Iteration :  94   Loss :  0.39086637999
Iteration :  95   Loss :  0.382666419437
Iteration :  96   Loss :  0.374638485327
Iteration :  97   Loss :  0.366778968729
Iteration :  98   Loss :  0.359084336421
Iteration :  99   Loss :  0.351551129309
[ -9.18971123e-03  -3.05378475e-04  -7.85217023e-04 ...,   5.77841592e-03
   2.72619248e-03  -5.79999269e-06]
CROSS VALIDATION 4
Iteration :  0   Loss :  175.023419988
Iteration :  1   Loss :  2.80781800136
Iteration :  2   Loss :  2.74891296877
Iteration :  3   Loss :  2.69124370106
Iteration :  4   Loss :  2.63478427319
Iteration :  5   Loss :  2.57950930401
Iteration :  6   Loss :  2.52539394485
Iteration :  7   Loss :  2.47241386831
Iteration :  8   Loss :  2.42054525737
Iteration :  9   Loss :  2.36976479468
Iteration :  10   Loss :  2.32004965204
Iteration :  11   Loss :  2.27137748016
Iteration :  12   Loss :  2.22372639863
Iteration :  13   Loss :  2.17707498607
Iteration :  14   Loss :  2.13140227048
Iteration :  15   Loss :  2.08668771984
Iteration :  16   Loss :  2.04291123288
Iteration :  17   Loss :  2.00005313001
Iteration :  18   Loss :  1.95809414451
Iteration :  19   Loss :  1.91701541386
Iteration :  20   Loss :  1.87679847124
Iteration :  21   Loss :  1.83742523727
Iteration :  22   Loss :  1.79887801184
Iteration :  23   Loss :  1.76113946616
Iteration :  24   Loss :  1.72419263499
Iteration :  25   Loss :  1.688020909
Iteration :  26   Loss :  1.65260802733
Iteration :  27   Loss :  1.61793807021
Iteration :  28   Loss :  1.58399545188
Iteration :  29   Loss :  1.55076491355
Iteration :  30   Loss :  1.51823151654
Iteration :  31   Loss :  1.48638063557
Iteration :  32   Loss :  1.45519795217
Iteration :  33   Loss :  1.42466944827
Iteration :  34   Loss :  1.39478139989
Iteration :  35   Loss :  1.36552037094
Iteration :  36   Loss :  1.33687320723
Iteration :  37   Loss :  1.30882703052
Iteration :  38   Loss :  1.28136923274
Iteration :  39   Loss :  1.25448747032
Iteration :  40   Loss :  1.22816965867
Iteration :  41   Loss :  1.20240396669
Iteration :  42   Loss :  1.17717881149
Iteration :  43   Loss :  1.1524828532
Iteration :  44   Loss :  1.12830498982
Iteration :  45   Loss :  1.10463435228
Iteration :  46   Loss :  1.08146029951
Iteration :  47   Loss :  1.0587724137
Iteration :  48   Loss :  1.03656049559
Iteration :  49   Loss :  1.01481455987
Iteration :  50   Loss :  0.993524830727
Iteration :  51   Loss :  0.972681737441
Iteration :  52   Loss :  0.952275910064
Iteration :  53   Loss :  0.932298175222
Iteration :  54   Loss :  0.912739551989
Iteration :  55   Loss :  0.893591247851
Iteration :  56   Loss :  0.874844654749
Iteration :  57   Loss :  0.856491345213
Iteration :  58   Loss :  0.838523068574
Iteration :  59   Loss :  0.82093174725
Iteration :  60   Loss :  0.80370947312
Iteration :  61   Loss :  0.786848503968
Iteration :  62   Loss :  0.770341259999
Iteration :  63   Loss :  0.754180320436
Iteration :  64   Loss :  0.73835842018
Iteration :  65   Loss :  0.722868446549
Iteration :  66   Loss :  0.707703436074
Iteration :  67   Loss :  0.692856571375
Iteration :  68   Loss :  0.678321178092
Iteration :  69   Loss :  0.664090721887
Iteration :  70   Loss :  0.650158805503
Iteration :  71   Loss :  0.636519165894
Iteration :  72   Loss :  0.623165671404
Iteration :  73   Loss :  0.610092319013
Iteration :  74   Loss :  0.597293231637
Iteration :  75   Loss :  0.58476265549
Iteration :  76   Loss :  0.57249495749
Iteration :  77   Loss :  0.560484622735
Iteration :  78   Loss :  0.548726252017
Iteration :  79   Loss :  0.5372145594
Iteration :  80   Loss :  0.525944369838
Iteration :  81   Loss :  0.514910616856
Iteration :  82   Loss :  0.504108340266
Iteration :  83   Loss :  0.493532683939
Iteration :  84   Loss :  0.483178893623
Iteration :  85   Loss :  0.473042314806
Iteration :  86   Loss :  0.463118390621
Iteration :  87   Loss :  0.453402659801
Iteration :  88   Loss :  0.443890754671
Iteration :  89   Loss :  0.434578399185
Iteration :  90   Loss :  0.425461407003
Iteration :  91   Loss :  0.416535679611
Iteration :  92   Loss :  0.407797204479
Iteration :  93   Loss :  0.399242053252
Iteration :  94   Loss :  0.39086637999
Iteration :  95   Loss :  0.382666419437
Iteration :  96   Loss :  0.374638485327
Iteration :  97   Loss :  0.366778968729
Iteration :  98   Loss :  0.359084336421
Iteration :  99   Loss :  0.351551129309
[ -9.18971123e-03  -3.05378475e-04  -7.85217023e-04 ...,   5.77841592e-03
   2.72619248e-03  -5.79999269e-06]
CROSS VALIDATION 5
Iteration :  0   Loss :  156.447309224
Iteration :  1   Loss :  22.9872341699
Iteration :  2   Loss :  2.69897230071
Iteration :  3   Loss :  2.64235073505
Iteration :  4   Loss :  2.58691702956
Iteration :  5   Loss :  2.53264626419
Iteration :  6   Loss :  2.47951404172
Iteration :  7   Loss :  2.4274964767
Iteration :  8   Loss :  2.37657018483
Iteration :  9   Loss :  2.32671227234
Iteration :  10   Loss :  2.27790032578
Iteration :  11   Loss :  2.2301124019
Iteration :  12   Loss :  2.18332701779
Iteration :  13   Loss :  2.13752314123
Iteration :  14   Loss :  2.09268018124
Iteration :  15   Loss :  2.04877797881
Iteration :  16   Loss :  2.00579679785
Iteration :  17   Loss :  1.96371731632
Iteration :  18   Loss :  1.92252061751
Iteration :  19   Loss :  1.88218818158
Iteration :  20   Loss :  1.84270187723
Iteration :  21   Loss :  1.8040439535
Iteration :  22   Loss :  1.76619703187
Iteration :  23   Loss :  1.72914409835
Iteration :  24   Loss :  1.69286849594
Iteration :  25   Loss :  1.65735391705
Iteration :  26   Loss :  1.62258439622
Iteration :  27   Loss :  1.58854430291
Iteration :  28   Loss :  1.55521833453
Iteration :  29   Loss :  1.52259150949
Iteration :  30   Loss :  1.49064916051
Iteration :  31   Loss :  1.459376928
Iteration :  32   Loss :  1.42876075364
Iteration :  33   Loss :  1.39878687402
Iteration :  34   Loss :  1.36944181449
Iteration :  35   Loss :  1.34071238307
Iteration :  36   Loss :  1.31258566454
Iteration :  37   Loss :  1.28504901462
Iteration :  38   Loss :  1.2580900543
Iteration :  39   Loss :  1.23169666427
Iteration :  40   Loss :  1.20585697946
Iteration :  41   Loss :  1.18055938373
Iteration :  42   Loss :  1.15579250463
Iteration :  43   Loss :  1.13154520829
Iteration :  44   Loss :  1.10780659442
Iteration :  45   Loss :  1.08456599139
Iteration :  46   Loss :  1.06181295147
Iteration :  47   Loss :  1.03953724611
Iteration :  48   Loss :  1.01772886133
Iteration :  49   Loss :  0.996377993247
Iteration :  50   Loss :  0.975475043649
Iteration :  51   Loss :  0.955010615681
Iteration :  52   Loss :  0.934975509626
Iteration :  53   Loss :  0.915360718768
Iteration :  54   Loss :  0.896157425341
Iteration :  55   Loss :  0.877356996567
Iteration :  56   Loss :  0.858950980775
Iteration :  57   Loss :  0.840931103599
Iteration :  58   Loss :  0.823289264264
Iteration :  59   Loss :  0.806017531937
Iteration :  60   Loss :  0.789108142168
Iteration :  61   Loss :  0.772553493395
Iteration :  62   Loss :  0.75634614353
Iteration :  63   Loss :  0.740478806612
Iteration :  64   Loss :  0.72494434953
Iteration :  65   Loss :  0.709735788821
Iteration :  66   Loss :  0.694846287525
Iteration :  67   Loss :  0.680269152115
Iteration :  68   Loss :  0.665997829489
Iteration :  69   Loss :  0.652025904018
Iteration :  70   Loss :  0.63834709467
Iteration :  71   Loss :  0.62495525218
Iteration :  72   Loss :  0.611844356289
Iteration :  73   Loss :  0.599008513036
Iteration :  74   Loss :  0.586441952111
Iteration :  75   Loss :  0.574139024256
Iteration :  76   Loss :  0.56209419873
Iteration :  77   Loss :  0.550302060822
Iteration :  78   Loss :  0.538757309413
Iteration :  79   Loss :  0.527454754599
Iteration :  80   Loss :  0.516389315352
Iteration :  81   Loss :  0.505556017242
Iteration :  82   Loss :  0.494949990193
Iteration :  83   Loss :  0.4845664663
Iteration :  84   Loss :  0.474400777686
Iteration :  85   Loss :  0.464448354396
Iteration :  86   Loss :  0.454704722352
Iteration :  87   Loss :  0.445165501336
Iteration :  88   Loss :  0.435826403021
Iteration :  89   Loss :  0.426683229047
Iteration :  90   Loss :  0.41773186913
Iteration :  91   Loss :  0.408968299215
Iteration :  92   Loss :  0.400388579667
Iteration :  93   Loss :  0.391988853502
Iteration :  94   Loss :  0.383765344649
Iteration :  95   Loss :  0.375714356258
Iteration :  96   Loss :  0.367832269033
Iteration :  97   Loss :  0.360115539607
Iteration :  98   Loss :  0.352560698949
Iteration :  99   Loss :  0.345164350805
[-0.00200957  0.00114841 -0.00446655 ...,  0.00104578  0.00040526
  0.0017314 ]
CROSS VALIDATION 6
Iteration :  0   Loss :  174.99306699
Iteration :  1   Loss :  2.80786950666
Iteration :  2   Loss :  2.74896339355
Iteration :  3   Loss :  2.69129306797
Iteration :  4   Loss :  2.63483260444
Iteration :  5   Loss :  2.57955662133
Iteration :  6   Loss :  2.52544026949
Iteration :  7   Loss :  2.47245922111
Iteration :  8   Loss :  2.42058965872
Iteration :  9   Loss :  2.36980826454
Iteration :  10   Loss :  2.32009220994
Iteration :  11   Loss :  2.27141914524
Iteration :  12   Loss :  2.22376718963
Iteration :  13   Loss :  2.17711492132
Iteration :  14   Loss :  2.13144136793
Iteration :  15   Loss :  2.08672599707
Iteration :  16   Loss :  2.04294870709
Iteration :  17   Loss :  2.00008981804
Iteration :  18   Loss :  1.95813006287
Iteration :  19   Loss :  1.91705057869
Iteration :  20   Loss :  1.87683289836
Iteration :  21   Loss :  1.83745894214
Iteration :  22   Loss :  1.79891100961
Iteration :  23   Loss :  1.76117177167
Iteration :  24   Loss :  1.72422426277
Iteration :  25   Loss :  1.68805187327
Iteration :  26   Loss :  1.65263834199
Iteration :  27   Loss :  1.61796774891
Iteration :  28   Loss :  1.58402450795
Iteration :  29   Loss :  1.55079336006
Iteration :  30   Loss :  1.51825936627
Iteration :  31   Loss :  1.48640790104
Iteration :  32   Loss :  1.45522464564
Iteration :  33   Loss :  1.42469558174
Iteration :  34   Loss :  1.3948069851
Iteration :  35   Loss :  1.36554541941
Iteration :  36   Loss :  1.33689773021
Iteration :  37   Loss :  1.30885103903
Iteration :  38   Loss :  1.28139273758
Iteration :  39   Loss :  1.25451048206
Iteration :  40   Loss :  1.22819218764
Iteration :  41   Loss :  1.20242602302
Iteration :  42   Loss :  1.17720040511
Iteration :  43   Loss :  1.15250399381
Iteration :  44   Loss :  1.12832568692
Iteration :  45   Loss :  1.10465461517
Iteration :  46   Loss :  1.08148013731
Iteration :  47   Loss :  1.05879183533
Iteration :  48   Loss :  1.03657950977
Iteration :  49   Loss :  1.01483317515
Iteration :  50   Loss :  0.993543055482
Iteration :  51   Loss :  0.97269957986
Iteration :  52   Loss :  0.952293378168
Iteration :  53   Loss :  0.932315276863
Iteration :  54   Loss :  0.912756294857
Iteration :  55   Loss :  0.893607639471
Iteration :  56   Loss :  0.87486070249
Iteration :  57   Loss :  0.85650705629
Iteration :  58   Loss :  0.838538450049
Iteration :  59   Loss :  0.820946806038
Iteration :  60   Loss :  0.803724215991
Iteration :  61   Loss :  0.786862937549
Iteration :  62   Loss :  0.770355390779
Iteration :  63   Loss :  0.754194154767
Iteration :  64   Loss :  0.738371964282
Iteration :  65   Loss :  0.722881706509
Iteration :  66   Loss :  0.707716417855
Iteration :  67   Loss :  0.692869280812
Iteration :  68   Loss :  0.678333620899
Iteration :  69   Loss :  0.664102903656
Iteration :  70   Loss :  0.650170731712
Iteration :  71   Loss :  0.636530841904
Iteration :  72   Loss :  0.623177102464
Iteration :  73   Loss :  0.610103510261
Iteration :  74   Loss :  0.597304188105
Iteration :  75   Loss :  0.584773382102
Iteration :  76   Loss :  0.57250545907
Iteration :  77   Loss :  0.560494904003
Iteration :  78   Loss :  0.548736317595
Iteration :  79   Loss :  0.537224413812
Iteration :  80   Loss :  0.525954017516
Iteration :  81   Loss :  0.514920062136
Iteration :  82   Loss :  0.504117587394
Iteration :  83   Loss :  0.493541737071
Iteration :  84   Loss :  0.48318775683
Iteration :  85   Loss :  0.473050992073
Iteration :  86   Loss :  0.463126885848
Iteration :  87   Loss :  0.453410976808
Iteration :  88   Loss :  0.443898897196
Iteration :  89   Loss :  0.434586370888
Iteration :  90   Loss :  0.425469211468
Iteration :  91   Loss :  0.416543320347
Iteration :  92   Loss :  0.40780468492
Iteration :  93   Loss :  0.399249376761
Iteration :  94   Loss :  0.39087354986
Iteration :  95   Loss :  0.382673438891
Iteration :  96   Loss :  0.37464535752
Iteration :  97   Loss :  0.36678569675
Iteration :  98   Loss :  0.359090923296
Iteration :  99   Loss :  0.351557577998
[ -9.19001004e-03  -3.05385829e-04  -7.85246065e-04 ...,   5.77870930e-03
   2.72655838e-03  -5.86935971e-06]
CROSS VALIDATION 7
Iteration :  0   Loss :  174.99306699
Iteration :  1   Loss :  2.80786950666
Iteration :  2   Loss :  2.74896339355
Iteration :  3   Loss :  2.69129306797
Iteration :  4   Loss :  2.63483260444
Iteration :  5   Loss :  2.57955662133
Iteration :  6   Loss :  2.52544026949
Iteration :  7   Loss :  2.47245922111
Iteration :  8   Loss :  2.42058965872
Iteration :  9   Loss :  2.36980826454
Iteration :  10   Loss :  2.32009220994
Iteration :  11   Loss :  2.27141914524
Iteration :  12   Loss :  2.22376718963
Iteration :  13   Loss :  2.17711492132
Iteration :  14   Loss :  2.13144136793
Iteration :  15   Loss :  2.08672599707
Iteration :  16   Loss :  2.04294870709
Iteration :  17   Loss :  2.00008981804
Iteration :  18   Loss :  1.95813006287
Iteration :  19   Loss :  1.91705057869
Iteration :  20   Loss :  1.87683289836
Iteration :  21   Loss :  1.83745894214
Iteration :  22   Loss :  1.79891100961
Iteration :  23   Loss :  1.76117177167
Iteration :  24   Loss :  1.72422426277
Iteration :  25   Loss :  1.68805187327
Iteration :  26   Loss :  1.65263834199
Iteration :  27   Loss :  1.61796774891
Iteration :  28   Loss :  1.58402450795
Iteration :  29   Loss :  1.55079336006
Iteration :  30   Loss :  1.51825936627
Iteration :  31   Loss :  1.48640790104
Iteration :  32   Loss :  1.45522464564
Iteration :  33   Loss :  1.42469558174
Iteration :  34   Loss :  1.3948069851
Iteration :  35   Loss :  1.36554541941
Iteration :  36   Loss :  1.33689773021
Iteration :  37   Loss :  1.30885103903
Iteration :  38   Loss :  1.28139273758
Iteration :  39   Loss :  1.25451048206
Iteration :  40   Loss :  1.22819218764
Iteration :  41   Loss :  1.20242602302
Iteration :  42   Loss :  1.17720040511
Iteration :  43   Loss :  1.15250399381
Iteration :  44   Loss :  1.12832568692
Iteration :  45   Loss :  1.10465461517
Iteration :  46   Loss :  1.08148013731
Iteration :  47   Loss :  1.05879183533
Iteration :  48   Loss :  1.03657950977
Iteration :  49   Loss :  1.01483317515
Iteration :  50   Loss :  0.993543055482
Iteration :  51   Loss :  0.97269957986
Iteration :  52   Loss :  0.952293378168
Iteration :  53   Loss :  0.932315276863
Iteration :  54   Loss :  0.912756294857
Iteration :  55   Loss :  0.893607639471
Iteration :  56   Loss :  0.87486070249
Iteration :  57   Loss :  0.85650705629
Iteration :  58   Loss :  0.838538450049
Iteration :  59   Loss :  0.820946806038
Iteration :  60   Loss :  0.803724215991
Iteration :  61   Loss :  0.786862937549
Iteration :  62   Loss :  0.770355390779
Iteration :  63   Loss :  0.754194154767
Iteration :  64   Loss :  0.738371964282
Iteration :  65   Loss :  0.722881706509
Iteration :  66   Loss :  0.707716417855
Iteration :  67   Loss :  0.692869280812
Iteration :  68   Loss :  0.678333620899
Iteration :  69   Loss :  0.664102903656
Iteration :  70   Loss :  0.650170731712
Iteration :  71   Loss :  0.636530841904
Iteration :  72   Loss :  0.623177102464
Iteration :  73   Loss :  0.610103510261
Iteration :  74   Loss :  0.597304188105
Iteration :  75   Loss :  0.584773382102
Iteration :  76   Loss :  0.57250545907
Iteration :  77   Loss :  0.560494904003
Iteration :  78   Loss :  0.548736317595
Iteration :  79   Loss :  0.537224413812
Iteration :  80   Loss :  0.525954017516
Iteration :  81   Loss :  0.514920062136
Iteration :  82   Loss :  0.504117587394
Iteration :  83   Loss :  0.493541737071
Iteration :  84   Loss :  0.48318775683
Iteration :  85   Loss :  0.473050992073
Iteration :  86   Loss :  0.463126885848
Iteration :  87   Loss :  0.453410976808
Iteration :  88   Loss :  0.443898897196
Iteration :  89   Loss :  0.434586370888
Iteration :  90   Loss :  0.425469211468
Iteration :  91   Loss :  0.416543320347
Iteration :  92   Loss :  0.40780468492
Iteration :  93   Loss :  0.399249376761
Iteration :  94   Loss :  0.39087354986
Iteration :  95   Loss :  0.382673438891
Iteration :  96   Loss :  0.37464535752
Iteration :  97   Loss :  0.36678569675
Iteration :  98   Loss :  0.359090923296
Iteration :  99   Loss :  0.351557577998
[ -9.19001004e-03  -3.05385829e-04  -7.85246065e-04 ...,   5.77870930e-03
   2.72655838e-03  -5.86935971e-06]
CROSS VALIDATION 8
Iteration :  0   Loss :  169.006548337
Iteration :  1   Loss :  2.56338807859
Iteration :  2   Loss :  2.50961092557
Iteration :  3   Loss :  2.45696195998
Iteration :  4   Loss :  2.40541751363
Iteration :  5   Loss :  2.35495441489
Iteration :  6   Loss :  2.30554997825
Iteration :  7   Loss :  2.25718199409
Iteration :  8   Loss :  2.20982871875
Iteration :  9   Loss :  2.16346886472
Iteration :  10   Loss :  2.11808159107
Iteration :  11   Loss :  2.07364649411
Iteration :  12   Loss :  2.03014359818
Iteration :  13   Loss :  1.98755334669
Iteration :  14   Loss :  1.94585659334
Iteration :  15   Loss :  1.90503459348
Iteration :  16   Loss :  1.86506899573
Iteration :  17   Loss :  1.82594183367
Iteration :  18   Loss :  1.78763551781
Iteration :  19   Loss :  1.75013282769
Iteration :  20   Loss :  1.71341690408
Iteration :  21   Loss :  1.67747124146
Iteration :  22   Loss :  1.64227968057
Iteration :  23   Loss :  1.60782640117
Iteration :  24   Loss :  1.5740959149
Iteration :  25   Loss :  1.54107305832
Iteration :  26   Loss :  1.50874298611
Iteration :  27   Loss :  1.4770911644
Iteration :  28   Loss :  1.44610336422
Iteration :  29   Loss :  1.4157656551
Iteration :  30   Loss :  1.38606439882
Iteration :  31   Loss :  1.35698624327
Iteration :  32   Loss :  1.32851811648
Iteration :  33   Loss :  1.30064722067
Iteration :  34   Loss :  1.2733610266
Iteration :  35   Loss :  1.24664726782
Iteration :  36   Loss :  1.22049393527
Iteration :  37   Loss :  1.19488927179
Iteration :  38   Loss :  1.1698217669
Iteration :  39   Loss :  1.14528015157
Iteration :  40   Loss :  1.1212533932
Iteration :  41   Loss :  1.09773069065
Iteration :  42   Loss :  1.07470146936
Iteration :  43   Loss :  1.05215537661
Iteration :  44   Loss :  1.03008227689
Iteration :  45   Loss :  1.0084722473
Iteration :  46   Loss :  0.987315573123
Iteration :  47   Loss :  0.966602743452
Iteration :  48   Loss :  0.946324446897
Iteration :  49   Loss :  0.926471567417
Iteration :  50   Loss :  0.907035180213
Iteration :  51   Loss :  0.888006547722
Iteration :  52   Loss :  0.869377115683
Iteration :  53   Loss :  0.851138509297
Iteration :  54   Loss :  0.833282529457
Iteration :  55   Loss :  0.815801149064
Iteration :  56   Loss :  0.798686509423
Iteration :  57   Loss :  0.781930916702
Iteration :  58   Loss :  0.765526838479
Iteration :  59   Loss :  0.749466900354
Iteration :  60   Loss :  0.733743882634
Iteration :  61   Loss :  0.718350717087
Iteration :  62   Loss :  0.703280483767
Iteration :  63   Loss :  0.688526407899
Iteration :  64   Loss :  0.674081856837
Iteration :  65   Loss :  0.659940337078
Iteration :  66   Loss :  0.64609549135
Iteration :  67   Loss :  0.632541095746
Iteration :  68   Loss :  0.619271056933
Iteration :  69   Loss :  0.606279409407
Iteration :  70   Loss :  0.593560312816
Iteration :  71   Loss :  0.581108049331
Iteration :  72   Loss :  0.568917021078
Iteration :  73   Loss :  0.55698174762
Iteration :  74   Loss :  0.545296863493
Iteration :  75   Loss :  0.533857115796
Iteration :  76   Loss :  0.522657361828
Iteration :  77   Loss :  0.511692566774
Iteration :  78   Loss :  0.500957801449
Iteration :  79   Loss :  0.490448240072
Iteration :  80   Loss :  0.480159158105
Iteration :  81   Loss :  0.470085930124
Iteration :  82   Loss :  0.460224027743
Iteration :  83   Loss :  0.450569017575
Iteration :  84   Loss :  0.441116559242
Iteration :  85   Loss :  0.431862403423
Iteration :  86   Loss :  0.422802389941
Iteration :  87   Loss :  0.413932445897
Iteration :  88   Loss :  0.405248583836
Iteration :  89   Loss :  0.396746899957
Iteration :  90   Loss :  0.388423572355
Iteration :  91   Loss :  0.380274859306
Iteration :  92   Loss :  0.372297097581
Iteration :  93   Loss :  0.364486700805
Iteration :  94   Loss :  0.356840157839
Iteration :  95   Loss :  0.349354031204
Iteration :  96   Loss :  0.342024955536
Iteration :  97   Loss :  0.334849636074
Iteration :  98   Loss :  0.327824847175
Iteration :  99   Loss :  0.320947430869
[-0.00237368 -0.00211659 -0.00299142 ...,  0.000982    0.00092851
  0.0010437 ]
CROSS VALIDATION 9
Iteration :  0   Loss :  155.791998474
Iteration :  1   Loss :  2.80793042365
Iteration :  2   Loss :  2.74902303256
Iteration :  3   Loss :  2.69135145583
Iteration :  4   Loss :  2.63488976738
Iteration :  5   Loss :  2.57961258505
Iteration :  6   Loss :  2.52549505915
Iteration :  7   Loss :  2.47251286134
Iteration :  8   Loss :  2.42064217364
Iteration :  9   Loss :  2.36985967775
Iteration :  10   Loss :  2.32014254456
Iteration :  11   Loss :  2.27146842389
Iteration :  12   Loss :  2.22381543446
Iteration :  13   Loss :  2.17716215403
Iteration :  14   Loss :  2.13148760975
Iteration :  15   Loss :  2.08677126878
Iteration :  16   Loss :  2.04299302905
Iteration :  17   Loss :  2.00013321018
Iteration :  18   Loss :  1.95817254468
Iteration :  19   Loss :  1.91709216928
Iteration :  20   Loss :  1.87687361642
Iteration :  21   Loss :  1.83749880598
Iteration :  22   Loss :  1.79895003715
Iteration :  23   Loss :  1.76120998046
Iteration :  24   Loss :  1.72426166997
Iteration :  25   Loss :  1.68808849571
Iteration :  26   Loss :  1.65267419614
Iteration :  27   Loss :  1.61800285087
Iteration :  28   Loss :  1.58405887351
Iteration :  29   Loss :  1.55082700466
Iteration :  30   Loss :  1.51829230505
Iteration :  31   Loss :  1.48644014879
Iteration :  32   Loss :  1.45525621687
Iteration :  33   Loss :  1.42472649064
Iteration :  34   Loss :  1.39483724557
Iteration :  35   Loss :  1.36557504504
Iteration :  36   Loss :  1.33692673433
Iteration :  37   Loss :  1.30887943467
Iteration :  38   Loss :  1.28142053751
Iteration :  39   Loss :  1.25453769878
Iteration :  40   Loss :  1.22821883338
Iteration :  41   Loss :  1.20245210977
Iteration :  42   Loss :  1.17722594458
Iteration :  43   Loss :  1.15252899749
Iteration :  44   Loss :  1.12835016605
Iteration :  45   Loss :  1.10467858076
Iteration :  46   Loss :  1.08150360013
Iteration :  47   Loss :  1.05881480592
Iteration :  48   Loss :  1.03660199846
Iteration :  49   Loss :  1.01485519205
Iteration :  50   Loss :  0.99356461049
Iteration :  51   Loss :  0.972720682667
Iteration :  52   Loss :  0.952314038261
Iteration :  53   Loss :  0.932335503529
Iteration :  54   Loss :  0.912776097189
Iteration :  55   Loss :  0.893627026371
Iteration :  56   Loss :  0.874879682674
Iteration :  57   Loss :  0.85652563829
Iteration :  58   Loss :  0.838556642218
Iteration :  59   Loss :  0.820964616555
Iteration :  60   Loss :  0.803741652862
Iteration :  61   Loss :  0.786880008613
Iteration :  62   Loss :  0.77037210371
Iteration :  63   Loss :  0.754210517079
Iteration :  64   Loss :  0.73838798333
Iteration :  65   Loss :  0.722897389495
Iteration :  66   Loss :  0.707731771828
Iteration :  67   Loss :  0.692884312675
Iteration :  68   Loss :  0.678348337409
Iteration :  69   Loss :  0.66411731143
Iteration :  70   Loss :  0.650184837226
Iteration :  71   Loss :  0.636544651499
Iteration :  72   Loss :  0.623190622348
Iteration :  73   Loss :  0.610116746513
Iteration :  74   Loss :  0.597317146674
Iteration :  75   Loss :  0.584786068815
Iteration :  76   Loss :  0.572517879629
Iteration :  77   Loss :  0.560507063991
Iteration :  78   Loss :  0.54874822248
Iteration :  79   Loss :  0.537236068945
Iteration :  80   Loss :  0.525965428137
Iteration :  81   Loss :  0.514931233374
Iteration :  82   Loss :  0.504128524271
Iteration :  83   Loss :  0.493552444505
Iteration :  84   Loss :  0.483198239633
Iteration :  85   Loss :  0.473061254958
Iteration :  86   Loss :  0.463136933429
Iteration :  87   Loss :  0.453420813601
Iteration :  88   Loss :  0.443908527623
Iteration :  89   Loss :  0.434595799279
Iteration :  90   Loss :  0.425478442062
Iteration :  91   Loss :  0.416552357293
Iteration :  92   Loss :  0.40781353228
Iteration :  93   Loss :  0.399258038513
Iteration :  94   Loss :  0.390882029898
Iteration :  95   Loss :  0.382681741026
Iteration :  96   Loss :  0.374653485486
Iteration :  97   Loss :  0.3667936542
Iteration :  98   Loss :  0.359098713807
Iteration :  99   Loss :  0.351565205072
[ -9.19018730e-03  -3.05437392e-04  -7.85552020e-04 ...,   5.77871737e-03
   2.72679238e-03  -5.79995105e-06]
CROSS VALIDATION 10
Iteration :  0   Loss :  75.0288649203
Iteration :  1   Loss :  2.81336307143
Iteration :  2   Loss :  2.75434170917
Iteration :  3   Loss :  2.69655855226
Iteration :  4   Loss :  2.63998762448
Iteration :  5   Loss :  2.58460349453
Iteration :  6   Loss :  2.53038126467
Iteration :  7   Loss :  2.47729655947
Iteration :  8   Loss :  2.42532551487
Iteration :  9   Loss :  2.37444476746
Iteration :  10   Loss :  2.32463144397
Iteration :  11   Loss :  2.27586315098
Iteration :  12   Loss :  2.22811796485
Iteration :  13   Loss :  2.18137442191
Iteration :  14   Loss :  2.13561150873
Iteration :  15   Loss :  2.09080865276
Iteration :  16   Loss :  2.046945713
Iteration :  17   Loss :  2.00400297103
Iteration :  18   Loss :  1.96196112207
Iteration :  19   Loss :  1.92080126635
Iteration :  20   Loss :  1.88050490058
Iteration :  21   Loss :  1.84105390967
Iteration :  22   Loss :  1.80243055855
Iteration :  23   Loss :  1.76461748422
Iteration :  24   Loss :  1.72759768794
Iteration :  25   Loss :  1.69135452757
Iteration :  26   Loss :  1.65587171012
Iteration :  27   Loss :  1.62113328441
Iteration :  28   Loss :  1.58712363389
Iteration :  29   Loss :  1.55382746963
Iteration :  30   Loss :  1.52122982346
Iteration :  31   Loss :  1.4893160412
Iteration :  32   Loss :  1.45807177611
Iteration :  33   Loss :  1.42748298245
Iteration :  34   Loss :  1.39753590911
Iteration :  35   Loss :  1.36821709349
Iteration :  36   Loss :  1.33951335542
Iteration :  37   Loss :  1.31141179121
Iteration :  38   Loss :  1.2838997679
Iteration :  39   Loss :  1.25696491755
Iteration :  40   Loss :  1.23059513169
Iteration :  41   Loss :  1.20477855586
Iteration :  42   Loss :  1.17950358432
Iteration :  43   Loss :  1.15475885477
Iteration :  44   Loss :  1.1305332433
Iteration :  45   Loss :  1.10681585937
Iteration :  46   Loss :  1.08359604091
Iteration :  47   Loss :  1.06086334952
Iteration :  48   Loss :  1.03860756579
Iteration :  49   Loss :  1.01681868472
Iteration :  50   Loss :  0.995486911174
Iteration :  51   Loss :  0.974602655529
Iteration :  52   Loss :  0.954156529335
Iteration :  53   Loss :  0.934139341103
Iteration :  54   Loss :  0.91454209217
Iteration :  55   Loss :  0.895355972658
Iteration :  56   Loss :  0.876572357508
Iteration :  57   Loss :  0.858182802608
Iteration :  58   Loss :  0.840179040993
Iteration :  59   Loss :  0.82255297913
Iteration :  60   Loss :  0.805296693282
Iteration :  61   Loss :  0.78840242594
Iteration :  62   Loss :  0.771862582342
Iteration :  63   Loss :  0.755669727056
Iteration :  64   Loss :  0.739816580635
Iteration :  65   Loss :  0.724296016349
Iteration :  66   Loss :  0.70910105698
Iteration :  67   Loss :  0.694224871682
Iteration :  68   Loss :  0.679660772916
Iteration :  69   Loss :  0.665402213437
Iteration :  70   Loss :  0.651442783357
Iteration :  71   Loss :  0.637776207259
Iteration :  72   Loss :  0.62439634138
Iteration :  73   Loss :  0.611297170842
Iteration :  74   Loss :  0.598472806958
Iteration :  75   Loss :  0.585917484576
Iteration :  76   Loss :  0.573625559491
Iteration :  77   Loss :  0.56159150591
Iteration :  78   Loss :  0.549809913961
Iteration :  79   Loss :  0.53827548727
Iteration :  80   Loss :  0.526983040571
Iteration :  81   Loss :  0.515927497383
Iteration :  82   Loss :  0.505103887722
Iteration :  83   Loss :  0.494507345869
Iteration :  84   Loss :  0.484133108183
Iteration :  85   Loss :  0.47397651096
Iteration :  86   Loss :  0.464032988335
Iteration :  87   Loss :  0.454298070229
Iteration :  88   Loss :  0.444767380341
Iteration :  89   Loss :  0.435436634181
Iteration :  90   Loss :  0.426301637143
Iteration :  91   Loss :  0.417358282618
Iteration :  92   Loss :  0.408602550149
Iteration :  93   Loss :  0.400030503628
Iteration :  94   Loss :  0.391638289516
Iteration :  95   Loss :  0.383422135123
Iteration :  96   Loss :  0.375378346903
Iteration :  97   Loss :  0.367503308796
Iteration :  98   Loss :  0.359793480605
Iteration :  99   Loss :  0.352245396402
[-0.00668681  0.00097797 -0.00089519 ...,  0.0060311   0.00255303
  0.00069234]
CROSS VALIDATION 11
Iteration :  0   Loss :  175.013645117
Iteration :  1   Loss :  2.80799564901
Iteration :  2   Loss :  2.74908688956
Iteration :  3   Loss :  2.69141397317
Iteration :  4   Loss :  2.63495097318
Iteration :  5   Loss :  2.57967250681
Iteration :  6   Loss :  2.52555372382
Iteration :  7   Loss :  2.47257029529
Iteration :  8   Loss :  2.42069840269
Iteration :  9   Loss :  2.36991472717
Iteration :  10   Loss :  2.3201964391
Iteration :  11   Loss :  2.27152118778
Iteration :  12   Loss :  2.22386709143
Iteration :  13   Loss :  2.17721272728
Iteration :  14   Loss :  2.13153712203
Iteration :  15   Loss :  2.08681974235
Iteration :  16   Loss :  2.04304048569
Iteration :  17   Loss :  2.00017967123
Iteration :  18   Loss :  1.95821803103
Iteration :  19   Loss :  1.91713670137
Iteration :  20   Loss :  1.87691721428
Iteration :  21   Loss :  1.8375414892
Iteration :  22   Loss :  1.79899182493
Iteration :  23   Loss :  1.76125089157
Iteration :  24   Loss :  1.72430172281
Iteration :  25   Loss :  1.68812770828
Iteration :  26   Loss :  1.65271258607
Iteration :  27   Loss :  1.61804043542
Iteration :  28   Loss :  1.58409566958
Iteration :  29   Loss :  1.55086302879
Iteration :  30   Loss :  1.51832757343
Iteration :  31   Loss :  1.48647467728
Iteration :  32   Loss :  1.45529002099
Iteration :  33   Loss :  1.42475958558
Iteration :  34   Loss :  1.39486964621
Iteration :  35   Loss :  1.36560676595
Iteration :  36   Loss :  1.33695778977
Iteration :  37   Loss :  1.30890983861
Iteration :  38   Loss :  1.2814503036
Iteration :  39   Loss :  1.25456684041
Iteration :  40   Loss :  1.22824736365
Iteration :  41   Loss :  1.2024800415
Iteration :  42   Loss :  1.17725329034
Iteration :  43   Loss :  1.15255576956
Iteration :  44   Loss :  1.12837637647
Iteration :  45   Loss :  1.10470424131
Iteration :  46   Loss :  1.08152872235
Iteration :  47   Loss :  1.0588394011
Iteration :  48   Loss :  1.03662607766
Iteration :  49   Loss :  1.0148787661
Iteration :  50   Loss :  0.993587689981
Iteration :  51   Loss :  0.972743277975
Iteration :  52   Loss :  0.952336159543
Iteration :  53   Loss :  0.932357160731
Iteration :  54   Loss :  0.912797300045
Iteration :  55   Loss :  0.893647784414
Iteration :  56   Loss :  0.874900005235
Iteration :  57   Loss :  0.856545534506
Iteration :  58   Loss :  0.838576121032
Iteration :  59   Loss :  0.820983686724
Iteration :  60   Loss :  0.80376032296
Iteration :  61   Loss :  0.786898287032
Iteration :  62   Loss :  0.770389998667
Iteration :  63   Loss :  0.754228036619
Iteration :  64   Loss :  0.738405135328
Iteration :  65   Loss :  0.722914181663
Iteration :  66   Loss :  0.707748211714
Iteration :  67   Loss :  0.69290040767
Iteration :  68   Loss :  0.678364094748
Iteration :  69   Loss :  0.664132738197
Iteration :  70   Loss :  0.650199940356
Iteration :  71   Loss :  0.636559437782
Iteration :  72   Loss :  0.62320509843
Iteration :  73   Loss :  0.610130918902
Iteration :  74   Loss :  0.597331021742
Iteration :  75   Loss :  0.584799652798
Iteration :  76   Loss :  0.572531178634
Iteration :  77   Loss :  0.560520083998
Iteration :  78   Loss :  0.548760969341
Iteration :  79   Loss :  0.537248548391
Iteration :  80   Loss :  0.525977645777
Iteration :  81   Loss :  0.514943194701
Iteration :  82   Loss :  0.504140234662
Iteration :  83   Loss :  0.493563909224
Iteration :  84   Loss :  0.483209463835
Iteration :  85   Loss :  0.473072243687
Iteration :  86   Loss :  0.463147691627
Iteration :  87   Loss :  0.453431346103
Iteration :  88   Loss :  0.443918839165
Iteration :  89   Loss :  0.434605894496
Iteration :  90   Loss :  0.425488325491
Iteration :  91   Loss :  0.416562033379
Iteration :  92   Loss :  0.407823005372
Iteration :  93   Loss :  0.39926731287
Iteration :  94   Loss :  0.390891109688
Iteration :  95   Loss :  0.382690630332
Iteration :  96   Loss :  0.374662188304
Iteration :  97   Loss :  0.366802174442
Iteration :  98   Loss :  0.359107055304
Iteration :  99   Loss :  0.351573371573
[ -9.19039976e-03  -3.05501077e-04  -7.85489534e-04 ...,   5.77876394e-03
   2.72691647e-03  -5.86938748e-06]
CROSS VALIDATION 12
Iteration :  0   Loss :  122.031276974
Iteration :  1   Loss :  37.1803411044
Iteration :  2   Loss :  15.7157725034
Iteration :  3   Loss :  3.08948850134
Iteration :  4   Loss :  3.02467432152
Iteration :  5   Loss :  2.96121987419
Iteration :  6   Loss :  2.89909663361
Iteration :  7   Loss :  2.83827667249
Iteration :  8   Loss :  2.77873264941
Iteration :  9   Loss :  2.72043779654
Iteration :  10   Loss :  2.66336590762
Iteration :  11   Loss :  2.60749132618
Iteration :  12   Loss :  2.55278893398
Iteration :  13   Loss :  2.49923413973
Iteration :  14   Loss :  2.44680286806
Iteration :  15   Loss :  2.39547154864
Iteration :  16   Loss :  2.34521710567
Iteration :  17   Loss :  2.2960169474
Iteration :  18   Loss :  2.24784895609
Iteration :  19   Loss :  2.20069147795
Iteration :  20   Loss :  2.1545233135
Iteration :  21   Loss :  2.10932370799
Iteration :  22   Loss :  2.06507234209
Iteration :  23   Loss :  2.02174932274
Iteration :  24   Loss :  1.97933517422
Iteration :  25   Loss :  1.93781082937
Iteration :  26   Loss :  1.89715762108
Iteration :  27   Loss :  1.85735727382
Iteration :  28   Loss :  1.81839189548
Iteration :  29   Loss :  1.78024396929
Iteration :  30   Loss :  1.74289634598
Iteration :  31   Loss :  1.70633223606
Iteration :  32   Loss :  1.67053520223
Iteration :  33   Loss :  1.63548915207
Iteration :  34   Loss :  1.60117833073
Iteration :  35   Loss :  1.56758731389
Iteration :  36   Loss :  1.53470100083
Iteration :  37   Loss :  1.50250460761
Iteration :  38   Loss :  1.47098366044
Iteration :  39   Loss :  1.44012398919
Iteration :  40   Loss :  1.40991172099
Iteration :  41   Loss :  1.38033327401
Iteration :  42   Loss :  1.35137535136
Iteration :  43   Loss :  1.32302493511
Iteration :  44   Loss :  1.2952692804
Iteration :  45   Loss :  1.26809590979
Iteration :  46   Loss :  1.24149260756
Iteration :  47   Loss :  1.21544741429
Iteration :  48   Loss :  1.18994862145
Iteration :  49   Loss :  1.16498476614
Iteration :  50   Loss :  1.14054462594
Iteration :  51   Loss :  1.11661721386
Iteration :  52   Loss :  1.09319177341
Iteration :  53   Loss :  1.07025777377
Iteration :  54   Loss :  1.04780490503
Iteration :  55   Loss :  1.02582307357
Iteration :  56   Loss :  1.00430239753
Iteration :  57   Loss :  0.983233202365
Iteration :  58   Loss :  0.962606016486
Iteration :  59   Loss :  0.942411567008
Iteration :  60   Loss :  0.922640775582
Iteration :  61   Loss :  0.903284754314
Iteration :  62   Loss :  0.884334801765
Iteration :  63   Loss :  0.865782399048
Iteration :  64   Loss :  0.84761920599
Iteration :  65   Loss :  0.829837057386
Iteration :  66   Loss :  0.81242795933
Iteration :  67   Loss :  0.795384085619
Iteration :  68   Loss :  0.778697774233
Iteration :  69   Loss :  0.762361523897
Iteration :  70   Loss :  0.746367990702
Iteration :  71   Loss :  0.730709984808
Iteration :  72   Loss :  0.715380467208
Iteration :  73   Loss :  0.700372546569
Iteration :  74   Loss :  0.685679476128
Iteration :  75   Loss :  0.671294650663
Iteration :  76   Loss :  0.657211603523
Iteration :  77   Loss :  0.643424003719
Iteration :  78   Loss :  0.629925653082
Iteration :  79   Loss :  0.616710483471
Iteration :  80   Loss :  0.603772554051
Iteration :  81   Loss :  0.591106048617
Iteration :  82   Loss :  0.578705272982
Iteration :  83   Loss :  0.566564652419
Iteration :  84   Loss :  0.554678729151
Iteration :  85   Loss :  0.5430421599
Iteration :  86   Loss :  0.531649713484
Iteration :  87   Loss :  0.520496268466
Iteration :  88   Loss :  0.509576810851
Iteration :  89   Loss :  0.498886431833
Iteration :  90   Loss :  0.488420325585
Iteration :  91   Loss :  0.478173787105
Iteration :  92   Loss :  0.468142210095
Iteration :  93   Loss :  0.45832108489
Iteration :  94   Loss :  0.448705996438
Iteration :  95   Loss :  0.439292622305
Iteration :  96   Loss :  0.430076730741
Iteration :  97   Loss :  0.421054178771
Iteration :  98   Loss :  0.412220910336
Iteration :  99   Loss :  0.403572954469
[ -1.52324524e-02  -3.29202836e-03  -7.42096656e-03 ...,   7.62463651e-03
   3.72922480e-03   1.20186388e-05]
CROSS VALIDATION 13
Iteration :  0   Loss :  175.080686053
Iteration :  1   Loss :  2.80805190658
Iteration :  2   Loss :  2.7491419669
Iteration :  3   Loss :  2.69146789505
Iteration :  4   Loss :  2.63500376383
Iteration :  5   Loss :  2.57972418998
Iteration :  6   Loss :  2.52560432273
Iteration :  7   Loss :  2.47261983269
Iteration :  8   Loss :  2.42074690084
Iteration :  9   Loss :  2.36996220788
Iteration :  10   Loss :  2.32024292372
Iteration :  11   Loss :  2.27156669721
Iteration :  12   Loss :  2.22391164611
Iteration :  13   Loss :  2.17725634725
Iteration :  14   Loss :  2.1315798269
Iteration :  15   Loss :  2.08686155132
Iteration :  16   Loss :  2.04308141755
Iteration :  17   Loss :  2.00021974438
Iteration :  18   Loss :  1.95825726349
Iteration :  19   Loss :  1.91717511078
Iteration :  20   Loss :  1.8769548179
Iteration :  21   Loss :  1.83757830394
Iteration :  22   Loss :  1.79902786733
Iteration :  23   Loss :  1.76128617784
Iteration :  24   Loss :  1.72433626881
Iteration :  25   Loss :  1.68816152955
Iteration :  26   Loss :  1.6527456978
Iteration :  27   Loss :  1.6180728525
Iteration :  28   Loss :  1.58412740658
Iteration :  29   Loss :  1.55089409999
Iteration :  30   Loss :  1.51835799278
Iteration :  31   Loss :  1.48650445847
Iteration :  32   Loss :  1.4553191774
Iteration :  33   Loss :  1.42478813033
Iteration :  34   Loss :  1.39489759212
Iteration :  35   Loss :  1.36563412558
Iteration :  36   Loss :  1.33698457542
Iteration :  37   Loss :  1.30893606232
Iteration :  38   Loss :  1.28147597717
Iteration :  39   Loss :  1.25459197537
Iteration :  40   Loss :  1.22827197131
Iteration :  41   Loss :  1.20250413292
Iteration :  42   Loss :  1.17727687634
Iteration :  43   Loss :  1.15257886076
Iteration :  44   Loss :  1.12839898324
Iteration :  45   Loss :  1.10472637382
Iteration :  46   Loss :  1.08155039054
Iteration :  47   Loss :  1.05886061472
Iteration :  48   Loss :  1.03664684624
Iteration :  49   Loss :  1.01489909897
Iteration :  50   Loss :  0.99360759629
Iteration :  51   Loss :  0.972762766671
Iteration :  52   Loss :  0.952355239387
Iteration :  53   Loss :  0.9323758403
Iteration :  54   Loss :  0.912815587737
Iteration :  55   Loss :  0.893665688449
Iteration :  56   Loss :  0.874917533663
Iteration :  57   Loss :  0.856562695205
Iteration :  58   Loss :  0.838592921719
Iteration :  59   Loss :  0.82100013495
Iteration :  60   Loss :  0.803776426119
Iteration :  61   Loss :  0.786914052364
Iteration :  62   Loss :  0.77040543326
Iteration :  63   Loss :  0.75424314741
Iteration :  64   Loss :  0.738419929111
Iteration :  65   Loss :  0.722928665088
Iteration :  66   Loss :  0.707762391292
Iteration :  67   Loss :  0.692914289776
Iteration :  68   Loss :  0.678377685622
Iteration :  69   Loss :  0.664146043949
Iteration :  70   Loss :  0.650212966968
Iteration :  71   Loss :  0.636572191109
Iteration :  72   Loss :  0.623217584206
Iteration :  73   Loss :  0.610143142739
Iteration :  74   Loss :  0.597342989136
Iteration :  75   Loss :  0.584811369129
Iteration :  76   Loss :  0.572542649169
Iteration :  77   Loss :  0.560531313893
Iteration :  78   Loss :  0.548771963645
Iteration :  79   Loss :  0.537259312046
Iteration :  80   Loss :  0.525988183622
Iteration :  81   Loss :  0.514953511473
Iteration :  82   Loss :  0.504150335
Iteration :  83   Loss :  0.493573797668
Iteration :  84   Loss :  0.483219144829
Iteration :  85   Loss :  0.473081721585
Iteration :  86   Loss :  0.463156970688
Iteration :  87   Loss :  0.453440430499
Iteration :  88   Loss :  0.44392773298
Iteration :  89   Loss :  0.434614601728
Iteration :  90   Loss :  0.425496850056
Iteration :  91   Loss :  0.416570379107
Iteration :  92   Loss :  0.407831176015
Iteration :  93   Loss :  0.399275312102
Iteration :  94   Loss :  0.390898941105
Iteration :  95   Loss :  0.382698297454
Iteration :  96   Loss :  0.374669694577
Iteration :  97   Loss :  0.366809523242
Iteration :  98   Loss :  0.359114249934
Iteration :  99   Loss :  0.351580415267
[ -9.19001124e-03  -3.05322254e-04  -7.85485875e-04 ...,   5.77876379e-03
   2.72688908e-03  -5.79992327e-06]
CROSS VALIDATION 14
Iteration :  0   Loss :  175.080686053
Iteration :  1   Loss :  2.80805190658
Iteration :  2   Loss :  2.7491419669
Iteration :  3   Loss :  2.69146789505
Iteration :  4   Loss :  2.63500376383
Iteration :  5   Loss :  2.57972418998
Iteration :  6   Loss :  2.52560432273
Iteration :  7   Loss :  2.47261983269
Iteration :  8   Loss :  2.42074690084
Iteration :  9   Loss :  2.36996220788
Iteration :  10   Loss :  2.32024292372
Iteration :  11   Loss :  2.27156669721
Iteration :  12   Loss :  2.22391164611
Iteration :  13   Loss :  2.17725634725
Iteration :  14   Loss :  2.1315798269
Iteration :  15   Loss :  2.08686155132
Iteration :  16   Loss :  2.04308141755
Iteration :  17   Loss :  2.00021974438
Iteration :  18   Loss :  1.95825726349
Iteration :  19   Loss :  1.91717511078
Iteration :  20   Loss :  1.8769548179
Iteration :  21   Loss :  1.83757830394
Iteration :  22   Loss :  1.79902786733
Iteration :  23   Loss :  1.76128617784
Iteration :  24   Loss :  1.72433626881
Iteration :  25   Loss :  1.68816152955
Iteration :  26   Loss :  1.6527456978
Iteration :  27   Loss :  1.6180728525
Iteration :  28   Loss :  1.58412740658
Iteration :  29   Loss :  1.55089409999
Iteration :  30   Loss :  1.51835799278
Iteration :  31   Loss :  1.48650445847
Iteration :  32   Loss :  1.4553191774
Iteration :  33   Loss :  1.42478813033
Iteration :  34   Loss :  1.39489759212
Iteration :  35   Loss :  1.36563412558
Iteration :  36   Loss :  1.33698457542
Iteration :  37   Loss :  1.30893606232
Iteration :  38   Loss :  1.28147597717
Iteration :  39   Loss :  1.25459197537
Iteration :  40   Loss :  1.22827197131
Iteration :  41   Loss :  1.20250413292
Iteration :  42   Loss :  1.17727687634
Iteration :  43   Loss :  1.15257886076
Iteration :  44   Loss :  1.12839898324
Iteration :  45   Loss :  1.10472637382
Iteration :  46   Loss :  1.08155039054
Iteration :  47   Loss :  1.05886061472
Iteration :  48   Loss :  1.03664684624
Iteration :  49   Loss :  1.01489909897
Iteration :  50   Loss :  0.99360759629
Iteration :  51   Loss :  0.972762766671
Iteration :  52   Loss :  0.952355239387
Iteration :  53   Loss :  0.9323758403
Iteration :  54   Loss :  0.912815587737
Iteration :  55   Loss :  0.893665688449
Iteration :  56   Loss :  0.874917533663
Iteration :  57   Loss :  0.856562695205
Iteration :  58   Loss :  0.838592921719
Iteration :  59   Loss :  0.82100013495
Iteration :  60   Loss :  0.803776426119
Iteration :  61   Loss :  0.786914052364
Iteration :  62   Loss :  0.77040543326
Iteration :  63   Loss :  0.75424314741
Iteration :  64   Loss :  0.738419929111
Iteration :  65   Loss :  0.722928665088
Iteration :  66   Loss :  0.707762391292
Iteration :  67   Loss :  0.692914289776
Iteration :  68   Loss :  0.678377685622
Iteration :  69   Loss :  0.664146043949
Iteration :  70   Loss :  0.650212966968
Iteration :  71   Loss :  0.636572191109
Iteration :  72   Loss :  0.623217584206
Iteration :  73   Loss :  0.610143142739
Iteration :  74   Loss :  0.597342989136
Iteration :  75   Loss :  0.584811369129
Iteration :  76   Loss :  0.572542649169
Iteration :  77   Loss :  0.560531313893
Iteration :  78   Loss :  0.548771963645
Iteration :  79   Loss :  0.537259312046
Iteration :  80   Loss :  0.525988183622
Iteration :  81   Loss :  0.514953511473
Iteration :  82   Loss :  0.504150335
Iteration :  83   Loss :  0.493573797668
Iteration :  84   Loss :  0.483219144829
Iteration :  85   Loss :  0.473081721585
Iteration :  86   Loss :  0.463156970688
Iteration :  87   Loss :  0.453440430499
Iteration :  88   Loss :  0.44392773298
Iteration :  89   Loss :  0.434614601728
Iteration :  90   Loss :  0.425496850056
Iteration :  91   Loss :  0.416570379107
Iteration :  92   Loss :  0.407831176015
Iteration :  93   Loss :  0.399275312102
Iteration :  94   Loss :  0.390898941105
Iteration :  95   Loss :  0.382698297454
Iteration :  96   Loss :  0.374669694577
Iteration :  97   Loss :  0.366809523242
Iteration :  98   Loss :  0.359114249934
Iteration :  99   Loss :  0.351580415267
[ -9.19001124e-03  -3.05322254e-04  -7.85485875e-04 ...,   5.77876379e-03
   2.72688908e-03  -5.79992327e-06]
CROSS VALIDATION 15
Iteration :  0   Loss :  175.080686053
Iteration :  1   Loss :  2.80805190658
Iteration :  2   Loss :  2.7491419669
Iteration :  3   Loss :  2.69146789505
Iteration :  4   Loss :  2.63500376383
Iteration :  5   Loss :  2.57972418998
Iteration :  6   Loss :  2.52560432273
Iteration :  7   Loss :  2.47261983269
Iteration :  8   Loss :  2.42074690084
Iteration :  9   Loss :  2.36996220788
Iteration :  10   Loss :  2.32024292372
Iteration :  11   Loss :  2.27156669721
Iteration :  12   Loss :  2.22391164611
Iteration :  13   Loss :  2.17725634725
Iteration :  14   Loss :  2.1315798269
Iteration :  15   Loss :  2.08686155132
Iteration :  16   Loss :  2.04308141755
Iteration :  17   Loss :  2.00021974438
Iteration :  18   Loss :  1.95825726349
Iteration :  19   Loss :  1.91717511078
Iteration :  20   Loss :  1.8769548179
Iteration :  21   Loss :  1.83757830394
Iteration :  22   Loss :  1.79902786733
Iteration :  23   Loss :  1.76128617784
Iteration :  24   Loss :  1.72433626881
Iteration :  25   Loss :  1.68816152955
Iteration :  26   Loss :  1.6527456978
Iteration :  27   Loss :  1.6180728525
Iteration :  28   Loss :  1.58412740658
Iteration :  29   Loss :  1.55089409999
Iteration :  30   Loss :  1.51835799278
Iteration :  31   Loss :  1.48650445847
Iteration :  32   Loss :  1.4553191774
Iteration :  33   Loss :  1.42478813033
Iteration :  34   Loss :  1.39489759212
Iteration :  35   Loss :  1.36563412558
Iteration :  36   Loss :  1.33698457542
Iteration :  37   Loss :  1.30893606232
Iteration :  38   Loss :  1.28147597717
Iteration :  39   Loss :  1.25459197537
Iteration :  40   Loss :  1.22827197131
Iteration :  41   Loss :  1.20250413292
Iteration :  42   Loss :  1.17727687634
Iteration :  43   Loss :  1.15257886076
Iteration :  44   Loss :  1.12839898324
Iteration :  45   Loss :  1.10472637382
Iteration :  46   Loss :  1.08155039054
Iteration :  47   Loss :  1.05886061472
Iteration :  48   Loss :  1.03664684624
Iteration :  49   Loss :  1.01489909897
Iteration :  50   Loss :  0.99360759629
Iteration :  51   Loss :  0.972762766671
Iteration :  52   Loss :  0.952355239387
Iteration :  53   Loss :  0.9323758403
Iteration :  54   Loss :  0.912815587737
Iteration :  55   Loss :  0.893665688449
Iteration :  56   Loss :  0.874917533663
Iteration :  57   Loss :  0.856562695205
Iteration :  58   Loss :  0.838592921719
Iteration :  59   Loss :  0.82100013495
Iteration :  60   Loss :  0.803776426119
Iteration :  61   Loss :  0.786914052364
Iteration :  62   Loss :  0.77040543326
Iteration :  63   Loss :  0.75424314741
Iteration :  64   Loss :  0.738419929111
Iteration :  65   Loss :  0.722928665088
Iteration :  66   Loss :  0.707762391292
Iteration :  67   Loss :  0.692914289776
Iteration :  68   Loss :  0.678377685622
Iteration :  69   Loss :  0.664146043949
Iteration :  70   Loss :  0.650212966968
Iteration :  71   Loss :  0.636572191109
Iteration :  72   Loss :  0.623217584206
Iteration :  73   Loss :  0.610143142739
Iteration :  74   Loss :  0.597342989136
Iteration :  75   Loss :  0.584811369129
Iteration :  76   Loss :  0.572542649169
Iteration :  77   Loss :  0.560531313893
Iteration :  78   Loss :  0.548771963645
Iteration :  79   Loss :  0.537259312046
Iteration :  80   Loss :  0.525988183622
Iteration :  81   Loss :  0.514953511473
Iteration :  82   Loss :  0.504150335
Iteration :  83   Loss :  0.493573797668
Iteration :  84   Loss :  0.483219144829
Iteration :  85   Loss :  0.473081721585
Iteration :  86   Loss :  0.463156970688
Iteration :  87   Loss :  0.453440430499
Iteration :  88   Loss :  0.44392773298
Iteration :  89   Loss :  0.434614601728
Iteration :  90   Loss :  0.425496850056
Iteration :  91   Loss :  0.416570379107
Iteration :  92   Loss :  0.407831176015
Iteration :  93   Loss :  0.399275312102
Iteration :  94   Loss :  0.390898941105
Iteration :  95   Loss :  0.382698297454
Iteration :  96   Loss :  0.374669694577
Iteration :  97   Loss :  0.366809523242
Iteration :  98   Loss :  0.359114249934
Iteration :  99   Loss :  0.351580415267
[ -9.19001124e-03  -3.05322254e-04  -7.85485875e-04 ...,   5.77876379e-03
   2.72688908e-03  -5.79992327e-06]
CROSS VALIDATION 16
Iteration :  0   Loss :  175.080686053
Iteration :  1   Loss :  2.57433815412
Iteration :  2   Loss :  2.52033128017
Iteration :  3   Loss :  2.46745741295
Iteration :  4   Loss :  2.41569278317
Iteration :  5   Loss :  2.36501412022
Iteration :  6   Loss :  2.31539864167
Iteration :  7   Loss :  2.26682404304
Iteration :  8   Loss :  2.21926848777
Iteration :  9   Loss :  2.17271059744
Iteration :  10   Loss :  2.12712944207
Iteration :  11   Loss :  2.08250453083
Iteration :  12   Loss :  2.03881580271
Iteration :  13   Loss :  1.99604361761
Iteration :  14   Loss :  1.95416874741
Iteration :  15   Loss :  1.91317236741
Iteration :  16   Loss :  1.87303604782
Iteration :  17   Loss :  1.83374174549
Iteration :  18   Loss :  1.79527179579
Iteration :  19   Loss :  1.75760890468
Iteration :  20   Loss :  1.72073614093
Iteration :  21   Loss :  1.68463692852
Iteration :  22   Loss :  1.64929503915
Iteration :  23   Loss :  1.61469458499
Iteration :  24   Loss :  1.58082001153
Iteration :  25   Loss :  1.54765609056
Iteration :  26   Loss :  1.51518791335
Iteration :  27   Loss :  1.48340088393
Iteration :  28   Loss :  1.45228071255
Iteration :  29   Loss :  1.42181340923
Iteration :  30   Loss :  1.39198527751
Iteration :  31   Loss :  1.36278290823
Iteration :  32   Loss :  1.33419317357
Iteration :  33   Loss :  1.30620322112
Iteration :  34   Loss :  1.27880046806
Iteration :  35   Loss :  1.2519725956
Iteration :  36   Loss :  1.22570754334
Iteration :  37   Loss :  1.19999350391
Iteration :  38   Loss :  1.17481891766
Iteration :  39   Loss :  1.15017246743
Iteration :  40   Loss :  1.12604307349
Iteration :  41   Loss :  1.10241988854
Iteration :  42   Loss :  1.07929229287
Iteration :  43   Loss :  1.05664988953
Iteration :  44   Loss :  1.03448249971
Iteration :  45   Loss :  1.01278015813
Iteration :  46   Loss :  0.991533108574
Iteration :  47   Loss :  0.970731799498
Iteration :  48   Loss :  0.950366879743
Iteration :  49   Loss :  0.930429194325
Iteration :  50   Loss :  0.910909780322
Iteration :  51   Loss :  0.891799862845
Iteration :  52   Loss :  0.873090851094
Iteration :  53   Loss :  0.854774334492
Iteration :  54   Loss :  0.836842078909
Iteration :  55   Loss :  0.819286022959
Iteration :  56   Loss :  0.802098274373
Iteration :  57   Loss :  0.785271106455
Iteration :  58   Loss :  0.768796954607
Iteration :  59   Loss :  0.752668412927
Iteration :  60   Loss :  0.736878230881
Iteration :  61   Loss :  0.721419310045
Iteration :  62   Loss :  0.706284700912
Iteration :  63   Loss :  0.691467599766
Iteration :  64   Loss :  0.676961345629
Iteration :  65   Loss :  0.662759417261
Iteration :  66   Loss :  0.648855430231
Iteration :  67   Loss :  0.635243134048
Iteration :  68   Loss :  0.621916409348
Iteration :  69   Loss :  0.608869265144
Iteration :  70   Loss :  0.596095836137
Iteration :  71   Loss :  0.583590380072
Iteration :  72   Loss :  0.571347275163
Iteration :  73   Loss :  0.559361017561
Iteration :  74   Loss :  0.547626218884
Iteration :  75   Loss :  0.536137603791
Iteration :  76   Loss :  0.524890007612
Iteration :  77   Loss :  0.513878374027
Iteration :  78   Loss :  0.503097752792
Iteration :  79   Loss :  0.492543297514
Iteration :  80   Loss :  0.482210263472
Iteration :  81   Loss :  0.472094005484
Iteration :  82   Loss :  0.46218997582
Iteration :  83   Loss :  0.452493722154
Iteration :  84   Loss :  0.443000885568
Iteration :  85   Loss :  0.433707198588
Iteration :  86   Loss :  0.424608483267
Iteration :  87   Loss :  0.415700649308
Iteration :  88   Loss :  0.406979692222
Iteration :  89   Loss :  0.398441691532
Iteration :  90   Loss :  0.390082809007
Iteration :  91   Loss :  0.381899286939
Iteration :  92   Loss :  0.373887446452
Iteration :  93   Loss :  0.36604368585
Iteration :  94   Loss :  0.358364478995
Iteration :  95   Loss :  0.350846373724
Iteration :  96   Loss :  0.343485990299
Iteration :  97   Loss :  0.336280019883
Iteration :  98   Loss :  0.329225223055
Iteration :  99   Loss :  0.322318428354
[ -5.81287875e-03   4.94293245e-04  -5.90491364e-05 ...,   5.72667736e-03
   2.74194929e-03   3.45433366e-04]
CROSS VALIDATION 17
Iteration :  0   Loss :  175.080686053
Iteration :  1   Loss :  2.80811410221
Iteration :  2   Loss :  2.74920285774
Iteration :  3   Loss :  2.69152750846
Iteration :  4   Loss :  2.63506212662
Iteration :  5   Loss :  2.57978132837
Iteration :  6   Loss :  2.52566026243
Iteration :  7   Loss :  2.47267459883
Iteration :  8   Loss :  2.42080051804
Iteration :  9   Loss :  2.37001470025
Iteration :  10   Loss :  2.32029431486
Iteration :  11   Loss :  2.27161701021
Iteration :  12   Loss :  2.2239609036
Iteration :  13   Loss :  2.17730457138
Iteration :  14   Loss :  2.13162703933
Iteration :  15   Loss :  2.08690777328
Iteration :  16   Loss :  2.04312666983
Iteration :  17   Loss :  2.00026404732
Iteration :  18   Loss :  1.958300637
Iteration :  19   Loss :  1.91721757435
Iteration :  20   Loss :  1.87699639063
Iteration :  21   Loss :  1.83761900452
Iteration :  22   Loss :  1.79906771406
Iteration :  23   Loss :  1.76132518862
Iteration :  24   Loss :  1.7243744612
Iteration :  25   Loss :  1.68819892069
Iteration :  26   Loss :  1.65278230452
Iteration :  27   Loss :  1.61810869125
Iteration :  28   Loss :  1.58416249347
Iteration :  29   Loss :  1.55092845079
Iteration :  30   Loss :  1.51839162294
Iteration :  31   Loss :  1.48653738311
Iteration :  32   Loss :  1.45535141131
Iteration :  33   Loss :  1.42481968801
Iteration :  34   Loss :  1.39492848775
Iteration :  35   Loss :  1.36566437306
Iteration :  36   Loss :  1.33701418834
Iteration :  37   Loss :  1.30896505399
Iteration :  38   Loss :  1.28150436063
Iteration :  39   Loss :  1.25461976337
Iteration :  40   Loss :  1.22829917635
Iteration :  41   Loss :  1.20253076722
Iteration :  42   Loss :  1.17730295189
Iteration :  43   Loss :  1.15260438926
Iteration :  44   Loss :  1.12842397619
Iteration :  45   Loss :  1.10475084244
Iteration :  46   Loss :  1.08157434583
Iteration :  47   Loss :  1.05888406745
Iteration :  48   Loss :  1.03666980696
Iteration :  49   Loss :  1.014921578
Iteration :  50   Loss :  0.993629603738
Iteration :  51   Loss :  0.972784312426
Iteration :  52   Loss :  0.952376333134
Iteration :  53   Loss :  0.932396491523
Iteration :  54   Loss :  0.912835805719
Iteration :  55   Loss :  0.89368548228
Iteration :  56   Loss :  0.87493691224
Iteration :  57   Loss :  0.856581667241
Iteration :  58   Loss :  0.838611495741
Iteration :  59   Loss :  0.821018319309
Iteration :  60   Loss :  0.80379422899
Iteration :  61   Loss :  0.78693148175
Iteration :  62   Loss :  0.770422496995
Iteration :  63   Loss :  0.754259853166
Iteration :  64   Loss :  0.738436284399
Iteration :  65   Loss :  0.722944677259
Iteration :  66   Loss :  0.707778067545
Iteration :  67   Loss :  0.692929637157
Iteration :  68   Loss :  0.678392711032
Iteration :  69   Loss :  0.664160754142
Iteration :  70   Loss :  0.650227368556
Iteration :  71   Loss :  0.636586290567
Iteration :  72   Loss :  0.623231387873
Iteration :  73   Loss :  0.61015665682
Iteration :  74   Loss :  0.597356219706
Iteration :  75   Loss :  0.584824322135
Iteration :  76   Loss :  0.572555330435
Iteration :  77   Loss :  0.56054372912
Iteration :  78   Loss :  0.548784118413
Iteration :  79   Loss :  0.53727121182
Iteration :  80   Loss :  0.525999833752
Iteration :  81   Loss :  0.514964917196
Iteration :  82   Loss :  0.504161501442
Iteration :  83   Loss :  0.49358472985
Iteration :  84   Loss :  0.483229847666
Iteration :  85   Loss :  0.473092199887
Iteration :  86   Loss :  0.463167229167
Iteration :  87   Loss :  0.453450473766
Iteration :  88   Loss :  0.44393756555
Iteration :  89   Loss :  0.434624228021
Iteration :  90   Loss :  0.425506274399
Iteration :  91   Loss :  0.416579605738
Iteration :  92   Loss :  0.407840209082
Iteration :  93   Loss :  0.399284155664
Iteration :  94   Loss :  0.390907599138
Iteration :  95   Loss :  0.382706773851
Iteration :  96   Loss :  0.374677993149
Iteration :  97   Loss :  0.366817647718
Iteration :  98   Loss :  0.359122203967
Iteration :  99   Loss :  0.351588202433
[ -9.19030283e-03  -3.05339261e-04  -7.85578641e-04 ...,   5.77877421e-03
   2.72688607e-03  -5.87018398e-06]
CROSS VALIDATION 18
Iteration :  0   Loss :  13.8082838623
Iteration :  1   Loss :  7.14947466082
Iteration :  2   Loss :  2.78243310803
Iteration :  3   Loss :  2.72406062348
Iteration :  4   Loss :  2.66691273151
Iteration :  5   Loss :  2.61096374148
Iteration :  6   Loss :  2.5561885017
Iteration :  7   Loss :  2.50256238814
Iteration :  8   Loss :  2.45006129336
Iteration :  9   Loss :  2.39866161565
Iteration :  10   Loss :  2.34834024847
Iteration :  11   Loss :  2.29907456999
Iteration :  12   Loss :  2.25084243301
Iteration :  13   Loss :  2.20362215491
Iteration :  14   Loss :  2.15739250798
Iteration :  15   Loss :  2.11213270983
Iteration :  16   Loss :  2.06782241406
Iteration :  17   Loss :  2.02444170112
Iteration :  18   Loss :  1.98197106937
Iteration :  19   Loss :  1.94039142625
Iteration :  20   Loss :  1.89968407978
Iteration :  21   Loss :  1.85983073011
Iteration :  22   Loss :  1.82081346129
Iteration :  23   Loss :  1.78261473323
Iteration :  24   Loss :  1.74521737382
Iteration :  25   Loss :  1.70860457119
Iteration :  26   Loss :  1.6727598662
Iteration :  27   Loss :  1.63766714495
Iteration :  28   Loss :  1.60331063164
Iteration :  29   Loss :  1.5696748814
Iteration :  30   Loss :  1.53674477339
Iteration :  31   Loss :  1.504505504
Iteration :  32   Loss :  1.47294258016
Iteration :  33   Loss :  1.44204181285
Iteration :  34   Loss :  1.41178931075
Iteration :  35   Loss :  1.38217147394
Iteration :  36   Loss :  1.35317498782
Iteration :  37   Loss :  1.32478681711
Iteration :  38   Loss :  1.29699419999
Iteration :  39   Loss :  1.2697846424
Iteration :  40   Loss :  1.24314591236
Iteration :  41   Loss :  1.21706603452
Iteration :  42   Loss :  1.19153328475
Iteration :  43   Loss :  1.16653618489
Iteration :  44   Loss :  1.14206349757
Iteration :  45   Loss :  1.11810422118
Iteration :  46   Loss :  1.09464758491
Iteration :  47   Loss :  1.07168304389
Iteration :  48   Loss :  1.04920027449
Iteration :  49   Loss :  1.02718916967
Iteration :  50   Loss :  1.00563983439
Iteration :  51   Loss :  0.984542581223
Iteration :  52   Loss :  0.96388792597
Iteration :  53   Loss :  0.943666583395
Iteration :  54   Loss :  0.923869463062
Iteration :  55   Loss :  0.904487665237
Iteration :  56   Loss :  0.885512476899
Iteration :  57   Loss :  0.866935367811
Iteration :  58   Loss :  0.848747986695
Iteration :  59   Loss :  0.830942157474
Iteration :  60   Loss :  0.813509875595
Iteration :  61   Loss :  0.796443304432
Iteration :  62   Loss :  0.779734771763
Iteration :  63   Loss :  0.763376766323
Iteration :  64   Loss :  0.747361934423
Iteration :  65   Loss :  0.731683076647
Iteration :  66   Loss :  0.716333144617
Iteration :  67   Loss :  0.701305237819
Iteration :  68   Loss :  0.686592600509
Iteration :  69   Loss :  0.672188618666
Iteration :  70   Loss :  0.658086817029
Iteration :  71   Loss :  0.644280856178
Iteration :  72   Loss :  0.630764529688
Iteration :  73   Loss :  0.61753176134
Iteration :  74   Loss :  0.604576602384
Iteration :  75   Loss :  0.591893228872
Iteration :  76   Loss :  0.579475939033
Iteration :  77   Loss :  0.567319150717
Iteration :  78   Loss :  0.555417398878
Iteration :  79   Loss :  0.543765333122
Iteration :  80   Loss :  0.532357715302
Iteration :  81   Loss :  0.52118941716
Iteration :  82   Loss :  0.510255418023
Iteration :  83   Loss :  0.499550802548
Iteration :  84   Loss :  0.489070758511
Iteration :  85   Loss :  0.47881057464
Iteration :  86   Loss :  0.468765638505
Iteration :  87   Loss :  0.458931434436
Iteration :  88   Loss :  0.4493035415
Iteration :  89   Loss :  0.439877631508
Iteration :  90   Loss :  0.430649467074
Iteration :  91   Loss :  0.421614899705
Iteration :  92   Loss :  0.412769867942
Iteration :  93   Loss :  0.404110395529
Iteration :  94   Loss :  0.395632589629
Iteration :  95   Loss :  0.387332639072
Iteration :  96   Loss :  0.379206812641
Iteration :  97   Loss :  0.371251457399
Iteration :  98   Loss :  0.36346299704
Iteration :  99   Loss :  0.355837930289
[ -9.63024748e-03  -1.33339689e-03  -2.32680093e-03 ...,   8.58946216e-03
   4.84141400e-03  -1.21625412e-05]
CROSS VALIDATION 19
Iteration :  0   Loss :  200.122157945
Iteration :  1   Loss :  54.7560043595
Iteration :  2   Loss :  2.67579697732
Iteration :  3   Loss :  2.61966160527
Iteration :  4   Loss :  2.56470389357
Iteration :  5   Loss :  2.51089913615
Iteration :  6   Loss :  2.45822314527
Iteration :  7   Loss :  2.4066522406
Iteration :  8   Loss :  2.35616323861
Iteration :  9   Loss :  2.30673344213
Iteration :  10   Loss :  2.25834063016
Iteration :  11   Loss :  2.21096304787
Iteration :  12   Loss :  2.16457939682
Iteration :  13   Loss :  2.11916882539
Iteration :  14   Loss :  2.0747109194
Iteration :  15   Loss :  2.03118569296
Iteration :  16   Loss :  1.98857357943
Iteration :  17   Loss :  1.94685542269
Iteration :  18   Loss :  1.90601246845
Iteration :  19   Loss :  1.86602635592
Iteration :  20   Loss :  1.82687910947
Iteration :  21   Loss :  1.78855313056
Iteration :  22   Loss :  1.7510311899
Iteration :  23   Loss :  1.71429641961
Iteration :  24   Loss :  1.67833230569
Iteration :  25   Loss :  1.64312268059
Iteration :  26   Loss :  1.60865171595
Iteration :  27   Loss :  1.57490391545
Iteration :  28   Loss :  1.54186410788
Iteration :  29   Loss :  1.50951744029
Iteration :  30   Loss :  1.47784937136
Iteration :  31   Loss :  1.4468456648
Iteration :  32   Loss :  1.41649238301
Iteration :  33   Loss :  1.38677588075
Iteration :  34   Loss :  1.35768279907
Iteration :  35   Loss :  1.32920005927
Iteration :  36   Loss :  1.30131485704
Iteration :  37   Loss :  1.27401465666
Iteration :  38   Loss :  1.24728718542
Iteration :  39   Loss :  1.22112042808
Iteration :  40   Loss :  1.19550262145
Iteration :  41   Loss :  1.17042224913
Iteration :  42   Loss :  1.14586803632
Iteration :  43   Loss :  1.12182894476
Iteration :  44   Loss :  1.09829416775
Iteration :  45   Loss :  1.07525312531
Iteration :  46   Loss :  1.05269545941
Iteration :  47   Loss :  1.03061102932
Iteration :  48   Loss :  1.00898990707
Iteration :  49   Loss :  0.987822372944
Iteration :  50   Loss :  0.967098911152
Iteration :  51   Loss :  0.946810205527
Iteration :  52   Loss :  0.926947135347
Iteration :  53   Loss :  0.907500771235
Iteration :  54   Loss :  0.88846237114
Iteration :  55   Loss :  0.869823376411
Iteration :  56   Loss :  0.85157540795
Iteration :  57   Loss :  0.833710262441
Iteration :  58   Loss :  0.816219908666
Iteration :  59   Loss :  0.799096483894
Iteration :  60   Loss :  0.782332290345
Iteration :  61   Loss :  0.76591979173
Iteration :  62   Loss :  0.749851609864
Iteration :  63   Loss :  0.734120521348
Iteration :  64   Loss :  0.718719454322
Iteration :  65   Loss :  0.703641485288
Iteration :  66   Loss :  0.688879835992
Iteration :  67   Loss :  0.674427870384
Iteration :  68   Loss :  0.66027909163
Iteration :  69   Loss :  0.646427139192
Iteration :  70   Loss :  0.632865785969
Iteration :  71   Loss :  0.619588935501
Iteration :  72   Loss :  0.606590619221
Iteration :  73   Loss :  0.593864993779
Iteration :  74   Loss :  0.581406338412
Iteration :  75   Loss :  0.569209052372
Iteration :  76   Loss :  0.557267652408
Iteration :  77   Loss :  0.545576770303
Iteration :  78   Loss :  0.534131150459
Iteration :  79   Loss :  0.522925647535
Iteration :  80   Loss :  0.511955224134
Iteration :  81   Loss :  0.501214948536
Iteration :  82   Loss :  0.490699992487
Iteration :  83   Loss :  0.480405629023
Iteration :  84   Loss :  0.470327230344
Iteration :  85   Loss :  0.460460265741
Iteration :  86   Loss :  0.450800299552
Iteration :  87   Loss :  0.441342989169
Iteration :  88   Loss :  0.432084083091
Iteration :  89   Loss :  0.423019419005
Iteration :  90   Loss :  0.414144921923
Iteration :  91   Loss :  0.405456602341
Iteration :  92   Loss :  0.396950554455
Iteration :  93   Loss :  0.388622954399
Iteration :  94   Loss :  0.380470058526
Iteration :  95   Loss :  0.37248820173
Iteration :  96   Loss :  0.364673795792
Iteration :  97   Loss :  0.357023327772
Iteration :  98   Loss :  0.349533358427
Iteration :  99   Loss :  0.342200520665
[-0.00732032 -0.00215004 -0.00557102 ...,  0.00383517  0.00130619  0.000697  ]
Accuracy (Hinge Loss):	0.8
lmda : 0.5  eta : 0.001
Logistic Loss
CROSS VALIDATION 0
Iteration :  0   Loss :  128.417053587
Iteration :  1   Loss :  62.7932309253
Iteration :  2   Loss :  11.2461000893
Iteration :  3   Loss :  10.1144831756
Iteration :  4   Loss :  9.09673300956
Iteration :  5   Loss :  8.18139197165
Iteration :  6   Loss :  7.35815534344
Iteration :  7   Loss :  6.61775529959
Iteration :  8   Loss :  5.95185657372
Iteration :  9   Loss :  5.35296262404
Iteration :  10   Loss :  4.81433124351
Iteration :  11   Loss :  4.32989866611
Iteration :  12   Loss :  3.89421131789
Iteration :  13   Loss :  3.50236444803
Iteration :  14   Loss :  3.14994695421
Iteration :  15   Loss :  2.8329917855
Iteration :  16   Loss :  2.54793136501
Iteration :  17   Loss :  2.29155751663
Iteration :  18   Loss :  2.06098539665
Iteration :  19   Loss :  1.85362091019
Iteration :  20   Loss :  1.66713105415
Iteration :  21   Loss :  1.49941669371
Iteration :  22   Loss :  1.34858771339
Iteration :  23   Loss :  1.21294131531
Iteration :  24   Loss :  1.09094441839
Iteration :  25   Loss :  0.981219303153
Iteration :  26   Loss :  0.882529744672
Iteration :  27   Loss :  0.793766336085
Iteration :  28   Loss :  0.713932384634
Iteration :  29   Loss :  0.642131771794
Iteration :  30   Loss :  0.577558789826
Iteration :  31   Loss :  0.51948939996
Iteration :  32   Loss :  0.467273467102
Iteration :  33   Loss :  0.420327656734
Iteration :  34   Loss :  0.378128655635
Iteration :  35   Loss :  0.340206299868
Iteration :  36   Loss :  0.306136303259
Iteration :  37   Loss :  0.275532895326
Iteration :  38   Loss :  0.248042799564
Iteration :  39   Loss :  0.223342655739
Iteration :  40   Loss :  0.201140540961
Iteration :  41   Loss :  0.181178493183
Iteration :  42   Loss :  0.163230904889
Iteration :  43   Loss :  0.147097966836
Iteration :  44   Loss :  0.132599270949
Iteration :  45   Loss :  0.11957129873
Iteration :  46   Loss :  0.107866865915
Iteration :  47   Loss :  0.0973539180425
Iteration :  48   Loss :  0.0879142075691
Iteration :  49   Loss :  0.0794424629811
Iteration :  50   Loss :  0.0718453067928
Iteration :  51   Loss :  0.0650394550887
Iteration :  52   Loss :  0.0589492749315
Iteration :  53   Loss :  0.0535045342494
Iteration :  54   Loss :  0.0486394414589
Iteration :  55   Loss :  0.0442929120591
Iteration :  56   Loss :  0.0404089231122
Iteration :  57   Loss :  0.0369364597382
Iteration :  58   Loss :  0.0338298120193
Iteration :  59   Loss :  0.0310491078984
Iteration :  60   Loss :  0.0285597797482
Iteration :  61   Loss :  0.0263312559187
Iteration :  62   Loss :  0.0243363118594
Iteration :  63   Loss :  0.0225511546732
Iteration :  64   Loss :  0.0209556600782
Iteration :  65   Loss :  0.0195339276114
Iteration :  66   Loss :  0.0182749681019
Iteration :  67   Loss :  0.0171720726479
Iteration :  68   Loss :  0.0162194378048
Iteration :  69   Loss :  0.0154065835212
Iteration :  70   Loss :  0.0147142914846
Iteration :  71   Loss :  0.0141168947128
Iteration :  72   Loss :  0.0135901361713
Iteration :  73   Loss :  0.0131177520454
Iteration :  74   Loss :  0.0126934313926
Iteration :  75   Loss :  0.0123193032778
Iteration :  76   Loss :  0.0120007458655
Iteration :  77   Loss :  0.0117377762238
Iteration :  78   Loss :  0.0115180466525
Iteration :  79   Loss :  0.0113198121383
Iteration :  80   Loss :  0.0111252144533
Iteration :  81   Loss :  0.010929965464
Iteration :  82   Loss :  0.010740610487
Iteration :  83   Loss :  0.0105675099847
Iteration :  84   Loss :  0.0104220119267
Iteration :  85   Loss :  0.0103172576939
Iteration :  86   Loss :  0.010266377454
[ -3.31475065e-04  -1.10738751e-04   7.47956692e-06 ...,   1.50494588e-04
   1.05495072e-04   2.64727266e-05]
CROSS VALIDATION 1
Iteration :  0   Loss :  71.2668859387
Iteration :  1   Loss :  11.4680285148
Iteration :  2   Loss :  10.3140804856
Iteration :  3   Loss :  9.27624622884
Iteration :  4   Loss :  8.34284202248
Iteration :  5   Loss :  7.50335979608
Iteration :  6   Loss :  6.74834883338
Iteration :  7   Loss :  6.06930937801
Iteration :  8   Loss :  5.45859694505
Iteration :  9   Loss :  4.90933626097
Iteration :  10   Loss :  4.41534386325
Iteration :  11   Loss :  3.97105848824
Iteration :  12   Loss :  3.57147846362
Iteration :  13   Loss :  3.2121054006
Iteration :  14   Loss :  2.88889355208
Iteration :  15   Loss :  2.59820426647
Iteration :  16   Loss :  2.33676502478
Iteration :  17   Loss :  2.10163259982
Iteration :  18   Loss :  1.89015992313
Iteration :  19   Loss :  1.69996628768
Iteration :  20   Loss :  1.528910552
Iteration :  21   Loss :  1.37506704708
Iteration :  22   Loss :  1.23670391885
Iteration :  23   Loss :  1.11226366921
Iteration :  24   Loss :  1.00034568574
Iteration :  25   Loss :  0.899690573809
Iteration :  26   Loss :  0.8091661217
Iteration :  27   Loss :  0.727754730103
Iteration :  28   Loss :  0.654542106779
Iteration :  29   Loss :  0.588706950261
Iteration :  30   Loss :  0.529511249652
Iteration :  31   Loss :  0.476290863235
Iteration :  32   Loss :  0.428446455643
Iteration :  33   Loss :  0.385435556013
Iteration :  34   Loss :  0.346766627783
Iteration :  35   Loss :  0.311995433749
Iteration :  36   Loss :  0.280723480969
Iteration :  37   Loss :  0.252597141105
Iteration :  38   Loss :  0.227304855193
Iteration :  39   Loss :  0.204571855602
Iteration :  40   Loss :  0.184154285968
Iteration :  41   Loss :  0.165833710368
Iteration :  42   Loss :  0.149411412308
Iteration :  43   Loss :  0.134702334665
Iteration :  44   Loss :  0.121530073271
Iteration :  45   Loss :  0.10972505142
Iteration :  46   Loss :  0.0991277898221
Iteration :  47   Loss :  0.0895970838476
Iteration :  48   Loss :  0.0810167390159
Iteration :  49   Loss :  0.0732931719785
Iteration :  50   Loss :  0.0663463916115
Iteration :  51   Loss :  0.0601033583739
Iteration :  52   Loss :  0.0544971847595
Iteration :  53   Loss :  0.0494683752471
Iteration :  54   Loss :  0.0449639328985
Iteration :  55   Loss :  0.0409349498999
Iteration :  56   Loss :  0.0373354356768
Iteration :  57   Loss :  0.0341228809506
Iteration :  58   Loss :  0.0312588139022
Iteration :  59   Loss :  0.0287080453801
Iteration :  60   Loss :  0.026437207513
Iteration :  61   Loss :  0.0244140303466
Iteration :  62   Loss :  0.0226081835303
Iteration :  63   Loss :  0.0209933544585
Iteration :  64   Loss :  0.0195494115336
Iteration :  65   Loss :  0.0182634069759
Iteration :  66   Loss :  0.0171284826925
Iteration :  67   Loss :  0.0161402641277
Iteration :  68   Loss :  0.0152912004035
Iteration :  69   Loss :  0.0145652755208
Iteration :  70   Loss :  0.0139377096319
Iteration :  71   Loss :  0.0133818248173
Iteration :  72   Loss :  0.0128781134544
Iteration :  73   Loss :  0.0124189470661
Iteration :  74   Loss :  0.0120079920899
Iteration :  75   Loss :  0.0116564341336
Iteration :  76   Loss :  0.0113766569821
Iteration :  77   Loss :  0.0111729171839
Iteration :  78   Loss :  0.0110325442087
Iteration :  79   Loss :  0.0109283185958
Iteration :  80   Loss :  0.0108352074367
[ -2.77772649e-04  -3.11044595e-05  -3.98386036e-05 ...,   1.19096848e-04
   8.01788200e-05   6.98081394e-05]
CROSS VALIDATION 2
Iteration :  0   Loss :  312.043760785
Iteration :  1   Loss :  73.9646337056
Iteration :  2   Loss :  17.5337728491
Iteration :  3   Loss :  11.5900720856
Iteration :  4   Loss :  10.3603753702
Iteration :  5   Loss :  9.31788278088
Iteration :  6   Loss :  8.38028897758
Iteration :  7   Loss :  7.53703872427
Iteration :  8   Loss :  6.77863888503
Iteration :  9   Loss :  6.09655155222
Iteration :  10   Loss :  5.48309792855
Iteration :  11   Loss :  4.93137188074
Iteration :  12   Loss :  4.43516219172
Iteration :  13   Loss :  3.98888263607
Iteration :  14   Loss :  3.58750909146
Iteration :  15   Loss :  3.2265229783
Iteration :  16   Loss :  2.9018603907
Iteration :  17   Loss :  2.60986634663
Iteration :  18   Loss :  2.34725364236
Iteration :  19   Loss :  2.1110658492
Iteration :  20   Loss :  1.89864403728
Iteration :  21   Loss :  1.70759685486
Iteration :  22   Loss :  1.53577363051
Iteration :  23   Loss :  1.38124020198
Iteration :  24   Loss :  1.24225720791
Iteration :  25   Loss :  1.11726060713
Iteration :  26   Loss :  1.00484420981
Iteration :  27   Loss :  0.903744009116
Iteration :  28   Loss :  0.812824075422
Iteration :  29   Loss :  0.731063708008
Iteration :  30   Loss :  0.657545467473
Iteration :  31   Loss :  0.59144380319
Iteration :  32   Loss :  0.532014490816
Iteration :  33   Loss :  0.478585743554
Iteration :  34   Loss :  0.430551414935
Iteration :  35   Loss :  0.387365083248
Iteration :  36   Loss :  0.348533980495
Iteration :  37   Loss :  0.31361399225
Iteration :  38   Loss :  0.282206700281
Iteration :  39   Loss :  0.253956759634
Iteration :  40   Loss :  0.228547657961
Iteration :  41   Loss :  0.205696362719
Iteration :  42   Loss :  0.185148774506
Iteration :  43   Loss :  0.166676900097
Iteration :  44   Loss :  0.150076839088
Iteration :  45   Loss :  0.135166135835
Iteration :  46   Loss :  0.121780369267
Iteration :  47   Loss :  0.109770196308
Iteration :  48   Loss :  0.0989996662032
Iteration :  49   Loss :  0.0893453546657
Iteration :  50   Loss :  0.0806954502758
Iteration :  51   Loss :  0.072948522429
Iteration :  52   Loss :  0.0660126574241
Iteration :  53   Loss :  0.0598055208872
Iteration :  54   Loss :  0.0542548449634
Iteration :  55   Loss :  0.0492982651696
Iteration :  56   Loss :  0.0448805475151
Iteration :  57   Loss :  0.040947458893
Iteration :  58   Loss :  0.0374417222566
Iteration :  59   Loss :  0.0343073971545
Iteration :  60   Loss :  0.0314962908236
Iteration :  61   Loss :  0.0289683140274
Iteration :  62   Loss :  0.0266901359437
Iteration :  63   Loss :  0.024635073743
Iteration :  64   Loss :  0.0227823958361
Iteration :  65   Loss :  0.0211158839296
Iteration :  66   Loss :  0.0196227311205
Iteration :  67   Loss :  0.0182931425279
Iteration :  68   Loss :  0.0171200451074
Iteration :  69   Loss :  0.0160979239426
Iteration :  70   Loss :  0.0152201274684
Iteration :  71   Loss :  0.0144751806114
Iteration :  72   Loss :  0.0138446964651
Iteration :  73   Loss :  0.0133062965187
Iteration :  74   Loss :  0.0128409393229
Iteration :  75   Loss :  0.0124381487801
Iteration :  76   Loss :  0.0120940112432
Iteration :  77   Loss :  0.0118037719223
Iteration :  78   Loss :  0.0115551438217
Iteration :  79   Loss :  0.0113292922549
Iteration :  80   Loss :  0.011109937898
Iteration :  81   Loss :  0.0108906703621
Iteration :  82   Loss :  0.0106741587674
Iteration :  83   Loss :  0.010467657579
Iteration :  84   Loss :  0.0102799995859
Iteration :  85   Loss :  0.0101210491577
Iteration :  86   Loss :  0.0100015460672
Iteration :  87   Loss :  0.00992917871999
[ -4.02121895e-04  -1.90240070e-04  -9.21241403e-05 ...,   2.77876239e-04
   1.03277104e-04   2.79185879e-05]
CROSS VALIDATION 3
Iteration :  0   Loss :  71.3590293456
Iteration :  1   Loss :  11.4732242534
Iteration :  2   Loss :  10.3187534131
Iteration :  3   Loss :  9.28044895218
Iteration :  4   Loss :  8.34662185496
Iteration :  5   Loss :  7.50675929029
Iteration :  6   Loss :  6.7514062601
Iteration :  7   Loss :  6.07205915713
Iteration :  8   Loss :  5.461070033
Iteration :  9   Loss :  4.91156049926
Iteration :  10   Loss :  4.4173442919
Iteration :  11   Loss :  3.97285762765
Iteration :  12   Loss :  3.57309656813
Iteration :  13   Loss :  3.21356068649
Iteration :  14   Loss :  2.89020240264
Iteration :  15   Loss :  2.59938141644
Iteration :  16   Loss :  2.33782372617
Iteration :  17   Loss :  2.10258477103
Iteration :  18   Loss :  1.89101628293
Iteration :  19   Loss :  1.70073647544
Iteration :  20   Loss :  1.52960323546
Iteration :  21   Loss :  1.37569001833
Iteration :  22   Loss :  1.2372641794
Iteration :  23   Loss :  1.1127675043
Iteration :  24   Loss :  1.00079872806
Iteration :  25   Loss :  0.900097857202
Iteration :  26   Loss :  0.809532126585
Iteration :  27   Loss :  0.728083426725
Iteration :  28   Loss :  0.654837011178
Iteration :  29   Loss :  0.588971220658
Iteration :  30   Loss :  0.529747855126
Iteration :  31   Loss :  0.476502818145
Iteration :  32   Loss :  0.428637017288
Iteration :  33   Loss :  0.385608251284
Iteration :  34   Loss :  0.346925193726
Iteration :  35   Loss :  0.312143998672
Iteration :  36   Loss :  0.280867109757
Iteration :  37   Loss :  0.25274246426
Iteration :  38   Loss :  0.227460288597
Iteration :  39   Loss :  0.204746723278
Iteration :  40   Loss :  0.184356184674
Iteration :  41   Loss :  0.166064574982
Iteration :  42   Loss :  0.14966485735
Iteration :  43   Loss :  0.134965084526
Iteration :  44   Loss :  0.121786817033
Iteration :  45   Loss :  0.109963397176
Iteration :  46   Loss :  0.0993412212555
Iteration :  47   Loss :  0.0897853666895
Iteration :  48   Loss :  0.081184096203
Iteration :  49   Loss :  0.0734456334916
Iteration :  50   Loss :  0.0664895964097
Iteration :  51   Loss :  0.0602412963844
Iteration :  52   Loss :  0.0546319953983
Iteration :  53   Loss :  0.0496008109344
Iteration :  54   Loss :  0.0450938463285
Iteration :  55   Loss :  0.041061656198
Iteration :  56   Loss :  0.0374581011319
Iteration :  57   Loss :  0.0342408758154
Iteration :  58   Loss :  0.0313718752757
Iteration :  59   Loss :  0.0288162531014
Iteration :  60   Loss :  0.0265409673494
Iteration :  61   Loss :  0.0245142329478
Iteration :  62   Loss :  0.0227063218787
Iteration :  63   Loss :  0.0210913071612
Iteration :  64   Loss :  0.0196491126169
Iteration :  65   Loss :  0.0183668096755
Iteration :  66   Loss :  0.0172377213557
Iteration :  67   Loss :  0.0162574113119
Iteration :  68   Loss :  0.0154172503669
Iteration :  69   Loss :  0.0146990145007
Iteration :  70   Loss :  0.0140757823647
Iteration :  71   Loss :  0.0135201307383
Iteration :  72   Loss :  0.0130130600901
Iteration :  73   Loss :  0.0125475889634
Iteration :  74   Loss :  0.0121274150968
Iteration :  75   Loss :  0.0117632343415
Iteration :  76   Loss :  0.0114675243647
Iteration :  77   Loss :  0.011246987909
Iteration :  78   Loss :  0.0110940436075
Iteration :  79   Loss :  0.0109854438158
Iteration :  80   Loss :  0.0108947730677
[ -3.02677812e-04  -2.83304548e-05  -5.63622444e-05 ...,   1.59902590e-04
   7.79783874e-05   6.60931133e-05]
CROSS VALIDATION 4
Iteration :  0   Loss :  71.3590293456
Iteration :  1   Loss :  11.4732242534
Iteration :  2   Loss :  10.3187534131
Iteration :  3   Loss :  9.28044895218
Iteration :  4   Loss :  8.34662185496
Iteration :  5   Loss :  7.50675929029
Iteration :  6   Loss :  6.7514062601
Iteration :  7   Loss :  6.07205915713
Iteration :  8   Loss :  5.461070033
Iteration :  9   Loss :  4.91156049926
Iteration :  10   Loss :  4.4173442919
Iteration :  11   Loss :  3.97285762765
Iteration :  12   Loss :  3.57309656813
Iteration :  13   Loss :  3.21356068649
Iteration :  14   Loss :  2.89020240264
Iteration :  15   Loss :  2.59938141644
Iteration :  16   Loss :  2.33782372617
Iteration :  17   Loss :  2.10258477103
Iteration :  18   Loss :  1.89101628293
Iteration :  19   Loss :  1.70073647544
Iteration :  20   Loss :  1.52960323546
Iteration :  21   Loss :  1.37569001833
Iteration :  22   Loss :  1.2372641794
Iteration :  23   Loss :  1.1127675043
Iteration :  24   Loss :  1.00079872806
Iteration :  25   Loss :  0.900097857214
Iteration :  26   Loss :  0.809532126627
Iteration :  27   Loss :  0.728083426871
Iteration :  28   Loss :  0.654837011649
Iteration :  29   Loss :  0.588971222079
Iteration :  30   Loss :  0.529747859166
Iteration :  31   Loss :  0.476502828974
Iteration :  32   Loss :  0.428637044716
Iteration :  33   Loss :  0.385608317051
Iteration :  34   Loss :  0.346925343393
Iteration :  35   Loss :  0.312144322859
Iteration :  36   Loss :  0.280867780198
Iteration :  37   Loss :  0.252743791949
Iteration :  38   Loss :  0.227462812227
Iteration :  39   Loss :  0.204751333622
Iteration :  40   Loss :  0.184364279554
Iteration :  41   Loss :  0.166078212932
Iteration :  42   Loss :  0.14968683429
Iteration :  43   Loss :  0.134998812334
Iteration :  44   Loss :  0.121835869334
Iteration :  45   Loss :  0.110030651711
Iteration :  46   Loss :  0.0994277280021
Iteration :  47   Loss :  0.0898893835056
Iteration :  48   Loss :  0.0813008830465
Iteration :  49   Loss :  0.0735682463491
Iteration :  50   Loss :  0.0666102623872
Iteration :  51   Loss :  0.0603526701939
Iteration :  52   Loss :  0.0547279882858
Iteration :  53   Loss :  0.0496772071115
Iteration :  54   Loss :  0.0451488945823
Iteration :  55   Loss :  0.0410963583193
Iteration :  56   Loss :  0.0374757680892
Iteration :  57   Loss :  0.0342459520056
Iteration :  58   Loss :  0.0313685360805
Iteration :  59   Loss :  0.0288074044616
Iteration :  60   Loss :  0.0265280206448
Iteration :  61   Loss :  0.0244975915437
Iteration :  62   Loss :  0.0226861532235
Iteration :  63   Loss :  0.0210680971665
Iteration :  64   Loss :  0.0196237175726
Iteration :  65   Loss :  0.0183400920817
Iteration :  66   Loss :  0.0172101281203
Iteration :  67   Loss :  0.0162288394136
Iteration :  68   Loss :  0.0153873095498
Iteration :  69   Loss :  0.014667495028
Iteration :  70   Loss :  0.0140429477669
Iteration :  71   Loss :  0.0134866357728
Iteration :  72   Loss :  0.0129797003334
Iteration :  73   Loss :  0.0125151773636
Iteration :  74   Loss :  0.0120968551034
Iteration :  75   Loss :  0.0117356385279
Iteration :  76   Loss :  0.0114440860689
Iteration :  77   Loss :  0.0112283503932
Iteration :  78   Loss :  0.0110794421815
Iteration :  79   Loss :  0.01097273055
Iteration :  80   Loss :  0.0108817818619
[ -2.92654696e-04  -2.58887340e-05  -5.39940526e-05 ...,   1.58420557e-04
   8.49322275e-05   6.65538441e-05]
CROSS VALIDATION 5
Iteration :  0   Loss :  71.2272721342
Iteration :  1   Loss :  11.466944612
Iteration :  2   Loss :  10.3131056484
Iteration :  3   Loss :  9.27536948277
Iteration :  4   Loss :  8.34205349727
Iteration :  5   Loss :  7.50265061469
Iteration :  6   Loss :  6.74771101198
Iteration :  7   Loss :  6.06873573615
Iteration :  8   Loss :  5.45808102479
Iteration :  9   Loss :  4.90887225418
Iteration :  10   Loss :  4.41492654624
Iteration :  11   Loss :  3.97068316294
Iteration :  12   Loss :  3.57114090469
Iteration :  13   Loss :  3.21180180789
Iteration :  14   Loss :  2.88862050779
Iteration :  15   Loss :  2.59795869674
Iteration :  16   Loss :  2.33654416507
Iteration :  17   Loss :  2.10143396378
Iteration :  18   Loss :  1.8899812747
Iteration :  19   Loss :  1.69980561596
Iteration :  20   Loss :  1.52876604882
Iteration :  21   Loss :  1.37493708691
Iteration :  22   Loss :  1.23658704104
Iteration :  23   Loss :  1.11215856246
Iteration :  24   Loss :  1.00025117462
Iteration :  25   Loss :  0.899605607501
Iteration :  26   Loss :  0.809089764547
Iteration :  27   Loss :  0.727686153128
Iteration :  28   Loss :  0.654480578199
Iteration :  29   Loss :  0.588651822204
Iteration :  30   Loss :  0.529461942831
Iteration :  31   Loss :  0.476246870714
Iteration :  32   Loss :  0.428407420267
Iteration :  33   Loss :  0.385401477362
Iteration :  34   Loss :  0.346738169601
Iteration :  35   Loss :  0.311974225935
Iteration :  36   Loss :  0.280712389491
Iteration :  37   Loss :  0.252600539962
Iteration :  38   Loss :  0.227328676649
Iteration :  39   Loss :  0.204622739081
Iteration :  40   Loss :  0.184237012788
Iteration :  41   Loss :  0.165947052825
Iteration :  42   Loss :  0.149544759095
Iteration :  43   Loss :  0.134836658924
Iteration :  44   Loss :  0.121644041168
Iteration :  45   Loss :  0.109802974109
Iteration :  46   Loss :  0.0991649140793
Iteration :  47   Loss :  0.0895989744291
Iteration :  48   Loss :  0.0809935616227
Iteration :  49   Loss :  0.0732538965051
Iteration :  50   Loss :  0.0662964903094
Iteration :  51   Loss :  0.0600454143712
Iteration :  52   Loss :  0.0544323436021
Iteration :  53   Loss :  0.0493973522626
Iteration :  54   Loss :  0.044887440756
Iteration :  55   Loss :  0.040853864866
Iteration :  56   Loss :  0.0372508075578
Iteration :  57   Loss :  0.0340357691058
Iteration :  58   Loss :  0.0311700431727
Iteration :  59   Loss :  0.0286180257383
Iteration :  60   Loss :  0.0263459806295
Iteration :  61   Loss :  0.0243216226882
Iteration :  62   Loss :  0.0225149948013
Iteration :  63   Loss :  0.0209002789582
Iteration :  64   Loss :  0.0194578399565
Iteration :  65   Loss :  0.0181753292314
Iteration :  66   Loss :  0.0170464799285
Iteration :  67   Loss :  0.0160669081382
Iteration :  68   Loss :  0.0152277303692
Iteration :  69   Loss :  0.0145103909793
Iteration :  70   Loss :  0.0138878292965
Iteration :  71   Loss :  0.0133327694788
Iteration :  72   Loss :  0.0128264927386
Iteration :  73   Loss :  0.0123622733639
Iteration :  74   Loss :  0.0119440410977
Iteration :  75   Loss :  0.0115827170425
Iteration :  76   Loss :  0.0112908179092
Iteration :  77   Loss :  0.0110745059403
Iteration :  78   Loss :  0.0109249285529
Iteration :  79   Loss :  0.0108176310716
Iteration :  80   Loss :  0.01072618114
[ -2.75996551e-04  -2.44487583e-05  -4.38454972e-05 ...,   1.33429130e-04
   5.43416304e-05   7.01974835e-05]
CROSS VALIDATION 6
Iteration :  0   Loss :  71.3599851786
Iteration :  1   Loss :  11.4732649335
Iteration :  2   Loss :  10.3187899999
Iteration :  3   Loss :  9.28048185747
Iteration :  4   Loss :  8.34665144923
Iteration :  5   Loss :  7.50678590668
Iteration :  6   Loss :  6.75143019827
Iteration :  7   Loss :  6.07208068657
Iteration :  8   Loss :  5.46108939608
Iteration :  9   Loss :  4.91157791397
Iteration :  10   Loss :  4.41735995429
Iteration :  11   Loss :  3.97287171404
Iteration :  12   Loss :  3.5731092371
Iteration :  13   Loss :  3.21357208068
Iteration :  14   Loss :  2.89021265031
Iteration :  15   Loss :  2.59939063295
Iteration :  16   Loss :  2.33783201529
Iteration :  17   Loss :  2.10259222607
Iteration :  18   Loss :  1.89102298781
Iteration :  19   Loss :  1.70074250564
Iteration :  20   Loss :  1.52960865884
Iteration :  21   Loss :  1.3756948959
Iteration :  22   Loss :  1.23726856598
Iteration :  23   Loss :  1.1127714491
Iteration :  24   Loss :  1.00080227521
Iteration :  25   Loss :  0.900101046121
Iteration :  26   Loss :  0.809534992389
Iteration :  27   Loss :  0.728086000576
Iteration :  28   Loss :  0.654839320783
Iteration :  29   Loss :  0.588973291211
Iteration :  30   Loss :  0.529749711109
Iteration :  31   Loss :  0.476504486333
Iteration :  32   Loss :  0.428638530854
Iteration :  33   Loss :  0.385609655137
Iteration :  34   Loss :  0.346926553962
Iteration :  35   Loss :  0.31214542265
Iteration :  36   Loss :  0.280868783667
Iteration :  37   Loss :  0.252744713712
Iteration :  38   Loss :  0.22746366865
Iteration :  39   Loss :  0.204752143698
Iteration :  40   Loss :  0.184365067068
Iteration :  41   Loss :  0.166079013895
Iteration :  42   Loss :  0.149687713131
Iteration :  43   Loss :  0.13499988567
Iteration :  44   Loss :  0.121837340104
Iteration :  45   Loss :  0.110032876883
Iteration :  46   Loss :  0.0994313430312
Iteration :  47   Loss :  0.0898954504684
Iteration :  48   Loss :  0.0813109838578
Iteration :  49   Loss :  0.0735844872415
Iteration :  50   Loss :  0.066635192876
Iteration :  51   Loss :  0.0603890704956
Iteration :  52   Loss :  0.0547784487293
Iteration :  53   Loss :  0.0497434947062
Iteration :  54   Loss :  0.0452312560766
Iteration :  55   Loss :  0.0411930555384
Iteration :  56   Loss :  0.0375831274592
Iteration :  57   Loss :  0.0343589641728
Iteration :  58   Loss :  0.0314817290493
Iteration :  59   Loss :  0.0289156186129
Iteration :  60   Loss :  0.026626937345
Iteration :  61   Loss :  0.0245841487202
Iteration :  62   Loss :  0.0227589451375
Iteration :  63   Loss :  0.0211275610248
Iteration :  64   Loss :  0.0196718324913
Iteration :  65   Loss :  0.0183796280888
Iteration :  66   Loss :  0.0172437954328
Iteration :  67   Loss :  0.0162586617066
Iteration :  68   Loss :  0.0154143474489
Iteration :  69   Loss :  0.0146919746869
Iteration :  70   Loss :  0.0140647771462
Iteration :  71   Loss :  0.0135059579592
Iteration :  72   Loss :  0.0129969890648
Iteration :  73   Loss :  0.012530844134
Iteration :  74   Loss :  0.0121107610833
Iteration :  75   Loss :  0.0117468720177
Iteration :  76   Loss :  0.0114512171618
Iteration :  77   Loss :  0.0112302510688
Iteration :  78   Loss :  0.0110762973975
Iteration :  79   Loss :  0.0109661294219
Iteration :  80   Loss :  0.0108734570642
[ -3.05079303e-04  -3.35260355e-05  -5.22434435e-05 ...,   1.50571095e-04
   7.78377844e-05   6.38518011e-05]
CROSS VALIDATION 7
Iteration :  0   Loss :  71.3599851786
Iteration :  1   Loss :  11.4732649335
Iteration :  2   Loss :  10.3187899999
Iteration :  3   Loss :  9.28048185747
Iteration :  4   Loss :  8.34665144923
Iteration :  5   Loss :  7.50678590668
Iteration :  6   Loss :  6.75143019827
Iteration :  7   Loss :  6.07208068657
Iteration :  8   Loss :  5.46108939608
Iteration :  9   Loss :  4.91157791397
Iteration :  10   Loss :  4.41735995429
Iteration :  11   Loss :  3.97287171404
Iteration :  12   Loss :  3.5731092371
Iteration :  13   Loss :  3.21357208068
Iteration :  14   Loss :  2.89021265031
Iteration :  15   Loss :  2.59939063295
Iteration :  16   Loss :  2.33783201529
Iteration :  17   Loss :  2.10259222607
Iteration :  18   Loss :  1.89102298781
Iteration :  19   Loss :  1.70074250564
Iteration :  20   Loss :  1.52960865884
Iteration :  21   Loss :  1.3756948959
Iteration :  22   Loss :  1.23726856598
Iteration :  23   Loss :  1.11277144908
Iteration :  24   Loss :  1.00080227514
Iteration :  25   Loss :  0.900101045886
Iteration :  26   Loss :  0.809534991652
Iteration :  27   Loss :  0.728085998391
Iteration :  28   Loss :  0.654839314669
Iteration :  29   Loss :  0.588973274999
Iteration :  30   Loss :  0.529749670303
Iteration :  31   Loss :  0.476504388626
Iteration :  32   Loss :  0.428638307873
Iteration :  33   Loss :  0.385609169205
Iteration :  34   Loss :  0.346925540748
Iteration :  35   Loss :  0.312143397449
Iteration :  36   Loss :  0.280864897117
Iteration :  37   Loss :  0.252737546282
Iteration :  38   Loss :  0.227450970737
Iteration :  39   Loss :  0.204730573854
Iteration :  40   Loss :  0.184330067231
Iteration :  41   Loss :  0.166025069738
Iteration :  42   Loss :  0.149609275644
Iteration :  43   Loss :  0.134892966669
Iteration :  44   Loss :  0.121701152826
Iteration :  45   Loss :  0.109870422259
Iteration :  46   Loss :  0.099248411311
Iteration :  47   Loss :  0.0896987251651
Iteration :  48   Loss :  0.0811064252523
Iteration :  49   Loss :  0.0733766973477
Iteration :  50   Loss :  0.0664276785066
Iteration :  51   Loss :  0.0601847543278
Iteration :  52   Loss :  0.0545797958409
Iteration :  53   Loss :  0.0495522115775
Iteration :  54   Loss :  0.0450480241472
Iteration :  55   Loss :  0.0410176706484
Iteration :  56   Loss :  0.0374150254846
Iteration :  57   Loss :  0.034197842185
Iteration :  58   Loss :  0.0313280310251
Iteration :  59   Loss :  0.0287707991842
Iteration :  60   Loss :  0.0264933070184
Iteration :  61   Loss :  0.0244641066635
Iteration :  62   Loss :  0.0226538941444
Iteration :  63   Loss :  0.0210372314782
Iteration :  64   Loss :  0.0195944872034
Iteration :  65   Loss :  0.018312938204
Iteration :  66   Loss :  0.017185749318
Iteration :  67   Loss :  0.0162079746456
Iteration :  68   Loss :  0.0153702438379
Iteration :  69   Loss :  0.0146536299105
Iteration :  70   Loss :  0.0140309384702
Iteration :  71   Loss :  0.0134750290563
Iteration :  72   Loss :  0.0129673624828
Iteration :  73   Loss :  0.0125011761262
Iteration :  74   Loss :  0.0120800945765
Iteration :  75   Loss :  0.0117146231253
Iteration :  76   Loss :  0.0114171338037
Iteration :  77   Loss :  0.0111945065552
Iteration :  78   Loss :  0.01103967725
Iteration :  79   Loss :  0.010929898305
Iteration :  80   Loss :  0.0108387171683
[ -3.12693278e-04  -3.24091090e-05  -5.81833693e-05 ...,   1.47793144e-04
   7.57605370e-05   6.69988939e-05]
CROSS VALIDATION 8
Iteration :  0   Loss :  71.3599851786
Iteration :  1   Loss :  11.4732649335
Iteration :  2   Loss :  10.3187899999
Iteration :  3   Loss :  9.28048185747
Iteration :  4   Loss :  8.34665144923
Iteration :  5   Loss :  7.50678590668
Iteration :  6   Loss :  6.75143019827
Iteration :  7   Loss :  6.07208068657
Iteration :  8   Loss :  5.46108939608
Iteration :  9   Loss :  4.91157791397
Iteration :  10   Loss :  4.41735995429
Iteration :  11   Loss :  3.97287171404
Iteration :  12   Loss :  3.5731092371
Iteration :  13   Loss :  3.21357208068
Iteration :  14   Loss :  2.89021265031
Iteration :  15   Loss :  2.59939063295
Iteration :  16   Loss :  2.33783201529
Iteration :  17   Loss :  2.10259222607
Iteration :  18   Loss :  1.89102298781
Iteration :  19   Loss :  1.70074250564
Iteration :  20   Loss :  1.52960865884
Iteration :  21   Loss :  1.3756948959
Iteration :  22   Loss :  1.23726856598
Iteration :  23   Loss :  1.1127714491
Iteration :  24   Loss :  1.00080227521
Iteration :  25   Loss :  0.900101046121
Iteration :  26   Loss :  0.809534992389
Iteration :  27   Loss :  0.728086000576
Iteration :  28   Loss :  0.654839320785
Iteration :  29   Loss :  0.588973291214
Iteration :  30   Loss :  0.529749711116
Iteration :  31   Loss :  0.476504486348
Iteration :  32   Loss :  0.428638530881
Iteration :  33   Loss :  0.385609655179
Iteration :  34   Loss :  0.346926554012
Iteration :  35   Loss :  0.312145422663
Iteration :  36   Loss :  0.280868783452
Iteration :  37   Loss :  0.252744712607
Iteration :  38   Loss :  0.227463664608
Iteration :  39   Loss :  0.204752131071
Iteration :  40   Loss :  0.184365032062
Iteration :  41   Loss :  0.166078927172
Iteration :  42   Loss :  0.149687521998
Iteration :  43   Loss :  0.13499951403
Iteration :  44   Loss :  0.121836706089
Iteration :  45   Loss :  0.110031923605
Iteration :  46   Loss :  0.0994300555154
Iteration :  47   Loss :  0.0898938367858
Iteration :  48   Loss :  0.0813090279063
Iteration :  49   Loss :  0.0735820899965
Iteration :  50   Loss :  0.0666321038317
Iteration :  51   Loss :  0.0603847977108
Iteration :  52   Loss :  0.0547721335689
Iteration :  53   Loss :  0.0497337460722
Iteration :  54   Loss :  0.045215965607
Iteration :  55   Loss :  0.0411692723669
Iteration :  56   Loss :  0.0375471657961
Iteration :  57   Loss :  0.0343070124902
Iteration :  58   Loss :  0.0314111683254
Iteration :  59   Loss :  0.0288267824922
Iteration :  60   Loss :  0.0265242192199
Iteration :  61   Loss :  0.0244751202766
Iteration :  62   Loss :  0.0226515123444
Iteration :  63   Loss :  0.0210270792925
Iteration :  64   Loss :  0.0195801422388
Iteration :  65   Loss :  0.0182961190619
Iteration :  66   Loss :  0.0171671224534
Iteration :  67   Loss :  0.0161878318141
Iteration :  68   Loss :  0.0153488198626
Iteration :  69   Loss :  0.0146311734995
Iteration :  70   Loss :  0.0140077061771
Iteration :  71   Loss :  0.0134512749983
Iteration :  72   Loss :  0.0129432901943
Iteration :  73   Loss :  0.012476828139
Iteration :  74   Loss :  0.0120552231918
Iteration :  75   Loss :  0.0116886162717
Iteration :  76   Loss :  0.0113890669412
Iteration :  77   Loss :  0.0111633790807
Iteration :  78   Loss :  0.0110047825755
Iteration :  79   Loss :  0.0108910207905
Iteration :  80   Loss :  0.0107959193682
[ -2.90816672e-04  -2.51925999e-05  -3.35041287e-05 ...,   1.53162627e-04
   7.13447962e-05   6.31931729e-05]
CROSS VALIDATION 9
Iteration :  0   Loss :  166.632250618
Iteration :  1   Loss :  95.0601611451
Iteration :  2   Loss :  12.3987176158
Iteration :  3   Loss :  11.1511382942
Iteration :  4   Loss :  10.0291018732
Iteration :  5   Loss :  9.01997593145
Iteration :  6   Loss :  8.11239695849
Iteration :  7   Loss :  7.29614305904
Iteration :  8   Loss :  6.5620209816
Iteration :  9   Loss :  5.90176442788
Iteration :  10   Loss :  5.3079409192
Iteration :  11   Loss :  4.77386686406
Iteration :  12   Loss :  4.29353131456
Iteration :  13   Loss :  3.86152797951
Iteration :  14   Loss :  3.47299444949
Iteration :  15   Loss :  3.12355766934
Iteration :  16   Loss :  2.80928492439
Iteration :  17   Loss :  2.52663973796
Iteration :  18   Loss :  2.27244211186
Iteration :  19   Loss :  2.04383252305
Iteration :  20   Loss :  1.83823910942
Iteration :  21   Loss :  1.6533477129
Iteration :  22   Loss :  1.48707499108
Iteration :  23   Loss :  1.3375451466
Iteration :  24   Loss :  1.20307010644
Iteration :  25   Loss :  1.08213200479
Iteration :  26   Loss :  0.973367288098
Iteration :  27   Loss :  0.875552137851
Iteration :  28   Loss :  0.787588024081
Iteration :  29   Loss :  0.70848691641
Iteration :  30   Loss :  0.637357658784
Iteration :  31   Loss :  0.573395270915
Iteration :  32   Loss :  0.51587353493
Iteration :  33   Loss :  0.464139585663
Iteration :  34   Loss :  0.417608639847
Iteration :  35   Loss :  0.375757836936
Iteration :  36   Loss :  0.338119331367
Iteration :  37   Loss :  0.304273312776
Iteration :  38   Loss :  0.27384157564
Iteration :  39   Loss :  0.246482136769
Iteration :  40   Loss :  0.221885349452
Iteration :  41   Loss :  0.199771480143
Iteration :  42   Loss :  0.179888896758
Iteration :  43   Loss :  0.162012113219
Iteration :  44   Loss :  0.145939857789
Iteration :  45   Loss :  0.131493412094
Iteration :  46   Loss :  0.118514745093
Iteration :  47   Loss :  0.106863721879
Iteration :  48   Loss :  0.096414093509
Iteration :  49   Loss :  0.0870488915238
Iteration :  50   Loss :  0.0786570674718
Iteration :  51   Loss :  0.071133027342
Iteration :  52   Loss :  0.0643784847414
Iteration :  53   Loss :  0.0583053331582
Iteration :  54   Loss :  0.0528386736357
Iteration :  55   Loss :  0.0479176960276
Iteration :  56   Loss :  0.0434928400008
Iteration :  57   Loss :  0.0395211065288
Iteration :  58   Loss :  0.0359623439271
Iteration :  59   Loss :  0.032777808164
Iteration :  60   Loss :  0.0299311751431
Iteration :  61   Loss :  0.0273910897019
Iteration :  62   Loss :  0.0251326774585
Iteration :  63   Loss :  0.0231356283717
Iteration :  64   Loss :  0.0213796069049
Iteration :  65   Loss :  0.0198409837304
Iteration :  66   Loss :  0.01849461611
Iteration :  67   Loss :  0.0173194631888
Iteration :  68   Loss :  0.0163024078786
Iteration :  69   Loss :  0.0154363988503
Iteration :  70   Loss :  0.0147139018594
Iteration :  71   Loss :  0.0141202321787
Iteration :  72   Loss :  0.0136316147317
Iteration :  73   Loss :  0.0132183511974
Iteration :  74   Loss :  0.0128495983732
Iteration :  75   Loss :  0.0124990392798
Iteration :  76   Loss :  0.0121511142268
Iteration :  77   Loss :  0.0118029078631
Iteration :  78   Loss :  0.011459951601
Iteration :  79   Loss :  0.0111302558677
Iteration :  80   Loss :  0.0108204790954
Iteration :  81   Loss :  0.0105349342384
Iteration :  82   Loss :  0.010276383621
Iteration :  83   Loss :  0.0100476633458
Iteration :  84   Loss :  0.00985381052666
Iteration :  85   Loss :  0.0097042401878
Iteration :  86   Loss :  0.00961226497262
[ -3.81614256e-04  -8.20542579e-05  -4.18191451e-05 ...,   2.67304198e-04
   1.44103668e-04   3.59412616e-05]
CROSS VALIDATION 10
Iteration :  0   Loss :  71.3772006023
Iteration :  1   Loss :  11.4752537782
Iteration :  2   Loss :  10.3205787209
Iteration :  3   Loss :  9.28209059193
Iteration :  4   Loss :  8.34809830791
Iteration :  5   Loss :  7.50808717803
Iteration :  6   Loss :  6.75260053173
Iteration :  7   Loss :  6.0731332575
Iteration :  8   Loss :  5.46203605412
Iteration :  9   Loss :  4.91242931639
Iteration :  10   Loss :  4.41812568599
Iteration :  11   Loss :  3.97356039548
Iteration :  12   Loss :  3.57372862132
Iteration :  13   Loss :  3.21412914056
Iteration :  14   Loss :  2.89071365712
Iteration :  15   Loss :  2.59984122691
Iteration :  16   Loss :  2.33823726908
Iteration :  17   Loss :  2.10295670187
Iteration :  18   Loss :  1.89135078867
Iteration :  19   Loss :  1.70103732151
Iteration :  20   Loss :  1.52987380792
Iteration :  21   Loss :  1.37593336159
Iteration :  22   Loss :  1.2374830297
Iteration :  23   Loss :  1.11296431927
Iteration :  24   Loss :  1.00097571242
Iteration :  25   Loss :  0.900256984734
Iteration :  26   Loss :  0.809675158964
Iteration :  27   Loss :  0.728211930737
Iteration :  28   Loss :  0.654952378202
Iteration :  29   Loss :  0.589074695769
Iteration :  30   Loss :  0.52984058418
Iteration :  31   Loss :  0.47658590986
Iteration :  32   Loss :  0.428711587202
Iteration :  33   Loss :  0.385675399283
Iteration :  34   Loss :  0.346985930008
Iteration :  35   Loss :  0.312199219478
Iteration :  36   Loss :  0.280917676474
Iteration :  37   Loss :  0.252789322531
Iteration :  38   Loss :  0.227504554132
Iteration :  39   Loss :  0.204789716007
Iteration :  40   Loss :  0.184399439798
Iteration :  41   Loss :  0.166109904482
Iteration :  42   Loss :  0.149714464474
Iteration :  43   Loss :  0.135021446548
Iteration :  44   Loss :  0.121851937084
Iteration :  45   Loss :  0.110037432513
Iteration :  46   Loss :  0.0994209564057
Iteration :  47   Loss :  0.0898632066868
Iteration :  48   Loss :  0.081248439316
Iteration :  49   Loss :  0.0734834284302
Iteration :  50   Loss :  0.0664907394981
Iteration :  51   Loss :  0.0602024099347
Iteration :  52   Loss :  0.0545566115414
Iteration :  53   Loss :  0.0494961031775
Iteration :  54   Loss :  0.0449671855356
Iteration :  55   Loss :  0.0409191259852
Iteration :  56   Loss :  0.0373044739296
Iteration :  57   Loss :  0.0340799398486
Iteration :  58   Loss :  0.0312065619371
Iteration :  59   Loss :  0.0286484686917
Iteration :  60   Loss :  0.0263714003393
Iteration :  61   Loss :  0.0243425831984
Iteration :  62   Loss :  0.0225320629568
Iteration :  63   Loss :  0.0209145253231
Iteration :  64   Loss :  0.0194708345938
Iteration :  65   Loss :  0.018188698349
Iteration :  66   Loss :  0.0170614836907
Iteration :  67   Loss :  0.0160842413916
Iteration :  68   Loss :  0.0152474061581
Iteration :  69   Loss :  0.0145316934725
Iteration :  70   Loss :  0.0139095707637
Iteration :  71   Loss :  0.0133537969419
Iteration :  72   Loss :  0.0128459910582
Iteration :  73   Loss :  0.0123796751039
Iteration :  74   Loss :  0.0119587523909
Iteration :  75   Loss :  0.0115939228503
Iteration :  76   Loss :  0.0112976018659
Iteration :  77   Loss :  0.0110764412913
Iteration :  78   Loss :  0.0109228352688
Iteration :  79   Loss :  0.0108135197816
Iteration :  80   Loss :  0.0107220711456
[ -2.84431020e-04  -2.25341982e-05  -5.01078617e-05 ...,   1.50027617e-04
   7.04038502e-05   6.93584393e-05]
CROSS VALIDATION 11
Iteration :  0   Loss :  71.377140107
Iteration :  1   Loss :  11.4752519102
Iteration :  2   Loss :  10.3205770409
Iteration :  3   Loss :  9.28208908093
Iteration :  4   Loss :  8.34809694896
Iteration :  5   Loss :  7.50808595582
Iteration :  6   Loss :  6.7525994325
Iteration :  7   Loss :  6.07313226888
Iteration :  8   Loss :  5.46203516497
Iteration :  9   Loss :  4.91242851671
Iteration :  10   Loss :  4.41812496678
Iteration :  11   Loss :  3.97355974865
Iteration :  12   Loss :  3.57372803956
Iteration :  13   Loss :  3.21412861735
Iteration :  14   Loss :  2.89071318655
Iteration :  15   Loss :  2.5998408037
Iteration :  16   Loss :  2.33823688844
Iteration :  17   Loss :  2.10295635954
Iteration :  18   Loss :  1.89135048078
Iteration :  19   Loss :  1.70103704461
Iteration :  20   Loss :  1.52987355888
Iteration :  21   Loss :  1.37593313761
Iteration :  22   Loss :  1.23748282827
Iteration :  23   Loss :  1.11296413813
Iteration :  24   Loss :  1.00097554955
Iteration :  25   Loss :  0.90025683833
Iteration :  26   Loss :  0.809675027423
Iteration :  27   Loss :  0.728211812646
Iteration :  28   Loss :  0.654952272323
Iteration :  29   Loss :  0.589074601002
Iteration :  30   Loss :  0.529840499511
Iteration :  31   Loss :  0.476585834286
Iteration :  32   Loss :  0.42871151973
Iteration :  33   Loss :  0.385675339118
Iteration :  34   Loss :  0.346985877073
Iteration :  35   Loss :  0.312199175453
Iteration :  36   Loss :  0.280917646949
Iteration :  37   Loss :  0.252789321726
Iteration :  38   Loss :  0.227504614924
Iteration :  39   Loss :  0.204789910046
Iteration :  40   Loss :  0.184399915848
Iteration :  41   Loss :  0.166110958223
Iteration :  42   Loss :  0.149716660008
Iteration :  43   Loss :  0.135025817605
Iteration :  44   Loss :  0.121860298084
Iteration :  45   Loss :  0.110052809092
Iteration :  46   Loss :  0.0994480675652
Iteration :  47   Loss :  0.0899087495784
Iteration :  48   Loss :  0.0813206447951
Iteration :  49   Loss :  0.0735902173289
Iteration :  50   Loss :  0.0666364689768
Iteration :  51   Loss :  0.0603849305274
Iteration :  52   Loss :  0.0547672508893
Iteration :  53   Loss :  0.0497228068542
Iteration :  54   Loss :  0.0451980309162
Iteration :  55   Loss :  0.0411441411131
Iteration :  56   Loss :  0.0375161139247
Iteration :  57   Loss :  0.0342732352395
Iteration :  58   Loss :  0.0313793195201
Iteration :  59   Loss :  0.0288013458818
Iteration :  60   Loss :  0.0265075233518
Iteration :  61   Loss :  0.0244665510818
Iteration :  62   Loss :  0.0226485210392
Iteration :  63   Loss :  0.0210268326321
Iteration :  64   Loss :  0.0195804091099
Iteration :  65   Loss :  0.0182951812887
Iteration :  66   Loss :  0.0171633727947
Iteration :  67   Loss :  0.0161796933697
Iteration :  68   Loss :  0.0153351863857
Iteration :  69   Loss :  0.0146120360774
Iteration :  70   Loss :  0.0139842802194
Iteration :  71   Loss :  0.0134254321641
Iteration :  72   Loss :  0.0129169472129
Iteration :  73   Loss :  0.0124518248222
Iteration :  74   Loss :  0.012033606458
Iteration :  75   Loss :  0.0116729552114
Iteration :  76   Loss :  0.0113822604262
Iteration :  77   Loss :  0.0111674676062
Iteration :  78   Loss :  0.0110192348271
Iteration :  79   Loss :  0.010912587927
Iteration :  80   Loss :  0.0108210644463
[ -3.02048607e-04  -2.23557563e-05  -3.88113803e-05 ...,   1.54265655e-04
   7.89902278e-05   6.36750608e-05]
CROSS VALIDATION 12
Iteration :  0   Loss :  138.198179969
Iteration :  1   Loss :  11.7072242501
Iteration :  2   Loss :  10.5292076159
Iteration :  3   Loss :  9.46972661081
Iteration :  4   Loss :  8.51685381792
Iteration :  5   Loss :  7.65986199358
Iteration :  6   Loss :  6.88910330213
Iteration :  7   Loss :  6.19590070254
Iteration :  8   Loss :  5.57245026415
Iteration :  9   Loss :  5.01173331163
Iteration :  10   Loss :  4.50743741016
Iteration :  11   Loss :  4.05388530139
Iteration :  12   Loss :  3.64597099003
Iteration :  13   Loss :  3.27910226162
Iteration :  14   Loss :  2.94914898434
Iteration :  15   Loss :  2.65239661282
Iteration :  16   Loss :  2.38550437061
Iteration :  17   Loss :  2.14546764035
Iteration :  18   Loss :  1.92958413848
Iteration :  19   Loss :  1.73542349343
Iteration :  20   Loss :  1.56079988507
Iteration :  21   Loss :  1.40374743712
Iteration :  22   Loss :  1.26249808581
Iteration :  23   Loss :  1.13546167534
Iteration :  24   Loss :  1.02120805628
Iteration :  25   Loss :  0.918450985306
Iteration :  26   Loss :  0.826033645164
Iteration :  27   Loss :  0.742915621956
Iteration :  28   Loss :  0.668161193643
Iteration :  29   Loss :  0.600928798743
Iteration :  30   Loss :  0.540461568415
Iteration :  31   Loss :  0.486078818498
Iteration :  32   Loss :  0.437168411295
Iteration :  33   Loss :  0.393179910162
Iteration :  34   Loss :  0.353618463718
Iteration :  35   Loss :  0.318039369956
Iteration :  36   Loss :  0.286043281144
Iteration :  37   Loss :  0.257272010175
Iteration :  38   Loss :  0.23140486961
Iteration :  39   Loss :  0.208155384465
Iteration :  40   Loss :  0.187268045317
Iteration :  41   Loss :  0.168514591706
Iteration :  42   Loss :  0.151689495056
Iteration :  43   Loss :  0.136605309612
Iteration :  44   Loss :  0.123089698873
Iteration :  45   Loss :  0.110984790274
Iteration :  46   Loss :  0.100146362483
Iteration :  47   Loss :  0.0904409218365
Iteration :  48   Loss :  0.0817435982479
Iteration :  49   Loss :  0.0739407616099
Iteration :  50   Loss :  0.0669352823556
Iteration :  51   Loss :  0.0606477132034
Iteration :  52   Loss :  0.0550107838362
Iteration :  53   Loss :  0.0499619928165
Iteration :  54   Loss :  0.0454402320498
Iteration :  55   Loss :  0.0413875577726
Iteration :  56   Loss :  0.0377529119487
Iteration :  57   Loss :  0.0344943493959
Iteration :  58   Loss :  0.0315780053568
Iteration :  59   Loss :  0.0289739464688
Iteration :  60   Loss :  0.0266517051623
Iteration :  61   Loss :  0.0245789598365
Iteration :  62   Loss :  0.022724319363
Iteration :  63   Loss :  0.0210616695519
Iteration :  64   Loss :  0.0195722283967
Iteration :  65   Loss :  0.0182436716077
Iteration :  66   Loss :  0.0170686793493
Iteration :  67   Loss :  0.0160442109065
Iteration :  68   Loss :  0.015170881232
Iteration :  69   Loss :  0.0144507140736
Iteration :  70   Loss :  0.0138807650286
Iteration :  71   Loss :  0.01344255204
Iteration :  72   Loss :  0.01309657383
Iteration :  73   Loss :  0.0127945490957
Iteration :  74   Loss :  0.0125010880245
Iteration :  75   Loss :  0.012202938266
Iteration :  76   Loss :  0.0119037192936
Iteration :  77   Loss :  0.0116146441636
Iteration :  78   Loss :  0.0113472138258
Iteration :  79   Loss :  0.011109261248
Iteration :  80   Loss :  0.0109035259598
Iteration :  81   Loss :  0.01072792273
Iteration :  82   Loss :  0.0105775191618
Iteration :  83   Loss :  0.0104481812259
Iteration :  84   Loss :  0.010340714883
Iteration :  85   Loss :  0.0102629719
[ -5.46796877e-04  -1.90415627e-04  -1.17558165e-04 ...,   1.54736128e-04
   9.54794561e-05   1.59868662e-05]
CROSS VALIDATION 13
Iteration :  0   Loss :  216.307521248
Iteration :  1   Loss :  60.4806631339
Iteration :  2   Loss :  11.8236307095
Iteration :  3   Loss :  10.6339009021
Iteration :  4   Loss :  9.56388533901
Iteration :  5   Loss :  8.60153800752
Iteration :  6   Loss :  7.73602500158
Iteration :  7   Loss :  6.95760255581
Iteration :  8   Loss :  6.25750735226
Iteration :  9   Loss :  5.62785786476
Iteration :  10   Loss :  5.06156563037
Iteration :  11   Loss :  4.55225544894
Iteration :  12   Loss :  4.09419361245
Iteration :  13   Loss :  3.68222335622
Iteration :  14   Loss :  3.31170680528
Iteration :  15   Loss :  2.97847276262
Iteration :  16   Loss :  2.67876975173
Iteration :  17   Loss :  2.40922378547
Iteration :  18   Loss :  2.16680038683
Iteration :  19   Loss :  1.94877043579
Iteration :  20   Loss :  1.75267946073
Iteration :  21   Loss :  1.57632003235
Iteration :  22   Loss :  1.4177069544
Iteration :  23   Loss :  1.27505497513
Iteration :  24   Loss :  1.14675876492
Iteration :  25   Loss :  1.03137490915
Iteration :  26   Loss :  0.927605642641
Iteration :  27   Loss :  0.834284010303
Iteration :  28   Loss :  0.750360163407
Iteration :  29   Loss :  0.674888790054
Iteration :  30   Loss :  0.607018260371
Iteration :  31   Loss :  0.545982131353
Iteration :  32   Loss :  0.491092239805
Iteration :  33   Loss :  0.441731322293
Iteration :  34   Loss :  0.397344450295
Iteration :  35   Loss :  0.357430803591
Iteration :  36   Loss :  0.32153771259
Iteration :  37   Loss :  0.289257597809
Iteration :  38   Loss :  0.260226238672
Iteration :  39   Loss :  0.234119972559
Iteration :  40   Loss :  0.210651408889
Iteration :  41   Loss :  0.189564937085
Iteration :  42   Loss :  0.170632611116
Iteration :  43   Loss :  0.153649973296
Iteration :  44   Loss :  0.138431675743
Iteration :  45   Loss :  0.124807577978
Iteration :  46   Loss :  0.112619721118
Iteration :  47   Loss :  0.101719989573
Iteration :  48   Loss :  0.0919696055157
Iteration :  49   Loss :  0.0832415277672
Iteration :  50   Loss :  0.0754227941955
Iteration :  51   Loss :  0.0684135127133
Iteration :  52   Loss :  0.0621244758267
Iteration :  53   Loss :  0.0564769269056
Iteration :  54   Loss :  0.0514037549751
Iteration :  55   Loss :  0.0468484746729
Iteration :  56   Loss :  0.0427611180026
Iteration :  57   Loss :  0.039094150909
Iteration :  58   Loss :  0.0358015198008
Iteration :  59   Loss :  0.0328411516408
Iteration :  60   Loss :  0.0301777930001
Iteration :  61   Loss :  0.0277827252158
Iteration :  62   Loss :  0.0256311751252
Iteration :  63   Loss :  0.0237005337442
Iteration :  64   Loss :  0.0219703217804
Iteration :  65   Loss :  0.0204227168428
Iteration :  66   Loss :  0.019042524123
Iteration :  67   Loss :  0.0178163820608
Iteration :  68   Loss :  0.016731420884
Iteration :  69   Loss :  0.0157737083
Iteration :  70   Loss :  0.0149272798154
Iteration :  71   Loss :  0.0141749644072
Iteration :  72   Loss :  0.0135013539938
Iteration :  73   Loss :  0.0128963775914
Iteration :  74   Loss :  0.0123573503299
Iteration :  75   Loss :  0.0118887231134
Iteration :  76   Loss :  0.0114996134188
Iteration :  77   Loss :  0.0111983525503
Iteration :  78   Loss :  0.0109834502239
Iteration :  79   Loss :  0.0108356751865
Iteration :  80   Loss :  0.0107222296524
Iteration :  81   Loss :  0.0106141336856
Iteration :  82   Loss :  0.010498668991
Iteration :  83   Loss :  0.0103771904067
Iteration :  84   Loss :  0.0102588799892
Iteration :  85   Loss :  0.010157920025
Iteration :  86   Loss :  0.0100911000992
[ -4.35084665e-04  -2.21655138e-04  -9.70226232e-05 ...,   1.94432039e-04
   7.49719431e-05   3.28279006e-05]
CROSS VALIDATION 14
Iteration :  0   Loss :  71.3085361568
Iteration :  1   Loss :  11.4746500167
Iteration :  2   Loss :  10.3200357118
Iteration :  3   Loss :  9.28160222203
Iteration :  4   Loss :  8.34765907928
Iteration :  5   Loss :  7.50769214593
Iteration :  6   Loss :  6.75224524896
Iteration :  7   Loss :  6.07281372437
Iteration :  8   Loss :  5.46174867338
Iteration :  9   Loss :  4.91217085278
Iteration :  10   Loss :  4.41789322978
Iteration :  11   Loss :  3.97335132973
Iteration :  12   Loss :  3.57354059239
Iteration :  13   Loss :  3.21396003168
Iteration :  14   Loss :  2.89056156449
Iteration :  15   Loss :  2.59970443829
Iteration :  16   Loss :  2.33811424446
Iteration :  17   Loss :  2.10284605609
Iteration :  18   Loss :  1.89125127549
Iteration :  19   Loss :  1.70094781894
Iteration :  20   Loss :  1.52979330383
Iteration :  21   Loss :  1.37586093846
Iteration :  22   Loss :  1.23741784596
Iteration :  23   Loss :  1.11290558295
Iteration :  24   Loss :  1.00092263974
Iteration :  25   Loss :  0.900208731897
Iteration :  26   Loss :  0.809630708959
Iteration :  27   Loss :  0.728169909881
Iteration :  28   Loss :  0.654910772118
Iteration :  29   Loss :  0.589030444678
Iteration :  30   Loss :  0.52978909577
Iteration :  31   Loss :  0.476520692346
Iteration :  32   Loss :  0.428624485417
Iteration :  33   Loss :  0.385558055143
Iteration :  34   Loss :  0.346832496149
Iteration :  35   Loss :  0.312008946839
Iteration :  36   Loss :  0.280695274876
Iteration :  37   Loss :  0.252542621299
Iteration :  38   Loss :  0.227241473084
Iteration :  39   Loss :  0.204516657996
Iteration :  40   Loss :  0.184121293961
Iteration :  41   Loss :  0.165830613741
Iteration :  42   Loss :  0.14943733625
Iteration :  43   Loss :  0.134749271527
Iteration :  44   Loss :  0.121587337208
Iteration :  45   Loss :  0.109783375652
Iteration :  46   Loss :  0.0991812839828
Iteration :  47   Loss :  0.0896433252069
Iteration :  48   Loss :  0.0810556658964
Iteration :  49   Loss :  0.0733256708471
Iteration :  50   Loss :  0.0663732804588
Iteration :  51   Loss :  0.0601249227824
Iteration :  52   Loss :  0.0545132281531
Iteration :  53   Loss :  0.0494786007405
Iteration :  54   Loss :  0.0449683634741
Iteration :  55   Loss :  0.04093420417
Iteration :  56   Loss :  0.0373307035769
Iteration :  57   Loss :  0.0341154789351
Iteration :  58   Loss :  0.0312494815859
Iteration :  59   Loss :  0.0286964354695
Iteration :  60   Loss :  0.0264220834297
Iteration :  61   Loss :  0.0243942211549
Iteration :  62   Loss :  0.0225834954818
Iteration :  63   Loss :  0.0209647277707
Iteration :  64   Loss :  0.019518592734
Iteration :  65   Loss :  0.0182326349052
Iteration :  66   Loss :  0.0171001726343
Iteration :  67   Loss :  0.0161164166818
Iteration :  68   Loss :  0.0152725131213
Iteration :  69   Loss :  0.014550562996
Iteration :  70   Loss :  0.0139244753949
Iteration :  71   Loss :  0.0133676479048
Iteration :  72   Loss :  0.0128613279642
Iteration :  73   Loss :  0.0123980831292
Iteration :  74   Loss :  0.0119808608853
Iteration :  75   Loss :  0.0116197438222
Iteration :  76   Loss :  0.0113267186905
Iteration :  77   Loss :  0.0111077110999
Iteration :  78   Loss :  0.0109539052901
Iteration :  79   Loss :  0.0108411494833
Iteration :  80   Loss :  0.0107435652858
[ -2.68286170e-04  -1.21827554e-05  -3.45420398e-05 ...,   1.33225179e-04
   6.01254530e-05   6.10268412e-05]
CROSS VALIDATION 15
Iteration :  0   Loss :  71.3085361568
Iteration :  1   Loss :  11.4746500167
Iteration :  2   Loss :  10.3200357118
Iteration :  3   Loss :  9.28160222203
Iteration :  4   Loss :  8.34765907928
Iteration :  5   Loss :  7.50769214593
Iteration :  6   Loss :  6.75224524896
Iteration :  7   Loss :  6.07281372437
Iteration :  8   Loss :  5.46174867338
Iteration :  9   Loss :  4.91217085278
Iteration :  10   Loss :  4.41789322978
Iteration :  11   Loss :  3.97335132973
Iteration :  12   Loss :  3.57354059239
Iteration :  13   Loss :  3.21396003169
Iteration :  14   Loss :  2.89056156451
Iteration :  15   Loss :  2.59970443836
Iteration :  16   Loss :  2.33811424471
Iteration :  17   Loss :  2.10284605687
Iteration :  18   Loss :  1.8912512778
Iteration :  19   Loss :  1.70094782538
Iteration :  20   Loss :  1.52979332091
Iteration :  21   Loss :  1.37586098151
Iteration :  22   Loss :  1.23741794942
Iteration :  23   Loss :  1.11290582049
Iteration :  24   Loss :  1.00092316214
Iteration :  25   Loss :  0.900209834342
Iteration :  26   Loss :  0.80963294525
Iteration :  27   Loss :  0.728174275056
Iteration :  28   Loss :  0.6549189743
Iteration :  29   Loss :  0.58904526829
Iteration :  30   Loss :  0.529814798562
Iteration :  31   Loss :  0.476563245454
Iteration :  32   Loss :  0.428691262571
Iteration :  33   Loss :  0.385656470023
Iteration :  34   Loss :  0.346967506852
Iteration :  35   Loss :  0.312180535622
Iteration :  36   Loss :  0.28089787103
Iteration :  37   Loss :  0.252767117315
Iteration :  38   Loss :  0.22747802783
Iteration :  39   Loss :  0.204756247023
Iteration :  40   Loss :  0.184355812861
Iteration :  41   Loss :  0.166052518683
Iteration :  42   Loss :  0.149639685029
Iteration :  43   Loss :  0.134926524339
Iteration :  44   Loss :  0.121736785359
Iteration :  45   Loss :  0.109906443564
Iteration :  46   Loss :  0.0992835095017
Iteration :  47   Loss :  0.0897324489755
Iteration :  48   Loss :  0.0811389319256
Iteration :  49   Loss :  0.0734081619945
Iteration :  50   Loss :  0.0664578173339
Iteration :  51   Loss :  0.060212765365
Iteration :  52   Loss :  0.0546047385057
Iteration :  53   Loss :  0.0495734447533
Iteration :  54   Loss :  0.0450654101793
Iteration :  55   Loss :  0.0410316520416
Iteration :  56   Loss :  0.0374266249466
Iteration :  57   Loss :  0.0342084351129
Iteration :  58   Loss :  0.0313389170606
Iteration :  59   Loss :  0.0287828366347
Iteration :  60   Loss :  0.0265068036568
Iteration :  61   Loss :  0.0244789565691
Iteration :  62   Loss :  0.0226697713832
Iteration :  63   Loss :  0.0210536633825
Iteration :  64   Loss :  0.0196108660992
Iteration :  65   Loss :  0.0183286061236
Iteration :  66   Loss :  0.0172001458011
Iteration :  67   Loss :  0.0162207680893
Iteration :  68   Loss :  0.0153814092666
Iteration :  69   Loss :  0.0146634473234
Iteration :  70   Loss :  0.014039874067
Iteration :  71   Loss :  0.0134835439488
Iteration :  72   Loss :  0.0129757723166
Iteration :  73   Loss :  0.0125096116922
Iteration :  74   Loss :  0.0120885297186
Iteration :  75   Loss :  0.0117229349925
Iteration :  76   Loss :  0.011425181725
Iteration :  77   Loss :  0.0112022301044
Iteration :  78   Loss :  0.0110471830225
Iteration :  79   Loss :  0.0109374350027
Iteration :  80   Loss :  0.0108464986137
[ -3.01033102e-04  -1.00126844e-05  -4.95965852e-05 ...,   1.54308158e-04
   7.71694269e-05   6.33294596e-05]
CROSS VALIDATION 16
Iteration :  0   Loss :  130.613425704
Iteration :  1   Loss :  10.8347160078
Iteration :  2   Loss :  9.74449704983
Iteration :  3   Loss :  8.76398176433
Iteration :  4   Loss :  7.8821329404
Iteration :  5   Loss :  7.08902443814
Iteration :  6   Loss :  6.37572908308
Iteration :  7   Loss :  5.73421733244
Iteration :  8   Loss :  5.15726570632
Iteration :  9   Loss :  4.63837483244
Iteration :  10   Loss :  4.17169742396
Iteration :  11   Loss :  3.75197514474
Iteration :  12   Loss :  3.37448128803
Iteration :  13   Loss :  3.03496709657
Iteration :  14   Loss :  2.72961231313
Iteration :  15   Loss :  2.4549810394
Iteration :  16   Loss :  2.20798271885
Iteration :  17   Loss :  1.98583741311
Iteration :  18   Loss :  1.78604466262
Iteration :  19   Loss :  1.60635546412
Iteration :  20   Loss :  1.44474703319
Iteration :  21   Loss :  1.29940007944
Iteration :  22   Loss :  1.16867835178
Iteration :  23   Loss :  1.05111023299
Iteration :  24   Loss :  0.945372182726
Iteration :  25   Loss :  0.850273846778
Iteration :  26   Loss :  0.764744669592
Iteration :  27   Loss :  0.687821864515
Iteration :  28   Loss :  0.618639614082
Iteration :  29   Loss :  0.556419389493
Iteration :  30   Loss :  0.500461294109
Iteration :  31   Loss :  0.450136347311
Iteration :  32   Loss :  0.404879624965
Iteration :  33   Loss :  0.364184144955
Iteration :  34   Loss :  0.327595305555
Iteration :  35   Loss :  0.294705542815
Iteration :  36   Loss :  0.265148774258
Iteration :  37   Loss :  0.23859449238
Iteration :  38   Loss :  0.214742434625
Iteration :  39   Loss :  0.193319780022
Iteration :  40   Loss :  0.174081287592
Iteration :  41   Loss :  0.156808738023
Iteration :  42   Loss :  0.141305882382
Iteration :  43   Loss :  0.127391639404
Iteration :  44   Loss :  0.11489790596
Iteration :  45   Loss :  0.103672736469
Iteration :  46   Loss :  0.0935838044339
Iteration :  47   Loss :  0.0845186638944
Iteration :  48   Loss :  0.0763817545169
Iteration :  49   Loss :  0.0690892483369
Iteration :  50   Loss :  0.0625638225013
Iteration :  51   Loss :  0.0567314677639
Iteration :  52   Loss :  0.0515208644594
Iteration :  53   Loss :  0.046864202747
Iteration :  54   Loss :  0.0426982535744
Iteration :  55   Loss :  0.0389663351968
Iteration :  56   Loss :  0.0356211141722
Iteration :  57   Loss :  0.0326249539037
Iteration :  58   Loss :  0.0299464896419
Iteration :  59   Loss :  0.0275563903476
Iteration :  60   Loss :  0.025425017211
Iteration :  61   Loss :  0.023522864236
Iteration :  62   Loss :  0.0218232409184
Iteration :  63   Loss :  0.0203048716784
Iteration :  64   Loss :  0.0189525549842
Iteration :  65   Loss :  0.0177563534177
Iteration :  66   Loss :  0.0167097926364
Iteration :  67   Loss :  0.0158061757606
Iteration :  68   Loss :  0.015033293116
Iteration :  69   Loss :  0.0143703139628
Iteration :  70   Loss :  0.0137911521716
Iteration :  71   Loss :  0.0132724875464
Iteration :  72   Loss :  0.0127998175458
Iteration :  73   Loss :  0.0123688538318
Iteration :  74   Loss :  0.0119841246438
Iteration :  75   Loss :  0.0116560266443
Iteration :  76   Loss :  0.0113957050448
Iteration :  77   Loss :  0.0112069860833
Iteration :  78   Loss :  0.0110785168952
Iteration :  79   Loss :  0.0109853151905
[ -3.50345296e-04  -2.75586893e-05  -1.40676670e-04 ...,   2.37674408e-04
   3.12955575e-05   6.27710281e-05]
CROSS VALIDATION 17
Iteration :  0   Loss :  71.3185291118
Iteration :  1   Loss :  11.475053766
Iteration :  2   Loss :  10.3203988345
Iteration :  3   Loss :  9.28192880628
Iteration :  4   Loss :  8.34795280163
Iteration :  5   Loss :  7.50795631304
Iteration :  6   Loss :  6.75248283477
Iteration :  7   Loss :  6.07302740357
Iteration :  8   Loss :  5.46194085153
Iteration :  9   Loss :  4.91234369337
Iteration :  10   Loss :  4.41804867863
Iteration :  11   Loss :  3.97349113684
Iteration :  12   Loss :  3.57366633169
Iteration :  13   Loss :  3.21407311871
Iteration :  14   Loss :  2.89066327237
Iteration :  15   Loss :  2.59979591206
Iteration :  16   Loss :  2.33819651403
Iteration :  17   Loss :  2.10292004796
Iteration :  18   Loss :  1.89131782359
Iteration :  19   Loss :  1.70100767491
Iteration :  20   Loss :  1.52984714768
Iteration :  21   Loss :  1.37590939097
Iteration :  22   Loss :  1.23746148549
Iteration :  23   Loss :  1.11294497133
Iteration :  24   Loss :  1.000958365
Iteration :  25   Loss :  0.90024147963
Iteration :  26   Loss :  0.809661379987
Iteration :  27   Loss :  0.72819980627
Iteration :  28   Loss :  0.654941873837
Iteration :  29   Loss :  0.5890657818
Iteration :  30   Loss :  0.529833161942
Iteration :  31   Loss :  0.476579708766
Iteration :  32   Loss :  0.428706112966
Iteration :  33   Loss :  0.385670043454
Iteration :  34   Loss :  0.346980201708
Iteration :  35   Loss :  0.312192867885
Iteration :  36   Loss :  0.280910593425
Iteration :  37   Loss :  0.25278138982
Iteration :  38   Loss :  0.227495619292
Iteration :  39   Loss :  0.204779738663
Iteration :  40   Loss :  0.184388731834
Iteration :  41   Loss :  0.166099263491
Iteration :  42   Loss :  0.149705044956
Iteration :  43   Loss :  0.135014643495
Iteration :  44   Loss :  0.121849764008
Iteration :  45   Loss :  0.110043286577
Iteration :  46   Loss :  0.0994401874053
Iteration :  47   Loss :  0.0899031526684
Iteration :  48   Loss :  0.0813177673915
Iteration :  49   Loss :  0.0735903181445
Iteration :  50   Loss :  0.0666398347892
Iteration :  51   Loss :  0.0603922548847
Iteration :  52   Loss :  0.0547800925442
Iteration :  53   Loss :  0.0497437872225
Iteration :  54   Loss :  0.045230586153
Iteration :  55   Loss :  0.041191934126
Iteration :  56   Loss :  0.0375821964588
Iteration :  57   Loss :  0.0343590618173
Iteration :  58   Loss :  0.0314839436663
Iteration :  59   Loss :  0.028921262128
Iteration :  60   Loss :  0.0266373733042
Iteration :  61   Loss :  0.0246004409108
Iteration :  62   Loss :  0.0227814197964
Iteration :  63   Loss :  0.0211555632277
Iteration :  64   Loss :  0.0197039431051
Iteration :  65   Loss :  0.018414259293
Iteration :  66   Loss :  0.0172797596607
Iteration :  67   Loss :  0.0162953803525
Iteration :  68   Loss :  0.0154516822944
Iteration :  69   Loss :  0.0147299014721
Iteration :  70   Loss :  0.014103162929
Iteration :  71   Loss :  0.0135445522332
Iteration :  72   Loss :  0.0130355381613
Iteration :  73   Loss :  0.0125692067599
Iteration :  74   Loss :  0.0121489766397
Iteration :  75   Loss :  0.0117851690431
Iteration :  76   Loss :  0.0114899489618
Iteration :  77   Loss :  0.0112697377669
Iteration :  78   Loss :  0.0111166112256
Iteration :  79   Loss :  0.0110070123657
Iteration :  80   Loss :  0.0109145103001
[ -2.96971655e-04  -2.39388364e-05  -4.30213473e-05 ...,   1.57367191e-04
   7.43565289e-05   6.40975859e-05]
CROSS VALIDATION 18
Iteration :  0   Loss :  139.490108248
Iteration :  1   Loss :  137.010076246
Iteration :  2   Loss :  10.448622196
Iteration :  3   Loss :  9.39724997841
Iteration :  4   Loss :  8.45167008975
Iteration :  5   Loss :  7.60123743654
Iteration :  6   Loss :  6.83637811762
Iteration :  7   Loss :  6.14848167633
Iteration :  8   Loss :  5.52980421561
Iteration :  9   Loss :  4.97338128604
Iteration :  10   Loss :  4.4729495538
Iteration :  11   Loss :  4.02287632286
Iteration :  12   Loss :  3.61809601538
Iteration :  13   Loss :  3.25405273079
Iteration :  14   Loss :  2.92664813663
Iteration :  15   Loss :  2.63219442109
Iteration :  16   Loss :  2.36737274637
Iteration :  17   Loss :  2.12919748069
Iteration :  18   Loss :  1.91498474973
Iteration :  19   Loss :  1.72232292069
Iteration :  20   Loss :  1.54904449292
Iteration :  21   Loss :  1.39320054089
Iteration :  22   Loss :  1.25303827751
Iteration :  23   Loss :  1.12698134114
Iteration :  24   Loss :  1.0136122161
Iteration :  25   Loss :  0.911656329543
Iteration :  26   Loss :  0.819967446783
Iteration :  27   Loss :  0.737514062313
Iteration :  28   Loss :  0.663366815938
Iteration :  29   Loss :  0.596687643932
Iteration :  30   Loss :  0.53672161862
Iteration :  31   Loss :  0.482791022842
Iteration :  32   Loss :  0.434289394603
Iteration :  33   Loss :  0.390674124547
Iteration :  34   Loss :  0.351458526167
Iteration :  35   Loss :  0.31620469398
Iteration :  36   Loss :  0.284517505221
Iteration :  37   Loss :  0.256039617232
Iteration :  38   Loss :  0.230447347471
Iteration :  39   Loss :  0.207447809251
Iteration :  40   Loss :  0.186777989785
Iteration :  41   Loss :  0.168204888593
Iteration :  42   Loss :  0.151523687516
Iteration :  43   Loss :  0.136552373433
Iteration :  44   Loss :  0.123125123455
Iteration :  45   Loss :  0.111087933942
Iteration :  46   Loss :  0.10029735692
Iteration :  47   Loss :  0.0906204411024
Iteration :  48   Loss :  0.081935316525
Iteration :  49   Loss :  0.0741336972415
Iteration :  50   Loss :  0.0671235290466
Iteration :  51   Loss :  0.0608280978896
Iteration :  52   Loss :  0.0551817452752
Iteration :  53   Loss :  0.0501253379802
Iteration :  54   Loss :  0.045603370752
Iteration :  55   Loss :  0.0415626328626
Iteration :  56   Loss :  0.0379518221751
Iteration :  57   Loss :  0.0347219967666
Iteration :  58   Loss :  0.0318281543506
Iteration :  59   Loss :  0.0292310641052
Iteration :  60   Loss :  0.0268973721437
Iteration :  61   Loss :  0.0247980602097
Iteration :  62   Loss :  0.0229079926494
Iteration :  63   Loss :  0.0212074834469
Iteration :  64   Loss :  0.0196832469906
Iteration :  65   Loss :  0.0183272237476
Iteration :  66   Loss :  0.0171346247103
Iteration :  67   Loss :  0.0161017961152
Iteration :  68   Loss :  0.015222635714
Iteration :  69   Loss :  0.0144829671607
Iteration :  70   Loss :  0.0138563045203
Iteration :  71   Loss :  0.0133078639967
Iteration :  72   Loss :  0.0128077635135
Iteration :  73   Loss :  0.0123424565489
Iteration :  74   Loss :  0.0119150959659
Iteration :  75   Loss :  0.0115386000474
Iteration :  76   Loss :  0.0112281247316
Iteration :  77   Loss :  0.0109934514051
Iteration :  78   Loss :  0.010830483145
Iteration :  79   Loss :  0.0107178241603
Iteration :  80   Loss :  0.0106269100204
[ -3.22792403e-04  -1.76432445e-04  -1.20542063e-04 ...,   2.27937434e-04
   9.74851703e-05   5.44641242e-05]
CROSS VALIDATION 19
Iteration :  0   Loss :  128.3285755
Iteration :  1   Loss :  33.5202526268
Iteration :  2   Loss :  12.2767966669
Iteration :  3   Loss :  11.0414679178
Iteration :  4   Loss :  9.93044171773
Iteration :  5   Loss :  8.93121036469
Iteration :  6   Loss :  8.03252472002
Iteration :  7   Loss :  7.22426756767
Iteration :  8   Loss :  6.49733971677
Iteration :  9   Loss :  5.84355756479
Iteration :  10   Loss :  5.25556096826
Iteration :  11   Loss :  4.72673038383
Iteration :  12   Loss :  4.25111234677
Iteration :  13   Loss :  3.82335244818
Iteration :  14   Loss :  3.43863505611
Iteration :  15   Loss :  3.09262910221
Iteration :  16   Loss :  2.78143932353
Iteration :  17   Loss :  2.50156241052
Iteration :  18   Loss :  2.24984756786
Iteration :  19   Loss :  2.02346104411
Iteration :  20   Loss :  1.81985423156
Iteration :  21   Loss :  1.63673497805
Iteration :  22   Loss :  1.47204178942
Iteration :  23   Loss :  1.32392063528
Iteration :  24   Loss :  1.19070410148
Iteration :  25   Loss :  1.07089266146
Iteration :  26   Loss :  0.963137864337
Iteration :  27   Loss :  0.866227259537
Iteration :  28   Loss :  0.779070891059
Iteration :  29   Loss :  0.700689190094
Iteration :  30   Loss :  0.630202053696
Iteration :  31   Loss :  0.566818801149
Iteration :  32   Loss :  0.509828576637
Iteration :  33   Loss :  0.458590813002
Iteration :  34   Loss :  0.41252600473
Iteration :  35   Loss :  0.371108318255
Iteration :  36   Loss :  0.33386194084
Iteration :  37   Loss :  0.30036047831
Iteration :  38   Loss :  0.270225511733
Iteration :  39   Loss :  0.243121788778
Iteration :  40   Loss :  0.218750593517
Iteration :  41   Loss :  0.196843605317
Iteration :  42   Loss :  0.177157690866
Iteration :  43   Loss :  0.159470597979
Iteration :  44   Loss :  0.143578642951
Iteration :  45   Loss :  0.129297244508
Iteration :  46   Loss :  0.116462668917
Iteration :  47   Loss :  0.104932334915
Iteration :  48   Loss :  0.0945824876375
Iteration :  49   Loss :  0.0853032933341
Iteration :  50   Loss :  0.0769930157206
Iteration :  51   Loss :  0.0695546768743
Iteration :  52   Loss :  0.0628973110239
Iteration :  53   Loss :  0.0569390295299
Iteration :  54   Loss :  0.0516071551963
Iteration :  55   Loss :  0.0468356359702
Iteration :  56   Loss :  0.0425638100064
Iteration :  57   Loss :  0.0387380710058
Iteration :  58   Loss :  0.0353137285621
Iteration :  59   Loss :  0.0322539658132
Iteration :  60   Loss :  0.0295262459481
Iteration :  61   Loss :  0.0270990015722
Iteration :  62   Loss :  0.0249407737481
Iteration :  63   Loss :  0.0230220296602
Iteration :  64   Loss :  0.0213182929416
Iteration :  65   Loss :  0.0198122958587
Iteration :  66   Loss :  0.0184928591277
Iteration :  67   Loss :  0.0173498866391
Iteration :  68   Loss :  0.016368022452
Iteration :  69   Loss :  0.0155236201064
Iteration :  70   Loss :  0.0147882098022
Iteration :  71   Loss :  0.0141363097118
Iteration :  72   Loss :  0.0135515971935
Iteration :  73   Loss :  0.0130286751934
Iteration :  74   Loss :  0.0125717272722
Iteration :  75   Loss :  0.0121901141571
Iteration :  76   Loss :  0.0118895102381
Iteration :  77   Loss :  0.0116611359747
Iteration :  78   Loss :  0.0114794292421
Iteration :  79   Loss :  0.0113154452017
Iteration :  80   Loss :  0.0111524243849
Iteration :  81   Loss :  0.0109869365659
Iteration :  82   Loss :  0.0108222235132
Iteration :  83   Loss :  0.0106650246925
Iteration :  84   Loss :  0.0105258683617
Iteration :  85   Loss :  0.0104190167806
Iteration :  86   Loss :  0.0103577255716
[ -3.23574653e-04  -1.46006715e-04  -7.38034165e-05 ...,   2.13475851e-04
   7.38710258e-05   2.89507546e-05]
Accuracy (Logistic Loss):	0.85
Hinge Loss
CROSS VALIDATION 0
Iteration :  0   Loss :  105.517191609
Iteration :  1   Loss :  12.5993310783
Iteration :  2   Loss :  11.3315479324
Iteration :  3   Loss :  10.1913329959
Iteration :  4   Loss :  9.16584996626
Iteration :  5   Loss :  8.24355416876
Iteration :  6   Loss :  7.41406258923
Iteration :  7   Loss :  6.66803698401
Iteration :  8   Loss :  5.99707875204
Iteration :  9   Loss :  5.39363438511
Iteration :  10   Loss :  4.85091043208
Iteration :  11   Loss :  4.36279701957
Iteration :  12   Loss :  3.92379906834
Iteration :  13   Loss :  3.52897443077
Iteration :  14   Loss :  3.17387825323
Iteration :  15   Loss :  2.85451293681
Iteration :  16   Loss :  2.56728313322
Iteration :  17   Loss :  2.30895526909
Iteration :  18   Loss :  2.07662114305
Iteration :  19   Loss :  1.86766518586
Iteration :  20   Loss :  1.6797350148
Iteration :  21   Loss :  1.51071495111
Iteration :  22   Loss :  1.35870220207
Iteration :  23   Loss :  1.22198543978
Iteration :  24   Loss :  1.09902553537
Iteration :  25   Loss :  0.98843823181
Iteration :  26   Loss :  0.888978560243
Iteration :  27   Loss :  0.799526824376
Iteration :  28   Loss :  0.719075995176
Iteration :  29   Loss :  0.646720373944
Iteration :  30   Loss :  0.581645396147
Iteration :  31   Loss :  0.523118461223
Iteration :  32   Loss :  0.470480685113
Iteration :  33   Loss :  0.423139482685
Iteration :  34   Loss :  0.380561896528
Iteration :  35   Loss :  0.342268597037
Iteration :  36   Loss :  0.30782848621
Iteration :  37   Loss :  0.276853844444
Iteration :  38   Loss :  0.248995965667
Iteration :  39   Loss :  0.223941231674
Iteration :  40   Loss :  0.201407581481
Iteration :  41   Loss :  0.181141335943
Iteration :  42   Loss :  0.162914341882
Iteration :  43   Loss :  0.146521403593
Iteration :  44   Loss :  0.131777972784
Iteration :  45   Loss :  0.11851807098
Iteration :  46   Loss :  0.106592420965
Iteration :  47   Loss :  0.0958667662506
Iteration :  48   Loss :  0.0862203596481
Iteration :  49   Loss :  0.0775446039184
Iteration :  50   Loss :  0.0697418292084
Iteration :  51   Loss :  0.0627241935036
Iteration :  52   Loss :  0.0564126937209
Iteration :  53   Loss :  0.0507362763089
Iteration :  54   Loss :  0.0456310373412
Iteration :  55   Loss :  0.0410395030994
Iteration :  56   Loss :  0.0369099830462
Iteration :  57   Loss :  0.033195987904
Iteration :  58   Loss :  0.029855706288
Iteration :  59   Loss :  0.0268515340026
Iteration :  60   Loss :  0.0241496507012
Iteration :  61   Loss :  0.0217196391437
Iteration :  62   Loss :  0.0195341427654
Iteration :  63   Loss :  0.0175685577027
Iteration :  64   Loss :  0.0326629103631
Iteration :  65   Loss :  301.511506527
Iteration :  66   Loss :  32.9164443681
Iteration :  67   Loss :  13.1418451188
Iteration :  68   Loss :  11.8194725545
Iteration :  69   Loss :  10.6301611534
Iteration :  70   Loss :  9.5605218952
Iteration :  71   Loss :  8.59851300369
Iteration :  72   Loss :  7.73330438287
Iteration :  73   Loss :  6.95515569406
Iteration :  74   Loss :  6.2553067012
Iteration :  75   Loss :  5.62587864992
Iteration :  76   Loss :  5.05978557016
Iteration :  77   Loss :  4.55065450378
Iteration :  78   Loss :  4.09275375915
Iteration :  79   Loss :  3.68092838495
Iteration :  80   Loss :  3.31054213678
Iteration :  81   Loss :  2.9774252833
Iteration :  82   Loss :  2.67782766429
Iteration :  83   Loss :  2.40837647206
Iteration :  84   Loss :  2.16603828115
Iteration :  85   Loss :  1.94808489862
Iteration :  86   Loss :  1.75206265063
Iteration :  87   Loss :  1.57576475948
Iteration :  88   Loss :  1.41720650019
Iteration :  89   Loss :  1.27460285687
Iteration :  90   Loss :  1.14634842737
Iteration :  91   Loss :  1.03099934999
Iteration :  92   Loss :  0.927257048812
Iteration :  93   Loss :  0.833953614601
Iteration :  94   Loss :  0.750038656699
Iteration :  95   Loss :  0.674567477967
Iteration :  96   Loss :  0.606690439575
Iteration :  97   Loss :  0.545643395945
Iteration :  98   Loss :  0.490739092158
Iteration :  99   Loss :  0.441359426984
[-0.0036214  -0.00098747 -0.00244887 ...,  0.00430852  0.00083543
  0.00032986]
CROSS VALIDATION 1
Iteration :  0   Loss :  100.8555092
Iteration :  1   Loss :  10.8982106432
Iteration :  2   Loss :  9.8015994273
Iteration :  3   Loss :  8.81533257875
Iteration :  4   Loss :  7.92830691055
Iteration :  5   Loss :  7.13053647227
Iteration :  6   Loss :  6.41304013025
Iteration :  7   Loss :  5.76774045994
Iteration :  8   Loss :  5.1873728119
Iteration :  9   Loss :  4.66540352786
Iteration :  10   Loss :  4.19595638622
Iteration :  11   Loss :  3.77374644871
Iteration :  12   Loss :  3.39402056369
Iteration :  13   Loss :  3.05250385612
Iteration :  14   Loss :  2.74535160197
Iteration :  15   Loss :  2.46910594505
Iteration :  16   Loss :  2.22065696923
Iteration :  17   Loss :  1.99720768761
Iteration :  18   Loss :  1.79624255466
Iteration :  19   Loss :  1.61549914674
Iteration :  20   Loss :  1.45294269214
Iteration :  21   Loss :  1.30674316412
Iteration :  22   Loss :  1.17525467881
Iteration :  23   Loss :  1.05699696618
Iteration :  24   Loss :  0.950638705525
Iteration :  25   Loss :  0.854982537655
Iteration :  26   Loss :  0.768951585336
Iteration :  27   Loss :  0.691577330003
Iteration :  28   Loss :  0.621988708385
Iteration :  29   Loss :  0.559402306257
Iteration :  30   Loss :  0.503113538923
Iteration :  31   Loss :  0.45248871915
Iteration :  32   Loss :  0.406957923247
Iteration :  33   Loss :  0.366008574987
Iteration :  34   Loss :  0.329179675125
Iteration :  35   Loss :  0.296056611568
Iteration :  36   Loss :  100.151466143
Iteration :  37   Loss :  84.1099031166
Iteration :  38   Loss :  5.24257335559
Iteration :  39   Loss :  4.71504962438
Iteration :  40   Loss :  4.24060694099
Iteration :  41   Loss :  3.81390412838
Iteration :  42   Loss :  3.43013745507
Iteration :  43   Loss :  3.08498655567
Iteration :  44   Loss :  2.77456579315
Iteration :  45   Loss :  2.49538051515
Iteration :  46   Loss :  2.2442877119
Iteration :  47   Loss :  2.01846063283
Iteration :  48   Loss :  1.81535696367
Iteration :  49   Loss :  1.63269020558
Iteration :  50   Loss :  1.46840393417
Iteration :  51   Loss :  1.32064864879
Iteration :  52   Loss :  1.18776095117
Iteration :  53   Loss :  1.06824481925
Iteration :  54   Loss :  0.960754765291
Iteration :  55   Loss :  0.864080688614
Iteration :  56   Loss :  0.777134252578
Iteration :  57   Loss :  0.698936632294
Iteration :  58   Loss :  0.628607495219
Iteration :  59   Loss :  0.56535509056
Iteration :  60   Loss :  0.508467335901
Iteration :  61   Loss :  0.457303800736
Iteration :  62   Loss :  0.411288496629
Iteration :  63   Loss :  0.369903392859
Iteration :  64   Loss :  0.332682584536
Iteration :  65   Loss :  0.299207047543
Iteration :  66   Loss :  0.269099921248
Iteration :  67   Loss :  0.242022265888
Iteration :  68   Loss :  0.21766924685
Iteration :  69   Loss :  0.195766702912
Iteration :  70   Loss :  0.176068059791
Iteration :  71   Loss :  0.15835155426
Iteration :  72   Loss :  0.142417737587
Iteration :  73   Loss :  0.128087230176
Iteration :  74   Loss :  0.115198702158
Iteration :  75   Loss :  0.103607057164
Iteration :  76   Loss :  0.0931817988663
Iteration :  77   Loss :  0.0838055618764
Iteration :  78   Loss :  0.0753727904684
Iteration :  79   Loss :  0.0677885502561
Iteration :  80   Loss :  0.0609674594407
Iteration :  81   Loss :  0.054832727601
Iteration :  82   Loss :  0.049315291202
Iteration :  83   Loss :  0.0443530360925
Iteration :  84   Loss :  0.0398900982368
Iteration :  85   Loss :  0.0358762348089
Iteration :  86   Loss :  0.0322662585692
Iteration :  87   Loss :  0.0290195291564
Iteration :  88   Loss :  0.026099495566
Iteration :  89   Loss :  0.0234732846673
Iteration :  90   Loss :  0.0211113311243
Iteration :  91   Loss :  0.0189870445555
Iteration :  92   Loss :  0.0170765101844
Iteration :  93   Loss :  0.0153582196127
Iteration :  94   Loss :  0.0138128286824
Iteration :  95   Loss :  0.0124229397041
Iteration :  96   Loss :  0.0174823581217
Iteration :  97   Loss :  41.2715978152
Iteration :  98   Loss :  10.8941343654
Iteration :  99   Loss :  9.79793331701
[-0.01073499 -0.00359245  0.00505716 ...,  0.00191258  0.00961851
  0.00175116]
CROSS VALIDATION 2
Iteration :  0   Loss :  189.43097139
Iteration :  1   Loss :  12.3468452465
Iteration :  2   Loss :  11.1044679956
Iteration :  3   Loss :  9.98710253539
Iteration :  4   Loss :  8.98216979795
Iteration :  5   Loss :  8.07835645958
Iteration :  6   Loss :  7.26548757772
Iteration :  7   Loss :  6.53441204361
Iteration :  8   Loss :  5.87689956096
Iteration :  9   Loss :  5.28554799102
Iteration :  10   Loss :  4.75370002083
Iteration :  11   Loss :  4.27536821659
Iteration :  12   Loss :  3.84516761835
Iteration :  13   Loss :  3.45825511726
Iteration :  14   Loss :  3.11027493287
Iteration :  15   Loss :  2.79730957666
Iteration :  16   Loss :  2.51583574976
Iteration :  17   Loss :  2.26268467836
Iteration :  18   Loss :  2.03500644038
Iteration :  19   Loss :  1.83023788157
Iteration :  20   Loss :  1.6460737601
Iteration :  21   Loss :  1.48044079459
Iteration :  22   Loss :  31.6773788976
Iteration :  23   Loss :  7.26923948362
Iteration :  24   Loss :  5.68993582567
Iteration :  25   Loss :  5.11739711738
Iteration :  26   Loss :  4.60246900129
Iteration :  27   Loss :  4.13935452379
Iteration :  28   Loss :  3.72284003842
Iteration :  29   Loss :  3.34823651176
Iteration :  30   Loss :  3.01132673524
Iteration :  31   Loss :  2.70831784866
Iteration :  32   Loss :  2.43579864103
Iteration :  33   Loss :  2.19070114778
Iteration :  34   Loss :  1.97026611234
Iteration :  35   Loss :  1.77201192293
Iteration :  36   Loss :  1.59370667512
Iteration :  37   Loss :  1.43334304552
Iteration :  38   Loss :  1.28911569376
Iteration :  39   Loss :  1.15940093832
Iteration :  40   Loss :  1.04273847745
Iteration :  41   Loss :  0.937814949441
Iteration :  42   Loss :  0.843449147046
Iteration :  43   Loss :  0.75857871969
Iteration :  44   Loss :  0.682248213756
Iteration :  45   Loss :  0.613598316287
Iteration :  46   Loss :  0.551856181019
Iteration :  47   Loss :  0.496326727837
Iteration :  48   Loss :  0.446384817708
Iteration :  49   Loss :  0.401468214998
Iteration :  50   Loss :  0.361071257936
Iteration :  51   Loss :  0.324739165984
Iteration :  52   Loss :  0.292062920009
Iteration :  53   Loss :  0.262674657631
Iteration :  54   Loss :  0.236243531905
Iteration :  55   Loss :  0.212471986717
Iteration :  56   Loss :  0.191092406955
Iteration :  57   Loss :  0.17186410576
Iteration :  58   Loss :  0.154570614914
Iteration :  59   Loss :  0.139017247897
Iteration :  60   Loss :  0.125028908137
Iteration :  61   Loss :  0.112448117816
Iteration :  62   Loss :  0.101133245013
Iteration :  63   Loss :  0.0909569092443
Iteration :  64   Loss :  0.0818045474385
Iteration :  65   Loss :  0.0735731242103
Iteration :  66   Loss :  0.0661699719093
Iteration :  67   Loss :  0.0595117473871
Iteration :  68   Loss :  0.053523493737
Iteration :  69   Loss :  0.0481377964451
Iteration :  70   Loss :  0.0432940244518
Iteration :  71   Loss :  0.0389376475794
Iteration :  72   Loss :  0.0350196226435
Iteration :  73   Loss :  0.0314958413343
Iteration :  74   Loss :  0.0283266336549
Iteration :  75   Loss :  0.0254763213245
Iteration :  76   Loss :  0.0229128161199
Iteration :  77   Loss :  0.0206072586327
Iteration :  78   Loss :  0.0185336933764
Iteration :  79   Loss :  0.0166687765846
Iteration :  80   Loss :  0.0149915134121
Iteration :  81   Loss :  0.0134830215789
Iteration :  82   Loss :  0.0121263187979
Iteration :  83   Loss :  0.0109061315913
Iteration :  84   Loss :  0.00980872334539
Iteration :  85   Loss :  0.00882173966646
Iteration :  86   Loss :  0.00793406929754
Iteration :  87   Loss :  0.00713571903029
Iteration :  88   Loss :  0.00641770120347
Iteration :  89   Loss :  0.00577193252175
Iteration :  90   Loss :  0.00519114305565
Iteration :  91   Loss :  757.40290464
Iteration :  92   Loss :  18.7232695826
Iteration :  93   Loss :  9.19952217527
Iteration :  94   Loss :  8.27383817733
Iteration :  95   Loss :  7.44129932843
Iteration :  96   Loss :  6.69253307939
Iteration :  97   Loss :  6.01910997554
Iteration :  98   Loss :  5.41344875966
Iteration :  99   Loss :  4.86873102379
[-0.01150502  0.00262919 -0.00464397 ...,  0.01418999  0.00471084
  0.00061058]
CROSS VALIDATION 3
Iteration :  0   Loss :  185.30515051
Iteration :  1   Loss :  12.4533119201
Iteration :  2   Loss :  23.8943581334
Iteration :  3   Loss :  11.5819251752
Iteration :  4   Loss :  10.4165165164
Iteration :  5   Loss :  9.36837483364
Iteration :  6   Loss :  8.42570036589
Iteration :  7   Loss :  7.5778806801
Iteration :  8   Loss :  6.81537119861
Iteration :  9   Loss :  6.1295877483
Iteration :  10   Loss :  5.51280992176
Iteration :  11   Loss :  4.95809416251
Iteration :  12   Loss :  4.45919559592
Iteration :  13   Loss :  4.01049772574
Iteration :  14   Loss :  3.60694920466
Iteration :  15   Loss :  3.24400696739
Iteration :  16   Loss :  2.91758508572
Iteration :  17   Loss :  2.62400877002
Iteration :  18   Loss :  2.35997299918
Iteration :  19   Loss :  2.12250531344
Iteration :  20   Loss :  1.90893235098
Iteration :  21   Loss :  1.71684975182
Iteration :  22   Loss :  1.54409509002
Iteration :  23   Loss :  1.38872352953
Iteration :  24   Loss :  1.24898592965
Iteration :  25   Loss :  1.12330915354
Iteration :  26   Loss :  1.01027835821
Iteration :  27   Loss :  0.908621066468
Iteration :  28   Loss :  0.817192841676
Iteration :  29   Loss :  0.7349644039
Iteration :  30   Loss :  0.661010042491
Iteration :  31   Loss :  0.594497194633
Iteration :  32   Loss :  0.534677072522
Iteration :  33   Loss :  0.480876233667
Iteration :  34   Loss :  0.432488999416
Iteration :  35   Loss :  0.388970636352
Iteration :  36   Loss :  0.349831223796
Iteration :  37   Loss :  0.314630138383
Iteration :  38   Loss :  0.282971093618
Iteration :  39   Loss :  0.254497678559
Iteration :  40   Loss :  0.228889345423
Iteration :  41   Loss :  0.205857800924
Iteration :  42   Loss :  0.185143760723
Iteration :  43   Loss :  0.166514030466
Iteration :  44   Loss :  0.149758880525
Iteration :  45   Loss :  0.134689684907
Iteration :  46   Loss :  0.121136797743
Iteration :  47   Loss :  0.108947643448
Iteration :  48   Loss :  0.0979849990597
Iteration :  49   Loss :  0.0881254494071
Iteration :  50   Loss :  0.0792579977316
Iteration :  51   Loss :  0.0712828161069
Iteration :  52   Loss :  0.0641101215973
Iteration :  53   Loss :  0.0576591655001
Iteration :  54   Loss :  0.0518573242935
Iteration :  55   Loss :  0.0466392820561
Iteration :  56   Loss :  0.041946295154
Iteration :  57   Loss :  0.0377255309168
Iteration :  58   Loss :  0.0339294728588
Iteration :  59   Loss :  0.030515385748
Iteration :  60   Loss :  0.0274448345021
Iteration :  61   Loss :  0.0246832514937
Iteration :  62   Loss :  0.0221995473959
Iteration :  63   Loss :  329.219912676
Iteration :  64   Loss :  28.9886402189
Iteration :  65   Loss :  27.7951342723
Iteration :  66   Loss :  10.3297438617
Iteration :  67   Loss :  9.2903335083
Iteration :  68   Loss :  8.35551179688
Iteration :  69   Loss :  7.51475469912
Iteration :  70   Loss :  6.75859714649
Iteration :  71   Loss :  6.07852647458
Iteration :  72   Loss :  5.46688658924
Iteration :  73   Loss :  4.91679177586
Iteration :  74   Loss :  4.42204918148
Iteration :  75   Loss :  3.97708909688
Iteration :  76   Loss :  3.576902254
Iteration :  77   Loss :  3.21698343261
Iteration :  78   Loss :  2.89328074148
Iteration :  79   Loss :  2.60215000307
Iteration :  80   Loss :  2.34031372808
Iteration :  81   Loss :  2.1048242182
Iteration :  82   Loss :  1.8930303815
Iteration :  83   Loss :  1.70254788705
Iteration :  84   Loss :  1.53123232253
Iteration :  85   Loss :  1.37715505296
Iteration :  86   Loss :  1.2385815085
Iteration :  87   Loss :  1.113951657
Iteration :  88   Loss :  1.00186244153
Iteration :  89   Loss :  0.901051985008
Iteration :  90   Loss :  0.810385384294
Iteration :  91   Loss :  0.728841933655
Iteration :  92   Loss :  0.655503633887
Iteration :  93   Loss :  0.589544857669
Iteration :  94   Loss :  0.530223054818
Iteration :  95   Loss :  0.476870392818
Iteration :  96   Loss :  0.428886238499
Iteration :  97   Loss :  0.385730396233
Iteration :  98   Loss :  0.346917026527
Iteration :  99   Loss :  0.312009176538
[-0.00247139 -0.00230791 -0.00303548 ...,  0.00332114 -0.00031039
  0.00010807]
CROSS VALIDATION 4
Iteration :  0   Loss :  185.30515051
Iteration :  1   Loss :  12.4533119201
Iteration :  2   Loss :  23.8943581334
Iteration :  3   Loss :  11.5819251752
Iteration :  4   Loss :  10.4165165164
Iteration :  5   Loss :  9.36837483364
Iteration :  6   Loss :  8.42570036589
Iteration :  7   Loss :  7.5778806801
Iteration :  8   Loss :  6.81537119861
Iteration :  9   Loss :  6.1295877483
Iteration :  10   Loss :  5.51280992176
Iteration :  11   Loss :  4.95809416251
Iteration :  12   Loss :  4.45919559592
Iteration :  13   Loss :  4.01049772574
Iteration :  14   Loss :  3.60694920466
Iteration :  15   Loss :  3.24400696739
Iteration :  16   Loss :  2.91758508572
Iteration :  17   Loss :  2.62400877002
Iteration :  18   Loss :  2.35997299918
Iteration :  19   Loss :  2.12250531344
Iteration :  20   Loss :  1.90893235098
Iteration :  21   Loss :  1.71684975182
Iteration :  22   Loss :  1.54409509002
Iteration :  23   Loss :  1.38872352953
Iteration :  24   Loss :  1.24898592965
Iteration :  25   Loss :  1.12330915354
Iteration :  26   Loss :  1.01027835821
Iteration :  27   Loss :  0.908621066468
Iteration :  28   Loss :  0.817192841676
Iteration :  29   Loss :  0.7349644039
Iteration :  30   Loss :  0.661010042491
Iteration :  31   Loss :  0.594497194633
Iteration :  32   Loss :  0.534677072522
Iteration :  33   Loss :  0.480876233667
Iteration :  34   Loss :  0.432488999416
Iteration :  35   Loss :  0.388970636352
Iteration :  36   Loss :  0.349831223796
Iteration :  37   Loss :  0.314630138383
Iteration :  38   Loss :  0.282971093618
Iteration :  39   Loss :  0.254497678559
Iteration :  40   Loss :  0.228889345423
Iteration :  41   Loss :  0.205857800924
Iteration :  42   Loss :  0.185143760723
Iteration :  43   Loss :  0.166514030466
Iteration :  44   Loss :  0.149758880525
Iteration :  45   Loss :  0.134689684907
Iteration :  46   Loss :  0.121136797743
Iteration :  47   Loss :  0.108947643448
Iteration :  48   Loss :  0.0979849990597
Iteration :  49   Loss :  0.0881254494071
Iteration :  50   Loss :  0.0792579977316
Iteration :  51   Loss :  0.0712828161069
Iteration :  52   Loss :  0.0641101215973
Iteration :  53   Loss :  0.0576591655001
Iteration :  54   Loss :  0.0518573242935
Iteration :  55   Loss :  0.0466392820561
Iteration :  56   Loss :  0.041946295154
Iteration :  57   Loss :  0.0377255309168
Iteration :  58   Loss :  0.0339294728588
Iteration :  59   Loss :  0.030515385748
Iteration :  60   Loss :  0.0274448345021
Iteration :  61   Loss :  0.0246832514937
Iteration :  62   Loss :  0.0221995473959
Iteration :  63   Loss :  329.219912676
Iteration :  64   Loss :  28.9886402189
Iteration :  65   Loss :  27.7951342723
Iteration :  66   Loss :  10.3297438617
Iteration :  67   Loss :  9.2903335083
Iteration :  68   Loss :  8.35551179688
Iteration :  69   Loss :  7.51475469912
Iteration :  70   Loss :  6.75859714649
Iteration :  71   Loss :  6.07852647458
Iteration :  72   Loss :  5.46688658924
Iteration :  73   Loss :  4.91679177586
Iteration :  74   Loss :  4.42204918148
Iteration :  75   Loss :  3.97708909688
Iteration :  76   Loss :  3.576902254
Iteration :  77   Loss :  3.21698343261
Iteration :  78   Loss :  2.89328074148
Iteration :  79   Loss :  2.60215000307
Iteration :  80   Loss :  2.34031372808
Iteration :  81   Loss :  2.1048242182
Iteration :  82   Loss :  1.8930303815
Iteration :  83   Loss :  1.70254788705
Iteration :  84   Loss :  1.53123232253
Iteration :  85   Loss :  1.37715505296
Iteration :  86   Loss :  1.2385815085
Iteration :  87   Loss :  1.113951657
Iteration :  88   Loss :  1.00186244153
Iteration :  89   Loss :  0.901051985008
Iteration :  90   Loss :  0.810385384294
Iteration :  91   Loss :  0.728841933655
Iteration :  92   Loss :  0.655503633887
Iteration :  93   Loss :  0.589544857669
Iteration :  94   Loss :  0.530223054818
Iteration :  95   Loss :  0.476870392818
Iteration :  96   Loss :  0.428886238499
Iteration :  97   Loss :  0.385730396233
Iteration :  98   Loss :  0.346917026527
Iteration :  99   Loss :  0.312009176538
[-0.00247139 -0.00230791 -0.00303548 ...,  0.00332114 -0.00031039
  0.00010807]
CROSS VALIDATION 5
Iteration :  0   Loss :  165.885136261
Iteration :  1   Loss :  12.291894984
Iteration :  2   Loss :  11.0550469962
Iteration :  3   Loss :  9.94265442782
Iteration :  4   Loss :  8.94219419462
Iteration :  5   Loss :  8.04240332346
Iteration :  6   Loss :  7.23315215588
Iteration :  7   Loss :  6.5053303106
Iteration :  8   Loss :  5.85074412068
Iteration :  9   Loss :  5.26202439097
Iteration :  10   Loss :  4.73254343721
Iteration :  11   Loss :  4.25634047299
Iteration :  12   Loss :  3.82805450437
Iteration :  13   Loss :  3.44286397704
Iteration :  14   Loss :  3.09643249616
Iteration :  15   Loss :  2.78486000818
Iteration :  16   Loss :  2.50463889485
Iteration :  17   Loss :  2.25261448517
Iteration :  18   Loss :  2.02594954076
Iteration :  19   Loss :  1.82209231483
Iteration :  20   Loss :  1.63874782515
Iteration :  21   Loss :  1.47385201757
Iteration :  22   Loss :  1.32554852941
Iteration :  23   Loss :  1.19216779085
Iteration :  24   Loss :  1.07220822928
Iteration :  25   Loss :  0.9643193649
Iteration :  26   Loss :  0.867286607331
Iteration :  27   Loss :  0.780017581969
Iteration :  28   Loss :  0.701529832282
Iteration :  29   Loss :  0.630939759511
Iteration :  30   Loss :  0.5674526753
Iteration :  31   Loss :  0.510353855263
Iteration :  32   Loss :  0.459000492763
Iteration :  33   Loss :  0.412814462326
Iteration :  34   Loss :  0.371275811229
Iteration :  35   Loss :  0.333916905981
Iteration :  36   Loss :  0.300317167797
Iteration :  37   Loss :  0.270098337815
Iteration :  38   Loss :  0.242920218732
Iteration :  39   Loss :  0.218476844938
Iteration :  40   Loss :  0.196493038016
Iteration :  41   Loss :  0.17672130884
Iteration :  42   Loss :  0.158939071397
Iteration :  43   Loss :  0.142946136956
Iteration :  44   Loss :  0.128562460388
Iteration :  45   Loss :  0.115626113255
Iteration :  46   Loss :  0.103991460852
Iteration :  47   Loss :  0.0935275226824
Iteration :  48   Loss :  0.084116497907
Iteration :  49   Loss :  0.0756524391667
Iteration :  50   Loss :  0.0680400598488
Iteration :  51   Loss :  0.0611936613707
Iteration :  52   Loss :  0.0550361684024
Iteration :  53   Loss :  0.0494982611691
Iteration :  54   Loss :  0.0445175950631
Iteration :  55   Loss :  0.0400380987815
Iteration :  56   Loss :  0.0360093430871
Iteration :  57   Loss :  0.0323859730862
Iteration :  58   Loss :  0.0291271976333
Iteration :  59   Loss :  0.0261963301122
Iteration :  60   Loss :  0.0235603754259
Iteration :  61   Loss :  0.021189658545
Iteration :  62   Loss :  0.019057490432
Iteration :  63   Loss :  0.0171398675819
Iteration :  64   Loss :  0.0154152017954
Iteration :  65   Loss :  0.0138640771441
Iteration :  66   Loss :  0.0124690313891
Iteration :  67   Loss :  0.0112143593956
Iteration :  68   Loss :  0.0100859363274
Iteration :  69   Loss :  0.00907105863213
Iteration :  70   Loss :  0.00815830102798
Iteration :  71   Loss :  450.056839443
Iteration :  72   Loss :  133.598416546
Iteration :  73   Loss :  18.5877266852
Iteration :  74   Loss :  44.5754489908
Iteration :  75   Loss :  7.752879649
Iteration :  76   Loss :  6.97276123717
Iteration :  77   Loss :  6.27114072083
Iteration :  78   Loss :  5.64011940217
Iteration :  79   Loss :  5.07259337446
Iteration :  80   Loss :  4.56217354772
Iteration :  81   Loss :  4.10311372174
Iteration :  82   Loss :  3.69024589648
Iteration :  83   Loss :  3.31892209186
Iteration :  84   Loss :  2.98496202173
Iteration :  85   Loss :  2.68460603309
Iteration :  86   Loss :  2.41447278071
Iteration :  87   Loss :  2.17152116062
Iteration :  88   Loss :  1.95301607402
Iteration :  89   Loss :  1.75649763611
Iteration :  90   Loss :  1.57975348319
Iteration :  91   Loss :  1.42079386635
Iteration :  92   Loss :  1.2778292513
Iteration :  93   Loss :  1.14925017214
Iteration :  94   Loss :  1.03360911234
Iteration :  95   Loss :  0.929604208914
Iteration :  96   Loss :  0.83606459629
Iteration :  97   Loss :  0.751937224968
Iteration :  98   Loss :  0.676275006502
Iteration :  99   Loss :  0.608226151378
[ -8.63257980e-05  -1.17084689e-04   2.61353835e-03 ...,   3.00670459e-03
   1.62585035e-03   4.68075100e-04]
CROSS VALIDATION 6
Iteration :  0   Loss :  185.159968443
Iteration :  1   Loss :  12.4544025409
Iteration :  2   Loss :  11.9502333144
Iteration :  3   Loss :  10.7477643665
Iteration :  4   Loss :  9.66629151412
Iteration :  5   Loss :  8.69363976075
Iteration :  6   Loss :  7.8188591953
Iteration :  7   Loss :  7.03210172016
Iteration :  8   Loss :  6.32451018332
Iteration :  9   Loss :  5.68811866646
Iteration :  10   Loss :  5.11576280628
Iteration :  11   Loss :  4.60099913956
Iteration :  12   Loss :  4.13803256403
Iteration :  13   Loss :  3.72165109829
Iteration :  14   Loss :  3.34716720642
Iteration :  15   Loss :  3.01036502666
Iteration :  16   Loss :  2.70745291014
Iteration :  17   Loss :  2.43502073525
Iteration :  18   Loss :  2.19000151727
Iteration :  19   Loss :  1.96963688078
Iteration :  20   Loss :  1.77144600657
Iteration :  21   Loss :  1.59319770299
Iteration :  22   Loss :  1.43288528772
Iteration :  23   Loss :  1.28870399695
Iteration :  24   Loss :  1.15903066769
Iteration :  25   Loss :  1.04240546458
Iteration :  26   Loss :  0.937515445348
Iteration :  27   Loss :  0.843179779969
Iteration :  28   Loss :  0.75833645715
Iteration :  29   Loss :  0.682030328412
Iteration :  30   Loss :  0.613402355232
Iteration :  31   Loss :  0.551679938163
Iteration :  32   Loss :  0.496168219075
Iteration :  33   Loss :  0.446242258582
Iteration :  34   Loss :  0.401340000606
Iteration :  35   Loss :  0.360955944868
Iteration :  36   Loss :  0.324635456069
Iteration :  37   Loss :  0.291969645702
Iteration :  38   Loss :  0.262590768869
Iteration :  39   Loss :  0.236168084286
Iteration :  40   Loss :  0.212404130868
Iteration :  41   Loss :  0.191031378969
Iteration :  42   Loss :  0.171809218595
Iteration :  43   Loss :  0.154521250664
Iteration :  44   Loss :  0.138972850828
Iteration :  45   Loss :  0.124988978437
Iteration :  46   Loss :  0.112412205964
Iteration :  47   Loss :  0.101100946721
Iteration :  48   Loss :  0.0909278609053
Iteration :  49   Loss :  0.081778422032
Iteration :  50   Loss :  0.0735496276219
Iteration :  51   Loss :  0.0661488396194
Iteration :  52   Loss :  0.0594927414927
Iteration :  53   Loss :  0.0535064002737
Iteration :  54   Loss :  579.923093947
Iteration :  55   Loss :  11.3319191902
Iteration :  56   Loss :  10.9305663467
Iteration :  57   Loss :  8.08307221469
Iteration :  58   Loss :  7.26972881916
Iteration :  59   Loss :  6.53822651839
Iteration :  60   Loss :  5.88033021164
Iteration :  61   Loss :  5.28863343916
Iteration :  62   Loss :  4.75647500177
Iteration :  63   Loss :  4.27786397048
Iteration :  64   Loss :  3.84741224186
Iteration :  65   Loss :  3.4602738799
Iteration :  66   Loss :  3.11209056145
Iteration :  67   Loss :  2.79894251116
Iteration :  68   Loss :  2.5173043734
Iteration :  69   Loss :  2.26400552461
Iteration :  70   Loss :  2.03619437905
Iteration :  71   Loss :  1.83130628623
Iteration :  72   Loss :  1.64703465862
Iteration :  73   Loss :  1.48130500457
Iteration :  74   Loss :  1.33225157411
Iteration :  75   Loss :  1.19819635473
Iteration :  76   Loss :  1.07763018066
Iteration :  77   Loss :  0.969195742991
Iteration :  78   Loss :  0.871672309376
Iteration :  79   Loss :  0.783961981289
Iteration :  80   Loss :  0.705077334104
Iteration :  81   Loss :  0.634130300873
Iteration :  82   Loss :  0.570322174653
Iteration :  83   Loss :  0.512934616834
Iteration :  84   Loss :  0.461321570227
Iteration :  85   Loss :  0.414901985892
Iteration :  86   Loss :  0.373153281803
Iteration :  87   Loss :  0.335605459736
Iteration :  88   Loss :  0.30183581412
Iteration :  89   Loss :  0.27146417331
Iteration :  90   Loss :  0.244148619691
Iteration :  91   Loss :  0.219581640444
Iteration :  92   Loss :  0.197486665626
Iteration :  93   Loss :  0.177614954607
Iteration :  94   Loss :  0.159742795799
Iteration :  95   Loss :  0.143668988156
Iteration :  96   Loss :  0.129212576094
Iteration :  97   Loss :  0.116210812335
Iteration :  98   Loss :  0.104517325726
Iteration :  99   Loss :  0.0940004734279
[  1.45558899e-03  -4.87175557e-04   4.28064043e-04 ...,   4.61886477e-04
   7.13316715e-05   2.88532326e-04]
CROSS VALIDATION 7
Iteration :  0   Loss :  185.159968443
Iteration :  1   Loss :  12.4544025409
Iteration :  2   Loss :  23.8736325834
Iteration :  3   Loss :  11.5845156944
Iteration :  4   Loss :  10.4188463697
Iteration :  5   Loss :  9.37047024997
Iteration :  6   Loss :  8.42758493503
Iteration :  7   Loss :  7.57957561814
Iteration :  8   Loss :  6.81689558681
Iteration :  9   Loss :  6.13095874791
Iteration :  10   Loss :  5.5140429672
Iteration :  11   Loss :  4.95920313515
Iteration :  12   Loss :  4.46019298036
Iteration :  13   Loss :  4.01139475031
Iteration :  14   Loss :  3.60775596788
Iteration :  15   Loss :  3.24473255163
Iteration :  16   Loss :  2.91823765946
Iteration :  17   Loss :  2.6245956798
Iteration :  18   Loss :  2.36050085231
Iteration :  19   Loss :  2.12298005237
Iteration :  20   Loss :  1.90935932024
Iteration :  21   Loss :  1.71723375813
Iteration :  22   Loss :  1.54444045644
Iteration :  23   Loss :  1.38903414413
Iteration :  24   Loss :  1.24926528926
Iteration :  25   Loss :  1.12356040313
Iteration :  26   Loss :  1.0105043263
Iteration :  27   Loss :  0.908824296959
Iteration :  28   Loss :  0.817375622495
Iteration :  29   Loss :  0.735128792754
Iteration :  30   Loss :  0.661157890037
Iteration :  31   Loss :  0.594630165308
Iteration :  32   Loss :  0.534796663282
Iteration :  33   Loss :  0.48098379084
Iteration :  34   Loss :  0.432585733858
Iteration :  35   Loss :  0.389057637079
Iteration :  36   Loss :  0.349909470244
Iteration :  37   Loss :  0.314700511435
Iteration :  38   Loss :  0.283034385518
Iteration :  39   Loss :  0.254554601835
Iteration :  40   Loss :  0.228940540906
Iteration :  41   Loss :  0.205903844961
Iteration :  42   Loss :  0.185185171668
Iteration :  43   Loss :  0.166551274516
Iteration :  44   Loss :  0.149792376965
Iteration :  45   Loss :  0.134719810833
Iteration :  46   Loss :  0.121163892306
Iteration :  47   Loss :  0.108972011674
Iteration :  48   Loss :  0.09800691528
Iteration :  49   Loss :  0.0881451603504
Iteration :  50   Loss :  0.0792757252997
Iteration :  51   Loss :  0.071298759873
Iteration :  52   Loss :  0.0641244610529
Iteration :  53   Loss :  0.0576720620759
Iteration :  54   Loss :  0.0518689231765
Iteration :  55   Loss :  0.0466497138241
Iteration :  56   Loss :  0.0419556772457
Iteration :  57   Loss :  0.0377339689538
Iteration :  58   Loss :  0.0339370618347
Iteration :  59   Loss :  0.0305222110981
Iteration :  60   Loss :  0.0274509730645
Iteration :  61   Loss :  0.0246887723753
Iteration :  62   Loss :  0.0222045127495
Iteration :  63   Loss :  329.168984618
Iteration :  64   Loss :  29.0868420033
Iteration :  65   Loss :  27.7407769838
Iteration :  66   Loss :  10.3309594094
Iteration :  67   Loss :  9.29142674384
Iteration :  68   Loss :  8.35649502773
Iteration :  69   Loss :  7.51563899427
Iteration :  70   Loss :  6.75939246117
Iteration :  71   Loss :  6.07924176227
Iteration :  72   Loss :  5.4675299025
Iteration :  73   Loss :  4.91737035697
Iteration :  74   Loss :  4.42256954399
Iteration :  75   Loss :  3.97755709893
Iteration :  76   Loss :  3.57732316426
Iteration :  77   Loss :  3.21736198959
Iteration :  78   Loss :  2.89362120691
Iteration :  79   Loss :  2.60245620983
Iteration :  80   Loss :  2.34058912338
Iteration :  81   Loss :  2.10507190238
Iteration :  82   Loss :  1.89325314295
Iteration :  83   Loss :  1.70274823355
Iteration :  84   Loss :  1.53141250956
Iteration :  85   Loss :  1.37731710902
Iteration :  86   Loss :  1.23872725798
Iteration :  87   Loss :  1.11408274073
Iteration :  88   Loss :  1.00198033521
Iteration :  89   Loss :  0.901158015868
Iteration :  90   Loss :  0.810480746005
Iteration :  91   Loss :  0.72892769978
Iteration :  92   Loss :  0.655580769963
Iteration :  93   Loss :  0.589614232077
Iteration :  94   Loss :  0.530285448561
Iteration :  95   Loss :  0.476926508312
Iteration :  96   Loss :  0.428936707481
Iteration :  97   Loss :  0.385775786873
Iteration :  98   Loss :  0.346957849822
Iteration :  99   Loss :  0.312045892068
[-0.00247203 -0.00230791 -0.00303553 ...,  0.0033218  -0.00030957
  0.00010793]
CROSS VALIDATION 8
Iteration :  0   Loss :  177.731834695
Iteration :  1   Loss :  11.3446695003
Iteration :  2   Loss :  10.2031342316
Iteration :  3   Loss :  9.17646372556
Iteration :  4   Loss :  8.25309993921
Iteration :  5   Loss :  7.42264783512
Iteration :  6   Loss :  6.67575835626
Iteration :  7   Loss :  6.00402317625
Iteration :  8   Loss :  5.39988004016
Iteration :  9   Loss :  4.85652763025
Iteration :  10   Loss :  4.36784899812
Iteration :  11   Loss :  3.92834270139
Iteration :  12   Loss :  3.5330608696
Iteration :  13   Loss :  3.17755350211
Iteration :  14   Loss :  2.85781837093
Iteration :  15   Loss :  2.57025596447
Iteration :  16   Loss :  2.31162896499
Iteration :  17   Loss :  2.07902580351
Iteration :  18   Loss :  1.86982788204
Iteration :  19   Loss :  1.68168009389
Iteration :  20   Loss :  1.51246431041
Iteration :  21   Loss :  1.3602755355
Iteration :  22   Loss :  1.2234004596
Iteration :  23   Loss :  1.10029817157
Iteration :  24   Loss :  0.989582811474
Iteration :  25   Loss :  0.890007968808
Iteration :  26   Loss :  0.800452650709
Iteration :  27   Loss :  0.719908662038
Iteration :  28   Loss :  0.647469255325
Iteration :  29   Loss :  0.582318922798
Iteration :  30   Loss :  0.523724215567
Iteration :  31   Loss :  0.471025486606
Iteration :  32   Loss :  0.423629464589
Iteration :  33   Loss :  0.381002574958
Iteration :  34   Loss :  0.342664933058
Iteration :  35   Loss :  0.308184941691
Iteration :  36   Loss :  0.277174432287
Iteration :  37   Loss :  0.249284294982
Iteration :  38   Loss :  0.224200548413
Iteration :  39   Loss :  0.201640804979
Iteration :  40   Loss :  0.18135109178
Iteration :  41   Loss :  0.163102991447
Iteration :  42   Loss :  0.146691070663
Iteration :  43   Loss :  0.131930567436
Iteration :  44   Loss :  0.118655311093
Iteration :  45   Loss :  0.106715851558
Iteration :  46   Loss :  0.0959777768809
Iteration :  47   Loss :  0.0863202000499
Iteration :  48   Loss :  0.0776343980743
Iteration :  49   Loss :  0.0698225880023
Iteration :  50   Loss :  0.0627968261011
Iteration :  51   Loss :  0.0564780178049
Iteration :  52   Loss :  0.0507950272842
Iteration :  53   Loss :  0.045683876614
Iteration :  54   Loss :  0.0410870255233
Iteration :  55   Loss :  0.0369527236188
Iteration :  56   Loss :  0.0332344277897
Iteration :  57   Loss :  0.0298902782351
Iteration :  58   Loss :  0.0268826272149
Iteration :  59   Loss :  0.0241776152196
Iteration :  60   Loss :  0.0217447897868
Iteration :  61   Loss :  0.0195567626741
Iteration :  62   Loss :  0.0542657105426
Iteration :  63   Loss :  182.663521983
Iteration :  64   Loss :  165.40224852
Iteration :  65   Loss :  10.4542639759
Iteration :  66   Loss :  9.40232402854
Iteration :  67   Loss :  8.45623348918
Iteration :  68   Loss :  7.60534146734
Iteration :  69   Loss :  6.84006879763
Iteration :  70   Loss :  6.15180019953
Iteration :  71   Loss :  5.53278728835
Iteration :  72   Loss :  4.9760613455
Iteration :  73   Loss :  4.47535486613
Iteration :  74   Loss :  4.02503100086
Iteration :  75   Loss :  3.62002009728
Iteration :  76   Loss :  3.2557626269
Iteration :  77   Loss :  2.9281578549
Iteration :  78   Loss :  2.63351767489
Iteration :  79   Loss :  2.36852508903
Iteration :  80   Loss :  2.13019686591
Iteration :  81   Loss :  1.91584995596
Iteration :  82   Loss :  1.72307128628
Iteration :  83   Loss :  1.54969059469
Iteration :  84   Loss :  1.39375599745
Iteration :  85   Loss :  1.25351201528
Iteration :  86   Loss :  1.12737981063
Iteration :  87   Loss :  1.01393941335
Iteration :  88   Loss :  0.91191373507
Iteration :  89   Loss :  0.820154191915
Iteration :  90   Loss :  0.737627774039
Iteration :  91   Loss :  0.663405416198
Iteration :  92   Loss :  0.596651538528
Iteration :  93   Loss :  0.536614639759
Iteration :  94   Loss :  0.482618836975
Iteration :  95   Loss :  0.434056256661
Iteration :  96   Loss :  0.390380191389
Iteration :  97   Loss :  0.351098945104
Iteration :  98   Loss :  0.315770297704
Iteration :  99   Loss :  0.283996526628
[ -3.26063589e-03  -1.38995010e-03  -2.44646762e-03 ...,   1.19038019e-03
  -1.57988871e-05   2.40592648e-05]
CROSS VALIDATION 9
Iteration :  0   Loss :  165.355780973
Iteration :  1   Loss :  12.4557042518
Iteration :  2   Loss :  11.2023732756
Iteration :  3   Loss :  23.01836619
Iteration :  4   Loss :  10.5715210019
Iteration :  5   Loss :  9.50778229476
Iteration :  6   Loss :  8.5510802228
Iteration :  7   Loss :  7.69064443315
Iteration :  8   Loss :  6.91678831868
Iteration :  9   Loss :  6.22079996823
Iteration :  10   Loss :  5.59484408974
Iteration :  11   Loss :  5.03187380214
Iteration :  12   Loss :  4.52555130304
Iteration :  13   Loss :  4.07017651909
Iteration :  14   Loss :  3.66062293569
Iteration :  15   Loss :  3.29227988379
Iteration :  16   Loss :  2.96100063395
Iteration :  17   Loss :  2.66305571328
Iteration :  18   Loss :  2.39509091984
Iteration :  19   Loss :  2.15408956174
Iteration :  20   Loss :  1.93733849583
Iteration :  21   Loss :  1.74239758368
Iteration :  22   Loss :  1.56707222107
Iteration :  23   Loss :  1.40938863153
Iteration :  24   Loss :  1.26757164601
Iteration :  25   Loss :  1.1400247184
Iteration :  26   Loss :  1.02531195192
Iteration :  27   Loss :  0.922141934093
Iteration :  28   Loss :  0.829353198336
Iteration :  29   Loss :  0.745901148359
Iteration :  30   Loss :  0.670846298344
Iteration :  31   Loss :  0.603343696403
Iteration :  32   Loss :  0.542633412284
Iteration :  33   Loss :  0.48803198224
Iteration :  34   Loss :  0.438924714728
Iteration :  35   Loss :  0.394758770348
Iteration :  36   Loss :  0.355036938084
Iteration :  37   Loss :  0.319312037812
Iteration :  38   Loss :  0.287181886037
Iteration :  39   Loss :  0.258284768194
Iteration :  40   Loss :  0.232295366541
Iteration :  41   Loss :  0.208921097801
Iteration :  42   Loss :  0.18789881932
Iteration :  43   Loss :  0.168991866659
Iteration :  44   Loss :  0.151987389277
Iteration :  45   Loss :  0.136693954307
Iteration :  46   Loss :  0.122939391439
Iteration :  47   Loss :  0.110568854666
Iteration :  48   Loss :  0.0994430790563
Iteration :  49   Loss :  0.0894368129445
Iteration :  50   Loss :  0.0804374078676
Iteration :  51   Loss :  0.0723435503955
Iteration :  52   Loss :  0.0650641215644
Iteration :  53   Loss :  0.0585171710789
Iteration :  54   Loss :  0.0526289947324
Iteration :  55   Loss :  0.0473333046603
Iteration :  56   Loss :  0.0425704830856
Iteration :  57   Loss :  0.0382869111538
Iteration :  58   Loss :  0.034434365303
Iteration :  59   Loss :  0.0309694743736
Iteration :  60   Loss :  0.0278532313443
Iteration :  61   Loss :  0.0250505541992
Iteration :  62   Loss :  0.0225298909822
Iteration :  63   Loss :  0.020262864591
Iteration :  64   Loss :  0.0182239533142
Iteration :  65   Loss :  0.0163902035128
Iteration :  66   Loss :  0.014740971213
Iteration :  67   Loss :  0.0132576897005
Iteration :  68   Loss :  0.0119236605008
Iteration :  69   Loss :  0.0107238653906
Iteration :  70   Loss :  0.00964479732606
Iteration :  71   Loss :  0.00867430838349
Iteration :  72   Loss :  0.00780147300022
Iteration :  73   Loss :  0.00701646497708
Iteration :  74   Loss :  0.00631044685705
Iteration :  75   Loss :  0.00567547043501
Iteration :  76   Loss :  0.00510438727848
Iteration :  77   Loss :  698.739379471
Iteration :  78   Loss :  100.103550951
Iteration :  79   Loss :  126.118300509
Iteration :  80   Loss :  38.7757244595
Iteration :  81   Loss :  11.3110534873
Iteration :  82   Loss :  10.1729007644
Iteration :  83   Loss :  9.14927244214
Iteration :  84   Loss :  8.22864472572
Iteration :  85   Loss :  7.40065337984
Iteration :  86   Loss :  6.6559770502
Iteration :  87   Loss :  5.98623232557
Iteration :  88   Loss :  5.38387935918
Iteration :  89   Loss :  4.84213698662
Iteration :  90   Loss :  4.35490638498
Iteration :  91   Loss :  3.91670241348
Iteration :  92   Loss :  3.52259186298
Iteration :  93   Loss :  3.16813791889
Iteration :  94   Loss :  2.84935021243
Iteration :  95   Loss :  2.56263989793
Iteration :  96   Loss :  2.3047792503
Iteration :  97   Loss :  2.07286532802
Iteration :  98   Loss :  1.86428729239
Iteration :  99   Loss :  1.67669701529
[-0.00524278 -0.00357487 -0.00227646 ...,  0.00077016  0.00117253
  0.00027003]
CROSS VALIDATION 10
Iteration :  0   Loss :  87.7530511774
Iteration :  1   Loss :  12.3950581192
Iteration :  2   Loss :  11.1478295419
Iteration :  3   Loss :  10.0261009106
Iteration :  4   Loss :  9.01724403763
Iteration :  5   Loss :  8.10990142222
Iteration :  6   Loss :  7.29385838994
Iteration :  7   Loss :  6.55992809812
Iteration :  8   Loss :  5.89984811219
Iteration :  9   Loss :  5.30618738899
Iteration :  10   Loss :  4.77226261959
Iteration :  11   Loss :  4.29206299001
Iteration :  12   Loss :  3.86018251271
Iteration :  13   Loss :  3.47175916712
Iteration :  14   Loss :  3.12242016402
Iteration :  15   Loss :  2.80823271759
Iteration :  16   Loss :  2.52565977091
Iteration :  17   Loss :  2.27152017652
Iteration :  18   Loss :  2.04295288375
Iteration :  19   Loss :  1.83738472956
Iteration :  20   Loss :  1.65250147043
Iteration :  21   Loss :  1.48622172909
Iteration :  22   Loss :  1.33667356281
Iteration :  23   Loss :  1.20217338943
Iteration :  24   Loss :  1.08120703399
Iteration :  25   Loss :  0.972412682421
Iteration :  26   Loss :  0.874565550542
Iteration :  27   Loss :  0.7865640957
Iteration :  28   Loss :  0.707417615821
Iteration :  29   Loss :  0.636235096301
Iteration :  30   Loss :  0.572215179141
Iteration :  31   Loss :  0.514637141433
Iteration :  32   Loss :  0.462852781605
Iteration :  33   Loss :  0.416279122108
Iteration :  34   Loss :  0.37439184637
Iteration :  35   Loss :  0.336719396156
Iteration :  36   Loss :  0.302837662858
Iteration :  37   Loss :  0.27236521297
Iteration :  38   Loss :  0.244958993991
Iteration :  39   Loss :  0.220310472408
Iteration :  40   Loss :  0.198142160294
Iteration :  41   Loss :  0.178204491401
Iteration :  42   Loss :  0.160273011602
Iteration :  43   Loss :  0.144145852026
Iteration :  44   Loss :  0.129641456467
Iteration :  45   Loss :  0.116596537455
Iteration :  46   Loss :  0.104864238007
Iteration :  47   Loss :  0.0943124783367
Iteration :  48   Loss :  0.0848224689277
Iteration :  49   Loss :  0.0762873732287
Iteration :  50   Loss :  0.0686111049077
Iteration :  51   Loss :  0.0617072461329
Iteration :  52   Loss :  0.0554980746984
Iteration :  53   Loss :  0.0499136890439
Iteration :  54   Loss :  0.0448912213172
Iteration :  55   Loss :  0.0403741296217
Iteration :  56   Loss :  0.0363115614785
Iteration :  57   Loss :  0.0326577813408
Iteration :  58   Loss :  0.0293716557118
Iteration :  59   Loss :  0.0264161900727
Iteration :  60   Loss :  0.0237581124062
Iteration :  61   Loss :  0.0213674986268
Iteration :  62   Loss :  0.019217435702
Iteration :  63   Loss :  0.0172837186707
Iteration :  64   Loss :  0.0155445781487
Iteration :  65   Loss :  0.038228777168
Iteration :  66   Loss :  132.0100007
Iteration :  67   Loss :  18.2734782865
Iteration :  68   Loss :  15.8755156279
Iteration :  69   Loss :  10.0569020932
Iteration :  70   Loss :  9.04494591124
Iteration :  71   Loss :  8.13481584876
Iteration :  72   Loss :  7.31626585085
Iteration :  73   Loss :  6.58008085192
Iteration :  74   Loss :  5.9179730344
Iteration :  75   Loss :  5.32248852621
Iteration :  76   Loss :  4.78692348663
Iteration :  77   Loss :  4.30524863586
Iteration :  78   Loss :  3.87204137864
Iteration :  79   Loss :  3.48242475777
Iteration :  80   Loss :  3.13201255039
Iteration :  81   Loss :  2.81685988876
Iteration :  82   Loss :  2.53341884979
Iteration :  83   Loss :  2.27849851321
Iteration :  84   Loss :  2.0492290389
Iteration :  85   Loss :  1.84302935882
Iteration :  86   Loss :  1.65757811987
Iteration :  87   Loss :  1.49078755057
Iteration :  88   Loss :  1.34077995739
Iteration :  89   Loss :  1.20586658606
Iteration :  90   Loss :  1.08452860991
Iteration :  91   Loss :  0.975400031229
Iteration :  92   Loss :  0.877252303196
Iteration :  93   Loss :  0.788980499102
Iteration :  94   Loss :  0.709590873338
Iteration :  95   Loss :  0.638189674013
Iteration :  96   Loss :  0.573973081279
Iteration :  97   Loss :  0.516218158093
Iteration :  98   Loss :  0.464274711544
Iteration :  99   Loss :  0.417557973116
[-0.00350613 -0.00103981 -0.0020139  ...,  0.00249101 -0.00029416
  0.00063232]
CROSS VALIDATION 11
Iteration :  0   Loss :  185.261695635
Iteration :  1   Loss :  12.4571091316
Iteration :  2   Loss :  11.20973362
Iteration :  3   Loss :  23.0323404959
Iteration :  4   Loss :  10.5725251055
Iteration :  5   Loss :  9.50868536235
Iteration :  6   Loss :  8.55189242097
Iteration :  7   Loss :  7.69137490547
Iteration :  8   Loss :  6.91744528864
Iteration :  9   Loss :  6.22139083187
Iteration :  10   Loss :  5.59537549887
Iteration :  11   Loss :  5.03235173926
Iteration :  12   Loss :  4.52598114868
Iteration :  13   Loss :  4.07056311234
Iteration :  14   Loss :  3.66097062875
Iteration :  15   Loss :  3.29259259091
Iteration :  16   Loss :  2.96128187552
Iteration :  17   Loss :  2.66330865547
Iteration :  18   Loss :  2.39531841022
Iteration :  19   Loss :  2.15429416135
Iteration :  20   Loss :  1.93752250799
Iteration :  21   Loss :  1.74256307998
Iteration :  22   Loss :  1.56722106462
Iteration :  23   Loss :  1.40952249799
Iteration :  24   Loss :  1.26769204242
Iteration :  25   Loss :  1.14013300016
Iteration :  26   Loss :  1.02540933803
Iteration :  27   Loss :  0.922229520921
Iteration :  28   Loss :  0.82943197191
Iteration :  29   Loss :  0.745971995496
Iteration :  30   Loss :  0.670910016626
Iteration :  31   Loss :  0.603401003156
Iteration :  32   Loss :  0.542684952657
Iteration :  33   Loss :  0.488078336463
Iteration :  34   Loss :  0.438966404648
Iteration :  35   Loss :  0.394796265301
Iteration :  36   Loss :  0.35507066018
Iteration :  37   Loss :  0.319342366688
Iteration :  38   Loss :  0.287209163129
Iteration :  39   Loss :  0.258309300582
Iteration :  40   Loss :  0.232317430405
Iteration :  41   Loss :  0.208940941532
Iteration :  42   Loss :  0.187916666314
Iteration :  43   Loss :  0.169007917834
Iteration :  44   Loss :  0.152001825334
Iteration :  45   Loss :  0.136706937764
Iteration :  46   Loss :  0.122951068461
Iteration :  47   Loss :  0.11057935671
Iteration :  48   Loss :  0.0994525243527
Iteration :  49   Loss :  0.0894453078263
Iteration :  50   Loss :  0.0804450479685
Iteration :  51   Loss :  0.0723504217261
Iteration :  52   Loss :  0.0650703014808
Iteration :  53   Loss :  0.0585227291533
Iteration :  54   Loss :  0.0526339935363
Iteration :  55   Loss :  0.0473378004694
Iteration :  56   Loss :  0.0425745265126
Iteration :  57   Loss :  0.0382905477188
Iteration :  58   Loss :  0.0344376359458
Iteration :  59   Loss :  0.0309724159143
Iteration :  60   Loss :  0.0278558768982
Iteration :  61   Loss :  0.0250529335494
Iteration :  62   Loss :  0.0225320309149
Iteration :  63   Loss :  202.264177818
Iteration :  64   Loss :  10.2100687301
Iteration :  65   Loss :  9.18270045362
Iteration :  66   Loss :  8.25870910866
Iteration :  67   Loss :  7.42769259282
Iteration :  68   Loss :  6.68029549504
Iteration :  69   Loss :  6.00810377428
Iteration :  70   Loss :  5.40355003596
Iteration :  71   Loss :  4.85982833988
Iteration :  72   Loss :  4.37081758027
Iteration :  73   Loss :  3.93101257575
Iteration :  74   Loss :  3.53546209305
Iteration :  75   Loss :  3.17971310712
Iteration :  76   Loss :  2.8597606699
Iteration :  77   Loss :  2.57200282339
Iteration :  78   Loss :  2.31320004963
Iteration :  79   Loss :  2.08043880082
Iteration :  80   Loss :  1.87109869925
Iteration :  81   Loss :  1.68282303759
Iteration :  82   Loss :  1.51349224763
Iteration :  83   Loss :  1.36120003854
Iteration :  84   Loss :  1.22423193632
Iteration :  85   Loss :  1.10104598257
Iteration :  86   Loss :  0.990255375446
Iteration :  87   Loss :  0.89061285734
Iteration :  88   Loss :  0.800996673512
Iteration :  89   Loss :  0.720397943606
Iteration :  90   Loss :  0.647909303888
Iteration :  91   Loss :  0.582714692332
Iteration :  92   Loss :  0.524080161562
Iteration :  93   Loss :  0.471345616229
Iteration :  94   Loss :  0.423917381792
Iteration :  95   Loss :  0.381261521053
Iteration :  96   Loss :  0.342897823207
Iteration :  97   Loss :  0.308394397722
Iteration :  98   Loss :  0.277362812213
Iteration :  99   Loss :  0.249453719545
[-0.00259224 -0.00308839 -0.0025533  ...,  0.0019142  -0.00029885
  0.00060119]
CROSS VALIDATION 12
Iteration :  0   Loss :  128.597043815
Iteration :  1   Loss :  49.5883909288
Iteration :  2   Loss :  33.4614129345
Iteration :  3   Loss :  12.0244959235
Iteration :  4   Loss :  10.8145544453
Iteration :  5   Loss :  9.72636097133
Iteration :  6   Loss :  8.74766484585
Iteration :  7   Loss :  7.86744811146
Iteration :  8   Loss :  7.07580147127
Iteration :  9   Loss :  6.36381273211
Iteration :  10   Loss :  5.72346647284
Iteration :  11   Loss :  5.1475538085
Iteration :  12   Loss :  4.62959123412
Iteration :  13   Loss :  4.16374763477
Iteration :  14   Loss :  3.74477864013
Iteration :  15   Loss :  3.36796758441
Iteration :  16   Loss :  3.02907240714
Iteration :  17   Loss :  2.72427789689
Iteration :  18   Loss :  2.4501527405
Iteration :  19   Loss :  2.20361089396
Iteration :  20   Loss :  1.98187684046
Iteration :  21   Loss :  1.78245434414
Iteration :  22   Loss :  1.60309834804
Iteration :  23   Loss :  1.44178969965
Iteration :  24   Loss :  1.29671241978
Iteration :  25   Loss :  1.16623325857
Iteration :  26   Loss :  1.04888330879
Iteration :  27   Loss :  0.943341469097
Iteration :  28   Loss :  0.848419571428
Iteration :  29   Loss :  0.763049004801
Iteration :  30   Loss :  0.686268685136
Iteration :  31   Loss :  0.617214235566
Iteration :  32   Loss :  0.555108255465
Iteration :  33   Loss :  0.499251568627
Iteration :  34   Loss :  0.449015352091
Iteration :  35   Loss :  0.403834056983
Iteration :  36   Loss :  0.36319904168
Iteration :  37   Loss :  0.326652845634
Iteration :  38   Loss :  0.293784039372
Iteration :  39   Loss :  0.264222592711
Iteration :  40   Loss :  0.237635709033
Iteration :  41   Loss :  0.213724078733
Iteration :  42   Loss :  0.192218509652
Iteration :  43   Loss :  0.172876896566
Iteration :  44   Loss :  0.155481495618
Iteration :  45   Loss :  0.139836473005
Iteration :  46   Loss :  0.125765700314
Iteration :  47   Loss :  0.113110771716
Iteration :  48   Loss :  0.101729220655
Iteration :  49   Loss :  0.0914929159982
Iteration :  50   Loss :  0.082286619557
Iteration :  51   Loss :  0.0758873207045
Iteration :  52   Loss :  51.606634209
Iteration :  53   Loss :  105.384839686
Iteration :  54   Loss :  6.22245848279
Iteration :  55   Loss :  5.5963357195
Iteration :  56   Loss :  5.03321533956
Iteration :  57   Loss :  4.52675785088
Iteration :  58   Loss :  4.0712616604
Iteration :  59   Loss :  3.66159888676
Iteration :  60   Loss :  3.29315763169
Iteration :  61   Loss :  2.96179006018
Iteration :  62   Loss :  2.66376570503
Iteration :  63   Loss :  2.39572947006
Iteration :  64   Loss :  2.15466385909
Iteration :  65   Loss :  1.93785500562
Iteration :  66   Loss :  1.74286212068
Iteration :  67   Loss :  1.56749001494
Iteration :  68   Loss :  1.4097643857
Iteration :  69   Loss :  1.26790959066
Iteration :  70   Loss :  1.14032865802
Iteration :  71   Loss :  1.02558530821
Iteration :  72   Loss :  0.922387784439
Iteration :  73   Loss :  0.82957431047
Iteration :  74   Loss :  0.746100011516
Iteration :  75   Loss :  0.671025151283
Iteration :  76   Loss :  0.603504552613
Iteration :  77   Loss :  0.542778082652
Iteration :  78   Loss :  0.488162095433
Iteration :  79   Loss :  0.439041735535
Iteration :  80   Loss :  0.394864016164
Iteration :  81   Loss :  0.355131593745
Iteration :  82   Loss :  0.319397168931
Iteration :  83   Loss :  0.287258451003
Iteration :  84   Loss :  0.258353628959
Iteration :  85   Loss :  0.232357298326
Iteration :  86   Loss :  0.208976797821
Iteration :  87   Loss :  0.187948914634
Iteration :  88   Loss :  0.169036921229
Iteration :  89   Loss :  0.15202791032
Iteration :  90   Loss :  0.136730397999
Iteration :  91   Loss :  0.122972168055
Iteration :  92   Loss :  0.110598333198
Iteration :  93   Loss :  0.0994695913692
Iteration :  94   Loss :  0.0894606575075
Iteration :  95   Loss :  0.0804588531179
Iteration :  96   Loss :  0.0723628377593
Iteration :  97   Loss :  0.0650814681748
Iteration :  98   Loss :  0.0585327722204
Iteration :  99   Loss :  0.0526430260394
[ -5.91046941e-04  -1.17992919e-03  -6.74894509e-05 ...,  -1.79673784e-04
  -3.07827331e-04   1.66059670e-04]
CROSS VALIDATION 13
Iteration :  0   Loss :  185.58744746
Iteration :  1   Loss :  12.4583211574
Iteration :  2   Loss :  11.2047268602
Iteration :  3   Loss :  10.0818364114
Iteration :  4   Loss :  22.4136850263
Iteration :  5   Loss :  9.66107732339
Iteration :  6   Loss :  8.6889502378
Iteration :  7   Loss :  7.81464154647
Iteration :  8   Loss :  7.02830846402
Iteration :  9   Loss :  6.32109861619
Iteration :  10   Loss :  5.68505038163
Iteration :  11   Loss :  5.11300326163
Iteration :  12   Loss :  4.59851726872
Iteration :  13   Loss :  4.13580042661
Iteration :  14   Loss :  3.71964356535
Iteration :  15   Loss :  3.34536167757
Iteration :  16   Loss :  3.00874117563
Iteration :  17   Loss :  2.70599245595
Iteration :  18   Loss :  2.43370723643
Iteration :  19   Loss :  2.1888201867
Iteration :  20   Loss :  1.9685744193
Iteration :  21   Loss :  1.7704904532
Iteration :  22   Loss :  1.59233830032
Iteration :  23   Loss :  1.43211236078
Iteration :  24   Loss :  1.28800884427
Iteration :  25   Loss :  1.1584054634
Iteration :  26   Loss :  1.04184317025
Iteration :  27   Loss :  0.937009730789
Iteration :  28   Loss :  0.842724951953
Iteration :  29   Loss :  0.757927395317
Iteration :  30   Loss :  0.681662427629
Iteration :  31   Loss :  0.613071473748
Iteration :  32   Loss :  0.551382350985
Iteration :  33   Loss :  0.495900576027
Iteration :  34   Loss :  0.446001546594
Iteration :  35   Loss :  0.401123509793
Iteration :  36   Loss :  0.360761238021
Iteration :  37   Loss :  0.324460341219
Iteration :  38   Loss :  0.291812151443
Iteration :  39   Loss :  0.262449122163
Iteration :  40   Loss :  0.236040690504
Iteration :  41   Loss :  0.212289555837
Iteration :  42   Loss :  0.190928332827
Iteration :  43   Loss :  0.171716541271
Iteration :  44   Loss :  0.154437898815
Iteration :  45   Loss :  0.138897886096
Iteration :  46   Loss :  0.124921556885
Iteration :  47   Loss :  0.112351568575
Iteration :  48   Loss :  0.101046410851
Iteration :  49   Loss :  0.0908788126008
Iteration :  50   Loss :  0.0817343091176
Iteration :  51   Loss :  0.0735099534836
Iteration :  52   Loss :  0.066113157614
Iteration :  53   Loss :  0.0594606499195
Iteration :  54   Loss :  0.0534775378526
Iteration :  55   Loss :  0.048096464782
Iteration :  56   Loss :  0.0432568517068
Iteration :  57   Loss :  0.0389042152695
Iteration :  58   Loss :  0.0349895543946
Iteration :  59   Loss :  0.0314687986444
Iteration :  60   Loss :  0.028302312083
Iteration :  61   Loss :  0.0254544470634
Iteration :  62   Loss :  0.0228931429137
Iteration :  63   Loss :  329.086647555
Iteration :  64   Loss :  29.4923433199
Iteration :  65   Loss :  27.622340003
Iteration :  66   Loss :  10.3351767813
Iteration :  67   Loss :  9.29521975097
Iteration :  68   Loss :  8.3599063709
Iteration :  69   Loss :  7.51870707769
Iteration :  70   Loss :  6.76215182467
Iteration :  71   Loss :  6.08172347019
Iteration :  72   Loss :  5.46976189339
Iteration :  73   Loss :  4.91937775813
Iteration :  74   Loss :  4.42437495432
Iteration :  75   Loss :  3.97918084336
Iteration :  76   Loss :  3.57878352257
Iteration :  77   Loss :  3.21867540219
Iteration :  78   Loss :  2.89480245992
Iteration :  79   Loss :  2.60351860156
Iteration :  80   Loss :  2.34154461401
Iteration :  81   Loss :  2.10593124863
Iteration :  82   Loss :  1.89402601915
Iteration :  83   Loss :  1.7034433406
Iteration :  84   Loss :  1.53203767281
Iteration :  85   Loss :  1.37787936644
Iteration :  86   Loss :  1.23923293934
Iteration :  87   Loss :  1.11453753889
Iteration :  88   Loss :  1.00238937019
Iteration :  89   Loss :  0.9015258925
Iteration :  90   Loss :  0.810811605768
Iteration :  91   Loss :  0.729225267424
Iteration :  92   Loss :  0.655848395442
Iteration :  93   Loss :  0.589854928264
Iteration :  94   Loss :  0.530501925163
Iteration :  95   Loss :  0.477121202378
Iteration :  96   Loss :  0.429111810835
Iteration :  97   Loss :  0.385933270793
Iteration :  98   Loss :  0.34709948723
Iteration :  99   Loss :  0.312173277488
[-0.00247272 -0.00230771 -0.00303651 ...,  0.00332547 -0.00030604
  0.00010937]
CROSS VALIDATION 14
Iteration :  0   Loss :  185.58744746
Iteration :  1   Loss :  12.4583211574
Iteration :  2   Loss :  11.2047268602
Iteration :  3   Loss :  10.077273047
Iteration :  4   Loss :  9.06326707749
Iteration :  5   Loss :  8.15129348334
Iteration :  6   Loss :  7.33108545555
Iteration :  7   Loss :  6.59340926277
Iteration :  8   Loss :  5.92996029987
Iteration :  9   Loss :  5.33326959644
Iteration :  10   Loss :  4.79661973267
Iteration :  11   Loss :  4.31396921604
Iteration :  12   Loss :  3.87988446742
Iteration :  13   Loss :  3.48947865102
Iteration :  14   Loss :  3.13835665937
Iteration :  15   Loss :  2.82256563414
Iteration :  16   Loss :  2.53855046565
Iteration :  17   Loss :  2.28311377022
Iteration :  18   Loss :  2.05337989467
Iteration :  19   Loss :  1.84676254281
Iteration :  20   Loss :  1.66093565949
Iteration :  21   Loss :  1.49380724431
Iteration :  22   Loss :  1.34349580033
Iteration :  23   Loss :  1.2083091526
Iteration :  24   Loss :  1.08672539794
Iteration :  25   Loss :  0.977375771748
Iteration :  26   Loss :  0.879029238674
Iteration :  27   Loss :  0.790578633909
Iteration :  28   Loss :  0.711028198943
Iteration :  29   Loss :  0.63948237153
Iteration :  30   Loss :  0.575135703626
Iteration :  31   Loss :  0.517263793831
Iteration :  32   Loss :  0.465215132222
Iteration :  33   Loss :  0.418403765796
Iteration :  34   Loss :  0.376302701926
Iteration :  35   Loss :  0.338437975594
Iteration :  36   Loss :  0.304383313588
Iteration :  37   Loss :  0.307295691099
Iteration :  38   Loss :  23.724828248
Iteration :  39   Loss :  62.2906746434
Iteration :  40   Loss :  6.63617124429
Iteration :  41   Loss :  5.9684194403
Iteration :  42   Loss :  5.36785886078
Iteration :  43   Loss :  4.82772851967
Iteration :  44   Loss :  4.34194774194
Iteration :  45   Loss :  3.90504770866
Iteration :  46   Loss :  3.51210989013
Iteration :  47   Loss :  3.15871067413
Iteration :  48   Loss :  2.84087156582
Iteration :  49   Loss :  2.55501439861
Iteration :  50   Loss :  2.297921052
Iteration :  51   Loss :  2.06669722256
Iteration :  52   Loss :  1.85873984053
Iteration :  53   Loss :  1.67170776496
Iteration :  54   Loss :  1.50349542765
Iteration :  55   Loss :  1.3522091291
Iteration :  56   Loss :  1.21614571963
Iteration :  57   Loss :  1.09377342568
Iteration :  58   Loss :  0.983714605426
Iteration :  59   Loss :  0.884730239561
Iteration :  60   Loss :  0.795705982687
Iteration :  61   Loss :  0.715639618239
Iteration :  62   Loss :  0.643629775742
Iteration :  63   Loss :  0.578865783369
Iteration :  64   Loss :  0.520618541566
Iteration :  65   Loss :  0.468232315002
Iteration :  66   Loss :  0.421117350436
Iteration :  67   Loss :  0.378743237398
Iteration :  68   Loss :  0.340632936938
Iteration :  69   Loss :  0.306357411222
Iteration :  70   Loss :  0.275530793512
Iteration :  71   Loss :  0.247806044157
Iteration :  72   Loss :  0.222871043698
Iteration :  73   Loss :  0.200445079085
Iteration :  74   Loss :  0.180275683476
Iteration :  75   Loss :  0.162135794008
Iteration :  76   Loss :  0.145821195582
Iteration :  77   Loss :  0.131148221841
Iteration :  78   Loss :  0.117951687499
Iteration :  79   Loss :  0.106083028718
Iteration :  80   Loss :  0.095408630606
Iteration :  81   Loss :  0.0858083230103
Iteration :  82   Loss :  653.050146496
Iteration :  83   Loss :  176.523321146
Iteration :  84   Loss :  9.89011289682
Iteration :  85   Loss :  8.89493955282
Iteration :  86   Loss :  7.99990358794
Iteration :  87   Loss :  7.19492887346
Iteration :  88   Loss :  6.47095317151
Iteration :  89   Loss :  5.81982611425
Iteration :  90   Loss :  5.2342174487
Iteration :  91   Loss :  4.70753451434
Iteration :  92   Loss :  4.23384802426
Iteration :  93   Loss :  3.80782531449
Iteration :  94   Loss :  3.42467030998
Iteration :  95   Loss :  3.0800695314
Iteration :  96   Loss :  2.77014353487
Iteration :  97   Loss :  2.491403238
Iteration :  98   Loss :  2.24071064051
Iteration :  99   Loss :  2.01524349728
[-0.00215028  0.00118122  0.00335318 ...,  0.00682953 -0.00040276
  0.00010698]
CROSS VALIDATION 15
Iteration :  0   Loss :  185.58744746
Iteration :  1   Loss :  12.4583211574
Iteration :  2   Loss :  11.2047268602
Iteration :  3   Loss :  10.0818364114
Iteration :  4   Loss :  22.4323638754
Iteration :  5   Loss :  9.66241026259
Iteration :  6   Loss :  8.69014905259
Iteration :  7   Loss :  7.81571973285
Iteration :  8   Loss :  7.02927816
Iteration :  9   Loss :  6.3219707384
Iteration :  10   Loss :  5.68583474824
Iteration :  11   Loss :  5.11370870288
Iteration :  12   Loss :  4.59915172632
Iteration :  13   Loss :  4.13637104315
Iteration :  14   Loss :  3.72015676471
Iteration :  15   Loss :  3.34582323724
Iteration :  16   Loss :  3.00915629176
Iteration :  17   Loss :  2.70636580183
Iteration :  18   Loss :  2.43404301511
Iteration :  19   Loss :  2.1891221783
Iteration :  20   Loss :  1.96884602359
Iteration :  21   Loss :  1.77073472784
Iteration :  22   Loss :  1.5925579953
Iteration :  23   Loss :  1.43230994937
Iteration :  24   Loss :  1.2881865509
Iteration :  25   Loss :  1.15856528864
Iteration :  26   Loss :  1.04198691339
Iteration :  27   Loss :  0.937139010057
Iteration :  28   Loss :  0.842841222746
Iteration :  29   Loss :  0.758031966588
Iteration :  30   Loss :  0.681756476619
Iteration :  31   Loss :  0.613156059242
Iteration :  32   Loss :  0.551458425228
Iteration :  33   Loss :  0.495968995448
Iteration :  34   Loss :  0.446063081444
Iteration :  35   Loss :  0.401178852818
Iteration :  36   Loss :  0.360811012262
Iteration :  37   Loss :  0.324505107024
Iteration :  38   Loss :  0.291852412776
Iteration :  39   Loss :  0.262485332278
Iteration :  40   Loss :  0.236073257047
Iteration :  41   Loss :  0.212318845435
Iteration :  42   Loss :  0.190954675216
Iteration :  43   Loss :  0.171740233008
Iteration :  44   Loss :  0.154459206617
Iteration :  45   Loss :  0.138917049843
Iteration :  46   Loss :  0.124938792317
Iteration :  47   Loss :  0.112367069725
Iteration :  48   Loss :  0.101060352228
Iteration :  49   Loss :  0.0908913511539
Iteration :  50   Loss :  0.0817455860032
Iteration :  51   Loss :  0.0735200956546
Iteration :  52   Loss :  0.0661222792488
Iteration :  53   Loss :  0.0594688537076
Iteration :  54   Loss :  0.0534849161504
Iteration :  55   Loss :  0.0481031006531
Iteration :  56   Loss :  0.0432628198562
Iteration :  57   Loss :  0.0389095828855
Iteration :  58   Loss :  0.0349943819047
Iteration :  59   Loss :  0.0314731403957
Iteration :  60   Loss :  0.028306216954
Iteration :  61   Loss :  0.0254579590144
Iteration :  62   Loss :  0.0228963014815
Iteration :  63   Loss :  329.081521276
Iteration :  64   Loss :  29.4935993903
Iteration :  65   Loss :  27.6219550835
Iteration :  66   Loss :  10.3351828213
Iteration :  67   Loss :  9.29522518313
Iteration :  68   Loss :  8.35991125646
Iteration :  69   Loss :  7.51871147164
Iteration :  70   Loss :  6.76215577649
Iteration :  71   Loss :  6.08172702437
Iteration :  72   Loss :  5.46976508994
Iteration :  73   Loss :  4.91938063303
Iteration :  74   Loss :  4.42437753994
Iteration :  75   Loss :  3.97918316881
Iteration :  76   Loss :  3.57878561402
Iteration :  77   Loss :  3.21867728319
Iteration :  78   Loss :  2.89480415166
Iteration :  79   Loss :  2.60352012306
Iteration :  80   Loss :  2.34154598242
Iteration :  81   Loss :  2.10593247934
Iteration :  82   Loss :  1.89402712603
Iteration :  83   Loss :  1.7034443361
Iteration :  84   Loss :  1.53203856814
Iteration :  85   Loss :  1.37788017168
Iteration :  86   Loss :  1.23923366356
Iteration :  87   Loss :  1.11453819023
Iteration :  88   Loss :  1.00238995599
Iteration :  89   Loss :  0.901526419355
Iteration :  90   Loss :  0.810812079609
Iteration :  91   Loss :  0.729225693585
Iteration :  92   Loss :  0.655848778722
Iteration :  93   Loss :  0.589855272977
Iteration :  94   Loss :  0.53050223519
Iteration :  95   Loss :  0.477121481209
Iteration :  96   Loss :  0.42911206161
Iteration :  97   Loss :  0.385933496334
Iteration :  98   Loss :  0.347099690076
Iteration :  99   Loss :  0.312173459923
[-0.00247276 -0.00230773 -0.00303653 ...,  0.00332549 -0.00030602
  0.00010937]
CROSS VALIDATION 16
Iteration :  0   Loss :  185.58744746
Iteration :  1   Loss :  11.3462637399
Iteration :  2   Loss :  10.2045680539
Iteration :  3   Loss :  9.17775327231
Iteration :  4   Loss :  8.25425972783
Iteration :  5   Loss :  7.42369092227
Iteration :  6   Loss :  6.6766964848
Iteration :  7   Loss :  6.00486690743
Iteration :  8   Loss :  5.40063887253
Iteration :  9   Loss :  4.8572101066
Iteration :  10   Loss :  4.36846280162
Iteration :  11   Loss :  3.9288947421
Iteration :  12   Loss :  3.53355736229
Iteration :  13   Loss :  3.17800003619
Iteration :  14   Loss :  2.85821997339
Iteration :  15   Loss :  2.57061715647
Iteration :  16   Loss :  2.31195381274
Iteration :  17   Loss :  2.07931796409
Iteration :  18   Loss :  1.87009064453
Iteration :  19   Loss :  1.68191641642
Iteration :  20   Loss :  1.51267685344
Iteration :  21   Loss :  1.3604666918
Iteration :  22   Loss :  1.22357238117
Iteration :  23   Loss :  1.10045279387
Iteration :  24   Loss :  0.989721875207
Iteration :  25   Loss :  0.890133039523
Iteration :  26   Loss :  0.800565136428
Iteration :  27   Loss :  0.7200098291
Iteration :  28   Loss :  0.647560242649
Iteration :  29   Loss :  0.5824007547
Iteration :  30   Loss :  0.523797813293
Iteration :  31   Loss :  0.471091678704
Iteration :  32   Loss :  0.423688996236
Iteration :  33   Loss :  0.381056116349
Iteration :  34   Loss :  0.34271308695
Iteration :  35   Loss :  0.308228250192
Iteration :  36   Loss :  0.277213382954
Iteration :  37   Loss :  0.249319326315
Iteration :  38   Loss :  0.224232054786
Iteration :  39   Loss :  0.201669141084
Iteration :  40   Loss :  0.18137657662
Iteration :  41   Loss :  0.163125911925
Iteration :  42   Loss :  0.146711684812
Iteration :  43   Loss :  0.131949107327
Iteration :  44   Loss :  0.118671985443
Iteration :  45   Loss :  626.52148854
Iteration :  46   Loss :  54.3991659609
Iteration :  47   Loss :  52.8406984589
Iteration :  48   Loss :  12.6040026048
Iteration :  49   Loss :  8.68732563127
Iteration :  50   Loss :  7.8131804128
Iteration :  51   Loss :  7.02699435408
Iteration :  52   Loss :  6.319916736
Iteration :  53   Loss :  5.68398742584
Iteration :  54   Loss :  5.11204726369
Iteration :  55   Loss :  4.59765746621
Iteration :  56   Loss :  4.13502714005
Iteration :  57   Loss :  3.71894808925
Iteration :  58   Loss :  3.34473618239
Iteration :  59   Loss :  3.00817861969
Iteration :  60   Loss :  2.70548650611
Iteration :  61   Loss :  2.4332521968
Iteration :  62   Loss :  2.18841093455
Iteration :  63   Loss :  1.96820634735
Iteration :  64   Loss :  1.77015941778
Iteration :  65   Loss :  1.59204057469
Iteration :  66   Loss :  1.4318445932
Iteration :  67   Loss :  1.28776802029
Iteration :  68   Loss :  1.15818887186
Iteration :  69   Loss :  1.04164837282
Iteration :  70   Loss :  0.936834534466
Iteration :  71   Loss :  0.842567384418
Iteration :  72   Loss :  0.757785682709
Iteration :  73   Loss :  0.681534974576
Iteration :  74   Loss :  0.612956845411
Iteration :  75   Loss :  0.5512792569
Iteration :  76   Loss :  0.495807855584
Iteration :  77   Loss :  0.445918155964
Iteration :  78   Loss :  0.401048510183
Iteration :  79   Loss :  0.360693785101
Iteration :  80   Loss :  0.324399675618
Iteration :  81   Loss :  0.291757590199
Iteration :  82   Loss :  0.262400051038
Iteration :  83   Loss :  0.235996557066
Iteration :  84   Loss :  0.21224986324
Iteration :  85   Loss :  0.190892634221
Iteration :  86   Loss :  0.171684434767
Iteration :  87   Loss :  0.154409022965
Iteration :  88   Loss :  0.138871915823
Iteration :  89   Loss :  0.12489819982
Iteration :  90   Loss :  0.112330561769
Iteration :  91   Loss :  0.101027517814
Iteration :  92   Loss :  0.0908618206387
Iteration :  93   Loss :  0.0817190269386
Iteration :  94   Loss :  0.0734962090441
Iteration :  95   Loss :  0.0661007961819
Iteration :  96   Loss :  0.0594495323325
Iteration :  97   Loss :  0.0534675389511
Iteration :  98   Loss :  0.0480874720005
Iteration :  99   Loss :  0.0432487638064
[-0.00030949 -0.00046105 -0.00054635 ...,  0.00073909  0.00012872
  0.0003749 ]
CROSS VALIDATION 17
Iteration :  0   Loss :  185.58744746
Iteration :  1   Loss :  12.4597869151
Iteration :  2   Loss :  11.2060451288
Iteration :  3   Loss :  10.1101073263
Iteration :  4   Loss :  22.4732146455
Iteration :  5   Loss :  9.66331099083
Iteration :  6   Loss :  8.69095914681
Iteration :  7   Loss :  7.81644831292
Iteration :  8   Loss :  7.02993342811
Iteration :  9   Loss :  6.32256007144
Iteration :  10   Loss :  5.68636478079
Iteration :  11   Loss :  5.11418540194
Iteration :  12   Loss :  4.59958045846
Iteration :  13   Loss :  4.13675663495
Iteration :  14   Loss :  3.72050355709
Iteration :  15   Loss :  3.34613513431
Iteration :  16   Loss :  3.0094368048
Iteration :  17   Loss :  2.70661808879
Iteration :  18   Loss :  2.43426991618
Iteration :  19   Loss :  2.1893262479
Iteration :  20   Loss :  1.96902955908
Iteration :  21   Loss :  1.77089979543
Iteration :  22   Loss :  1.59270645328
Iteration :  23   Loss :  1.43244346906
Iteration :  24   Loss :  1.28830663543
Iteration :  25   Loss :  1.1586732899
Iteration :  26   Loss :  1.04208404724
Iteration :  27   Loss :  0.937226369996
Iteration :  28   Loss :  0.842919792261
Iteration :  29   Loss :  0.758102630199
Iteration :  30   Loss :  0.681820029842
Iteration :  31   Loss :  0.613213217546
Iteration :  32   Loss :  0.551509832089
Iteration :  33   Loss :  0.496015229593
Iteration :  34   Loss :  0.446104663368
Iteration :  35   Loss :  0.401216250642
Iteration :  36   Loss :  0.360844647003
Iteration :  37   Loss :  0.324535357335
Iteration :  38   Loss :  0.291879619208
Iteration :  39   Loss :  0.262509801116
Iteration :  40   Loss :  0.236095263756
Iteration :  41   Loss :  0.212338637761
Iteration :  42   Loss :  0.190972475979
Iteration :  43   Loss :  0.171756242604
Iteration :  44   Loss :  0.154473605278
Iteration :  45   Loss :  0.138929999666
Iteration :  46   Loss :  0.12495043909
Iteration :  47   Loss :  0.112377544564
Iteration :  48   Loss :  0.101069773057
Iteration :  49   Loss :  0.0908998240302
Iteration :  50   Loss :  0.0817532063129
Iteration :  51   Loss :  0.0735269491854
Iteration :  52   Loss :  0.0661284431564
Iteration :  53   Loss :  0.059474397384
Iteration :  54   Loss :  0.0534899020052
Iteration :  55   Loss :  0.0481075848159
Iteration :  56   Loss :  0.0432668528089
Iteration :  57   Loss :  0.0389132100302
Iteration :  58   Loss :  0.034997644075
Iteration :  59   Loss :  0.0314760743165
Iteration :  60   Loss :  0.0283088556547
Iteration :  61   Loss :  0.025460332201
Iteration :  62   Loss :  0.0228984358708
Iteration :  63   Loss :  329.088216956
Iteration :  64   Loss :  29.4389583287
Iteration :  65   Loss :  27.7372737164
Iteration :  66   Loss :  10.3364647568
Iteration :  67   Loss :  9.29637812641
Iteration :  68   Loss :  8.36094818707
Iteration :  69   Loss :  7.51964406314
Iteration :  70   Loss :  6.76299452778
Iteration :  71   Loss :  6.08248137795
Iteration :  72   Loss :  5.47044353815
Iteration :  73   Loss :  4.91999081372
Iteration :  74   Loss :  4.42492632238
Iteration :  75   Loss :  3.97967673109
Iteration :  76   Loss :  3.57922951255
Iteration :  77   Loss :  3.2190765153
Iteration :  78   Loss :  2.89516321181
Iteration :  79   Loss :  2.60384305349
Iteration :  80   Loss :  2.3418364186
Iteration :  81   Loss :  2.10619369095
Iteration :  82   Loss :  1.89426205373
Iteration :  83   Loss :  1.70365562465
Iteration :  84   Loss :  1.5322285962
Iteration :  85   Loss :  1.37805107853
Iteration :  86   Loss :  1.23938737324
Iteration :  87   Loss :  1.11467643317
Iteration :  88   Loss :  1.00251428851
Iteration :  89   Loss :  0.901638241157
Iteration :  90   Loss :  0.81091264956
Iteration :  91   Loss :  0.729316143881
Iteration :  92   Loss :  0.655930127633
Iteration :  93   Loss :  0.589928436312
Iteration :  94   Loss :  0.530568036607
Iteration :  95   Loss :  0.477180661487
Iteration :  96   Loss :  0.429165286987
Iteration :  97   Loss :  0.385981366011
Iteration :  98   Loss :  0.34714274296
Iteration :  99   Loss :  0.312212180695
[-0.00247344 -0.00230777 -0.00303675 ...,  0.00332552 -0.00030602
  0.00010921]
CROSS VALIDATION 18
Iteration :  0   Loss :  23.2142383478
Iteration :  1   Loss :  16.8753612899
Iteration :  2   Loss :  14.0703254704
Iteration :  3   Loss :  11.9617227784
Iteration :  4   Loss :  10.7580977256
Iteration :  5   Loss :  9.67558509909
Iteration :  6   Loss :  8.70199819684
Iteration :  7   Loss :  7.82637658004
Iteration :  8   Loss :  7.0388626827
Iteration :  9   Loss :  6.33059083717
Iteration :  10   Loss :  5.69358746637
Iteration :  11   Loss :  5.12068131885
Iteration :  12   Loss :  4.60542273639
Iteration :  13   Loss :  4.14201104504
Iteration :  14   Loss :  3.7252292524
Iteration :  15   Loss :  3.35038531574
Iteration :  16   Loss :  3.01325932
Iteration :  17   Loss :  2.71005597085
Iteration :  18   Loss :  2.43736186805
Iteration :  19   Loss :  2.19210707814
Iteration :  20   Loss :  1.97153057369
Iteration :  21   Loss :  1.77314915032
Iteration :  22   Loss :  1.59472947122
Iteration :  23   Loss :  1.43426292476
Iteration :  24   Loss :  1.28994301194
Iteration :  25   Loss :  1.16014500921
Iteration :  26   Loss :  1.04340767766
Iteration :  27   Loss :  0.938416812686
Iteration :  28   Loss :  0.868023674178
Iteration :  29   Loss :  10.0364813543
Iteration :  30   Loss :  3.82697690603
Iteration :  31   Loss :  3.44189480993
Iteration :  32   Loss :  3.0955608496
Iteration :  33   Loss :  2.78407606936
Iteration :  34   Loss :  2.50393383834
Iteration :  35   Loss :  2.2519803736
Iteration :  36   Loss :  2.02537923543
Iteration :  37   Loss :  1.82157939537
Iteration :  38   Loss :  1.6382865172
Iteration :  39   Loss :  1.47343712784
Iteration :  40   Loss :  1.32517538716
Iteration :  41   Loss :  1.19183219531
Iteration :  42   Loss :  1.07190640238
Iteration :  43   Loss :  0.964047908746
Iteration :  44   Loss :  0.867042465924
Iteration :  45   Loss :  0.779798006816
Iteration :  46   Loss :  0.701332351451
Iteration :  47   Loss :  0.630762149803
Iteration :  48   Loss :  0.567292937223
Iteration :  49   Loss :  0.510210190519
Iteration :  50   Loss :  0.458871284004
Iteration :  51   Loss :  0.412698254946
Iteration :  52   Loss :  0.371171296991
Iteration :  53   Loss :  0.333822908285
Iteration :  54   Loss :  0.300232628436
Iteration :  55   Loss :  0.270022305062
Iteration :  56   Loss :  0.242851836627
Iteration :  57   Loss :  0.218415343649
Iteration :  58   Loss :  0.196437725174
Iteration :  59   Loss :  0.176671561746
Iteration :  60   Loss :  0.158894330007
Iteration :  61   Loss :  0.142905897581
Iteration :  62   Loss :  0.128526270022
Iteration :  63   Loss :  0.115593564474
Iteration :  64   Loss :  0.103962187229
Iteration :  65   Loss :  0.0935011946603
Iteration :  66   Loss :  0.0840928190908
Iteration :  67   Loss :  0.0756311429852
Iteration :  68   Loss :  0.0680209065542
Iteration :  69   Loss :  0.061176435339
Iteration :  70   Loss :  0.0550206757066
Iteration :  71   Loss :  0.0494843273956
Iteration :  72   Loss :  0.0445050633484
Iteration :  73   Loss :  0.0400268280462
Iteration :  74   Loss :  376.088675744
Iteration :  75   Loss :  165.889975407
Iteration :  76   Loss :  28.298751886
Iteration :  77   Loss :  9.36422477649
Iteration :  78   Loss :  8.42196790016
Iteration :  79   Loss :  7.57452378646
Iteration :  80   Loss :  6.81235208585
Iteration :  81   Loss :  6.12687242789
Iteration :  82   Loss :  5.51036782517
Iteration :  83   Loss :  4.95589779713
Iteration :  84   Loss :  4.45722023553
Iteration :  85   Loss :  4.00872113214
Iteration :  86   Loss :  3.60535137733
Iteration :  87   Loss :  3.24256991832
Iteration :  88   Loss :  2.91629263692
Iteration :  89   Loss :  2.62284637136
Iteration :  90   Loss :  2.35892756462
Iteration :  91   Loss :  2.1215650737
Iteration :  92   Loss :  1.90808672103
Iteration :  93   Loss :  1.71608921174
Iteration :  94   Loss :  1.5434110778
Iteration :  95   Loss :  1.38810834471
Iteration :  96   Loss :  1.2484326466
Iteration :  97   Loss :  1.12281154352
Iteration :  98   Loss :  1.00983081923
Iteration :  99   Loss :  0.908218560225
[ -1.80660763e-03  -3.86683913e-03  -8.87596024e-04 ...,   4.81113240e-03
   1.10853941e-03  -4.83001436e-05]
CROSS VALIDATION 19
Iteration :  0   Loss :  204.154935237
Iteration :  1   Loss :  64.8756701824
Iteration :  2   Loss :  11.0432161589
Iteration :  3   Loss :  9.9320140454
Iteration :  4   Loss :  8.93262447996
Iteration :  5   Loss :  8.03379654269
Iteration :  6   Loss :  7.22541141567
Iteration :  7   Loss :  6.49836846729
Iteration :  8   Loss :  5.84448279929
Iteration :  9   Loss :  5.25639310285
Iteration :  10   Loss :  4.72747878649
Iteration :  11   Loss :  4.25178544287
Iteration :  12   Loss :  3.82395781529
Iteration :  13   Loss :  3.43917950932
Iteration :  14   Loss :  3.09311877082
Iteration :  15   Loss :  2.78187972
Iteration :  16   Loss :  2.50195849236
Iteration :  17   Loss :  2.25020379296
Iteration :  18   Loss :  2.02378141976
Iteration :  19   Loss :  1.82014235679
Iteration :  20   Loss :  1.63699407783
Iteration :  21   Loss :  1.47227473766
Iteration :  22   Loss :  1.32412996022
Iteration :  23   Loss :  1.19089196242
Iteration :  24   Loss :  1.0710607786
Iteration :  25   Loss :  0.963287374209
Iteration :  26   Loss :  0.866358458693
Iteration :  27   Loss :  0.77918282648
Iteration :  28   Loss :  0.700779072438
Iteration :  29   Loss :  0.630264543413
Iteration :  30   Loss :  0.566845401507
Iteration :  31   Loss :  0.509807687213
Iteration :  32   Loss :  0.458509281808
Iteration :  33   Loss :  0.412372678516
Iteration :  34   Loss :  0.370878481055
Iteration :  35   Loss :  0.333559556382
Iteration :  36   Loss :  0.299995775805
Iteration :  37   Loss :  0.269809285266
Iteration :  38   Loss :  0.242660251533
Iteration :  39   Loss :  0.218243036432
Iteration :  40   Loss :  0.196282756035
Iteration :  41   Loss :  0.176532186074
Iteration :  42   Loss :  0.158768978741
Iteration :  43   Loss :  0.142793159542
Iteration :  44   Loss :  0.128424876028
Iteration :  45   Loss :  0.115502373054
Iteration :  46   Loss :  0.103880171767
Iteration :  47   Loss :  0.093427431845
Iteration :  48   Loss :  0.0840264785152
Iteration :  49   Loss :  0.0755714778007
Iteration :  50   Loss :  0.0679672450626
Iteration :  51   Loss :  0.0611281734305
Iteration :  52   Loss :  0.0549772700586
Iteration :  53   Loss :  0.0494452893564
Iteration :  54   Loss :  0.0444699534359
Iteration :  55   Loss :  0.0399952510002
Iteration :  56   Loss :  0.0359708067803
Iteration :  57   Loss :  0.0323513144203
Iteration :  58   Loss :  0.0290960264281
Iteration :  59   Loss :  0.0261682954487
Iteration :  60   Loss :  0.0235351616959
Iteration :  61   Loss :  0.0211669818974
Iteration :  62   Loss :  0.0190370955779
Iteration :  63   Loss :  0.0171215249203
Iteration :  64   Loss :  0.0153987048284
Iteration :  65   Loss :  0.0138492401522
Iteration :  66   Loss :  0.0124556873407
Iteration :  67   Loss :  0.0112023580661
Iteration :  68   Loss :  186.859397036
Iteration :  69   Loss :  67.3075748445
Iteration :  70   Loss :  99.2122514373
Iteration :  71   Loss :  9.52257247129
Iteration :  72   Loss :  8.56438216663
Iteration :  73   Loss :  7.7026078948
Iteration :  74   Loss :  6.9275479803
Iteration :  75   Loss :  6.23047695986
Iteration :  76   Loss :  5.60354735294
Iteration :  77   Loss :  5.03970131643
Iteration :  78   Loss :  4.53259118896
Iteration :  79   Loss :  4.0765080302
Iteration :  80   Loss :  3.66631735082
Iteration :  81   Loss :  3.2974013095
Iteration :  82   Loss :  2.96560672617
Iteration :  83   Loss :  2.66719832644
Iteration :  84   Loss :  2.39881669063
Iteration :  85   Loss :  2.1574404341
Iteration :  86   Loss :  1.94035219318
Iteration :  87   Loss :  1.74510803361
Iteration :  88   Loss :  1.56950993725
Iteration :  89   Loss :  1.41158105727
Iteration :  90   Loss :  1.26954346319
Iteration :  91   Loss :  1.14179812531
Iteration :  92   Loss :  1.02690691321
Iteration :  93   Loss :  0.923576405516
Iteration :  94   Loss :  0.830643328866
Iteration :  95   Loss :  0.747061462018
Iteration :  96   Loss :  0.671889857701
Iteration :  97   Loss :  0.604282249632
Iteration :  98   Loss :  0.543477525424
Iteration :  99   Loss :  0.488791158139
[ -3.67307873e-03  -3.69062495e-03  -2.76260666e-03 ...,   3.17876985e-03
  -1.73325302e-04   2.74165135e-05]
Accuracy (Hinge Loss):	0.75
lmda : 1  eta : 0.0001
Logistic Loss
CROSS VALIDATION 0
Iteration :  0   Loss :  6.11018664174
Iteration :  1   Loss :  0.207425270189
Iteration :  2   Loss :  0.201215635358
Iteration :  3   Loss :  0.196519396011
Iteration :  4   Loss :  0.192202256647
Iteration :  5   Loss :  0.188093819894
Iteration :  6   Loss :  0.184135440437
Iteration :  7   Loss :  0.180300001798
Iteration :  8   Loss :  0.176572519273
Iteration :  9   Loss :  0.172943592021
Iteration :  10   Loss :  0.169406719155
Iteration :  11   Loss :  0.165957045627
Iteration :  12   Loss :  0.162590717417
Iteration :  13   Loss :  0.159304524771
Iteration :  14   Loss :  0.156095693004
Iteration :  15   Loss :  0.15296175391
Iteration :  16   Loss :  0.14990046357
Iteration :  17   Loss :  0.146909748044
Iteration :  18   Loss :  0.143987666458
Iteration :  19   Loss :  0.141132385243
Iteration :  20   Loss :  0.13834215974
Iteration :  21   Loss :  0.135615320736
Iteration :  22   Loss :  0.132950264382
Iteration :  23   Loss :  0.130345444462
Iteration :  24   Loss :  0.127799366303
Iteration :  25   Loss :  0.125310581886
Iteration :  26   Loss :  0.122877685804
Iteration :  27   Loss :  0.120499311882
Iteration :  28   Loss :  0.118174130299
Iteration :  29   Loss :  0.115900845105
Iteration :  30   Loss :  0.113678192095
Iteration :  31   Loss :  0.111504936964
Iteration :  32   Loss :  0.109379873743
Iteration :  33   Loss :  0.107301823468
Iteration :  34   Loss :  0.105269633101
Iteration :  35   Loss :  0.103282174665
Iteration :  36   Loss :  0.101338344607
Iteration :  37   Loss :  0.0994370633761
Iteration :  38   Loss :  0.0975772752197
Iteration :  39   Loss :  0.0957579481856
Iteration :  40   Loss :  0.0939780743317
Iteration :  41   Loss :  0.0922366701227
Iteration :  42   Loss :  0.0905327769953
Iteration :  43   Loss :  0.0888654620617
Iteration :  44   Loss :  0.0872338189099
Iteration :  45   Loss :  0.0856369684542
Iteration :  46   Loss :  0.0840740597769
Iteration :  47   Loss :  0.0825442709062
Iteration :  48   Loss :  0.0810468094673
Iteration :  49   Loss :  0.0795809131568
Iteration :  50   Loss :  0.0781458499939
Iteration :  51   Loss :  0.0767409183176
Iteration :  52   Loss :  0.0753654465145
Iteration :  53   Loss :  0.074018792475
Iteration :  54   Loss :  0.0727003427957
Iteration :  55   Loss :  0.0714095117561
Iteration :  56   Loss :  0.0701457401094
Iteration :  57   Loss :  0.0689084937349
Iteration :  58   Loss :  0.0676972622021
Iteration :  59   Loss :  0.0665115572956
Iteration :  60   Loss :  0.0653509115451
Iteration :  61   Loss :  0.0642148767985
Iteration :  62   Loss :  0.0631030228657
Iteration :  63   Loss :  0.0620149362504
Iteration :  64   Loss :  0.0609502189775
Iteration :  65   Loss :  0.0599084875101
Iteration :  66   Loss :  0.0588893717469
Iteration :  67   Loss :  0.0578925140756
Iteration :  68   Loss :  0.0569175684599
Iteration :  69   Loss :  0.0559641995302
Iteration :  70   Loss :  0.055032081648
Iteration :  71   Loss :  0.0541208979166
Iteration :  72   Loss :  0.053230339114
Iteration :  73   Loss :  0.0523601025276
Iteration :  74   Loss :  0.0515098906812
Iteration :  75   Loss :  0.0506794099483
Iteration :  76   Loss :  0.0498683690612
Iteration :  77   Loss :  0.0490764775289
Iteration :  78   Loss :  0.048303443993
Iteration :  79   Loss :  0.0475489745575
Iteration :  80   Loss :  0.0468127711371
Iteration :  81   Loss :  0.0460945298777
Iteration :  82   Loss :  0.0453939397068
Iteration :  83   Loss :  0.0447106810722
Iteration :  84   Loss :  0.0440444249292
Iteration :  85   Loss :  0.0433948320279
Iteration :  86   Loss :  0.0427615525447
Iteration :  87   Loss :  0.0421442260911
Iteration :  88   Loss :  0.0415424821147
Iteration :  89   Loss :  0.0409559406931
Iteration :  90   Loss :  0.0403842137027
Iteration :  91   Loss :  0.0398269063273
Iteration :  92   Loss :  0.0392836188586
Iteration :  93   Loss :  0.0387539487281
Iteration :  94   Loss :  0.0382374927039
Iteration :  95   Loss :  0.0377338491827
Iteration :  96   Loss :  0.0372426205091
Iteration :  97   Loss :  0.0367634152611
Iteration :  98   Loss :  0.0362958504471
Iteration :  99   Loss :  0.0358395535709
[ -1.75691859e-04   2.42480868e-04   8.49903325e-05 ...,   2.24590483e-04
   1.86298667e-05   1.09732468e-04]
CROSS VALIDATION 1
Iteration :  0   Loss :  9.18373492998
Iteration :  1   Loss :  0.21300775046
Iteration :  2   Loss :  0.201131675652
Iteration :  3   Loss :  0.195456261364
Iteration :  4   Loss :  0.190614169893
Iteration :  5   Loss :  0.186175889736
Iteration :  6   Loss :  0.181993292565
Iteration :  7   Loss :  0.177997880197
Iteration :  8   Loss :  0.174152368914
Iteration :  9   Loss :  0.170434130072
Iteration :  10   Loss :  0.16682828127
Iteration :  11   Loss :  0.163324398611
Iteration :  12   Loss :  0.159914796394
Iteration :  13   Loss :  0.156593559489
Iteration :  14   Loss :  0.153355967024
Iteration :  15   Loss :  0.150198132753
Iteration :  16   Loss :  0.147116771749
Iteration :  17   Loss :  0.14410904395
Iteration :  18   Loss :  0.141172446221
Iteration :  19   Loss :  0.138304735985
Iteration :  20   Loss :  0.135503875999
Iteration :  21   Loss :  0.13276799361
Iteration :  22   Loss :  0.130095350154
Iteration :  23   Loss :  0.127484317617
Iteration :  24   Loss :  0.124933360546
Iteration :  25   Loss :  0.122441021866
Iteration :  26   Loss :  0.120005911606
Iteration :  27   Loss :  0.117626697852
Iteration :  28   Loss :  0.115302099385
Iteration :  29   Loss :  0.113030879655
Iteration :  30   Loss :  0.110811841764
Iteration :  31   Loss :  0.108643824248
Iteration :  32   Loss :  0.106525697467
Iteration :  33   Loss :  0.10445636045
Iteration :  34   Loss :  0.10243473809
Iteration :  35   Loss :  0.100459778577
Iteration :  36   Loss :  0.0985304510117
Iteration :  37   Loss :  0.0966457431688
Iteration :  38   Loss :  0.0948046593783
Iteration :  39   Loss :  0.0930062185576
Iteration :  40   Loss :  0.0912494524261
Iteration :  41   Loss :  0.0895334039604
Iteration :  42   Loss :  0.0878571261591
Iteration :  43   Loss :  0.0862196811832
Iteration :  44   Loss :  0.084620139929
Iteration :  45   Loss :  0.0830575820676
Iteration :  46   Loss :  0.0815310965585
Iteration :  47   Loss :  0.080039782609
Iteration :  48   Loss :  0.0785827510221
Iteration :  49   Loss :  0.0771591258406
Iteration :  50   Loss :  0.0757680461797
Iteration :  51   Loss :  0.0744086681244
Iteration :  52   Loss :  0.0730801665705
Iteration :  53   Loss :  0.0717817368962
Iteration :  54   Loss :  0.0705125963723
Iteration :  55   Loss :  0.0692719852423
Iteration :  56   Loss :  0.068059167434
Iteration :  57   Loss :  0.0668734308896
Iteration :  58   Loss :  0.0657140875285
Iteration :  59   Loss :  0.0645804728778
Iteration :  60   Loss :  0.0634719454169
Iteration :  61   Loss :  0.0623878856984
Iteration :  62   Loss :  0.0613276953023
Iteration :  63   Loss :  0.0602907956889
Iteration :  64   Loss :  0.0592766270025
Iteration :  65   Loss :  0.0582846468776
Iteration :  66   Loss :  0.0573143292862
Iteration :  67   Loss :  0.0563651634596
Iteration :  68   Loss :  0.0554366529052
Iteration :  69   Loss :  0.0545283145354
Iteration :  70   Loss :  0.0536396779145
Iteration :  71   Loss :  0.0527702846272
Iteration :  72   Loss :  0.051919687764
Iteration :  73   Loss :  0.05108745152
Iteration :  74   Loss :  0.0502731508958
Iteration :  75   Loss :  0.0494763714923
Iteration :  76   Loss :  0.0486967093877
Iteration :  77   Loss :  0.0479337710835
Iteration :  78   Loss :  0.0471871735103
Iteration :  79   Loss :  0.0464565440783
Iteration :  80   Loss :  0.0457415207629
Iteration :  81   Loss :  0.0450417522119
Iteration :  82   Loss :  0.0443568978645
Iteration :  83   Loss :  0.0436866280692
Iteration :  84   Loss :  0.0430306241916
Iteration :  85   Loss :  0.0423885787016
Iteration :  86   Loss :  0.0417601952307
Iteration :  87   Loss :  0.041145188593
Iteration :  88   Loss :  0.0405432847626
Iteration :  89   Loss :  0.0399542208037
Iteration :  90   Loss :  0.0393777447493
Iteration :  91   Loss :  0.0388136154286
Iteration :  92   Loss :  0.0382616022436
Iteration :  93   Loss :  0.0377214848966
Iteration :  94   Loss :  0.0371930530748
Iteration :  95   Loss :  0.0366761060952
Iteration :  96   Loss :  0.0361704525199
Iteration :  97   Loss :  0.0356759097482
Iteration :  98   Loss :  0.0351923035953
Iteration :  99   Loss :  0.0347194678683
[ -4.18643517e-04   1.47717833e-04  -1.68762327e-04 ...,   1.22388490e-04
  -4.88964670e-05   1.45264756e-04]
CROSS VALIDATION 2
Iteration :  0   Loss :  2.77431133292
Iteration :  1   Loss :  0.20799684412
Iteration :  2   Loss :  0.202685413698
Iteration :  3   Loss :  0.197997770506
Iteration :  4   Loss :  0.193604362664
Iteration :  5   Loss :  0.189401553811
Iteration :  6   Loss :  0.18534436465
Iteration :  7   Loss :  0.181409353118
Iteration :  8   Loss :  0.177582657547
Iteration :  9   Loss :  0.173855253473
Iteration :  10   Loss :  0.17022079335
Iteration :  11   Loss :  0.16667451926
Iteration :  12   Loss :  0.163212672248
Iteration :  13   Loss :  0.159832151399
Iteration :  14   Loss :  0.156530307026
Iteration :  15   Loss :  0.153304809885
Iteration :  16   Loss :  0.150153565435
Iteration :  17   Loss :  0.147074655844
Iteration :  18   Loss :  0.144066299615
Iteration :  19   Loss :  0.141126822759
Iteration :  20   Loss :  0.138254637684
Iteration :  21   Loss :  0.135448227386
Iteration :  22   Loss :  0.132706133338
Iteration :  23   Loss :  0.130026946002
Iteration :  24   Loss :  0.127409297265
Iteration :  25   Loss :  0.124851854271
Iteration :  26   Loss :  0.122353314331
Iteration :  27   Loss :  0.119912400654
Iteration :  28   Loss :  0.117527858741
Iteration :  29   Loss :  0.115198453335
Iteration :  30   Loss :  0.112922965848
Iteration :  31   Loss :  0.110700192233
Iteration :  32   Loss :  0.108528941266
Iteration :  33   Loss :  0.106408033242
Iteration :  34   Loss :  0.104336299083
Iteration :  35   Loss :  0.102312579853
Iteration :  36   Loss :  0.100335726694
Iteration :  37   Loss :  0.0984046011671
Iteration :  38   Loss :  0.0965180759929
Iteration :  39   Loss :  0.0946750361594
Iteration :  40   Loss :  0.092874380363
Iteration :  41   Loss :  0.0911150227314
Iteration :  42   Loss :  0.0893958947658
Iteration :  43   Loss :  0.0877159474327
Iteration :  44   Loss :  0.0860741533308
Iteration :  45   Loss :  0.0844695088552
Iteration :  46   Loss :  0.082901036284
Iteration :  47   Loss :  0.0813677857202
Iteration :  48   Loss :  0.079868836827
Iteration :  49   Loss :  0.0784033003122
Iteration :  50   Loss :  0.0769703191257
Iteration :  51   Loss :  0.075569069352
Iteration :  52   Loss :  0.0741987607925
Iteration :  53   Loss :  0.0728586372447
Iteration :  54   Loss :  0.0715479764977
Iteration :  55   Loss :  0.0702660900692
Iteration :  56   Loss :  0.0690123227152
Iteration :  57   Loss :  0.0677860517445
Iteration :  58   Loss :  0.0665866861669
Iteration :  59   Loss :  0.0654136657012
Iteration :  60   Loss :  0.0642664596579
Iteration :  61   Loss :  0.0631445657084
Iteration :  62   Loss :  0.0620475085374
Iteration :  63   Loss :  0.0609748383712
Iteration :  64   Loss :  0.0599261293668
Iteration :  65   Loss :  0.0589009778409
Iteration :  66   Loss :  0.0578990003202
Iteration :  67   Loss :  0.0569198313975
Iteration :  68   Loss :  0.0559631213844
Iteration :  69   Loss :  0.0550285337687
Iteration :  70   Loss :  0.0541157424984
Iteration :  71   Loss :  0.0532244291366
Iteration :  72   Loss :  0.0523542799533
Iteration :  73   Loss :  0.051504983042
Iteration :  74   Loss :  0.0506762255685
Iteration :  75   Loss :  0.0498676912682
Iteration :  76   Loss :  0.0490790583169
Iteration :  77   Loss :  0.0483099976862
Iteration :  78   Loss :  0.0475601720789
Iteration :  79   Loss :  0.046829235504
Iteration :  80   Loss :  0.0461168335093
Iteration :  81   Loss :  0.0454226040433
Iteration :  82   Loss :  0.0447461788671
Iteration :  83   Loss :  0.0440871853995
Iteration :  84   Loss :  0.0434452488434
Iteration :  85   Loss :  0.0428199944348
Iteration :  86   Loss :  0.0422110496557
Iteration :  87   Loss :  0.0416180462775
Iteration :  88   Loss :  0.0410406221364
Iteration :  89   Loss :  0.040478422588
Iteration :  90   Loss :  0.0399311016341
Iteration :  91   Loss :  0.0393983227564
Iteration :  92   Loss :  0.0388797595243
Iteration :  93   Loss :  0.0383750960637
Iteration :  94   Loss :  0.0378840274772
Iteration :  95   Loss :  0.0374062602975
Iteration :  96   Loss :  0.0369415130358
Iteration :  97   Loss :  0.0364895168564
Iteration :  98   Loss :  0.0360500163767
Iteration :  99   Loss :  0.0356227705581
[-0.00060005 -0.00022885 -0.00028811 ...,  0.00032985  0.00020163
  0.0001104 ]
CROSS VALIDATION 3
Iteration :  0   Loss :  8.16188162159
Iteration :  1   Loss :  0.213384198568
Iteration :  2   Loss :  0.19538342191
Iteration :  3   Loss :  0.19091222494
Iteration :  4   Loss :  0.186683954011
Iteration :  5   Loss :  0.18263252808
Iteration :  6   Loss :  0.178723401347
Iteration :  7   Loss :  0.17493615985
Iteration :  8   Loss :  0.171257596496
Iteration :  9   Loss :  0.167678533648
Iteration :  10   Loss :  0.164192208942
Iteration :  11   Loss :  0.160793389628
Iteration :  12   Loss :  0.157477856427
Iteration :  13   Loss :  0.154242087772
Iteration :  14   Loss :  0.151083058864
Iteration :  15   Loss :  0.14799810955
Iteration :  16   Loss :  0.144984855108
Iteration :  17   Loss :  0.142041124654
Iteration :  18   Loss :  0.139164917828
Iteration :  19   Loss :  0.136354373853
Iteration :  20   Loss :  0.133607749092
Iteration :  21   Loss :  0.130923400525
Iteration :  22   Loss :  0.128299773322
Iteration :  23   Loss :  0.125735391271
Iteration :  24   Loss :  0.123228849143
Iteration :  25   Loss :  0.120778806335
Iteration :  26   Loss :  0.118383981324
Iteration :  27   Loss :  0.116043146599
Iteration :  28   Loss :  0.11375512384
Iteration :  29   Loss :  0.111518779229
Iteration :  30   Loss :  0.109333018791
Iteration :  31   Loss :  0.107196783793
Iteration :  32   Loss :  0.105109046188
Iteration :  33   Loss :  0.103068804203
Iteration :  34   Loss :  0.101075078123
Iteration :  35   Loss :  0.099126906376
Iteration :  36   Loss :  0.0972233420149
Iteration :  37   Loss :  0.0953634496763
Iteration :  38   Loss :  0.0935463030986
Iteration :  39   Loss :  0.0917709832527
Iteration :  40   Loss :  0.0900365771201
Iteration :  41   Loss :  0.088342177123
Iteration :  42   Loss :  0.0866868811844
Iteration :  43   Loss :  0.0850697933688
Iteration :  44   Loss :  0.0834900250242
Iteration :  45   Loss :  0.0819466963292
Iteration :  46   Loss :  0.0804389381294
Iteration :  47   Loss :  0.0789658939387
Iteration :  48   Loss :  0.0775267219818
Iteration :  49   Loss :  0.0761205971556
Iteration :  50   Loss :  0.0747467128065
Iteration :  51   Loss :  0.0734042822327
Iteration :  52   Loss :  0.072092539849
Iteration :  53   Loss :  0.0708107419725
Iteration :  54   Loss :  0.0695581672165
Iteration :  55   Loss :  0.0683341164995
Iteration :  56   Loss :  0.0671379126997
Iteration :  57   Loss :  0.0659689000009
Iteration :  58   Loss :  0.0648264429852
Iteration :  59   Loss :  0.0637099255373
Iteration :  60   Loss :  0.0626187496241
Iteration :  61   Loss :  0.0615523340118
Iteration :  62   Loss :  0.0605101129771
Iteration :  63   Loss :  0.0594915350571
Iteration :  64   Loss :  0.058496061878
Iteration :  65   Loss :  0.057523167085
Iteration :  66   Loss :  0.0565723353899
Iteration :  67   Loss :  0.0556430617412
Iteration :  68   Loss :  0.0547348506144
Iteration :  69   Loss :  0.0538472154133
Iteration :  70   Loss :  0.0529796779714
Iteration :  71   Loss :  0.05213176814
Iteration :  72   Loss :  0.0513030234507
Iteration :  73   Loss :  0.0504929888442
Iteration :  74   Loss :  0.0497012164592
Iteration :  75   Loss :  0.0489272654807
Iteration :  76   Loss :  0.0481707020497
Iteration :  77   Loss :  0.0474310992417
Iteration :  78   Loss :  0.0467080371196
Iteration :  79   Loss :  0.0460011028713
Iteration :  80   Loss :  0.045309891037
Iteration :  81   Loss :  0.0446340038302
Iteration :  82   Loss :  0.0439730515526
Iteration :  83   Loss :  0.0433266530958
Iteration :  84   Loss :  0.042694436522
Iteration :  85   Loss :  0.0420760397041
Iteration :  86   Loss :  0.0414711110091
Iteration :  87   Loss :  0.040879309999
Iteration :  88   Loss :  0.0403003081255
Iteration :  89   Loss :  0.0397337893929
Iteration :  90   Loss :  0.0391794509652
Iteration :  91   Loss :  0.0386370036948
Iteration :  92   Loss :  0.0381061725556
Iteration :  93   Loss :  0.0375866969662
Iteration :  94   Loss :  0.0370783309928
Iteration :  95   Loss :  0.0365808434284
Iteration :  96   Loss :  0.0360940177482
Iteration :  97   Loss :  0.0356176519449
Iteration :  98   Loss :  0.0351515582525
Iteration :  99   Loss :  0.0346955627692
[ -4.26604455e-04   1.69986916e-04  -1.66226111e-04 ...,   1.36020105e-04
  -4.30481278e-05   1.47888934e-04]
CROSS VALIDATION 4
Iteration :  0   Loss :  8.01999962228
Iteration :  1   Loss :  0.21160948375
Iteration :  2   Loss :  0.195026229316
Iteration :  3   Loss :  0.19050811321
Iteration :  4   Loss :  0.186259779496
Iteration :  5   Loss :  0.182200587499
Iteration :  6   Loss :  0.1782901736
Iteration :  7   Loss :  0.17450536715
Iteration :  8   Loss :  0.170831492121
Iteration :  9   Loss :  0.167258516467
Iteration :  10   Loss :  0.163779145369
Iteration :  11   Loss :  0.160387795107
Iteration :  12   Loss :  0.157080004072
Iteration :  13   Loss :  0.153852076871
Iteration :  14   Loss :  0.150700860115
Iteration :  15   Loss :  0.147623596253
Iteration :  16   Loss :  0.144617825538
Iteration :  17   Loss :  0.141681318701
Iteration :  18   Loss :  0.138812029765
Iteration :  19   Loss :  0.136008062381
Iteration :  20   Loss :  0.133267645382
Iteration :  21   Loss :  0.130589114715
Iteration :  22   Loss :  0.127970899788
Iteration :  23   Loss :  0.125411512876
Iteration :  24   Loss :  0.122909540643
Iteration :  25   Loss :  0.120463637075
Iteration :  26   Loss :  0.118072517378
Iteration :  27   Loss :  0.115734952478
Iteration :  28   Loss :  0.113449763937
Iteration :  29   Loss :  0.111215819121
Iteration :  30   Loss :  0.109032026586
Iteration :  31   Loss :  0.106897331631
Iteration :  32   Loss :  0.104810712065
Iteration :  33   Loss :  0.102771174205
Iteration :  34   Loss :  0.100777749175
Iteration :  35   Loss :  0.0988294895548
Iteration :  36   Loss :  0.0969254664362
Iteration :  37   Loss :  0.0950647669384
Iteration :  38   Loss :  0.0932464922095
Iteration :  39   Loss :  0.0914697559348
Iteration :  40   Loss :  0.0897336833491
Iteration :  41   Loss :  0.0880374107296
Iteration :  42   Loss :  0.0863800853283
Iteration :  43   Loss :  0.0847608656847
Iteration :  44   Loss :  0.0831789222421
Iteration :  45   Loss :  0.0816334381824
Iteration :  46   Loss :  0.0801236103858
Iteration :  47   Loss :  0.078648650421
Iteration :  48   Loss :  0.0772077854737
Iteration :  49   Loss :  0.0758002591328
Iteration :  50   Loss :  0.0744253319644
Iteration :  51   Loss :  0.073082281822
Iteration :  52   Loss :  0.0717704038599
Iteration :  53   Loss :  0.0704890102363
Iteration :  54   Loss :  0.069237429514
Iteration :  55   Loss :  0.0680150057827
Iteration :  56   Loss :  0.0668210975443
Iteration :  57   Loss :  0.0656550764128
Iteration :  58   Loss :  0.0645163256923
Iteration :  59   Loss :  0.0634042388976
Iteration :  60   Loss :  0.0623182182865
Iteration :  61   Loss :  0.0612576734662
Iteration :  62   Loss :  0.0602220201357
Iteration :  63   Loss :  0.0592106790116
Iteration :  64   Loss :  0.0582230749796
Iteration :  65   Loss :  0.0572586364974
Iteration :  66   Loss :  0.0563167952658
Iteration :  67   Loss :  0.0553969861694
Iteration :  68   Loss :  0.0544986474769
Iteration :  69   Loss :  0.0536212212816
Iteration :  70   Loss :  0.0527641541531
Iteration :  71   Loss :  0.051926897964
Iteration :  72   Loss :  0.0511089108538
Iteration :  73   Loss :  0.0503096582889
Iteration :  74   Loss :  0.0495286141798
Iteration :  75   Loss :  0.0487652620184
Iteration :  76   Loss :  0.0480190960044
Iteration :  77   Loss :  0.0472896221325
Iteration :  78   Loss :  0.0465763592192
Iteration :  79   Loss :  0.0458788398529
Iteration :  80   Loss :  0.0451966112563
Iteration :  81   Loss :  0.0445292360514
Iteration :  82   Loss :  0.0438762929249
Iteration :  83   Loss :  0.0432373771908
Iteration :  84   Loss :  0.0426121012486
Iteration :  85   Loss :  0.0420000949392
Iteration :  86   Loss :  0.0414010057981
Iteration :  87   Loss :  0.0408144992076
Iteration :  88   Loss :  0.0402402584494
Iteration :  89   Loss :  0.0396779846586
Iteration :  90   Loss :  0.039127396683
Iteration :  91   Loss :  0.0385882308478
Iteration :  92   Loss :  0.0380602406326
Iteration :  93   Loss :  0.0375431962613
Iteration :  94   Loss :  0.0370368842145
Iteration :  95   Loss :  0.0365411066682
Iteration :  96   Loss :  0.0360556808685
Iteration :  97   Loss :  0.0355804384516
Iteration :  98   Loss :  0.0351152247178
Iteration :  99   Loss :  0.0346598978727
[ -4.15718609e-04   1.75806337e-04  -1.62317958e-04 ...,   1.33254841e-04
  -3.95627398e-05   1.48965165e-04]
CROSS VALIDATION 5
Iteration :  0   Loss :  6.86613587709
Iteration :  1   Loss :  0.202925907849
Iteration :  2   Loss :  0.198453118239
Iteration :  3   Loss :  0.19417684848
Iteration :  4   Loss :  0.190050118757
Iteration :  5   Loss :  0.186048813464
Iteration :  6   Loss :  0.182158660522
Iteration :  7   Loss :  0.178370295209
Iteration :  8   Loss :  0.174677060077
Iteration :  9   Loss :  0.171073907819
Iteration :  10   Loss :  0.167556806699
Iteration :  11   Loss :  0.164122396928
Iteration :  12   Loss :  0.160767781604
Iteration :  13   Loss :  0.157490394081
Iteration :  14   Loss :  0.154287910845
Iteration :  15   Loss :  0.151158192601
Iteration :  16   Loss :  0.148099243438
Iteration :  17   Loss :  0.145109181938
Iteration :  18   Loss :  0.142186220383
Iteration :  19   Loss :  0.139328649578
Iteration :  20   Loss :  0.136534827653
Iteration :  21   Loss :  0.133803171738
Iteration :  22   Loss :  0.131132151746
Iteration :  23   Loss :  0.128520285723
Iteration :  24   Loss :  0.125966136369
Iteration :  25   Loss :  0.123468308441
Iteration :  26   Loss :  0.12102544682
Iteration :  27   Loss :  0.118636235059
Iteration :  28   Loss :  0.116299394275
Iteration :  29   Loss :  0.114013682284
Iteration :  30   Loss :  0.111777892881
Iteration :  31   Loss :  0.109590855211
Iteration :  32   Loss :  0.107451433168
Iteration :  33   Loss :  0.105358524808
Iteration :  34   Loss :  0.103311061753
Iteration :  35   Loss :  0.101308008585
Iteration :  36   Loss :  0.0993483622313
Iteration :  37   Loss :  0.0974311513618
Iteration :  38   Loss :  0.0955554358074
Iteration :  39   Loss :  0.0937203060107
Iteration :  40   Loss :  0.0919248825191
Iteration :  41   Loss :  0.090168315522
Iteration :  42   Loss :  0.0884497844252
Iteration :  43   Loss :  0.0867684974493
Iteration :  44   Loss :  0.0851236912328
Iteration :  45   Loss :  0.0835146304136
Iteration :  46   Loss :  0.0819406071626
Iteration :  47   Loss :  0.0804009406384
Iteration :  48   Loss :  0.0788949763393
Iteration :  49   Loss :  0.0774220853273
Iteration :  50   Loss :  0.0759816633064
Iteration :  51   Loss :  0.0745731295446
Iteration :  52   Loss :  0.0731959256326
Iteration :  53   Loss :  0.0718495140802
Iteration :  54   Loss :  0.0705333767572
Iteration :  55   Loss :  0.0692470131876
Iteration :  56   Loss :  0.0679899387137
Iteration :  57   Loss :  0.0667616825445
Iteration :  58   Loss :  0.0655617857077
Iteration :  59   Loss :  0.0643897989254
Iteration :  60   Loss :  0.0632452804332
Iteration :  61   Loss :  0.0621277937645
Iteration :  62   Loss :  0.061036905523
Iteration :  63   Loss :  0.0599721831695
Iteration :  64   Loss :  0.0589331928489
Iteration :  65   Loss :  0.0579194972907
Iteration :  66   Loss :  0.0569306538175
Iteration :  67   Loss :  0.0559662125014
Iteration :  68   Loss :  0.0550257145107
Iteration :  69   Loss :  0.0541086906957
Iteration :  70   Loss :  0.0532146604584
Iteration :  71   Loss :  0.0523431309569
Iteration :  72   Loss :  0.0514935966869
Iteration :  73   Loss :  0.0506655394785
Iteration :  74   Loss :  0.0498584289375
Iteration :  75   Loss :  0.0490717233472
Iteration :  76   Loss :  0.0483048710308
Iteration :  77   Loss :  0.0475573121584
Iteration :  78   Loss :  0.0468284809649
Iteration :  79   Loss :  0.0461178083265
Iteration :  80   Loss :  0.0454247246276
Iteration :  81   Loss :  0.0447486628384
Iteration :  82   Loss :  0.0440890617111
Iteration :  83   Loss :  0.0434453689988
Iteration :  84   Loss :  0.0428170446001
Iteration :  85   Loss :  0.0422035635348
Iteration :  86   Loss :  0.0416044186648
Iteration :  87   Loss :  0.0410191230835
Iteration :  88   Loss :  0.0404472121146
Iteration :  89   Loss :  0.0398882448718
Iteration :  90   Loss :  0.0393418053553
Iteration :  91   Loss :  0.0388075030724
Iteration :  92   Loss :  0.0382849731927
Iteration :  93   Loss :  0.0377738762616
Iteration :  94   Loss :  0.0372738975135
Iteration :  95   Loss :  0.0367847458353
Iteration :  96   Loss :  0.0363061524453
Iteration :  97   Loss :  0.0358378693539
Iteration :  98   Loss :  0.0353796676797
Iteration :  99   Loss :  0.0349313358931
[ -3.21742332e-04   2.17931071e-04  -1.63539347e-04 ...,   6.68397546e-05
  -3.95099203e-05   1.78657037e-04]
CROSS VALIDATION 6
Iteration :  0   Loss :  8.01505676005
Iteration :  1   Loss :  0.21153734576
Iteration :  2   Loss :  0.195004767372
Iteration :  3   Loss :  0.190485825356
Iteration :  4   Loss :  0.18623755284
Iteration :  5   Loss :  0.182178820173
Iteration :  6   Loss :  0.178269089851
Iteration :  7   Loss :  0.174485117372
Iteration :  8   Loss :  0.170812192358
Iteration :  9   Loss :  0.167240266578
Iteration :  10   Loss :  0.163762038445
Iteration :  11   Loss :  0.160371922864
Iteration :  12   Loss :  0.157065460237
Iteration :  13   Loss :  0.153838959475
Iteration :  14   Loss :  0.150689273167
Iteration :  15   Loss :  0.147613651024
Iteration :  16   Loss :  0.144609641578
Iteration :  17   Loss :  0.141675024656
Iteration :  18   Loss :  0.138807764021
Iteration :  19   Loss :  0.13600597354
Iteration :  20   Loss :  0.133267892568
Iteration :  21   Loss :  0.13059186769
Iteration :  22   Loss :  0.127976338853
Iteration :  23   Loss :  0.125419828548
Iteration :  24   Loss :  0.122920933059
Iteration :  25   Loss :  0.120478315119
Iteration :  26   Loss :  0.118090697488
Iteration :  27   Loss :  0.115756857128
Iteration :  28   Loss :  0.113475619765
Iteration :  29   Loss :  0.111245854707
Iteration :  30   Loss :  0.109066469862
Iteration :  31   Loss :  0.106936406956
Iteration :  32   Loss :  0.10485463697
Iteration :  33   Loss :  0.102820155855
Iteration :  34   Loss :  0.100831980595
Iteration :  35   Loss :  0.098889145683
Iteration :  36   Loss :  0.0969907001002
Iteration :  37   Loss :  0.0951357048329
Iteration :  38   Loss :  0.0933232309934
Iteration :  39   Loss :  0.0915523585593
Iteration :  40   Loss :  0.0898221757358
Iteration :  41   Loss :  0.0881317789175
Iteration :  42   Loss :  0.0864802732007
Iteration :  43   Loss :  0.0848667733748
Iteration :  44   Loss :  0.0832904052989
Iteration :  45   Loss :  0.0817503075543
Iteration :  46   Loss :  0.0802456332547
Iteration :  47   Loss :  0.0787755518873
Iteration :  48   Loss :  0.0773392510672
Iteration :  49   Loss :  0.0759359380907
Iteration :  50   Loss :  0.0745648411941
Iteration :  51   Loss :  0.0732252104431
Iteration :  52   Loss :  0.0719163182008
Iteration :  53   Loss :  0.0706374591512
Iteration :  54   Loss :  0.0693879498796
Iteration :  55   Loss :  0.0681671280342
Iteration :  56   Loss :  0.0669743511179
Iteration :  57   Loss :  0.0658089949728
Iteration :  58   Loss :  0.0646704520364
Iteration :  59   Loss :  0.0635581294518
Iteration :  60   Loss :  0.0624714471215
Iteration :  61   Loss :  0.0614098357868
Iteration :  62   Loss :  0.0603727352148
Iteration :  63   Loss :  0.0593595925578
Iteration :  64   Loss :  0.0583698609441
Iteration :  65   Loss :  0.0574029983396
Iteration :  66   Loss :  0.0564584667075
Iteration :  67   Loss :  0.0555357314761
Iteration :  68   Loss :  0.0546342613109
Iteration :  69   Loss :  0.0537535281753
Iteration :  70   Loss :  0.0528930076515
Iteration :  71   Loss :  0.0520521794876
Iteration :  72   Loss :  0.0512305283303
Iteration :  73   Loss :  0.0504275446009
Iteration :  74   Loss :  0.0496427254715
Iteration :  75   Loss :  0.0488755759037
Iteration :  76   Loss :  0.0481256097112
Iteration :  77   Loss :  0.0473923506168
Iteration :  78   Loss :  0.0466753332784
Iteration :  79   Loss :  0.0459741042619
Iteration :  80   Loss :  0.0452882229478
Iteration :  81   Loss :  0.044617262358
Iteration :  82   Loss :  0.0439608098957
Iteration :  83   Loss :  0.0433184679932
Iteration :  84   Loss :  0.0426898546627
Iteration :  85   Loss :  0.0420746039496
Iteration :  86   Loss :  0.0414723662849
Iteration :  87   Loss :  0.0408828087373
Iteration :  88   Loss :  0.0403056151657
Iteration :  89   Loss :  0.0397404862715
Iteration :  90   Loss :  0.0391871395539
Iteration :  91   Loss :  0.0386453091717
Iteration :  92   Loss :  0.0381147457136
Iteration :  93   Loss :  0.0375952158857
Iteration :  94   Loss :  0.0370865021201
Iteration :  95   Loss :  0.0365884021142
Iteration :  96   Loss :  0.0361007283076
Iteration :  97   Loss :  0.0356233073087
Iteration :  98   Loss :  0.0351559792774
Iteration :  99   Loss :  0.034698597277
[ -4.24501365e-04   1.71308011e-04  -1.61583775e-04 ...,   1.28532682e-04
  -4.38321682e-05   1.47171415e-04]
CROSS VALIDATION 7
Iteration :  0   Loss :  8.03428044167
Iteration :  1   Loss :  0.21176609459
Iteration :  2   Loss :  0.194993185236
Iteration :  3   Loss :  0.190478065663
Iteration :  4   Loss :  0.186226311779
Iteration :  5   Loss :  0.182160633891
Iteration :  6   Loss :  0.178241948405
Iteration :  7   Loss :  0.174447670209
Iteration :  8   Loss :  0.170763430987
Iteration :  9   Loss :  0.167179382721
Iteration :  10   Loss :  0.16368835639
Iteration :  11   Loss :  0.160284866578
Iteration :  12   Loss :  0.15696453802
Iteration :  13   Loss :  0.153723757944
Iteration :  14   Loss :  0.150559456283
Iteration :  15   Loss :  0.147468961785
Iteration :  16   Loss :  0.144449904951
Iteration :  17   Loss :  0.141500150822
Iteration :  18   Loss :  0.138617751335
Iteration :  19   Loss :  0.135800910771
Iteration :  20   Loss :  0.133047960156
Iteration :  21   Loss :  0.130357337833
Iteration :  22   Loss :  0.127727574341
Iteration :  23   Loss :  0.125157280313
Iteration :  24   Loss :  0.122645136502
Iteration :  25   Loss :  0.120189885281
Iteration :  26   Loss :  0.1177903232
Iteration :  27   Loss :  0.115445294278
Iteration :  28   Loss :  0.113153683842
Iteration :  29   Loss :  0.110914412782
Iteration :  30   Loss :  0.108726432166
Iteration :  31   Loss :  0.106588718202
Iteration :  32   Loss :  0.104500267553
Iteration :  33   Loss :  0.102460093069
Iteration :  34   Loss :  0.100467219979
Iteration :  35   Loss :  0.0985206826136
Iteration :  36   Loss :  0.0966195217364
Iteration :  37   Loss :  0.0947627825173
Iteration :  38   Loss :  0.0929495132079
Iteration :  39   Loss :  0.0911787645282
Iteration :  40   Loss :  0.0894495897573
Iteration :  41   Loss :  0.087761045491
Iteration :  42   Loss :  0.0861121929968
Iteration :  43   Loss :  0.0845021000694
Iteration :  44   Loss :  0.0829298432659
Iteration :  45   Loss :  0.0813945103797
Iteration :  46   Loss :  0.0798952030045
Iteration :  47   Loss :  0.0784310390387
Iteration :  48   Loss :  0.0770011549876
Iteration :  49   Loss :  0.0756047079423
Iteration :  50   Loss :  0.0742408771353
Iteration :  51   Loss :  0.0729088650068
Iteration :  52   Loss :  0.0716078977462
Iteration :  53   Loss :  0.0703372253074
Iteration :  54   Loss :  0.0690961209256
Iteration :  55   Loss :  0.0678838801914
Iteration :  56   Loss :  0.0666998197547
Iteration :  57   Loss :  0.0655432757474
Iteration :  58   Loss :  0.0644136020189
Iteration :  59   Loss :  0.0633101682793
Iteration :  60   Loss :  0.0622323582422
Iteration :  61   Loss :  0.0611795678472
Iteration :  62   Loss :  0.0601512036325
Iteration :  63   Loss :  0.0591466813142
Iteration :  64   Loss :  0.0581654246097
Iteration :  65   Loss :  0.0572068643325
Iteration :  66   Loss :  0.0562704377674
Iteration :  67   Loss :  0.0553555883221
Iteration :  68   Loss :  0.0544617654408
Iteration :  69   Loss :  0.053588424755
Iteration :  70   Loss :  0.0527350284404
Iteration :  71   Loss :  0.0519010457443
Iteration :  72   Loss :  0.0510859536456
Iteration :  73   Loss :  0.050289237611
Iteration :  74   Loss :  0.0495103924127
Iteration :  75   Loss :  0.0487489229747
Iteration :  76   Loss :  0.0480043452232
Iteration :  77   Loss :  0.0472761869157
Iteration :  78   Loss :  0.0465639884337
Iteration :  79   Loss :  0.0458673035233
Iteration :  80   Loss :  0.0451856999742
Iteration :  81   Loss :  0.0445187602297
Iteration :  82   Loss :  0.0438660819227
Iteration :  83   Loss :  0.0432272783323
Iteration :  84   Loss :  0.0426019787608
Iteration :  85   Loss :  0.0419898288266
Iteration :  86   Loss :  0.041390490673
Iteration :  87   Loss :  0.040803643092
Iteration :  88   Loss :  0.0402289815618
Iteration :  89   Loss :  0.0396662182
Iteration :  90   Loss :  0.0391150816328
Iteration :  91   Loss :  0.0385753167823
Iteration :  92   Loss :  0.038046684577
Iteration :  93   Loss :  0.0375289615876
Iteration :  94   Loss :  0.0370219395955
Iteration :  95   Loss :  0.0365254251005
Iteration :  96   Loss :  0.0360392387743
Iteration :  97   Loss :  0.0355632148699
Iteration :  98   Loss :  0.0350972005945
Iteration :  99   Loss :  0.0346410554561
[ -4.30071222e-04   1.70483725e-04  -1.67495461e-04 ...,   1.25796296e-04
  -4.61596776e-05   1.49472066e-04]
CROSS VALIDATION 8
Iteration :  0   Loss :  8.02062735276
Iteration :  1   Loss :  0.212151093961
Iteration :  2   Loss :  0.195067871411
Iteration :  3   Loss :  0.19056333244
Iteration :  4   Loss :  0.186320643676
Iteration :  5   Loss :  0.182263695336
Iteration :  6   Loss :  0.178353797132
Iteration :  7   Loss :  0.174568517411
Iteration :  8   Loss :  0.170893546604
Iteration :  9   Loss :  0.167319049816
Iteration :  10   Loss :  0.163837845115
Iteration :  11   Loss :  0.160444417046
Iteration :  12   Loss :  0.157134347714
Iteration :  13   Loss :  0.15390397176
Iteration :  14   Loss :  0.150750158346
Iteration :  15   Loss :  0.147670168662
Iteration :  16   Loss :  0.14466156012
Iteration :  17   Loss :  0.141722120373
Iteration :  18   Loss :  0.138849820915
Iteration :  19   Loss :  0.136042783823
Iteration :  20   Loss :  0.133299257469
Iteration :  21   Loss :  0.130617598409
Iteration :  22   Loss :  0.127996257575
Iteration :  23   Loss :  0.125433769439
Iteration :  24   Loss :  0.122928743228
Iteration :  25   Loss :  0.12047985554
Iteration :  26   Loss :  0.1180858439
Iteration :  27   Loss :  0.115745500936
Iteration :  28   Loss :  0.113457668974
Iteration :  29   Loss :  0.111221234922
Iteration :  30   Loss :  0.109035125375
Iteration :  31   Loss :  0.106898301932
Iteration :  32   Loss :  0.104809756733
Iteration :  33   Loss :  0.102768508268
Iteration :  34   Loss :  0.100773597501
Iteration :  35   Loss :  0.0988240843683
Iteration :  36   Loss :  0.096919044722
Iteration :  37   Loss :  0.0950575677546
Iteration :  38   Loss :  0.093238753946
Iteration :  39   Loss :  0.0914617135542
Iteration :  40   Loss :  0.089725565647
Iteration :  41   Loss :  0.0880294376565
Iteration :  42   Loss :  0.086372465414
Iteration :  43   Loss :  0.084753793607
Iteration :  44   Loss :  0.0831725765821
Iteration :  45   Loss :  0.0816279794086
Iteration :  46   Loss :  0.0801191791075
Iteration :  47   Loss :  0.0786453659507
Iteration :  48   Loss :  0.0772057447392
Iteration :  49   Loss :  0.0757995359742
Iteration :  50   Loss :  0.0744259768497
Iteration :  51   Loss :  0.0730843220084
Iteration :  52   Loss :  0.0717738440198
Iteration :  53   Loss :  0.0704938335563
Iteration :  54   Loss :  0.0692435992619
Iteration :  55   Loss :  0.0680224673212
Iteration :  56   Loss :  0.0668297807524
Iteration :  57   Loss :  0.06566489846
Iteration :  58   Loss :  0.0645271940902
Iteration :  59   Loss :  0.0634160547403
Iteration :  60   Loss :  0.0623308795762
Iteration :  61   Loss :  0.0612710784135
Iteration :  62   Loss :  0.0602360703157
Iteration :  63   Loss :  0.0592252822614
Iteration :  64   Loss :  0.0582381479258
Iteration :  65   Loss :  0.057274106615
Iteration :  66   Loss :  0.0563326023858
Iteration :  67   Loss :  0.0554130833731
Iteration :  68   Loss :  0.0545150013415
Iteration :  69   Loss :  0.053637811466
Iteration :  70   Loss :  0.0527809723441
Iteration :  71   Loss :  0.0519439462296
Iteration :  72   Loss :  0.0511261994781
Iteration :  73   Loss :  0.0503272031858
Iteration :  74   Loss :  0.0495464340014
Iteration :  75   Loss :  0.0487833750895
Iteration :  76   Loss :  0.0480375172185
Iteration :  77   Loss :  0.0473083599483
Iteration :  78   Loss :  0.0465954128887
Iteration :  79   Loss :  0.045898197001
Iteration :  80   Loss :  0.0452162459112
Iteration :  81   Loss :  0.0445491072059
Iteration :  82   Loss :  0.0438963436809
Iteration :  83   Loss :  0.04325753451
Iteration :  84   Loss :  0.042632276309
Iteration :  85   Loss :  0.0420201840657
Iteration :  86   Loss :  0.0414208919148
Iteration :  87   Loss :  0.0408340537383
Iteration :  88   Loss :  0.0402593435781
Iteration :  89   Loss :  0.0396964558527
Iteration :  90   Loss :  0.0391451053767
Iteration :  91   Loss :  0.0386050271861
Iteration :  92   Loss :  0.0380759761794
Iteration :  93   Loss :  0.0375577265895
Iteration :  94   Loss :  0.0370500713046
Iteration :  95   Loss :  0.0365528210589
Iteration :  96   Loss :  0.0360658035191
Iteration :  97   Loss :  0.0355888622891
Iteration :  98   Loss :  0.0351218558593
Iteration :  99   Loss :  0.0346646565224
[ -4.16561807e-04   1.76262928e-04  -1.49587294e-04 ...,   1.30587005e-04
  -4.97631823e-05   1.46470267e-04]
CROSS VALIDATION 9
Iteration :  0   Loss :  8.08297741723
Iteration :  1   Loss :  0.247179929882
Iteration :  2   Loss :  0.227632498206
Iteration :  3   Loss :  0.222346602113
Iteration :  4   Loss :  0.217384420968
Iteration :  5   Loss :  0.212638227094
Iteration :  6   Loss :  0.208059298933
Iteration :  7   Loss :  0.203621184756
Iteration :  8   Loss :  0.199307679636
Iteration :  9   Loss :  0.195107926749
Iteration :  10   Loss :  0.191014120801
Iteration :  11   Loss :  0.187020319033
Iteration :  12   Loss :  0.183121778024
Iteration :  13   Loss :  0.179314561612
Iteration :  14   Loss :  0.175595298039
Iteration :  15   Loss :  0.171961023706
Iteration :  16   Loss :  0.168409079464
Iteration :  17   Loss :  0.164937039966
Iteration :  18   Loss :  0.161542664498
Iteration :  19   Loss :  0.158223862109
Iteration :  20   Loss :  0.154978666484
Iteration :  21   Loss :  0.151805217533
Iteration :  22   Loss :  0.148701747674
Iteration :  23   Loss :  0.145666571405
Iteration :  24   Loss :  0.142698077166
Iteration :  25   Loss :  0.1397947208
Iteration :  26   Loss :  0.136955020106
Iteration :  27   Loss :  0.134177550124
Iteration :  28   Loss :  0.131460938911
Iteration :  29   Loss :  0.12880386364
Iteration :  30   Loss :  0.126205046916
Iteration :  31   Loss :  0.123663253264
Iteration :  32   Loss :  0.121177285761
Iteration :  33   Loss :  0.118745982813
Iteration :  34   Loss :  0.116368215114
Iteration :  35   Loss :  0.114042882796
Iteration :  36   Loss :  0.111768912815
Iteration :  37   Loss :  0.109545256588
Iteration :  38   Loss :  0.107370887904
Iteration :  39   Loss :  0.10524480113
Iteration :  40   Loss :  0.10316600968
Iteration :  41   Loss :  0.101133544765
Iteration :  42   Loss :  0.099146454373
Iteration :  43   Loss :  0.0972038024674
Iteration :  44   Loss :  0.095304668346
Iteration :  45   Loss :  0.0934481461381
Iteration :  46   Loss :  0.0916333443942
Iteration :  47   Loss :  0.0898593857413
Iteration :  48   Loss :  0.0881254065804
Iteration :  49   Loss :  0.0864305568148
Iteration :  50   Loss :  0.0847739996082
Iteration :  51   Loss :  0.0831549111783
Iteration :  52   Loss :  0.0815724806436
Iteration :  53   Loss :  0.0800259099398
Iteration :  54   Loss :  0.0785144138287
Iteration :  55   Loss :  0.0770372200137
Iteration :  56   Loss :  0.0755935693755
Iteration :  57   Loss :  0.0741827163282
Iteration :  58   Loss :  0.0728039292892
Iteration :  59   Loss :  0.0714564912446
Iteration :  60   Loss :  0.0701397003807
Iteration :  61   Loss :  0.0688528707474
Iteration :  62   Loss :  0.0675953329106
Iteration :  63   Loss :  0.066366434551
Iteration :  64   Loss :  0.0651655409667
Iteration :  65   Loss :  0.0639920354407
Iteration :  66   Loss :  0.0628453194414
Iteration :  67   Loss :  0.0617248126339
Iteration :  68   Loss :  0.060629952687
Iteration :  69   Loss :  0.0595601948742
Iteration :  70   Loss :  0.0585150114747
Iteration :  71   Loss :  0.0574938909902
Iteration :  72   Loss :  0.0564963372004
Iteration :  73   Loss :  0.0555218680856
Iteration :  74   Loss :  0.0545700146489
Iteration :  75   Loss :  0.0536403196713
Iteration :  76   Loss :  0.0527323364329
Iteration :  77   Loss :  0.051845627432
Iteration :  78   Loss :  0.0509797631307
Iteration :  79   Loss :  0.0501343207522
Iteration :  80   Loss :  0.0493088831509
Iteration :  81   Loss :  0.0485030377739
Iteration :  82   Loss :  0.0477163757293
Iteration :  83   Loss :  0.0469484909739
Iteration :  84   Loss :  0.0461989796318
Iteration :  85   Loss :  0.0454674394564
Iteration :  86   Loss :  0.0447534694432
Iteration :  87   Loss :  0.0440566696068
Iteration :  88   Loss :  0.0433766409268
Iteration :  89   Loss :  0.0427129854725
Iteration :  90   Loss :  0.0420653067078
Iteration :  91   Loss :  0.0414332099775
Iteration :  92   Loss :  0.0408163031683
Iteration :  93   Loss :  0.0402141975335
Iteration :  94   Loss :  0.0396265086637
Iteration :  95   Loss :  0.0390528575792
Iteration :  96   Loss :  0.038492871915
Iteration :  97   Loss :  0.0379461871642
Iteration :  98   Loss :  0.0374124479443
Iteration :  99   Loss :  0.0368913092486
[ -3.83137533e-04   2.02460620e-04  -1.12092875e-04 ...,   2.57630342e-04
  -3.30756685e-05   1.43159658e-04]
CROSS VALIDATION 10
Iteration :  0   Loss :  7.47820113366
Iteration :  1   Loss :  0.254717584645
Iteration :  2   Loss :  0.249071636668
Iteration :  3   Loss :  0.243683518841
Iteration :  4   Loss :  0.238482363032
Iteration :  5   Loss :  0.233435318255
Iteration :  6   Loss :  0.228524113651
Iteration :  7   Loss :  0.223737220896
Iteration :  8   Loss :  0.21906663949
Iteration :  9   Loss :  0.214506387138
Iteration :  10   Loss :  0.21005171823
Iteration :  11   Loss :  0.205698688117
Iteration :  12   Loss :  0.201443895585
Iteration :  13   Loss :  0.197284323272
Iteration :  14   Loss :  0.193217234882
Iteration :  15   Loss :  0.189240106804
Iteration :  16   Loss :  0.185350581396
Iteration :  17   Loss :  0.181546434345
Iteration :  18   Loss :  0.177825551445
Iteration :  19   Loss :  0.174185911825
Iteration :  20   Loss :  0.170625575684
Iteration :  21   Loss :  0.167142675223
Iteration :  22   Loss :  0.16373540787
Iteration :  23   Loss :  0.160402031157
Iteration :  24   Loss :  0.157140858776
Iteration :  25   Loss :  0.153950257476
Iteration :  26   Loss :  0.150828644535
Iteration :  27   Loss :  0.147774485601
Iteration :  28   Loss :  0.144786292766
Iteration :  29   Loss :  0.141862622738
Iteration :  30   Loss :  0.139002075056
Iteration :  31   Loss :  0.136203290266
Iteration :  32   Loss :  0.133464948055
Iteration :  33   Loss :  0.130785765311
Iteration :  34   Loss :  0.128164494136
Iteration :  35   Loss :  0.125599919815
Iteration :  36   Loss :  0.123090858781
Iteration :  37   Loss :  0.120636156601
Iteration :  38   Loss :  0.11823468603
Iteration :  39   Loss :  0.115885345148
Iteration :  40   Loss :  0.11358705563
Iteration :  41   Loss :  0.111338761154
Iteration :  42   Loss :  0.109139425972
Iteration :  43   Loss :  0.10698803366
Iteration :  44   Loss :  0.104883586041
Iteration :  45   Loss :  0.102825102284
Iteration :  46   Loss :  0.100811618177
Iteration :  47   Loss :  0.0988421855651
Iteration :  48   Loss :  0.0969158719366
Iteration :  49   Loss :  0.0950317601541
Iteration :  50   Loss :  0.0931889483079
Iteration :  51   Loss :  0.0913865496813
Iteration :  52   Loss :  0.0896236928069
Iteration :  53   Loss :  0.0878995215975
Iteration :  54   Loss :  0.0862131955314
Iteration :  55   Loss :  0.084563889871
Iteration :  56   Loss :  0.0829507958958
Iteration :  57   Loss :  0.0813731211303
Iteration :  58   Loss :  0.079830089549
Iteration :  59   Loss :  0.0783209417453
Iteration :  60   Loss :  0.0768449350523
Iteration :  61   Loss :  0.0754013436111
Iteration :  62   Loss :  0.0739894583833
Iteration :  63   Loss :  0.0726085871112
Iteration :  64   Loss :  0.0712580542335
Iteration :  65   Loss :  0.0699372007656
Iteration :  66   Loss :  0.0686453841585
Iteration :  67   Loss :  0.0673819781491
Iteration :  68   Loss :  0.0661463726162
Iteration :  69   Loss :  0.064937973455
Iteration :  70   Loss :  0.0637562024783
Iteration :  71   Loss :  0.062600497352
Iteration :  72   Loss :  0.0614703115647
Iteration :  73   Loss :  0.0603651144303
Iteration :  74   Loss :  0.0592843911128
Iteration :  75   Loss :  0.0582276426611
Iteration :  76   Loss :  0.0571943860373
Iteration :  77   Loss :  0.0561841541161
Iteration :  78   Loss :  0.055196495635
Iteration :  79   Loss :  0.0542309750713
Iteration :  80   Loss :  0.0532871724269
Iteration :  81   Loss :  0.0523646829014
Iteration :  82   Loss :  0.0514631164428
Iteration :  83   Loss :  0.0505820971681
Iteration :  84   Loss :  0.0497212626538
Iteration :  85   Loss :  0.0488802631055
Iteration :  86   Loss :  0.0480587604178
Iteration :  87   Loss :  0.0472564271478
Iteration :  88   Loss :  0.0464729454233
Iteration :  89   Loss :  0.0457080058164
Iteration :  90   Loss :  0.0449613062078
Iteration :  91   Loss :  0.0442325506715
Iteration :  92   Loss :  0.043521448405
Iteration :  93   Loss :  0.0428277127258
Iteration :  94   Loss :  0.0421510601541
Iteration :  95   Loss :  0.0414912095937
Iteration :  96   Loss :  0.040847881623
Iteration :  97   Loss :  0.0402207979018
Iteration :  98   Loss :  0.0396096807009
Iteration :  99   Loss :  0.0390142525567
[ -3.89714754e-04   1.66322229e-04  -1.80285245e-04 ...,   2.45657513e-04
  -1.57387758e-05   1.53089928e-04]
CROSS VALIDATION 11
Iteration :  0   Loss :  8.01738795911
Iteration :  1   Loss :  0.211619619717
Iteration :  2   Loss :  0.19507806347
Iteration :  3   Loss :  0.190558636534
Iteration :  4   Loss :  0.186309108843
Iteration :  5   Loss :  0.182248734977
Iteration :  6   Loss :  0.178337129678
Iteration :  7   Loss :  0.174551116971
Iteration :  8   Loss :  0.170876018693
Iteration :  9   Loss :  0.167301800817
Iteration :  10   Loss :  0.163821165987
Iteration :  11   Loss :  0.160428527263
Iteration :  12   Loss :  0.157119419158
Iteration :  13   Loss :  0.153890141776
Iteration :  14   Loss :  0.150737536653
Iteration :  15   Loss :  0.147658840619
Iteration :  16   Loss :  0.144651587796
Iteration :  17   Loss :  0.141713542298
Iteration :  18   Loss :  0.138842651082
Iteration :  19   Loss :  0.136037010317
Iteration :  20   Loss :  0.133294841001
Iteration :  21   Loss :  0.130614470955
Iteration :  22   Loss :  0.12799432126
Iteration :  23   Loss :  0.125432895781
Iteration :  24   Loss :  0.122928772819
Iteration :  25   Loss :  0.120480598225
Iteration :  26   Loss :  0.118087079487
Iteration :  27   Loss :  0.115746980468
Iteration :  28   Loss :  0.113459116573
Iteration :  29   Loss :  0.111222350202
Iteration :  30   Loss :  0.10903558643
Iteration :  31   Loss :  0.106897768869
Iteration :  32   Loss :  0.104807875726
Iteration :  33   Loss :  0.102764916087
Iteration :  34   Loss :  0.100767926445
Iteration :  35   Loss :  0.0988159675355
Iteration :  36   Loss :  0.0969081214984
Iteration :  37   Loss :  0.095043489403
Iteration :  38   Loss :  0.0932211891549
Iteration :  39   Loss :  0.0914403537892
Iteration :  40   Loss :  0.0897001301457
Iteration :  41   Loss :  0.0879996779061
Iteration :  42   Loss :  0.0863381689635
Iteration :  43   Loss :  0.0847147870846
Iteration :  44   Loss :  0.0831287278176
Iteration :  45   Loss :  0.0815791985929
Iteration :  46   Loss :  0.0800654189621
Iteration :  47   Loss :  0.0785866209178
Iteration :  48   Loss :  0.0771420492412
Iteration :  49   Loss :  0.0757309618255
Iteration :  50   Loss :  0.0743526299298
Iteration :  51   Loss :  0.0730063383272
Iteration :  52   Loss :  0.0716913853157
Iteration :  53   Loss :  0.0704070825749
Iteration :  54   Loss :  0.0691527548596
Iteration :  55   Loss :  0.0679277395328
Iteration :  56   Loss :  0.0667313859523
Iteration :  57   Loss :  0.0655630547323
Iteration :  58   Loss :  0.0644221169127
Iteration :  59   Loss :  0.0633079530725
Iteration :  60   Loss :  0.0622199524283
Iteration :  61   Loss :  0.0611575119622
Iteration :  62   Loss :  0.0601200356182
Iteration :  63   Loss :  0.0591069336075
Iteration :  64   Loss :  0.0581176218542
Iteration :  65   Loss :  0.0571515216063
Iteration :  66   Loss :  0.0562080592278
Iteration :  67   Loss :  0.0552866661799
Iteration :  68   Loss :  0.054386779188
Iteration :  69   Loss :  0.0535078405843
Iteration :  70   Loss :  0.052649298807
Iteration :  71   Loss :  0.0518106090327
Iteration :  72   Loss :  0.0509912339143
Iteration :  73   Loss :  0.0501906443949
Iteration :  74   Loss :  0.0494083205689
Iteration :  75   Loss :  0.0486437525617
Iteration :  76   Loss :  0.047896441405
Iteration :  77   Loss :  0.0471658998864
Iteration :  78   Loss :  0.046451653356
Iteration :  79   Loss :  0.0457532404785
Iteration :  80   Loss :  0.0450702139207
Iteration :  81   Loss :  0.0444021409698
Iteration :  82   Loss :  0.0437486040777
Iteration :  83   Loss :  0.0431092013293
Iteration :  84   Loss :  0.0424835468359
Iteration :  85   Loss :  0.0418712710505
Iteration :  86   Loss :  0.041272021007
Iteration :  87   Loss :  0.0406854604828
Iteration :  88   Loss :  0.0401112700833
Iteration :  89   Loss :  0.0395491472504
Iteration :  90   Loss :  0.0389988061927
Iteration :  91   Loss :  0.0384599777394
Iteration :  92   Loss :  0.0379324091173
Iteration :  93   Loss :  0.0374158636544
Iteration :  94   Loss :  0.0369101204114
Iteration :  95   Loss :  0.0364149737451
Iteration :  96   Loss :  0.0359302328098
Iteration :  97   Loss :  0.0354557210024
Iteration :  98   Loss :  0.0349912753586
Iteration :  99   Loss :  0.0345367459093
[ -4.21005626e-04   1.78048800e-04  -1.51627421e-04 ...,   1.30642137e-04
  -4.35996267e-05   1.47142149e-04]
CROSS VALIDATION 12
Iteration :  0   Loss :  10.1521453937
Iteration :  1   Loss :  0.182981426958
Iteration :  2   Loss :  0.174206962921
Iteration :  3   Loss :  0.16989516107
Iteration :  4   Loss :  0.166026403573
Iteration :  5   Loss :  0.162386387858
Iteration :  6   Loss :  0.158902664552
Iteration :  7   Loss :  0.155541842355
Iteration :  8   Loss :  0.152285568491
Iteration :  9   Loss :  0.149122470917
Iteration :  10   Loss :  0.146044842312
Iteration :  11   Loss :  0.143047077379
Iteration :  12   Loss :  0.140124861038
Iteration :  13   Loss :  0.137274714437
Iteration :  14   Loss :  0.13449372602
Iteration :  15   Loss :  0.131779384656
Iteration :  16   Loss :  0.129129472048
Iteration :  17   Loss :  0.126541991051
Iteration :  18   Loss :  0.12401511646
Iteration :  19   Loss :  0.121547160251
Iteration :  20   Loss :  0.119136546302
Iteration :  21   Loss :  0.116781791409
Iteration :  22   Loss :  0.114481490571
Iteration :  23   Loss :  0.112234305176
Iteration :  24   Loss :  0.11003895322
Iteration :  25   Loss :  0.107894200982
Iteration :  26   Loss :  0.105798855798
Iteration :  27   Loss :  0.103751759733
Iteration :  28   Loss :  0.101751784028
Iteration :  29   Loss :  0.099797824273
Iteration :  30   Loss :  0.0978887962981
Iteration :  31   Loss :  0.0960236327855
Iteration :  32   Loss :  0.0942012806074
Iteration :  33   Loss :  0.092420698894
Iteration :  34   Loss :  0.0906808578186
Iteration :  35   Loss :  0.0889807380691
Iteration :  36   Loss :  0.0873193309584
Iteration :  37   Loss :  0.0856956391077
Iteration :  38   Loss :  0.0841086776232
Iteration :  39   Loss :  0.0825574756743
Iteration :  40   Loss :  0.0810410783763
Iteration :  41   Loss :  0.0795585488764
Iteration :  42   Loss :  0.0781089705454
Iteration :  43   Loss :  0.0766914491809
Iteration :  44   Loss :  0.0753051151372
Iteration :  45   Loss :  0.0739491253039
Iteration :  46   Loss :  0.0726226648695
Iteration :  47   Loss :  0.0713249488136
Iteration :  48   Loss :  0.0700552230853
Iteration :  49   Loss :  0.0688127654355
Iteration :  50   Loss :  0.0675968858824
Iteration :  51   Loss :  0.0664069267999
Iteration :  52   Loss :  0.0652422626317
Iteration :  53   Loss :  0.064102299242
Iteration :  54   Loss :  0.0629864729271
Iteration :  55   Loss :  0.061894249121
Iteration :  56   Loss :  0.0608251208358
Iteration :  57   Loss :  0.0597786068883
Iteration :  58   Loss :  0.058754249966
Iteration :  59   Loss :  0.0577516145922
Iteration :  60   Loss :  0.0567702850498
Iteration :  61   Loss :  0.0558098633206
Iteration :  62   Loss :  0.0548699670972
Iteration :  63   Loss :  0.0539502279143
Iteration :  64   Loss :  0.0530502894433
Iteration :  65   Loss :  0.0521698059838
Iteration :  66   Loss :  0.0513084411768
Iteration :  67   Loss :  0.0504658669564
Iteration :  68   Loss :  0.0496417627454
Iteration :  69   Loss :  0.0488358148946
Iteration :  70   Loss :  0.0480477163537
Iteration :  71   Loss :  0.0472771665571
Iteration :  72   Loss :  0.0465238714999
Iteration :  73   Loss :  0.0457875439742
Iteration :  74   Loss :  0.0450679039309
Iteration :  75   Loss :  0.0443646789294
Iteration :  76   Loss :  0.0436776046346
Iteration :  77   Loss :  0.0430064253207
Iteration :  78   Loss :  0.0423508943395
Iteration :  79   Loss :  0.0417107745147
Iteration :  80   Loss :  0.0410858384243
Iteration :  81   Loss :  0.0404758685375
Iteration :  82   Loss :  0.0398806571767
Iteration :  83   Loss :  0.0393000062817
Iteration :  84   Loss :  0.0387337269581
Iteration :  85   Loss :  0.0381816387999
Iteration :  86   Loss :  0.0376435689816
Iteration :  87   Loss :  0.0371193511262
Iteration :  88   Loss :  0.0366088239596
Iteration :  89   Loss :  0.0361118297729
Iteration :  90   Loss :  0.0356282127201
Iteration :  91   Loss :  0.0351578169878
Iteration :  92   Loss :  0.0347004848807
Iteration :  93   Loss :  0.0342560548732
Iteration :  94   Loss :  0.0338243596852
Iteration :  95   Loss :  0.0334052244415
Iteration :  96   Loss :  0.032998464982
Iteration :  97   Loss :  0.0326038863859
Iteration :  98   Loss :  0.0322212817748
Iteration :  99   Loss :  0.0318504314515
[ -7.32208821e-04  -3.78860179e-05  -2.25834036e-04 ...,   6.97550916e-05
  -4.89209248e-05   8.52395147e-05]
CROSS VALIDATION 13
Iteration :  0   Loss :  8.10834569578
Iteration :  1   Loss :  0.246437070947
Iteration :  2   Loss :  0.237502677113
Iteration :  3   Loss :  0.231441181678
Iteration :  4   Loss :  0.226051272916
Iteration :  5   Loss :  0.220999886921
Iteration :  6   Loss :  0.216172972957
Iteration :  7   Loss :  0.211518572906
Iteration :  8   Loss :  0.207008678802
Iteration :  9   Loss :  0.202626315234
Iteration :  10   Loss :  0.198360232074
Iteration :  11   Loss :  0.194202419931
Iteration :  12   Loss :  0.190146830634
Iteration :  13   Loss :  0.186188668117
Iteration :  14   Loss :  0.182323971771
Iteration :  15   Loss :  0.178549359602
Iteration :  16   Loss :  0.174861863433
Iteration :  17   Loss :  0.171258819461
Iteration :  18   Loss :  0.16773779342
Iteration :  19   Loss :  0.164296528052
Iteration :  20   Loss :  0.16093290542
Iteration :  21   Loss :  0.157644919338
Iteration :  22   Loss :  0.15443065487
Iteration :  23   Loss :  0.151288272904
Iteration :  24   Loss :  0.148215998439
Iteration :  25   Loss :  0.145212111663
Iteration :  26   Loss :  0.14227494116
Iteration :  27   Loss :  0.139402858805
Iteration :  28   Loss :  0.13659427599
Iteration :  29   Loss :  0.133847640927
Iteration :  30   Loss :  0.13116143683
Iteration :  31   Loss :  0.128534180808
Iteration :  32   Loss :  0.125964423319
Iteration :  33   Loss :  0.123450748065
Iteration :  34   Loss :  0.120991772216
Iteration :  35   Loss :  0.118586146847
Iteration :  36   Loss :  0.116232557508
Iteration :  37   Loss :  0.113929724837
Iteration :  38   Loss :  0.111676405144
Iteration :  39   Loss :  0.109471390917
Iteration :  40   Loss :  0.107313511199
Iteration :  41   Loss :  0.105201631808
Iteration :  42   Loss :  0.103134655382
Iteration :  43   Loss :  0.10111152124
Iteration :  44   Loss :  0.0991312050696
Iteration :  45   Loss :  0.0971927184414
Iteration :  46   Loss :  0.0952951081702
Iteration :  47   Loss :  0.093437455546
Iteration :  48   Loss :  0.0916188754497
Iteration :  49   Loss :  0.08983851538
Iteration :  50   Loss :  0.0880955544089
Iteration :  51   Loss :  0.0863892020881
Iteration :  52   Loss :  0.0847186973234
Iteration :  53   Loss :  0.083083307233
Iteration :  54   Loss :  0.081482326004
Iteration :  55   Loss :  0.0799150737601
Iteration :  56   Loss :  0.078380895449
Iteration :  57   Loss :  0.0768791597596
Iteration :  58   Loss :  0.075409258072
Iteration :  59   Loss :  0.0739706034469
Iteration :  60   Loss :  0.072562629652
Iteration :  61   Loss :  0.0711847902251
Iteration :  62   Loss :  0.0698365575682
Iteration :  63   Loss :  0.0685174220651
Iteration :  64   Loss :  0.0672268912113
Iteration :  65   Loss :  0.0659644887451
Iteration :  66   Loss :  0.0647297537645
Iteration :  67   Loss :  0.0635222398175
Iteration :  68   Loss :  0.0623415139511
Iteration :  69   Loss :  0.0611871557082
Iteration :  70   Loss :  0.0600587560639
Iteration :  71   Loss :  0.058955916299
Iteration :  72   Loss :  0.0578782468099
Iteration :  73   Loss :  0.0568253658661
Iteration :  74   Loss :  0.055796898328
Iteration :  75   Loss :  0.0547924743465
Iteration :  76   Loss :  0.0538117280725
Iteration :  77   Loss :  0.0528542964067
Iteration :  78   Loss :  0.0519198178248
Iteration :  79   Loss :  0.051007931315
Iteration :  80   Loss :  0.0501182754605
Iteration :  81   Loss :  0.0492504876994
Iteration :  82   Loss :  0.0484042037874
Iteration :  83   Loss :  0.0475790574798
Iteration :  84   Loss :  0.0467746804436
Iteration :  85   Loss :  0.0459907023985
Iteration :  86   Loss :  0.0452267514779
Iteration :  87   Loss :  0.0444824547912
Iteration :  88   Loss :  0.0437574391641
Iteration :  89   Loss :  0.0430513320249
Iteration :  90   Loss :  0.0423637624033
Iteration :  91   Loss :  0.0416943620064
Iteration :  92   Loss :  0.0410427663347
Iteration :  93   Loss :  0.0404086158013
Iteration :  94   Loss :  0.0397915568213
Iteration :  95   Loss :  0.0391912428368
Iteration :  96   Loss :  0.038607335246
Iteration :  97   Loss :  0.0380395042074
Iteration :  98   Loss :  0.037487429291
Iteration :  99   Loss :  0.0369507999507
[ -5.81883968e-04  -1.15178216e-04  -2.67993325e-04 ...,   7.19849931e-05
   3.29708354e-05   1.29757704e-04]
CROSS VALIDATION 14
Iteration :  0   Loss :  7.99743842155
Iteration :  1   Loss :  0.210225488651
Iteration :  2   Loss :  0.193535945662
Iteration :  3   Loss :  0.189249233698
Iteration :  4   Loss :  0.185154357537
Iteration :  5   Loss :  0.181204357156
Iteration :  6   Loss :  0.177375437104
Iteration :  7   Loss :  0.173653612147
Iteration :  8   Loss :  0.170029739355
Iteration :  9   Loss :  0.166497332308
Iteration :  10   Loss :  0.163051480679
Iteration :  11   Loss :  0.159688268193
Iteration :  12   Loss :  0.156404437608
Iteration :  13   Loss :  0.153197187509
Iteration :  14   Loss :  0.150064043786
Iteration :  15   Loss :  0.147002775556
Iteration :  16   Loss :  0.144011338643
Iteration :  17   Loss :  0.141087836741
Iteration :  18   Loss :  0.138230494235
Iteration :  19   Loss :  0.135437636866
Iteration :  20   Loss :  0.132707677742
Iteration :  21   Loss :  0.130039107001
Iteration :  22   Loss :  0.127430483957
Iteration :  23   Loss :  0.124880430885
Iteration :  24   Loss :  0.122387627858
Iteration :  25   Loss :  0.119950808218
Iteration :  26   Loss :  0.117568754371
Iteration :  27   Loss :  0.115240293728
Iteration :  28   Loss :  0.112964294666
Iteration :  29   Loss :  0.11073966248
Iteration :  30   Loss :  0.108565335299
Iteration :  31   Loss :  0.106440280045
Iteration :  32   Loss :  0.104363488465
Iteration :  33   Loss :  0.10233397336
Iteration :  34   Loss :  0.100350765083
Iteration :  35   Loss :  0.0984129084114
Iteration :  36   Loss :  0.0965194598889
Iteration :  37   Loss :  0.094669485701
Iteration :  38   Loss :  0.0928620601445
Iteration :  39   Loss :  0.0910962647182
Iteration :  40   Loss :  0.0893711878355
Iteration :  41   Loss :  0.0876859251306
Iteration :  42   Loss :  0.0860395803006
Iteration :  43   Loss :  0.0844312664006
Iteration :  44   Loss :  0.0828601074839
Iteration :  45   Loss :  0.0813252404689
Iteration :  46   Loss :  0.0798258170999
Iteration :  47   Loss :  0.0783610058725
Iteration :  48   Loss :  0.0769299938012
Iteration :  49   Loss :  0.0755319879184
Iteration :  50   Loss :  0.0741662164197
Iteration :  51   Loss :  0.0728319293918
Iteration :  52   Loss :  0.0715283990876
Iteration :  53   Loss :  0.0702549197428
Iteration :  54   Loss :  0.0690108069508
Iteration :  55   Loss :  0.0677953966404
Iteration :  56   Loss :  0.0666080437141
Iteration :  57   Loss :  0.0654481204244
Iteration :  58   Loss :  0.0643150145677
Iteration :  59   Loss :  0.0632081275842
Iteration :  60   Loss :  0.0621268726468
Iteration :  61   Loss :  0.0610706728168
Iteration :  62   Loss :  0.0600389593388
Iteration :  63   Loss :  0.0590311701295
Iteration :  64   Loss :  0.0580467485082
Iteration :  65   Loss :  0.0570851421998
Iteration :  66   Loss :  0.0561458026266
Iteration :  67   Loss :  0.055228184496
Iteration :  68   Loss :  0.0543317456747
Iteration :  69   Loss :  0.0534559473342
Iteration :  70   Loss :  0.0526002543414
Iteration :  71   Loss :  0.0517641358639
Iteration :  72   Loss :  0.0509470661551
Iteration :  73   Loss :  0.0501485254834
Iteration :  74   Loss :  0.0493680011677
Iteration :  75   Loss :  0.0486049886861
Iteration :  76   Loss :  0.0478589928221
Iteration :  77   Loss :  0.0471295288204
Iteration :  78   Loss :  0.0464161235247
Iteration :  79   Loss :  0.0457183164721
Iteration :  80   Loss :  0.0450356609254
Iteration :  81   Loss :  0.0443677248238
Iteration :  82   Loss :  0.0437140916372
Iteration :  83   Loss :  0.0430743611114
Iteration :  84   Loss :  0.0424481498942
Iteration :  85   Loss :  0.0418350920344
Iteration :  86   Loss :  0.0412348393486
Iteration :  87   Loss :  0.0406470616549
Iteration :  88   Loss :  0.0400714468708
Iteration :  89   Loss :  0.0395077009821
Iteration :  90   Loss :  0.0389555478846
Iteration :  91   Loss :  0.0384147291094
Iteration :  92   Loss :  0.0378850034387
Iteration :  93   Loss :  0.0373661464271
Iteration :  94   Loss :  0.0368579498369
Iteration :  95   Loss :  0.0363602210054
Iteration :  96   Loss :  0.0358727821549
Iteration :  97   Loss :  0.0353954696608
Iteration :  98   Loss :  0.0349281332921
Iteration :  99   Loss :  0.0344706354357
[ -3.95703504e-04   1.87212067e-04  -1.48549887e-04 ...,   1.13544561e-04
  -5.90732748e-05   1.44525656e-04]
CROSS VALIDATION 15
Iteration :  0   Loss :  8.0465233581
Iteration :  1   Loss :  0.212120039723
Iteration :  2   Loss :  0.195135045381
Iteration :  3   Loss :  0.190633547569
Iteration :  4   Loss :  0.186391589462
Iteration :  5   Loss :  0.182334223416
Iteration :  6   Loss :  0.178423268588
Iteration :  7   Loss :  0.174636550707
Iteration :  8   Loss :  0.170959904221
Iteration :  9   Loss :  0.167383580775
Iteration :  10   Loss :  0.163900452732
Iteration :  11   Loss :  0.160505039084
Iteration :  12   Loss :  0.157192943078
Iteration :  13   Loss :  0.153960510866
Iteration :  14   Loss :  0.150804615713
Iteration :  15   Loss :  0.147722516966
Iteration :  16   Loss :  0.14471176533
Iteration :  17   Loss :  0.141770137799
Iteration :  18   Loss :  0.138895592107
Iteration :  19   Loss :  0.136086234318
Iteration :  20   Loss :  0.133340295417
Iteration :  21   Loss :  0.130656114112
Iteration :  22   Loss :  0.128032123949
Iteration :  23   Loss :  0.125466843409
Iteration :  24   Loss :  0.122958868029
Iteration :  25   Loss :  0.120506863861
Iteration :  26   Loss :  0.118109561789
Iteration :  27   Loss :  0.115765752358
Iteration :  28   Loss :  0.113474280863
Iteration :  29   Loss :  0.111234042575
Iteration :  30   Loss :  0.109043978002
Iteration :  31   Loss :  0.10690306816
Iteration :  32   Loss :  0.104810329862
Iteration :  33   Loss :  0.102764811078
Iteration :  34   Loss :  0.100765586407
Iteration :  35   Loss :  0.0988117527533
Iteration :  36   Loss :  0.096902425294
Iteration :  37   Loss :  0.0950367338065
Iteration :  38   Loss :  0.0932138194542
Iteration :  39   Loss :  0.0914328320911
Iteration :  40   Loss :  0.0896929281428
Iteration :  41   Loss :  0.087993269097
Iteration :  42   Loss :  0.0863330206142
Iteration :  43   Loss :  0.0847113522399
Iteration :  44   Loss :  0.0831274376757
Iteration :  45   Loss :  0.0815804555374
Iteration :  46   Loss :  0.0800695905079
Iteration :  47   Loss :  0.078594034774
Iteration :  48   Loss :  0.0771529896271
Iteration :  49   Loss :  0.0757456671044
Iteration :  50   Loss :  0.0743712915556
Iteration :  51   Loss :  0.0730291010291
Iteration :  52   Loss :  0.0717183483965
Iteration :  53   Loss :  0.0704383021558
Iteration :  54   Loss :  0.0691882468787
Iteration :  55   Loss :  0.0679674832973
Iteration :  56   Loss :  0.0667753280463
Iteration :  57   Loss :  0.0656111131007
Iteration :  58   Loss :  0.0644741849614
Iteration :  59   Loss :  0.0633639036556
Iteration :  60   Loss :  0.0622796416216
Iteration :  61   Loss :  0.0612207825501
Iteration :  62   Loss :  0.0601867202475
Iteration :  63   Loss :  0.0591768575829
Iteration :  64   Loss :  0.0581906055676
Iteration :  65   Loss :  0.0572273826052
Iteration :  66   Loss :  0.0562866139394
Iteration :  67   Loss :  0.0553677313128
Iteration :  68   Loss :  0.054470172839
Iteration :  69   Loss :  0.0535933830818
Iteration :  70   Loss :  0.0527368133246
Iteration :  71   Loss :  0.0518999220094
Iteration :  72   Loss :  0.0510821753175
Iteration :  73   Loss :  0.0502830478657
Iteration :  74   Loss :  0.0495020234859
Iteration :  75   Loss :  0.0487385960622
Iteration :  76   Loss :  0.0479922703967
Iteration :  77   Loss :  0.0472625630801
Iteration :  78   Loss :  0.0465490033463
Iteration :  79   Loss :  0.0458511338903
Iteration :  80   Loss :  0.0451685116349
Iteration :  81   Loss :  0.0445007084312
Iteration :  82   Loss :  0.0438473116832
Iteration :  83   Loss :  0.0432079248851
Iteration :  84   Loss :  0.0425821680667
Iteration :  85   Loss :  0.0419696781395
Iteration :  86   Loss :  0.0413701091405
Iteration :  87   Loss :  0.0407831323729
Iteration :  88   Loss :  0.0402084364428
Iteration :  89   Loss :  0.0396457271936
Iteration :  90   Loss :  0.0390947275428
Iteration :  91   Loss :  0.0385551772245
Iteration :  92   Loss :  0.038026832446
Iteration :  93   Loss :  0.0375094654639
Iteration :  94   Loss :  0.0370028640903
Iteration :  95   Loss :  0.0365068311373
Iteration :  96   Loss :  0.0360211838103
Iteration :  97   Loss :  0.0355457530593
Iteration :  98   Loss :  0.0350803828996
Iteration :  99   Loss :  0.034624929712
[ -4.21454549e-04   1.85234444e-04  -1.60409216e-04 ...,   1.31181151e-04
  -4.45761022e-05   1.46760651e-04]
CROSS VALIDATION 16
Iteration :  0   Loss :  8.02212520354
Iteration :  1   Loss :  0.197602215577
Iteration :  2   Loss :  0.193158610974
Iteration :  3   Loss :  0.188941871458
Iteration :  4   Loss :  0.184887226644
Iteration :  5   Loss :  0.18096374432
Iteration :  6   Loss :  0.177153990534
Iteration :  7   Loss :  0.173446940319
Iteration :  8   Loss :  0.169834986226
Iteration :  9   Loss :  0.166312503363
Iteration :  10   Loss :  0.162875094108
Iteration :  11   Loss :  0.159519161454
Iteration :  12   Loss :  0.156241654264
Iteration :  13   Loss :  0.153039908269
Iteration :  14   Loss :  0.14991154317
Iteration :  15   Loss :  0.146854394086
Iteration :  16   Loss :  0.143866464762
Iteration :  17   Loss :  0.140945894996
Iteration :  18   Loss :  0.138090937561
Iteration :  19   Loss :  0.135299941607
Iteration :  20   Loss :  0.132571340492
Iteration :  21   Loss :  0.129903642684
Iteration :  22   Loss :  0.127295424757
Iteration :  23   Loss :  0.124745325784
Iteration :  24   Loss :  0.122252042646
Iteration :  25   Loss :  0.119814325893
Iteration :  26   Loss :  0.117430975912
Iteration :  27   Loss :  0.115100839234
Iteration :  28   Loss :  0.112822804875
Iteration :  29   Loss :  0.110595800675
Iteration :  30   Loss :  0.108418789606
Iteration :  31   Loss :  0.10629076608
Iteration :  32   Loss :  0.104210752309
Iteration :  33   Loss :  0.102177794746
Iteration :  34   Loss :  0.100190960705
Iteration :  35   Loss :  0.0982493351978
Iteration :  36   Loss :  0.0963520180564
Iteration :  37   Loss :  0.0944981214014
Iteration :  38   Loss :  0.0926867674871
Iteration :  39   Loss :  0.0909170869601
Iteration :  40   Loss :  0.089188217547
Iteration :  41   Loss :  0.0874993031737
Iteration :  42   Loss :  0.0858494935076
Iteration :  43   Loss :  0.0842379438988
Iteration :  44   Loss :  0.0826638156863
Iteration :  45   Loss :  0.0811262768236
Iteration :  46   Loss :  0.0796245027676
Iteration :  47   Loss :  0.0781576775697
Iteration :  48   Loss :  0.0767249950985
Iteration :  49   Loss :  0.0753256603252
Iteration :  50   Loss :  0.0739588905974
Iteration :  51   Loss :  0.0726239168368
Iteration :  52   Loss :  0.0713199845948
Iteration :  53   Loss :  0.0700463549178
Iteration :  54   Loss :  0.0688023049798
Iteration :  55   Loss :  0.0675871284557
Iteration :  56   Loss :  0.0664001356272
Iteration :  57   Loss :  0.0652406532213
Iteration :  58   Loss :  0.0641080240023
Iteration :  59   Loss :  0.0630016061467
Iteration :  60   Loss :  0.0619207724409
Iteration :  61   Loss :  0.0608649093509
Iteration :  62   Loss :  0.0598334160137
Iteration :  63   Loss :  0.0588257032055
Iteration :  64   Loss :  0.0578411923372
Iteration :  65   Loss :  0.0568793145248
Iteration :  66   Loss :  0.0559395097764
Iteration :  67   Loss :  0.0550212263297
Iteration :  68   Loss :  0.0541239201659
Iteration :  69   Loss :  0.0532470547159
Iteration :  70   Loss :  0.0523901007652
Iteration :  71   Loss :  0.0515525365573
Iteration :  72   Loss :  0.0507338480835
Iteration :  73   Loss :  0.0499335295421
Iteration :  74   Loss :  0.0491510839433
Iteration :  75   Loss :  0.048386023831
Iteration :  76   Loss :  0.0476378720893
Iteration :  77   Loss :  0.0469061628009
Iteration :  78   Loss :  0.0461904421233
Iteration :  79   Loss :  0.0454902691503
Iteration :  80   Loss :  0.0448052167314
Iteration :  81   Loss :  0.0441348722196
Iteration :  82   Loss :  0.0434788381307
Iteration :  83   Loss :  0.0428367326938
Iteration :  84   Loss :  0.0422081902843
Iteration :  85   Loss :  0.0415928617317
Iteration :  86   Loss :  0.0409904145014
Iteration :  87   Loss :  0.0404005327517
Iteration :  88   Loss :  0.0398229172739
Iteration :  89   Loss :  0.0392572853214
Iteration :  90   Loss :  0.0387033703419
Iteration :  91   Loss :  0.0381609216215
Iteration :  92   Loss :  0.0376297038563
Iteration :  93   Loss :  0.0371094966627
Iteration :  94   Loss :  0.03660009404
Iteration :  95   Loss :  0.0361013037971
Iteration :  96   Loss :  0.035612946955
Iteration :  97   Loss :  0.0351348571351
Iteration :  98   Loss :  0.0346668799418
Iteration :  99   Loss :  0.0342088723486
[ -4.03596668e-04   1.75529651e-04  -1.53428418e-04 ...,   1.27882399e-04
  -4.54109117e-05   1.51969466e-04]
CROSS VALIDATION 17
Iteration :  0   Loss :  8.01421224891
Iteration :  1   Loss :  0.211536387205
Iteration :  2   Loss :  0.195031174545
Iteration :  3   Loss :  0.190510506187
Iteration :  4   Loss :  0.186261073365
Iteration :  5   Loss :  0.182201382949
Iteration :  6   Loss :  0.178290767569
Iteration :  7   Loss :  0.174505927525
Iteration :  8   Loss :  0.170832125678
Iteration :  9   Loss :  0.167259299135
Iteration :  10   Loss :  0.163780137174
Iteration :  11   Loss :  0.160389048186
Iteration :  12   Loss :  0.157081567296
Iteration :  13   Loss :  0.153853998701
Iteration :  14   Loss :  0.15070319048
Iteration :  15   Loss :  0.147626387854
Iteration :  16   Loss :  0.144621134797
Iteration :  17   Loss :  0.141685206471
Iteration :  18   Loss :  0.138816561859
Iteration :  19   Loss :  0.136013309956
Iteration :  20   Loss :  0.133273685187
Iteration :  21   Loss :  0.130596029211
Iteration :  22   Loss :  0.127978777132
Iteration :  23   Loss :  0.12542044677
Iteration :  24   Loss :  0.122919630036
Iteration :  25   Loss :  0.120474985723
Iteration :  26   Loss :  0.118085233248
Iteration :  27   Loss :  0.115749147005
Iteration :  28   Loss :  0.113465551135
Iteration :  29   Loss :  0.111233314558
Iteration :  30   Loss :  0.109051346232
Iteration :  31   Loss :  0.106918590605
Iteration :  32   Loss :  0.104834023304
Iteration :  33   Loss :  0.102796647082
Iteration :  34   Loss :  0.10080548811
Iteration :  35   Loss :  0.0988595926556
Iteration :  36   Loss :  0.0969580242192
Iteration :  37   Loss :  0.0950998611718
Iteration :  38   Loss :  0.0932841949296
Iteration :  39   Loss :  0.0915101286774
Iteration :  40   Loss :  0.0897767766306
Iteration :  41   Loss :  0.0880832638055
Iteration :  42   Loss :  0.0864287262411
Iteration :  43   Loss :  0.0848123115986
Iteration :  44   Loss :  0.0832331800462
Iteration :  45   Loss :  0.0816905053264
Iteration :  46   Loss :  0.0801834758975
Iteration :  47   Loss :  0.0787112960418
Iteration :  48   Loss :  0.0772731868414
Iteration :  49   Loss :  0.0758683869357
Iteration :  50   Loss :  0.0744961529935
Iteration :  51   Loss :  0.0731557598566
Iteration :  52   Loss :  0.0718465003334
Iteration :  53   Loss :  0.0705676846465
Iteration :  54   Loss :  0.0693186395626
Iteration :  55   Loss :  0.0680987072472
Iteration :  56   Loss :  0.0669072439085
Iteration :  57   Loss :  0.0657436182972
Iteration :  58   Loss :  0.0646072101415
Iteration :  59   Loss :  0.0634974085904
Iteration :  60   Loss :  0.0624136107377
Iteration :  61   Loss :  0.0613552202897
Iteration :  62   Loss :  0.0603216464295
Iteration :  63   Loss :  0.059312302917
Iteration :  64   Loss :  0.0583266074509
Iteration :  65   Loss :  0.0573639813052
Iteration :  66   Loss :  0.0564238492387
Iteration :  67   Loss :  0.0555056396643
Iteration :  68   Loss :  0.0546087850565
Iteration :  69   Loss :  0.0537327225639
Iteration :  70   Loss :  0.052876894793
Iteration :  71   Loss :  0.0520407507216
Iteration :  72   Loss :  0.0512237467041
Iteration :  73   Loss :  0.0504253475297
Iteration :  74   Loss :  0.049645027499
Iteration :  75   Loss :  0.0488822714893
Iteration :  76   Loss :  0.0481365759837
Iteration :  77   Loss :  0.0474074500446
Iteration :  78   Loss :  0.0466944162177
Iteration :  79   Loss :  0.0459970113573
Iteration :  80   Loss :  0.0453147873657
Iteration :  81   Loss :  0.0446473118448
Iteration :  82   Loss :  0.0439941686574
Iteration :  83   Loss :  0.0433549583983
Iteration :  84   Loss :  0.0427292987746
Iteration :  85   Loss :  0.0421168248955
Iteration :  86   Loss :  0.0415171894718
Iteration :  87   Loss :  0.0409300629241
Iteration :  88   Loss :  0.0403551334014
Iteration :  89   Loss :  0.0397921067094
Iteration :  90   Loss :  0.0392407061503
Iteration :  91   Loss :  0.0387006722767
Iteration :  92   Loss :  0.0381717625635
Iteration :  93   Loss :  0.0376537510016
Iteration :  94   Loss :  0.0371464276203
Iteration :  95   Loss :  0.0366495979456
Iteration :  96   Loss :  0.0361630824021
Iteration :  97   Loss :  0.0356867156693
Iteration :  98   Loss :  0.0352203459989
Iteration :  99   Loss :  0.0347638345061
[ -4.19817472e-04   1.77275221e-04  -1.55593193e-04 ...,   1.33159786e-04
  -4.70041035e-05   1.47062048e-04]
CROSS VALIDATION 18
Iteration :  0   Loss :  7.13643175285
Iteration :  1   Loss :  0.202723602745
Iteration :  2   Loss :  0.197704865545
Iteration :  3   Loss :  0.193119076331
Iteration :  4   Loss :  0.188803179149
Iteration :  5   Loss :  0.184684476875
Iteration :  6   Loss :  0.180723527304
Iteration :  7   Loss :  0.176896249095
Iteration :  8   Loss :  0.173186675403
Iteration :  9   Loss :  0.169583533729
Iteration :  10   Loss :  0.1660784554
Iteration :  11   Loss :  0.162664964494
Iteration :  12   Loss :  0.159337872693
Iteration :  13   Loss :  0.156092899802
Iteration :  14   Loss :  0.152926426461
Iteration :  15   Loss :  0.149835327646
Iteration :  16   Loss :  0.146816857339
Iteration :  17   Loss :  0.143868566619
Iteration :  18   Loss :  0.140988244141
Iteration :  19   Loss :  0.138173872
Iteration :  20   Loss :  0.135423592363
Iteration :  21   Loss :  0.132735681818
Iteration :  22   Loss :  0.130108531347
Iteration :  23   Loss :  0.127540630482
Iteration :  24   Loss :  0.125030554632
Iteration :  25   Loss :  0.122576954861
Iteration :  26   Loss :  0.12017854959
Iteration :  27   Loss :  0.117834117822
Iteration :  28   Loss :  0.115542493604
Iteration :  29   Loss :  0.113302561456
Iteration :  30   Loss :  0.111113252573
Iteration :  31   Loss :  0.108973541606
Iteration :  32   Loss :  0.10688244385
Iteration :  33   Loss :  0.104839012684
Iteration :  34   Loss :  0.102842337104
Iteration :  35   Loss :  0.10089153922
Iteration :  36   Loss :  0.098985771604
Iteration :  37   Loss :  0.0971242143925
Iteration :  38   Loss :  0.0953060720844
Iteration :  39   Loss :  0.0935305700089
Iteration :  40   Loss :  0.091796950472
Iteration :  41   Loss :  0.0901044686306
Iteration :  42   Loss :  0.0884523881777
Iteration :  43   Loss :  0.0868399769556
Iteration :  44   Loss :  0.0852665026369
Iteration :  45   Loss :  0.083731228631
Iteration :  46   Loss :  0.0822334103824
Iteration :  47   Loss :  0.0807722922186
Iteration :  48   Loss :  0.0793471048964
Iteration :  49   Loss :  0.0779570639676
Iteration :  50   Loss :  0.0766013690523
Iteration :  51   Loss :  0.0752792040688
Iteration :  52   Loss :  0.073989738422
Iteration :  53   Loss :  0.0727321291076
Iteration :  54   Loss :  0.0715055236391
Iteration :  55   Loss :  0.0703090636664
Iteration :  56   Loss :  0.0691418891155
Iteration :  57   Loss :  0.0680031426564
Iteration :  58   Loss :  0.0668919742924
Iteration :  59   Loss :  0.0658075458633
Iteration :  60   Loss :  0.064749035271
Iteration :  61   Loss :  0.0637156402603
Iteration :  62   Loss :  0.0627065816222
Iteration :  63   Loss :  0.0617211057315
Iteration :  64   Loss :  0.0607584863705
Iteration :  65   Loss :  0.0598180258338
Iteration :  66   Loss :  0.0588990553482
Iteration :  67   Loss :  0.0580009348711
Iteration :  68   Loss :  0.0571230523553
Iteration :  69   Loss :  0.0562648225837
Iteration :  70   Loss :  0.0554256856869
Iteration :  71   Loss :  0.0546051054569
Iteration :  72   Loss :  0.0538025675685
Iteration :  73   Loss :  0.0530175778103
Iteration :  74   Loss :  0.0522496604166
Iteration :  75   Loss :  0.0514983565745
Iteration :  76   Loss :  0.0507632231652
Iteration :  77   Loss :  0.0500438317764
Iteration :  78   Loss :  0.0493397680059
Iteration :  79   Loss :  0.0486506310544
Iteration :  80   Loss :  0.047976033588
Iteration :  81   Loss :  0.0473156018366
Iteration :  82   Loss :  0.0466689758792
Iteration :  83   Loss :  0.0460358100623
Iteration :  84   Loss :  0.0454157734893
Iteration :  85   Loss :  0.0448085505249
Iteration :  86   Loss :  0.0442138412572
Iteration :  87   Loss :  0.0436313618707
Iteration :  88   Loss :  0.0430608448943
Iteration :  89   Loss :  0.0425020392961
Iteration :  90   Loss :  0.0419547104106
Iteration :  91   Loss :  0.0414186396949
Iteration :  92   Loss :  0.0408936243171
Iteration :  93   Loss :  0.0403794765909
Iteration :  94   Loss :  0.0398760232723
Iteration :  95   Loss :  0.0393831047411
Iteration :  96   Loss :  0.0389005740852
Iteration :  97   Loss :  0.0384282961117
Iteration :  98   Loss :  0.0379661463004
Iteration :  99   Loss :  0.0375140097157
[ -3.53617763e-04  -1.22877250e-04  -5.15969351e-04 ...,   3.69116686e-04
  -1.58131737e-05   1.27083188e-04]
CROSS VALIDATION 19
Iteration :  0   Loss :  7.8550761952
Iteration :  1   Loss :  0.206699147727
Iteration :  2   Loss :  0.189042627028
Iteration :  3   Loss :  0.184713990349
Iteration :  4   Loss :  0.180620116897
Iteration :  5   Loss :  0.17669966472
Iteration :  6   Loss :  0.172919573849
Iteration :  7   Loss :  0.169259882203
Iteration :  8   Loss :  0.165707473299
Iteration :  9   Loss :  0.162253132467
Iteration :  10   Loss :  0.158890020646
Iteration :  11   Loss :  0.155612823221
Iteration :  12   Loss :  0.152417247344
Iteration :  13   Loss :  0.14929971104
Iteration :  14   Loss :  0.146257143489
Iteration :  15   Loss :  0.143286852573
Iteration :  16   Loss :  0.1403864346
Iteration :  17   Loss :  0.137553711252
Iteration :  18   Loss :  0.134786684535
Iteration :  19   Loss :  0.132083503855
Iteration :  20   Loss :  0.129442441384
Iteration :  21   Loss :  0.126861873146
Iteration :  22   Loss :  0.124340264097
Iteration :  23   Loss :  0.121876156006
Iteration :  24   Loss :  0.119468157349
Iteration :  25   Loss :  0.11711493466
Iteration :  26   Loss :  0.114815205012
Iteration :  27   Loss :  0.112567729415
Iteration :  28   Loss :  0.110371306999
Iteration :  29   Loss :  0.108224769953
Iteration :  30   Loss :  0.106126979188
Iteration :  31   Loss :  0.10407682073
Iteration :  32   Loss :  0.102073202848
Iteration :  33   Loss :  0.100115053902
Iteration :  34   Loss :  0.0982013208934
Iteration :  35   Loss :  0.0963309686608
Iteration :  36   Loss :  0.0945029796532
Iteration :  37   Loss :  0.0927163541936
Iteration :  38   Loss :  0.0909701111259
Iteration :  39   Loss :  0.0892632887378
Iteration :  40   Loss :  0.0875949458502
Iteration :  41   Loss :  0.0859641629704
Iteration :  42   Loss :  0.0843700434208
Iteration :  43   Loss :  0.0828117143705
Iteration :  44   Loss :  0.0812883277137
Iteration :  45   Loss :  0.0797990607594
Iteration :  46   Loss :  0.0783431167088
Iteration :  47   Loss :  0.0769197249133
Iteration :  48   Loss :  0.0755281409101
Iteration :  49   Loss :  0.0741676462428
Iteration :  50   Loss :  0.072837548073
Iteration :  51   Loss :  0.0715371785907
Iteration :  52   Loss :  0.0702658942299
Iteration :  53   Loss :  0.0690230746946
Iteration :  54   Loss :  0.0678081217993
Iteration :  55   Loss :  0.0666204581301
Iteration :  56   Loss :  0.0654595255325
Iteration :  57   Loss :  0.0643247834388
Iteration :  58   Loss :  0.0632157070494
Iteration :  59   Loss :  0.0621317853952
Iteration :  60   Loss :  0.0610725193076
Iteration :  61   Loss :  0.0600374193379
Iteration :  62   Loss :  0.0590260036685
Iteration :  63   Loss :  0.0580377960679
Iteration :  64   Loss :  0.0570723239427
Iteration :  65   Loss :  0.0561291165422
Iteration :  66   Loss :  0.055207703371
Iteration :  67   Loss :  0.054307612859
Iteration :  68   Loss :  0.0534283713331
Iteration :  69   Loss :  0.0525695023262
Iteration :  70   Loss :  0.0517305262458
Iteration :  71   Loss :  0.0509109604132
Iteration :  72   Loss :  0.0501103194724
Iteration :  73   Loss :  0.0493281161498
Iteration :  74   Loss :  0.0485638623383
Iteration :  75   Loss :  0.0478170704642
Iteration :  76   Loss :  0.0470872550886
Iteration :  77   Loss :  0.0463739346866
Iteration :  78   Loss :  0.045676633546
Iteration :  79   Loss :  0.0449948837236
Iteration :  80   Loss :  0.0443282270011
Iteration :  81   Loss :  0.0436762167844
Iteration :  82   Loss :  0.0430384198976
Iteration :  83   Loss :  0.0424144182294
Iteration :  84   Loss :  0.0418038101969
Iteration :  85   Loss :  0.0412062120022
Iteration :  86   Loss :  0.0406212586636
Iteration :  87   Loss :  0.040048604812
Iteration :  88   Loss :  0.0394879252522
Iteration :  89   Loss :  0.0389389152916
Iteration :  90   Loss :  0.0384012908488
Iteration :  91   Loss :  0.0378747883549
Iteration :  92   Loss :  0.0373591644668
Iteration :  93   Loss :  0.0368541956127
Iteration :  94   Loss :  0.0363596773902
Iteration :  95   Loss :  0.0358754238414
Iteration :  96   Loss :  0.0354012666247
Iteration :  97   Loss :  0.0349370541054
Iteration :  98   Loss :  0.034482650385
Iteration :  99   Loss :  0.0340379342863
[ -4.82173816e-04   1.76352418e-04  -1.93666351e-04 ...,   1.15707333e-04
  -5.10882434e-05   1.56326527e-04]
Accuracy (Logistic Loss):	0.95
Hinge Loss
CROSS VALIDATION 0
Iteration :  0   Loss :  24.9873436673
Iteration :  1   Loss :  0.282621840961
Iteration :  2   Loss :  0.276692735604
Iteration :  3   Loss :  0.270888016566
Iteration :  4   Loss :  0.265205074354
Iteration :  5   Loss :  0.25964135422
Iteration :  6   Loss :  0.254194355011
Iteration :  7   Loss :  0.248861628047
Iteration :  8   Loss :  0.243640776018
Iteration :  9   Loss :  0.238529451906
Iteration :  10   Loss :  0.233525357933
Iteration :  11   Loss :  0.228626244524
Iteration :  12   Loss :  0.2238299093
Iteration :  13   Loss :  0.219134196082
Iteration :  14   Loss :  0.214536993929
Iteration :  15   Loss :  0.210036236183
Iteration :  16   Loss :  0.205629899543
Iteration :  17   Loss :  0.201316003155
Iteration :  18   Loss :  0.197092607721
Iteration :  19   Loss :  0.192957814627
Iteration :  20   Loss :  0.188909765091
Iteration :  21   Loss :  0.184946639324
Iteration :  22   Loss :  0.181066655717
Iteration :  23   Loss :  0.177268070036
Iteration :  24   Loss :  0.173549174638
Iteration :  25   Loss :  0.169908297707
Iteration :  26   Loss :  0.166343802498
Iteration :  27   Loss :  0.162854086604
Iteration :  28   Loss :  0.159437581234
Iteration :  29   Loss :  0.15609275051
Iteration :  30   Loss :  0.152818090774
Iteration :  31   Loss :  0.149612129913
Iteration :  32   Loss :  0.146473426698
Iteration :  33   Loss :  0.143400570135
Iteration :  34   Loss :  0.140392178832
Iteration :  35   Loss :  0.137446900375
Iteration :  36   Loss :  0.134563410725
Iteration :  37   Loss :  0.131740413618
Iteration :  38   Loss :  0.128976639986
Iteration :  39   Loss :  0.126270847382
Iteration :  40   Loss :  0.123621819426
Iteration :  41   Loss :  0.121028365257
Iteration :  42   Loss :  0.118489318995
Iteration :  43   Loss :  0.116003539221
Iteration :  44   Loss :  0.11356990846
Iteration :  45   Loss :  0.111187332682
Iteration :  46   Loss :  0.108854740808
Iteration :  47   Loss :  0.106571084228
Iteration :  48   Loss :  0.104335336332
Iteration :  49   Loss :  0.102146492046
Iteration :  50   Loss :  0.100003567384
Iteration :  51   Loss :  0.0979055989993
Iteration :  52   Loss :  0.0958516437583
Iteration :  53   Loss :  0.0938407783117
Iteration :  54   Loss :  0.0918720986815
Iteration :  55   Loss :  0.0899447198542
Iteration :  56   Loss :  0.0880577753828
Iteration :  57   Loss :  0.0862104169977
Iteration :  58   Loss :  0.0844018142249
Iteration :  59   Loss :  0.0826311540129
Iteration :  60   Loss :  0.0808976403672
Iteration :  61   Loss :  0.0792004939924
Iteration :  62   Loss :  0.0775389519418
Iteration :  63   Loss :  0.0759122672746
Iteration :  64   Loss :  0.07431970872
Iteration :  65   Loss :  0.0727605603486
Iteration :  66   Loss :  0.0712341212502
Iteration :  67   Loss :  0.069739705219
Iteration :  68   Loss :  0.0682766404454
Iteration :  69   Loss :  0.0668442692131
Iteration :  70   Loss :  0.0654419476045
Iteration :  71   Loss :  0.0640690452104
Iteration :  72   Loss :  0.062724944847
Iteration :  73   Loss :  0.0614090422783
Iteration :  74   Loss :  0.0601207459446
Iteration :  75   Loss :  0.0588594766966
Iteration :  76   Loss :  0.0576246675347
Iteration :  77   Loss :  0.0564157633545
Iteration :  78   Loss :  0.0552322206971
Iteration :  79   Loss :  0.0540735075047
Iteration :  80   Loss :  0.0529391028815
Iteration :  81   Loss :  0.0518284968597
Iteration :  82   Loss :  0.0507411901699
Iteration :  83   Loss :  0.049676694017
Iteration :  84   Loss :  0.0486345298601
Iteration :  85   Loss :  0.0476142291978
Iteration :  86   Loss :  0.0466153333573
Iteration :  87   Loss :  0.0456373932881
Iteration :  88   Loss :  0.0446799693605
Iteration :  89   Loss :  0.0437426311676
Iteration :  90   Loss :  0.0428249573322
Iteration :  91   Loss :  0.0419265353169
Iteration :  92   Loss :  0.0410469612391
Iteration :  93   Loss :  0.0401858396891
Iteration :  94   Loss :  0.0393427835525
Iteration :  95   Loss :  0.0385174138362
Iteration :  96   Loss :  0.0377093594978
Iteration :  97   Loss :  0.0369182572793
Iteration :  98   Loss :  0.0361437515431
Iteration :  99   Loss :  0.0353854941128
[ -7.19350667e-04   5.28731121e-05   8.03733400e-05 ...,   2.21954817e-04
   2.93276678e-04  -3.52224393e-05]
CROSS VALIDATION 1
Iteration :  0   Loss :  14.9480597088
Iteration :  1   Loss :  0.584770405395
Iteration :  2   Loss :  0.298205283045
Iteration :  3   Loss :  0.291949253662
Iteration :  4   Loss :  0.285824469117
Iteration :  5   Loss :  0.279828176031
Iteration :  6   Loss :  0.273957678791
Iteration :  7   Loss :  0.268210338333
Iteration :  8   Loss :  0.26258357096
Iteration :  9   Loss :  0.257074847176
Iteration :  10   Loss :  0.251681690553
Iteration :  11   Loss :  0.246401676614
Iteration :  12   Loss :  0.241232431747
Iteration :  13   Loss :  0.236171632134
Iteration :  14   Loss :  0.23121700271
Iteration :  15   Loss :  0.226366316137
Iteration :  16   Loss :  0.221617391804
Iteration :  17   Loss :  0.216968094848
Iteration :  18   Loss :  0.212416335193
Iteration :  19   Loss :  0.207960066609
Iteration :  20   Loss :  0.203597285797
Iteration :  21   Loss :  0.199326031482
Iteration :  22   Loss :  0.195144383535
Iteration :  23   Loss :  0.191050462111
Iteration :  24   Loss :  0.187042426799
Iteration :  25   Loss :  0.183118475802
Iteration :  26   Loss :  0.17927684512
Iteration :  27   Loss :  0.175515807759
Iteration :  28   Loss :  0.171833672959
Iteration :  29   Loss :  0.168228785427
Iteration :  30   Loss :  0.164699524598
Iteration :  31   Loss :  0.161244303903
Iteration :  32   Loss :  0.157861570061
Iteration :  33   Loss :  0.154549802374
Iteration :  34   Loss :  0.151307512047
Iteration :  35   Loss :  0.148133241519
Iteration :  36   Loss :  0.145025563808
Iteration :  37   Loss :  0.141983081868
Iteration :  38   Loss :  0.139004427959
Iteration :  39   Loss :  0.136088263038
Iteration :  40   Loss :  0.133233276153
Iteration :  41   Loss :  0.130438183854
Iteration :  42   Loss :  0.127701729616
Iteration :  43   Loss :  0.125022683274
Iteration :  44   Loss :  0.122399840472
Iteration :  45   Loss :  0.119832022121
Iteration :  46   Loss :  0.117318073865
Iteration :  47   Loss :  0.114856865567
Iteration :  48   Loss :  0.112447290799
Iteration :  49   Loss :  0.110088266344
Iteration :  50   Loss :  0.107778731711
Iteration :  51   Loss :  0.105517648656
Iteration :  52   Loss :  0.103304000717
Iteration :  53   Loss :  0.101136792755
Iteration :  54   Loss :  0.0990150505087
Iteration :  55   Loss :  0.0969378201561
Iteration :  56   Loss :  0.0949041678849
Iteration :  57   Loss :  0.0929131794735
Iteration :  58   Loss :  0.0909639598794
Iteration :  59   Loss :  0.0890556328374
Iteration :  60   Loss :  0.0871873404652
Iteration :  61   Loss :  0.085358242878
Iteration :  62   Loss :  0.0887266079926
Iteration :  63   Loss :  2.08725242368
Iteration :  64   Loss :  0.131199907584
Iteration :  65   Loss :  0.128447473193
Iteration :  66   Loss :  0.125752781945
Iteration :  67   Loss :  0.12311462245
Iteration :  68   Loss :  0.120531808734
Iteration :  69   Loss :  0.118003179699
Iteration :  70   Loss :  0.11552759861
Iteration :  71   Loss :  0.113103952577
Iteration :  72   Loss :  0.110731152057
Iteration :  73   Loss :  0.108408130367
Iteration :  74   Loss :  0.106133843198
Iteration :  75   Loss :  0.103907268153
Iteration :  76   Loss :  0.101727404283
Iteration :  77   Loss :  0.0995932716352
Iteration :  78   Loss :  0.0975039108189
Iteration :  79   Loss :  0.0954583825683
Iteration :  80   Loss :  0.0934557673228
Iteration :  81   Loss :  0.0914951648133
Iteration :  82   Loss :  0.0895756936572
Iteration :  83   Loss :  0.0876964909626
Iteration :  84   Loss :  0.0858567119401
Iteration :  85   Loss :  0.0840555295228
Iteration :  86   Loss :  0.0822921339953
Iteration :  87   Loss :  0.0805657326286
Iteration :  88   Loss :  0.0788755493248
Iteration :  89   Loss :  0.0781164604415
Iteration :  90   Loss :  0.107817087221
Iteration :  91   Loss :  0.105555199509
Iteration :  92   Loss :  0.103340763793
Iteration :  93   Loss :  0.10117278458
Iteration :  94   Loss :  0.0990502872639
Iteration :  95   Loss :  0.0969723176817
Iteration :  96   Loss :  0.0949379416892
Iteration :  97   Loss :  0.0929462447393
Iteration :  98   Loss :  0.0909963314711
Iteration :  99   Loss :  0.0890873253076
[ -9.39498318e-04   7.45379140e-05  -3.62801387e-04 ...,   1.06801468e-03
   4.06331795e-04   1.73370619e-04]
CROSS VALIDATION 2
Iteration :  0   Loss :  10.7071614343
Iteration :  1   Loss :  2.94761409594
Iteration :  2   Loss :  0.298680109247
Iteration :  3   Loss :  0.292414118517
Iteration :  4   Loss :  0.286279581601
Iteration :  5   Loss :  0.28027374074
Iteration :  6   Loss :  0.274393896026
Iteration :  7   Loss :  0.268637404195
Iteration :  8   Loss :  0.263001677435
Iteration :  9   Loss :  0.257484182222
Iteration :  10   Loss :  0.252082438186
Iteration :  11   Loss :  0.246794016988
Iteration :  12   Loss :  0.241616541238
Iteration :  13   Loss :  0.236547683417
Iteration :  14   Loss :  0.231585164837
Iteration :  15   Loss :  0.226726754614
Iteration :  16   Loss :  0.221970268666
Iteration :  17   Loss :  0.217313568729
Iteration :  18   Loss :  0.2127545614
Iteration :  19   Loss :  0.208291197191
Iteration :  20   Loss :  0.203921469611
Iteration :  21   Loss :  0.199643414265
Iteration :  22   Loss :  0.195455107965
Iteration :  23   Loss :  0.191354667873
Iteration :  24   Loss :  0.187340250648
Iteration :  25   Loss :  0.183410051624
Iteration :  26   Loss :  0.179562303991
Iteration :  27   Loss :  0.175795278008
Iteration :  28   Loss :  0.172107280219
Iteration :  29   Loss :  0.168496652698
Iteration :  30   Loss :  0.164961772298
Iteration :  31   Loss :  0.161501049927
Iteration :  32   Loss :  0.158112929826
Iteration :  33   Loss :  0.154795888878
Iteration :  34   Loss :  0.151548435919
Iteration :  35   Loss :  0.148369111065
Iteration :  36   Loss :  0.145256485062
Iteration :  37   Loss :  0.142209158639
Iteration :  38   Loss :  0.13922576188
Iteration :  39   Loss :  0.13630495361
Iteration :  40   Loss :  0.133445420787
Iteration :  41   Loss :  0.130645877919
Iteration :  42   Loss :  0.12790506648
Iteration :  43   Loss :  0.125221754348
Iteration :  44   Loss :  0.122594735247
Iteration :  45   Loss :  0.12002282821
Iteration :  46   Loss :  0.117504877046
Iteration :  47   Loss :  0.115039749816
Iteration :  48   Loss :  0.112626338331
Iteration :  49   Loss :  0.11026355765
Iteration :  50   Loss :  0.107950345592
Iteration :  51   Loss :  0.105685662261
Iteration :  52   Loss :  0.103468489575
Iteration :  53   Loss :  0.101297830811
Iteration :  54   Loss :  0.0991727101582
Iteration :  55   Loss :  0.097092172274
Iteration :  56   Loss :  0.0950552818597
Iteration :  57   Loss :  0.109468800417
Iteration :  58   Loss :  0.10717226152
Iteration :  59   Loss :  0.104923901564
Iteration :  60   Loss :  0.102722709806
Iteration :  61   Loss :  0.100567696709
Iteration :  62   Loss :  0.0984578934918
Iteration :  63   Loss :  0.0963923517003
Iteration :  64   Loss :  0.0943701427767
Iteration :  65   Loss :  0.0923903576435
Iteration :  66   Loss :  0.0904521062949
Iteration :  67   Loss :  0.0885545173961
Iteration :  68   Loss :  0.0866967378924
Iteration :  69   Loss :  0.0848779326249
Iteration :  70   Loss :  0.0830972839558
Iteration :  71   Loss :  0.0813539914001
Iteration :  72   Loss :  0.0796472712664
Iteration :  73   Loss :  0.0779763563042
Iteration :  74   Loss :  0.0763404953591
Iteration :  75   Loss :  0.0747389530352
Iteration :  76   Loss :  0.0731710093644
Iteration :  77   Loss :  0.0716359594825
Iteration :  78   Loss :  0.0701331133131
Iteration :  79   Loss :  0.0686617952564
Iteration :  80   Loss :  0.0672213438862
Iteration :  81   Loss :  0.0658111116524
Iteration :  82   Loss :  0.0644304645895
Iteration :  83   Loss :  0.0630787820323
Iteration :  84   Loss :  0.0617554563361
Iteration :  85   Loss :  0.0604598926044
Iteration :  86   Loss :  0.0591915084205
Iteration :  87   Loss :  0.0579497335865
Iteration :  88   Loss :  0.0567340098666
Iteration :  89   Loss :  0.0555437907361
Iteration :  90   Loss :  0.0543785411359
Iteration :  91   Loss :  0.0532377372319
Iteration :  92   Loss :  0.0521208661793
Iteration :  93   Loss :  0.0510274258925
Iteration :  94   Loss :  0.0499569248188
Iteration :  95   Loss :  0.048908881718
Iteration :  96   Loss :  0.0478828254458
Iteration :  97   Loss :  0.0468782947419
Iteration :  98   Loss :  0.0458948380228
Iteration :  99   Loss :  0.0449320131787
[ -4.48692799e-04  -1.20252892e-05  -2.92905430e-04 ...,   7.58254788e-04
   2.90507161e-04   9.86985257e-05]
CROSS VALIDATION 3
Iteration :  0   Loss :  21.0485112246
Iteration :  1   Loss :  1.63825419437
Iteration :  2   Loss :  1.09828607164
Iteration :  3   Loss :  0.308545151511
Iteration :  4   Loss :  0.302072202696
Iteration :  5   Loss :  0.295735049456
Iteration :  6   Loss :  0.289530842945
Iteration :  7   Loss :  0.283456794082
Iteration :  8   Loss :  0.277510172297
Iteration :  9   Loss :  0.271688304308
Iteration :  10   Loss :  0.265988572911
Iteration :  11   Loss :  0.260408415811
Iteration :  12   Loss :  0.254945324466
Iteration :  13   Loss :  0.249596842963
Iteration :  14   Loss :  0.244360566907
Iteration :  15   Loss :  0.239234142349
Iteration :  16   Loss :  0.234215264721
Iteration :  17   Loss :  0.229301677802
Iteration :  18   Loss :  0.224491172706
Iteration :  19   Loss :  0.219781586886
Iteration :  20   Loss :  0.215170803163
Iteration :  21   Loss :  0.210656748774
Iteration :  22   Loss :  0.206237394441
Iteration :  23   Loss :  0.201910753457
Iteration :  24   Loss :  0.197674880794
Iteration :  25   Loss :  0.193527872231
Iteration :  26   Loss :  0.189467863493
Iteration :  27   Loss :  0.185493029416
Iteration :  28   Loss :  0.181601583127
Iteration :  29   Loss :  0.177791775238
Iteration :  30   Loss :  0.174061893063
Iteration :  31   Loss :  0.170410259845
Iteration :  32   Loss :  0.166835234005
Iteration :  33   Loss :  0.163335208401
Iteration :  34   Loss :  0.159908609608
Iteration :  35   Loss :  0.15655389721
Iteration :  36   Loss :  0.153269563107
Iteration :  37   Loss :  0.150054130837
Iteration :  38   Loss :  0.146906154913
Iteration :  39   Loss :  0.143824220173
Iteration :  40   Loss :  0.140806941142
Iteration :  41   Loss :  0.137852961414
Iteration :  42   Loss :  0.134960953036
Iteration :  43   Loss :  0.132129615915
Iteration :  44   Loss :  0.129357677232
Iteration :  45   Loss :  0.126643890872
Iteration :  46   Loss :  0.123987036861
Iteration :  47   Loss :  0.121385920819
Iteration :  48   Loss :  0.118839373422
Iteration :  49   Loss :  0.116346249879
Iteration :  50   Loss :  0.113905429414
Iteration :  51   Loss :  0.111515814763
Iteration :  52   Loss :  0.109176331684
Iteration :  53   Loss :  0.10688592847
Iteration :  54   Loss :  0.104643575477
Iteration :  55   Loss :  0.102448264662
Iteration :  56   Loss :  0.100299009132
Iteration :  57   Loss :  0.0981948426939
Iteration :  58   Loss :  0.0961348194281
Iteration :  59   Loss :  0.0941180132574
Iteration :  60   Loss :  0.0921435175331
Iteration :  61   Loss :  0.0902104446272
Iteration :  62   Loss :  0.088317925533
Iteration :  63   Loss :  0.0864651094747
Iteration :  64   Loss :  0.0846511635249
Iteration :  65   Loss :  0.08287527223
Iteration :  66   Loss :  0.081136637244
Iteration :  67   Loss :  0.0794344769691
Iteration :  68   Loss :  0.0777680262048
Iteration :  69   Loss :  0.0761365358034
Iteration :  70   Loss :  0.0745392723338
Iteration :  71   Loss :  0.0729755177515
Iteration :  72   Loss :  0.0714445690756
Iteration :  73   Loss :  0.0699457380731
Iteration :  74   Loss :  0.0684783509495
Iteration :  75   Loss :  0.0670417480456
Iteration :  76   Loss :  0.0656352835413
Iteration :  77   Loss :  0.0642583251651
Iteration :  78   Loss :  0.0629102539097
Iteration :  79   Loss :  0.0615904637542
Iteration :  80   Loss :  0.0602983613911
Iteration :  81   Loss :  0.0590333659601
Iteration :  82   Loss :  0.0577949087866
Iteration :  83   Loss :  0.0565824331261
Iteration :  84   Loss :  0.0553953939143
Iteration :  85   Loss :  0.0542332575215
Iteration :  86   Loss :  0.053095501513
Iteration :  87   Loss :  0.0519816144144
Iteration :  88   Loss :  0.0508910954814
Iteration :  89   Loss :  0.0498234544746
Iteration :  90   Loss :  0.0487782114396
Iteration :  91   Loss :  0.0477548964907
Iteration :  92   Loss :  0.0467530496001
Iteration :  93   Loss :  0.0457722203907
Iteration :  94   Loss :  0.044811967934
Iteration :  95   Loss :  0.0438718605516
Iteration :  96   Loss :  0.0429514756213
Iteration :  97   Loss :  0.042050399387
Iteration :  98   Loss :  0.0411682267729
Iteration :  99   Loss :  0.040304561201
[ -1.02908247e-03  -1.03757346e-04  -1.84246388e-04 ...,   7.82692432e-04
   4.72304691e-04   7.07077384e-05]
CROSS VALIDATION 4
Iteration :  0   Loss :  21.0485112246
Iteration :  1   Loss :  1.63825419437
Iteration :  2   Loss :  1.09828607164
Iteration :  3   Loss :  0.308545151511
Iteration :  4   Loss :  0.302072202696
Iteration :  5   Loss :  0.295735049456
Iteration :  6   Loss :  0.289530842945
Iteration :  7   Loss :  0.283456794082
Iteration :  8   Loss :  0.277510172297
Iteration :  9   Loss :  0.271688304308
Iteration :  10   Loss :  0.265988572911
Iteration :  11   Loss :  0.260408415811
Iteration :  12   Loss :  0.254945324466
Iteration :  13   Loss :  0.249596842963
Iteration :  14   Loss :  0.244360566907
Iteration :  15   Loss :  0.239234142349
Iteration :  16   Loss :  0.234215264721
Iteration :  17   Loss :  0.229301677802
Iteration :  18   Loss :  0.224491172706
Iteration :  19   Loss :  0.219781586886
Iteration :  20   Loss :  0.215170803163
Iteration :  21   Loss :  0.210656748774
Iteration :  22   Loss :  0.206237394441
Iteration :  23   Loss :  0.201910753457
Iteration :  24   Loss :  0.197674880794
Iteration :  25   Loss :  0.193527872231
Iteration :  26   Loss :  0.189467863493
Iteration :  27   Loss :  0.185493029416
Iteration :  28   Loss :  0.181601583127
Iteration :  29   Loss :  0.177791775238
Iteration :  30   Loss :  0.174061893063
Iteration :  31   Loss :  0.170410259845
Iteration :  32   Loss :  0.166835234005
Iteration :  33   Loss :  0.163335208401
Iteration :  34   Loss :  0.159908609608
Iteration :  35   Loss :  0.15655389721
Iteration :  36   Loss :  0.153269563107
Iteration :  37   Loss :  0.150054130837
Iteration :  38   Loss :  0.146906154913
Iteration :  39   Loss :  0.143824220173
Iteration :  40   Loss :  0.140806941142
Iteration :  41   Loss :  0.137852961414
Iteration :  42   Loss :  0.134960953036
Iteration :  43   Loss :  0.132129615915
Iteration :  44   Loss :  0.129357677232
Iteration :  45   Loss :  0.126643890872
Iteration :  46   Loss :  0.123987036861
Iteration :  47   Loss :  0.121385920819
Iteration :  48   Loss :  0.118839373422
Iteration :  49   Loss :  0.116346249879
Iteration :  50   Loss :  0.113905429414
Iteration :  51   Loss :  0.111515814763
Iteration :  52   Loss :  0.109176331684
Iteration :  53   Loss :  0.10688592847
Iteration :  54   Loss :  0.104643575477
Iteration :  55   Loss :  0.102448264662
Iteration :  56   Loss :  0.100299009132
Iteration :  57   Loss :  0.0981948426939
Iteration :  58   Loss :  0.0961348194281
Iteration :  59   Loss :  0.0941180132574
Iteration :  60   Loss :  0.0921435175331
Iteration :  61   Loss :  0.0902104446272
Iteration :  62   Loss :  0.088317925533
Iteration :  63   Loss :  0.0864651094747
Iteration :  64   Loss :  0.0846511635249
Iteration :  65   Loss :  0.08287527223
Iteration :  66   Loss :  0.081136637244
Iteration :  67   Loss :  0.0794344769691
Iteration :  68   Loss :  0.0777680262048
Iteration :  69   Loss :  0.0761365358034
Iteration :  70   Loss :  0.0745392723338
Iteration :  71   Loss :  0.0729755177515
Iteration :  72   Loss :  0.0714445690756
Iteration :  73   Loss :  0.0699457380731
Iteration :  74   Loss :  0.0684783509495
Iteration :  75   Loss :  0.0670417480456
Iteration :  76   Loss :  0.0656352835413
Iteration :  77   Loss :  0.0642583251651
Iteration :  78   Loss :  0.0629102539097
Iteration :  79   Loss :  0.0615904637542
Iteration :  80   Loss :  0.0602983613911
Iteration :  81   Loss :  0.0590333659601
Iteration :  82   Loss :  0.0577949087866
Iteration :  83   Loss :  0.0565824331261
Iteration :  84   Loss :  0.0553953939143
Iteration :  85   Loss :  0.0542332575215
Iteration :  86   Loss :  0.053095501513
Iteration :  87   Loss :  0.0519816144144
Iteration :  88   Loss :  0.0508910954814
Iteration :  89   Loss :  0.0498234544746
Iteration :  90   Loss :  0.0487782114396
Iteration :  91   Loss :  0.0477548964907
Iteration :  92   Loss :  0.0467530496001
Iteration :  93   Loss :  0.0457722203907
Iteration :  94   Loss :  0.044811967934
Iteration :  95   Loss :  0.0438718605516
Iteration :  96   Loss :  0.0429514756213
Iteration :  97   Loss :  0.042050399387
Iteration :  98   Loss :  0.0411682267729
Iteration :  99   Loss :  0.040304561201
[ -1.02908247e-03  -1.03757346e-04  -1.84246388e-04 ...,   7.82692432e-04
   4.72304691e-04   7.07077384e-05]
CROSS VALIDATION 5
Iteration :  0   Loss :  9.89329423499
Iteration :  1   Loss :  0.292275507328
Iteration :  2   Loss :  0.286143878328
Iteration :  3   Loss :  0.280140884377
Iteration :  4   Loss :  0.274263826848
Iteration :  5   Loss :  0.26851006373
Iteration :  6   Loss :  0.262877008437
Iteration :  7   Loss :  0.257362128647
Iteration :  8   Loss :  0.251962945165
Iteration :  9   Loss :  0.246677030804
Iteration :  10   Loss :  0.241502009299
Iteration :  11   Loss :  0.236435554236
Iteration :  12   Loss :  0.231475388007
Iteration :  13   Loss :  0.226619280786
Iteration :  14   Loss :  0.221865049524
Iteration :  15   Loss :  0.217210556972
Iteration :  16   Loss :  0.21265371072
Iteration :  17   Loss :  0.208192462251
Iteration :  18   Loss :  0.203824806025
Iteration :  19   Loss :  0.199548778578
Iteration :  20   Loss :  0.195362457634
Iteration :  21   Loss :  0.191263961247
Iteration :  22   Loss :  0.187251446951
Iteration :  23   Loss :  0.183323110934
Iteration :  24   Loss :  0.179477187224
Iteration :  25   Loss :  0.1757119469
Iteration :  26   Loss :  0.172025697309
Iteration :  27   Loss :  0.168416781311
Iteration :  28   Loss :  0.164883576528
Iteration :  29   Loss :  0.16142449462
Iteration :  30   Loss :  0.158037980568
Iteration :  31   Loss :  0.154722511976
Iteration :  32   Loss :  0.151476598386
Iteration :  33   Loss :  0.148298780607
Iteration :  34   Loss :  0.145187630062
Iteration :  35   Loss :  0.142141748144
Iteration :  36   Loss :  0.139159765586
Iteration :  37   Loss :  0.136240341847
Iteration :  38   Loss :  0.13338216451
Iteration :  39   Loss :  0.130583948692
Iteration :  40   Loss :  0.127844436462
Iteration :  41   Loss :  0.125162396283
Iteration :  42   Loss :  0.122536622451
Iteration :  43   Loss :  0.119965934559
Iteration :  44   Loss :  0.117449176963
Iteration :  45   Loss :  0.114985218261
Iteration :  46   Loss :  0.11257295079
Iteration :  47   Loss :  0.110211290123
Iteration :  48   Loss :  0.107899174582
Iteration :  49   Loss :  0.105635564764
Iteration :  50   Loss :  0.10341944307
Iteration :  51   Loss :  0.101249813251
Iteration :  52   Loss :  0.0991256999546
Iteration :  53   Loss :  0.0970461482945
Iteration :  54   Loss :  0.0950102234143
Iteration :  55   Loss :  0.0930170100708
Iteration :  56   Loss :  0.0910656122213
Iteration :  57   Loss :  0.0891551526213
Iteration :  58   Loss :  0.08728477243
Iteration :  59   Loss :  0.0854536308239
Iteration :  60   Loss :  0.0836609046194
Iteration :  61   Loss :  0.0819057879022
Iteration :  62   Loss :  0.0801874916653
Iteration :  63   Loss :  0.0785052434543
Iteration :  64   Loss :  0.0768582870198
Iteration :  65   Loss :  0.0752458819781
Iteration :  66   Loss :  0.0736673034777
Iteration :  67   Loss :  0.0721218418737
Iteration :  68   Loss :  0.0706088024089
Iteration :  69   Loss :  0.0691275049014
Iteration :  70   Loss :  0.0676772834386
Iteration :  71   Loss :  0.0662574860784
Iteration :  72   Loss :  0.0648674745552
Iteration :  73   Loss :  0.0635066239941
Iteration :  74   Loss :  0.0621743226291
Iteration :  75   Loss :  0.0608699715284
Iteration :  76   Loss :  0.0595929843252
Iteration :  77   Loss :  0.0583427869541
Iteration :  78   Loss :  0.057118817393
Iteration :  79   Loss :  0.0559205254103
Iteration :  80   Loss :  0.0547473723177
Iteration :  81   Loss :  0.0535988307281
Iteration :  82   Loss :  0.0524743843184
Iteration :  83   Loss :  0.0513735275973
Iteration :  84   Loss :  0.0502957656783
Iteration :  85   Loss :  0.049240614057
Iteration :  86   Loss :  0.0482075983935
Iteration :  87   Loss :  0.0471962542989
Iteration :  88   Loss :  0.0462061271268
Iteration :  89   Loss :  0.0452367717687
Iteration :  90   Loss :  0.044287752454
Iteration :  91   Loss :  0.0433586425543
Iteration :  92   Loss :  0.0424490243912
Iteration :  93   Loss :  0.0415584890489
Iteration :  94   Loss :  0.04068663619
Iteration :  95   Loss :  0.0398330738759
Iteration :  96   Loss :  0.0389974183905
Iteration :  97   Loss :  0.0381792940676
Iteration :  98   Loss :  0.0373783331221
Iteration :  99   Loss :  0.0365941754845
[ -3.54146849e-04   2.83508198e-04  -3.34655430e-04 ...,   7.17442851e-05
   9.37207667e-05   2.08685346e-04]
CROSS VALIDATION 6
Iteration :  0   Loss :  21.0459817783
Iteration :  1   Loss :  1.63809714464
Iteration :  2   Loss :  0.296512169002
Iteration :  3   Loss :  0.29029165935
Iteration :  4   Loss :  0.284201649369
Iteration :  5   Loss :  0.278239401314
Iteration :  6   Loss :  0.272402234875
Iteration :  7   Loss :  0.266687525975
Iteration :  8   Loss :  0.261092705584
Iteration :  9   Loss :  0.255615258569
Iteration :  10   Loss :  0.250252722561
Iteration :  11   Loss :  0.245002686849
Iteration :  12   Loss :  0.239862791298
Iteration :  13   Loss :  0.234830725284
Iteration :  14   Loss :  0.229904226658
Iteration :  15   Loss :  0.225081080729
Iteration :  16   Loss :  0.220359119267
Iteration :  17   Loss :  0.215736219529
Iteration :  18   Loss :  0.211210303307
Iteration :  19   Loss :  0.206779335988
Iteration :  20   Loss :  0.202441325648
Iteration :  21   Loss :  0.198194322146
Iteration :  22   Loss :  0.194036416256
Iteration :  23   Loss :  0.189965738806
Iteration :  24   Loss :  0.185980459835
Iteration :  25   Loss :  0.182078787774
Iteration :  26   Loss :  0.178258968639
Iteration :  27   Loss :  0.174519285244
Iteration :  28   Loss :  0.170858056425
Iteration :  29   Loss :  0.167273636289
Iteration :  30   Loss :  0.163764413472
Iteration :  31   Loss :  0.160328810414
Iteration :  32   Loss :  0.15696528265
Iteration :  33   Loss :  0.153672318119
Iteration :  34   Loss :  0.150448436477
Iteration :  35   Loss :  0.147292188441
Iteration :  36   Loss :  0.144202155128
Iteration :  37   Loss :  0.141176947425
Iteration :  38   Loss :  0.138215205359
Iteration :  39   Loss :  0.135315597488
Iteration :  40   Loss :  0.132476820304
Iteration :  41   Loss :  0.129697597643
Iteration :  42   Loss :  0.126976680115
Iteration :  43   Loss :  0.12431284454
Iteration :  44   Loss :  0.121704893399
Iteration :  45   Loss :  0.119151654297
Iteration :  46   Loss :  0.116651979434
Iteration :  47   Loss :  0.114204745088
Iteration :  48   Loss :  0.111808851113
Iteration :  49   Loss :  0.109463220444
Iteration :  50   Loss :  0.107166798608
Iteration :  51   Loss :  0.104918553259
Iteration :  52   Loss :  0.102717473703
Iteration :  53   Loss :  0.100562570453
Iteration :  54   Loss :  0.0984528747795
Iteration :  55   Loss :  0.0963874382753
Iteration :  56   Loss :  0.0943653324301
Iteration :  57   Loss :  0.0923856482129
Iteration :  58   Loss :  0.090447495663
Iteration :  59   Loss :  0.0885500034905
Iteration :  60   Loss :  0.0866923186837
Iteration :  61   Loss :  0.0848736061265
Iteration :  62   Loss :  0.0830930482227
Iteration :  63   Loss :  0.0813498445282
Iteration :  64   Loss :  0.0796432113914
Iteration :  65   Loss :  0.0779723816011
Iteration :  66   Loss :  0.0763366040411
Iteration :  67   Loss :  0.0747351433529
Iteration :  68   Loss :  0.0731672796051
Iteration :  69   Loss :  0.0716323079696
Iteration :  70   Loss :  0.070129538405
Iteration :  71   Loss :  0.0686582953461
Iteration :  72   Loss :  0.0672179174003
Iteration :  73   Loss :  0.0658077570505
Iteration :  74   Loss :  0.0644271803636
Iteration :  75   Loss :  0.063075566706
Iteration :  76   Loss :  0.0617523084639
Iteration :  77   Loss :  0.0604568107712
Iteration :  78   Loss :  0.0591884912408
Iteration :  79   Loss :  0.0579467797041
Iteration :  80   Loss :  0.0567311179535
Iteration :  81   Loss :  0.0555409594923
Iteration :  82   Loss :  0.0543757692886
Iteration :  83   Loss :  0.0532350235349
Iteration :  84   Loss :  0.0521182094128
Iteration :  85   Loss :  0.0510248248621
Iteration :  86   Loss :  0.0499543783553
Iteration :  87   Loss :  0.0489063886766
Iteration :  88   Loss :  0.0478803847058
Iteration :  89   Loss :  0.046875905206
Iteration :  90   Loss :  0.0458924986168
Iteration :  91   Loss :  0.044929722851
Iteration :  92   Loss :  1.83442798999
Iteration :  93   Loss :  0.110956876061
Iteration :  94   Loss :  0.108629118921
Iteration :  95   Loss :  0.106350195648
Iteration :  96   Loss :  0.104119081759
Iteration :  97   Loss :  0.101934774264
Iteration :  98   Loss :  0.0997962912153
Iteration :  99   Loss :  0.0977026712644
[-0.00063591  0.00037123 -0.00022731 ...,  0.00098958  0.00074402
  0.00022141]
CROSS VALIDATION 7
Iteration :  0   Loss :  21.0459817783
Iteration :  1   Loss :  1.63809714464
Iteration :  2   Loss :  1.09780037881
Iteration :  3   Loss :  0.308557589914
Iteration :  4   Loss :  0.302084380155
Iteration :  5   Loss :  0.295746971445
Iteration :  6   Loss :  0.289542514823
Iteration :  7   Loss :  0.283468221096
Iteration :  8   Loss :  0.277521359585
Iteration :  9   Loss :  0.271699256898
Iteration :  10   Loss :  0.265999295728
Iteration :  11   Loss :  0.260418913675
Iteration :  12   Loss :  0.254955602096
Iteration :  13   Loss :  0.249606904979
Iteration :  14   Loss :  0.244370417833
Iteration :  15   Loss :  0.239243786613
Iteration :  16   Loss :  0.234224706658
Iteration :  17   Loss :  0.229310921658
Iteration :  18   Loss :  0.224500222635
Iteration :  19   Loss :  0.219790446958
Iteration :  20   Loss :  0.21517947736
Iteration :  21   Loss :  0.210665240995
Iteration :  22   Loss :  0.206245708504
Iteration :  23   Loss :  0.2019188931
Iteration :  24   Loss :  0.197682849677
Iteration :  25   Loss :  0.193535673935
Iteration :  26   Loss :  0.189475501525
Iteration :  27   Loss :  0.18550050721
Iteration :  28   Loss :  0.181608904045
Iteration :  29   Loss :  0.177798942571
Iteration :  30   Loss :  0.174068910033
Iteration :  31   Loss :  0.170417129606
Iteration :  32   Loss :  0.166841959646
Iteration :  33   Loss :  0.163341792945
Iteration :  34   Loss :  0.159915056015
Iteration :  35   Loss :  0.156560208379
Iteration :  36   Loss :  0.153275741874
Iteration :  37   Loss :  0.15006017998
Iteration :  38   Loss :  0.146912077152
Iteration :  39   Loss :  0.143830018169
Iteration :  40   Loss :  0.140812617503
Iteration :  41   Loss :  0.137858518691
Iteration :  42   Loss :  0.134966393727
Iteration :  43   Loss :  0.132134942466
Iteration :  44   Loss :  0.129362892038
Iteration :  45   Loss :  0.126648996277
Iteration :  46   Loss :  0.12399203516
Iteration :  47   Loss :  0.121390814259
Iteration :  48   Loss :  0.118844164203
Iteration :  49   Loss :  0.116350940154
Iteration :  50   Loss :  0.113910021292
Iteration :  51   Loss :  0.111520310309
Iteration :  52   Loss :  0.109180732918
Iteration :  53   Loss :  0.10689023737
Iteration :  54   Loss :  0.104647793981
Iteration :  55   Loss :  0.102452394667
Iteration :  56   Loss :  0.100303052493
Iteration :  57   Loss :  0.0981988012299
Iteration :  58   Loss :  0.0961386949182
Iteration :  59   Loss :  0.0941218074438
Iteration :  60   Loss :  0.0921472321216
Iteration :  61   Loss :  0.0902140812876
Iteration :  62   Loss :  0.0883214859001
Iteration :  63   Loss :  0.0864685951491
Iteration :  64   Loss :  0.0846545760735
Iteration :  65   Loss :  0.082878613187
Iteration :  66   Loss :  0.0811399081113
Iteration :  67   Loss :  0.0794376792171
Iteration :  68   Loss :  0.077771161273
Iteration :  69   Loss :  0.0761396051012
Iteration :  70   Loss :  0.0745422772411
Iteration :  71   Loss :  0.072978459619
Iteration :  72   Loss :  0.0714474492258
Iteration :  73   Loss :  0.0699485578009
Iteration :  74   Loss :  0.0684811115224
Iteration :  75   Loss :  0.0670444507046
Iteration :  76   Loss :  0.0656379295014
Iteration :  77   Loss :  0.0642609156157
Iteration :  78   Loss :  0.0629127900154
Iteration :  79   Loss :  0.0615929466551
Iteration :  80   Loss :  0.0603007922034
Iteration :  81   Loss :  0.0590357457766
Iteration :  82   Loss :  0.057797238677
Iteration :  83   Loss :  0.056584714138
Iteration :  84   Loss :  0.0553976270729
Iteration :  85   Loss :  0.0542354438308
Iteration :  86   Loss :  0.0530976419559
Iteration :  87   Loss :  0.0519837099531
Iteration :  88   Loss :  0.0508931470579
Iteration :  89   Loss :  0.0498254630112
Iteration :  90   Loss :  0.0487801778392
Iteration :  91   Loss :  0.0477568216374
Iteration :  92   Loss :  0.0467549343592
Iteration :  93   Loss :  0.0457740656096
Iteration :  94   Loss :  0.0448137744422
Iteration :  95   Loss :  0.0438736291612
Iteration :  96   Loss :  0.0429532071273
Iteration :  97   Loss :  0.0420520945679
Iteration :  98   Loss :  0.0411698863906
Iteration :  99   Loss :  0.0403061860018
[ -1.02908560e-03  -1.03732389e-04  -1.84231576e-04 ...,   7.82732225e-04
   4.72350742e-04   7.07079639e-05]
CROSS VALIDATION 8
Iteration :  0   Loss :  23.8803582642
Iteration :  1   Loss :  1.05900654787
Iteration :  2   Loss :  0.446621190111
Iteration :  3   Loss :  0.309678496783
Iteration :  4   Loss :  0.30318177159
Iteration :  5   Loss :  0.296821340776
Iteration :  6   Loss :  0.29059434503
Iteration :  7   Loss :  0.284497985025
Iteration :  8   Loss :  0.278529520163
Iteration :  9   Loss :  0.27268626734
Iteration :  10   Loss :  0.266965599741
Iteration :  11   Loss :  0.261364945659
Iteration :  12   Loss :  0.255881787336
Iteration :  13   Loss :  0.250513659838
Iteration :  14   Loss :  0.245258149941
Iteration :  15   Loss :  0.240112895046
Iteration :  16   Loss :  0.235075582122
Iteration :  17   Loss :  0.23014394666
Iteration :  18   Loss :  0.22531577166
Iteration :  19   Loss :  0.220588886632
Iteration :  20   Loss :  0.21596116662
Iteration :  21   Loss :  0.211430531247
Iteration :  22   Loss :  0.206994943781
Iteration :  23   Loss :  0.202652410218
Iteration :  24   Loss :  0.198400978386
Iteration :  25   Loss :  0.194238737069
Iteration :  26   Loss :  0.190163815143
Iteration :  27   Loss :  0.186174380741
Iteration :  28   Loss :  0.182268640427
Iteration :  29   Loss :  0.178444838386
Iteration :  30   Loss :  0.174701255641
Iteration :  31   Loss :  0.171036209277
Iteration :  32   Loss :  0.167448051684
Iteration :  33   Loss :  0.163935169817
Iteration :  34   Loss :  0.160495984473
Iteration :  35   Loss :  0.157128949575
Iteration :  36   Loss :  0.153832551486
Iteration :  37   Loss :  0.150605308319
Iteration :  38   Loss :  0.147445769278
Iteration :  39   Loss :  0.144352514003
Iteration :  40   Loss :  0.141324151931
Iteration :  41   Loss :  0.138359321671
Iteration :  42   Loss :  0.135456690393
Iteration :  43   Loss :  0.13261495323
Iteration :  44   Loss :  0.129832832687
Iteration :  45   Loss :  0.127109078072
Iteration :  46   Loss :  0.124442464929
Iteration :  47   Loss :  0.12183179449
Iteration :  48   Loss :  0.119275893139
Iteration :  49   Loss :  0.116773611876
Iteration :  50   Loss :  0.114323825811
Iteration :  51   Loss :  0.11192543365
Iteration :  52   Loss :  0.109577357203
Iteration :  53   Loss :  0.1072785409
Iteration :  54   Loss :  0.105027951316
Iteration :  55   Loss :  0.102824576705
Iteration :  56   Loss :  0.100667426549
Iteration :  57   Loss :  0.0985555311058
Iteration :  58   Loss :  0.0964879409811
Iteration :  59   Loss :  0.0944637266961
Iteration :  60   Loss :  0.092481978272
Iteration :  61   Loss :  0.0905418048201
Iteration :  62   Loss :  0.0886423341417
Iteration :  63   Loss :  0.0867827123362
Iteration :  64   Loss :  0.0849621034164
Iteration :  65   Loss :  0.0831796889337
Iteration :  66   Loss :  0.0814346676093
Iteration :  67   Loss :  0.0797262549746
Iteration :  68   Loss :  0.0780536830183
Iteration :  69   Loss :  0.0764161998411
Iteration :  70   Loss :  0.0748130693177
Iteration :  71   Loss :  0.0732435707661
Iteration :  72   Loss :  0.0717069986234
Iteration :  73   Loss :  0.0702026621284
Iteration :  74   Loss :  0.0687298850116
Iteration :  75   Loss :  0.0672880051908
Iteration :  76   Loss :  0.0658763744738
Iteration :  77   Loss :  0.0644943582664
Iteration :  78   Loss :  0.0631413352878
Iteration :  79   Loss :  0.061816697291
Iteration :  80   Loss :  0.0605198487891
Iteration :  81   Loss :  0.0592502067883
Iteration :  82   Loss :  0.0580072005251
Iteration :  83   Loss :  0.05679027121
Iteration :  84   Loss :  0.0555988717764
Iteration :  85   Loss :  0.0544324666346
Iteration :  86   Loss :  0.0532905314311
Iteration :  87   Loss :  0.0521725528124
Iteration :  88   Loss :  0.051078028195
Iteration :  89   Loss :  0.0500064655389
Iteration :  90   Loss :  0.0489573831265
Iteration :  91   Loss :  0.0479303093464
Iteration :  92   Loss :  0.0469247824808
Iteration :  93   Loss :  0.0459403504984
Iteration :  94   Loss :  0.0449765708511
Iteration :  95   Loss :  0.0440330102747
Iteration :  96   Loss :  0.0431092445947
Iteration :  97   Loss :  0.0422048585353
Iteration :  98   Loss :  0.0413194455327
Iteration :  99   Loss :  0.0404526075523
[ -7.94933003e-04   1.94359872e-04  -5.13605043e-04 ...,   6.34901754e-04
  -6.85688922e-05   3.30197941e-05]
CROSS VALIDATION 9
Iteration :  0   Loss :  11.3057900475
Iteration :  1   Loss :  0.727230887542
Iteration :  2   Loss :  0.308716651751
Iteration :  3   Loss :  0.302240105043
Iteration :  4   Loss :  0.295899429391
Iteration :  5   Loss :  0.289691774364
Iteration :  6   Loss :  0.283614349331
Iteration :  7   Loss :  0.277664422205
Iteration :  8   Loss :  0.271839318216
Iteration :  9   Loss :  0.266136418708
Iteration :  10   Loss :  0.260553159961
Iteration :  11   Loss :  0.255087032038
Iteration :  12   Loss :  0.249735577661
Iteration :  13   Loss :  0.244496391099
Iteration :  14   Loss :  0.239367117094
Iteration :  15   Loss :  0.234345449797
Iteration :  16   Loss :  0.229429131734
Iteration :  17   Loss :  0.22461595279
Iteration :  18   Loss :  0.219903749217
Iteration :  19   Loss :  0.215290402658
Iteration :  20   Loss :  0.210773839199
Iteration :  21   Loss :  0.206352028433
Iteration :  22   Loss :  0.20202298255
Iteration :  23   Loss :  0.197784755441
Iteration :  24   Loss :  0.193635441825
Iteration :  25   Loss :  0.189573176391
Iteration :  26   Loss :  0.185596132962
Iteration :  27   Loss :  0.18170252367
Iteration :  28   Loss :  0.177890598155
Iteration :  29   Loss :  0.174158642781
Iteration :  30   Loss :  0.170504979857
Iteration :  31   Loss :  0.166927966891
Iteration :  32   Loss :  0.16342599585
Iteration :  33   Loss :  0.159997492433
Iteration :  34   Loss :  0.156640915368
Iteration :  35   Loss :  0.153354755717
Iteration :  36   Loss :  0.150137536197
Iteration :  37   Loss :  0.146987810517
Iteration :  38   Loss :  0.143904162729
Iteration :  39   Loss :  0.140885206589
Iteration :  40   Loss :  0.137929584935
Iteration :  41   Loss :  0.135035969077
Iteration :  42   Loss :  0.132203058199
Iteration :  43   Loss :  0.129429578776
Iteration :  44   Loss :  0.126714283998
Iteration :  45   Loss :  0.124055953214
Iteration :  46   Loss :  0.12145339138
Iteration :  47   Loss :  0.118905428523
Iteration :  48   Loss :  0.116410919214
Iteration :  49   Loss :  0.113968742055
Iteration :  50   Loss :  0.111577799173
Iteration :  51   Loss :  0.109237015727
Iteration :  52   Loss :  0.106945339426
Iteration :  53   Loss :  0.104701740054
Iteration :  54   Loss :  0.102505209009
Iteration :  55   Loss :  0.100354758846
Iteration :  56   Loss :  0.0982494228388
Iteration :  57   Loss :  0.0961882545396
Iteration :  58   Loss :  0.0941703273572
Iteration :  59   Loss :  0.0921947341388
Iteration :  60   Loss :  0.090260586763
Iteration :  61   Loss :  0.0883670157402
Iteration :  62   Loss :  0.0865131698216
Iteration :  63   Loss :  0.0846982156169
Iteration :  64   Loss :  0.0829213372192
Iteration :  65   Loss :  0.0811817358388
Iteration :  66   Loss :  0.0794786294434
Iteration :  67   Loss :  0.0778112524071
Iteration :  68   Loss :  0.076178855166
Iteration :  69   Loss :  0.0745807038813
Iteration :  70   Loss :  0.0730160801092
Iteration :  71   Loss :  0.0714842804782
Iteration :  72   Loss :  0.0699846163728
Iteration :  73   Loss :  0.0685164136238
Iteration :  74   Loss :  0.0670790122056
Iteration :  75   Loss :  0.0656717659389
Iteration :  76   Loss :  0.0642940422007
Iteration :  77   Loss :  0.06294522164
Iteration :  78   Loss :  0.0616246978987
Iteration :  79   Loss :  0.0603318773397
Iteration :  80   Loss :  0.0590661787798
Iteration :  81   Loss :  0.0578270332282
Iteration :  82   Loss :  0.0566138836312
Iteration :  83   Loss :  0.0554261846213
Iteration :  84   Loss :  0.0542634022722
Iteration :  85   Loss :  0.053125013859
Iteration :  86   Loss :  0.0520105076228
Iteration :  87   Loss :  0.0509193825409
Iteration :  88   Loss :  0.0498511481017
Iteration :  89   Loss :  0.0488053240839
Iteration :  90   Loss :  0.0477814403406
Iteration :  91   Loss :  0.0467790365883
Iteration :  92   Loss :  0.0457976621996
Iteration :  93   Loss :  0.0448368760008
Iteration :  94   Loss :  0.0438962460737
Iteration :  95   Loss :  0.0429753495611
Iteration :  96   Loss :  0.0420737724769
Iteration :  97   Loss :  0.0411911095203
Iteration :  98   Loss :  0.0403269638928
Iteration :  99   Loss :  0.0394809471206
[ -1.04788956e-03  -9.61675301e-05  -2.96710925e-04 ...,   7.17600805e-04
   5.68616474e-04   6.97473877e-05]
CROSS VALIDATION 10
Iteration :  0   Loss :  28.8774535686
Iteration :  1   Loss :  0.339607147029
Iteration :  2   Loss :  0.332482550615
Iteration :  3   Loss :  0.32550742065
Iteration :  4   Loss :  0.318678621485
Iteration :  5   Loss :  0.311993083258
Iteration :  6   Loss :  0.305447800505
Iteration :  7   Loss :  0.299039830817
Iteration :  8   Loss :  0.292766293511
Iteration :  9   Loss :  0.286624368339
Iteration :  10   Loss :  0.280611294219
Iteration :  11   Loss :  0.274724367993
Iteration :  12   Loss :  0.268960943212
Iteration :  13   Loss :  0.263318428947
Iteration :  14   Loss :  0.257794288625
Iteration :  15   Loss :  0.252386038886
Iteration :  16   Loss :  0.247091248469
Iteration :  17   Loss :  0.241907537119
Iteration :  18   Loss :  0.236832574514
Iteration :  19   Loss :  0.231864079223
Iteration :  20   Loss :  0.226999817673
Iteration :  21   Loss :  0.222237603152
Iteration :  22   Loss :  0.217575294822
Iteration :  23   Loss :  0.213010796758
Iteration :  24   Loss :  0.208542057004
Iteration :  25   Loss :  0.204167066653
Iteration :  26   Loss :  0.199883858942
Iteration :  27   Loss :  0.195690508368
Iteration :  28   Loss :  0.191585129826
Iteration :  29   Loss :  0.187565877756
Iteration :  30   Loss :  0.183630945315
Iteration :  31   Loss :  0.179778563568
Iteration :  32   Loss :  0.176007000688
Iteration :  33   Loss :  0.172314561183
Iteration :  34   Loss :  0.168699585128
Iteration :  35   Loss :  0.165160447422
Iteration :  36   Loss :  0.161695557058
Iteration :  37   Loss :  0.158303356405
Iteration :  38   Loss :  0.15498232051
Iteration :  39   Loss :  0.151730956413
Iteration :  40   Loss :  0.148547802474
Iteration :  41   Loss :  0.145431427715
Iteration :  42   Loss :  0.142380431182
Iteration :  43   Loss :  0.139393441307
Iteration :  44   Loss :  0.136469115301
Iteration :  45   Loss :  0.133606138541
Iteration :  46   Loss :  0.130803223986
Iteration :  47   Loss :  0.128059111594
Iteration :  48   Loss :  0.125372567759
Iteration :  49   Loss :  0.122742384753
Iteration :  50   Loss :  0.120167380187
Iteration :  51   Loss :  0.117646396476
Iteration :  52   Loss :  0.115178300319
Iteration :  53   Loss :  0.112761982192
Iteration :  54   Loss :  0.110396355847
Iteration :  55   Loss :  0.108080357824
Iteration :  56   Loss :  0.105812946975
Iteration :  57   Loss :  0.103593103991
Iteration :  58   Loss :  0.10141983095
Iteration :  59   Loss :  0.0992921508637
Iteration :  60   Loss :  0.0972091072407
Iteration :  61   Loss :  0.0951697636553
Iteration :  62   Loss :  0.0931732033274
Iteration :  63   Loss :  0.0912185287096
Iteration :  64   Loss :  0.0893048610845
Iteration :  65   Loss :  0.087431340169
Iteration :  66   Loss :  0.0855971237278
Iteration :  67   Loss :  0.0838013871949
Iteration :  68   Loss :  0.0820433233029
Iteration :  69   Loss :  0.0803221417197
Iteration :  70   Loss :  0.0786370686938
Iteration :  71   Loss :  0.0769873467062
Iteration :  72   Loss :  0.0753722341296
Iteration :  73   Loss :  0.0737910048955
Iteration :  74   Loss :  0.0722429481675
Iteration :  75   Loss :  0.0707273680216
Iteration :  76   Loss :  0.0692435831338
Iteration :  77   Loss :  0.0677909264733
Iteration :  78   Loss :  0.0663687450031
Iteration :  79   Loss :  0.0649763993862
Iteration :  80   Loss :  0.0636132636981
Iteration :  81   Loss :  0.0622787251456
Iteration :  82   Loss :  0.0609721837912
Iteration :  83   Loss :  0.0596930522836
Iteration :  84   Loss :  0.0584407555933
Iteration :  85   Loss :  0.0572147307544
Iteration :  86   Loss :  0.0560144266114
Iteration :  87   Loss :  0.0548393035716
Iteration :  88   Loss :  0.0536888333622
Iteration :  89   Loss :  0.052562498793
Iteration :  90   Loss :  0.0514597935241
Iteration :  91   Loss :  0.0503802218378
Iteration :  92   Loss :  0.0493232984162
Iteration :  93   Loss :  0.0482885481228
Iteration :  94   Loss :  0.0472755057891
Iteration :  95   Loss :  0.046283716005
Iteration :  96   Loss :  0.0453127329148
Iteration :  97   Loss :  0.0443621200162
Iteration :  98   Loss :  0.0434314499643
Iteration :  99   Loss :  0.0425203043792
[-0.0007716  -0.00014186 -0.00055766 ...,  0.00049646  0.00016733
  0.00010442]
CROSS VALIDATION 11
Iteration :  0   Loss :  21.0460353497
Iteration :  1   Loss :  1.63748621224
Iteration :  2   Loss :  1.09676807325
Iteration :  3   Loss :  0.308572128938
Iteration :  4   Loss :  0.302098614166
Iteration :  5   Loss :  0.295760906841
Iteration :  6   Loss :  0.28955615787
Iteration :  7   Loss :  0.283481577926
Iteration :  8   Loss :  0.277534436204
Iteration :  9   Loss :  0.271712059183
Iteration :  10   Loss :  0.266011829434
Iteration :  11   Loss :  0.260431184437
Iteration :  12   Loss :  0.254967615431
Iteration :  13   Loss :  0.249618666287
Iteration :  14   Loss :  0.244381932401
Iteration :  15   Loss :  0.239255059617
Iteration :  16   Loss :  0.234235743167
Iteration :  17   Loss :  0.229321726632
Iteration :  18   Loss :  0.224510800934
Iteration :  19   Loss :  0.219800803334
Iteration :  20   Loss :  0.215189616471
Iteration :  21   Loss :  0.210675167398
Iteration :  22   Loss :  0.206255426662
Iteration :  23   Loss :  0.201928407381
Iteration :  24   Loss :  0.197692164358
Iteration :  25   Loss :  0.193544793204
Iteration :  26   Loss :  0.189484429482
Iteration :  27   Loss :  0.185509247868
Iteration :  28   Loss :  0.181617461333
Iteration :  29   Loss :  0.177807320336
Iteration :  30   Loss :  0.174077112042
Iteration :  31   Loss :  0.170425159546
Iteration :  32   Loss :  0.166849821126
Iteration :  33   Loss :  0.163349489499
Iteration :  34   Loss :  0.159922591104
Iteration :  35   Loss :  0.156567585389
Iteration :  36   Loss :  0.153282964123
Iteration :  37   Loss :  0.150067250714
Iteration :  38   Loss :  0.146918999549
Iteration :  39   Loss :  0.143836795342
Iteration :  40   Loss :  0.140819252498
Iteration :  41   Loss :  0.137865014491
Iteration :  42   Loss :  0.134972753252
Iteration :  43   Loss :  0.132141168574
Iteration :  44   Loss :  0.129368987529
Iteration :  45   Loss :  0.126654963892
Iteration :  46   Loss :  0.12399787758
Iteration :  47   Loss :  0.121396534111
Iteration :  48   Loss :  0.118849764059
Iteration :  49   Loss :  0.116356422531
Iteration :  50   Loss :  0.113915388655
Iteration :  51   Loss :  0.11152556507
Iteration :  52   Loss :  0.10918587744
Iteration :  53   Loss :  0.106895273966
Iteration :  54   Loss :  0.104652724914
Iteration :  55   Loss :  0.102457222154
Iteration :  56   Loss :  0.100307778705
Iteration :  57   Loss :  0.0982034282907
Iteration :  58   Loss :  0.0961432249082
Iteration :  59   Loss :  0.0941262423995
Iteration :  60   Loss :  0.0921515740366
Iteration :  61   Loss :  0.0902183321138
Iteration :  62   Loss :  0.0883256475485
Iteration :  63   Loss :  0.0864726694906
Iteration :  64   Loss :  0.0846585649397
Iteration :  65   Loss :  0.082882518371
Iteration :  66   Loss :  0.0811437313687
Iteration :  67   Loss :  0.0794414222666
Iteration :  68   Loss :  0.0777748257973
Iteration :  69   Loss :  0.0761431927477
Iteration :  70   Loss :  0.0745457896226
Iteration :  71   Loss :  0.0729818983144
Iteration :  72   Loss :  0.0714508157811
Iteration :  73   Loss :  0.0699518537294
Iteration :  74   Loss :  0.0684843383058
Iteration :  75   Loss :  0.0670476097936
Iteration :  76   Loss :  0.0656410223161
Iteration :  77   Loss :  0.0642639435464
Iteration :  78   Loss :  0.0629157544234
Iteration :  79   Loss :  0.0615958488729
Iteration :  80   Loss :  0.0603036335358
Iteration :  81   Loss :  0.0590385275008
Iteration :  82   Loss :  0.0577999620437
Iteration :  83   Loss :  0.0565873803713
Iteration :  84   Loss :  0.0554002373715
Iteration :  85   Loss :  0.0542379993681
Iteration :  86   Loss :  0.0531001438808
Iteration :  87   Loss :  0.0519861593902
Iteration :  88   Loss :  0.0508955451084
Iteration :  89   Loss :  0.0498278107532
Iteration :  90   Loss :  0.0487824763281
Iteration :  91   Loss :  0.0477590719064
Iteration :  92   Loss :  0.04675713742
Iteration :  93   Loss :  0.0457762224525
Iteration :  94   Loss :  0.0448158860368
Iteration :  95   Loss :  0.0438756964568
Iteration :  96   Loss :  0.0429552310532
Iteration :  97   Loss :  0.042054076034
Iteration :  98   Loss :  0.0411718262877
Iteration :  99   Loss :  0.0403080852019
[ -1.02912840e-03  -1.03762318e-04  -1.84274851e-04 ...,   7.82737689e-04
   4.72386551e-04   7.07079612e-05]
CROSS VALIDATION 12
Iteration :  0   Loss :  19.7020116313
Iteration :  1   Loss :  4.84712525298
Iteration :  2   Loss :  0.337964450631
Iteration :  3   Loss :  0.330874316239
Iteration :  4   Loss :  0.323932925319
Iteration :  5   Loss :  0.31713715739
Iteration :  6   Loss :  0.310483957439
Iteration :  7   Loss :  0.303970334539
Iteration :  8   Loss :  0.297593360514
Iteration :  9   Loss :  0.291350168615
Iteration :  10   Loss :  0.285237952236
Iteration :  11   Loss :  0.27925396365
Iteration :  12   Loss :  0.273395512774
Iteration :  13   Loss :  0.267659965961
Iteration :  14   Loss :  0.262044744814
Iteration :  15   Loss :  0.256547325029
Iteration :  16   Loss :  0.251165235259
Iteration :  17   Loss :  0.245896056003
Iteration :  18   Loss :  0.240737418518
Iteration :  19   Loss :  0.235687003756
Iteration :  20   Loss :  0.230742541319
Iteration :  21   Loss :  0.225901808441
Iteration :  22   Loss :  0.221162628985
Iteration :  23   Loss :  0.216522872468
Iteration :  24   Loss :  0.211980453105
Iteration :  25   Loss :  0.207533328864
Iteration :  26   Loss :  0.203179500555
Iteration :  27   Loss :  0.198917010929
Iteration :  28   Loss :  0.194743943798
Iteration :  29   Loss :  0.190658423173
Iteration :  30   Loss :  0.186658612422
Iteration :  31   Loss :  0.182742713442
Iteration :  32   Loss :  0.178908965853
Iteration :  33   Loss :  0.175155646208
Iteration :  34   Loss :  0.171481067214
Iteration :  35   Loss :  0.167883576975
Iteration :  36   Loss :  0.164361558252
Iteration :  37   Loss :  0.160913427732
Iteration :  38   Loss :  0.157537635319
Iteration :  39   Loss :  0.154232663438
Iteration :  40   Loss :  0.150997026348
Iteration :  41   Loss :  0.14782926948
Iteration :  42   Loss :  0.144727968779
Iteration :  43   Loss :  0.141691730065
Iteration :  44   Loss :  0.138719188407
Iteration :  45   Loss :  0.135809007509
Iteration :  46   Loss :  0.132959879107
Iteration :  47   Loss :  0.130170522387
Iteration :  48   Loss :  0.127439683401
Iteration :  49   Loss :  0.12476613451
Iteration :  50   Loss :  0.122148673828
Iteration :  51   Loss :  0.119586124686
Iteration :  52   Loss :  0.117077335096
Iteration :  53   Loss :  0.114621177241
Iteration :  54   Loss :  0.112216546963
Iteration :  55   Loss :  0.109862363269
Iteration :  56   Loss :  0.107557567842
Iteration :  57   Loss :  0.10530112457
Iteration :  58   Loss :  0.103092019075
Iteration :  59   Loss :  0.100929258262
Iteration :  60   Loss :  0.0988118698686
Iteration :  61   Loss :  0.0967389020295
Iteration :  62   Loss :  0.0947094228489
Iteration :  63   Loss :  0.092722519981
Iteration :  64   Loss :  0.0907773002201
Iteration :  65   Loss :  0.088872889099
Iteration :  66   Loss :  0.0870084304959
Iteration :  67   Loss :  0.0851830862494
Iteration :  68   Loss :  0.0833960357821
Iteration :  69   Loss :  0.0816464757311
Iteration :  70   Loss :  0.0799336195875
Iteration :  71   Loss :  0.0782566973424
Iteration :  72   Loss :  0.0766149551408
Iteration :  73   Loss :  0.0750076549428
Iteration :  74   Loss :  0.073434074192
Iteration :  75   Loss :  0.0718935054901
Iteration :  76   Loss :  0.0703852562796
Iteration :  77   Loss :  0.0689086485319
Iteration :  78   Loss :  0.0674630184428
Iteration :  79   Loss :  0.0660477161341
Iteration :  80   Loss :  0.0646621053611
Iteration :  81   Loss :  0.063305563227
Iteration :  82   Loss :  0.0619774799028
Iteration :  83   Loss :  0.060677258353
Iteration :  84   Loss :  0.059404314067
Iteration :  85   Loss :  0.0581580747971
Iteration :  86   Loss :  0.0569379803004
Iteration :  87   Loss :  0.0557434820874
Iteration :  88   Loss :  0.0545740431753
Iteration :  89   Loss :  0.0534291378466
Iteration :  90   Loss :  0.0523082514128
Iteration :  91   Loss :  0.0512108799833
Iteration :  92   Loss :  0.0501365302381
Iteration :  93   Loss :  0.0490847192068
Iteration :  94   Loss :  0.0480549740513
Iteration :  95   Loss :  0.0470468318529
Iteration :  96   Loss :  0.0460598394046
Iteration :  97   Loss :  0.0450935530072
Iteration :  98   Loss :  0.0441475382697
Iteration :  99   Loss :  0.0432213699143
[ -1.54691954e-03  -1.97480050e-04  -6.40781005e-04 ...,   7.28231770e-04
   4.41491697e-04   3.53283939e-05]
CROSS VALIDATION 13
Iteration :  0   Loss :  21.0512628041
Iteration :  1   Loss :  1.63622530749
Iteration :  2   Loss :  1.09894344672
Iteration :  3   Loss :  0.308577230923
Iteration :  4   Loss :  0.302103609116
Iteration :  5   Loss :  0.295765797003
Iteration :  6   Loss :  0.289560945441
Iteration :  7   Loss :  0.28348626506
Iteration :  8   Loss :  0.277539025006
Iteration :  9   Loss :  0.271716551717
Iteration :  10   Loss :  0.266016227719
Iteration :  11   Loss :  0.260435490451
Iteration :  12   Loss :  0.25497183111
Iteration :  13   Loss :  0.249622793525
Iteration :  14   Loss :  0.244385973054
Iteration :  15   Loss :  0.239259015502
Iteration :  16   Loss :  0.234239616061
Iteration :  17   Loss :  0.229325518277
Iteration :  18   Loss :  0.224514513034
Iteration :  19   Loss :  0.219804437559
Iteration :  20   Loss :  0.215193174453
Iteration :  21   Loss :  0.210678650738
Iteration :  22   Loss :  0.206258836925
Iteration :  23   Loss :  0.2019317461
Iteration :  24   Loss :  0.197695433035
Iteration :  25   Loss :  0.193547993307
Iteration :  26   Loss :  0.18948756245
Iteration :  27   Loss :  0.18551231511
Iteration :  28   Loss :  0.181620464228
Iteration :  29   Loss :  0.177810260233
Iteration :  30   Loss :  0.174079990263
Iteration :  31   Loss :  0.170427977385
Iteration :  32   Loss :  0.166852579849
Iteration :  33   Loss :  0.163352190348
Iteration :  34   Loss :  0.159925235292
Iteration :  35   Loss :  0.156570174105
Iteration :  36   Loss :  0.15328549853
Iteration :  37   Loss :  0.150069731952
Iteration :  38   Loss :  0.146921428733
Iteration :  39   Loss :  0.143839173564
Iteration :  40   Loss :  0.140821580828
Iteration :  41   Loss :  0.137867293975
Iteration :  42   Loss :  0.134974984914
Iteration :  43   Loss :  0.132143353419
Iteration :  44   Loss :  0.129371126539
Iteration :  45   Loss :  0.126657058027
Iteration :  46   Loss :  0.123999927783
Iteration :  47   Loss :  0.121398541303
Iteration :  48   Loss :  0.118851729142
Iteration :  49   Loss :  0.116358346389
Iteration :  50   Loss :  0.113917272151
Iteration :  51   Loss :  0.111527409053
Iteration :  52   Loss :  0.109187682738
Iteration :  53   Loss :  0.106897041391
Iteration :  54   Loss :  0.10465445526
Iteration :  55   Loss :  0.102458916199
Iteration :  56   Loss :  0.100309437211
Iteration :  57   Loss :  0.0982050520032
Iteration :  58   Loss :  0.0961448145569
Iteration :  59   Loss :  0.094127798699
Iteration :  60   Loss :  0.0921530976866
Iteration :  61   Loss :  0.0902198237993
Iteration :  62   Loss :  0.0883271079401
Iteration :  63   Loss :  0.0864740992446
Iteration :  64   Loss :  0.084659964699
Iteration :  65   Loss :  0.0828838887649
Iteration :  66   Loss :  0.0811450730131
Iteration :  67   Loss :  0.0794427357648
Iteration :  68   Loss :  0.0777761117397
Iteration :  69   Loss :  0.0761444517124
Iteration :  70   Loss :  0.0745470221755
Iteration :  71   Loss :  0.0729831050098
Iteration :  72   Loss :  0.0714519971612
Iteration :  73   Loss :  0.0699530103255
Iteration :  74   Loss :  0.0684854706377
Iteration :  75   Loss :  0.0670487183704
Iteration :  76   Loss :  0.0656421076361
Iteration :  77   Loss :  0.0642650060976
Iteration :  78   Loss :  0.0629167946833
Iteration :  79   Loss :  0.0615968673093
Iteration :  80   Loss :  0.0603046306065
Iteration :  81   Loss :  0.059039503654
Iteration :  82   Loss :  0.0578009177182
Iteration :  83   Loss :  0.0565883159968
Iteration :  84   Loss :  0.0554011533686
Iteration :  85   Loss :  0.0542388961486
Iteration :  86   Loss :  0.0531010218477
Iteration :  87   Loss :  0.0519870189384
Iteration :  88   Loss :  0.0508963866241
Iteration :  89   Loss :  0.0498286346148
Iteration :  90   Loss :  0.048783282906
Iteration :  91   Loss :  0.0477598615631
Iteration :  92   Loss :  0.0467579105106
Iteration :  93   Loss :  0.0457769793245
Iteration :  94   Loss :  0.0448166270304
Iteration :  95   Loss :  0.0438764219051
Iteration :  96   Loss :  0.0429559412825
Iteration :  97   Loss :  0.0420547713634
Iteration :  98   Loss :  0.0411725070298
Iteration :  99   Loss :  0.0403087516628
[ -1.02908954e-03  -1.03744436e-04  -1.84274485e-04 ...,   7.82737674e-04
   4.72383813e-04   7.07149076e-05]
CROSS VALIDATION 14
Iteration :  0   Loss :  21.0512628041
Iteration :  1   Loss :  0.286689411486
Iteration :  2   Loss :  0.28067497283
Iteration :  3   Loss :  0.274786710694
Iteration :  4   Loss :  0.26902197803
Iteration :  5   Loss :  0.26337818332
Iteration :  6   Loss :  0.257852789415
Iteration :  7   Loss :  0.252443312391
Iteration :  8   Loss :  0.247147320437
Iteration :  9   Loss :  0.241962432757
Iteration :  10   Loss :  0.2368863185
Iteration :  11   Loss :  0.231916695717
Iteration :  12   Loss :  0.227051330329
Iteration :  13   Loss :  0.222288035128
Iteration :  14   Loss :  0.217624668789
Iteration :  15   Loss :  0.213059134911
Iteration :  16   Loss :  0.208589381074
Iteration :  17   Loss :  0.204213397914
Iteration :  18   Loss :  0.199929218222
Iteration :  19   Loss :  0.19573491606
Iteration :  20   Loss :  0.191628605892
Iteration :  21   Loss :  0.18760844174
Iteration :  22   Loss :  0.183672616352
Iteration :  23   Loss :  0.17981936039
Iteration :  24   Loss :  0.176046941637
Iteration :  25   Loss :  0.172353664213
Iteration :  26   Loss :  0.168737867818
Iteration :  27   Loss :  0.165197926982
Iteration :  28   Loss :  0.161732250337
Iteration :  29   Loss :  0.158339279898
Iteration :  30   Loss :  0.155017490367
Iteration :  31   Loss :  0.151765388443
Iteration :  32   Loss :  0.148581512157
Iteration :  33   Loss :  0.145464430205
Iteration :  34   Loss :  0.142412741314
Iteration :  35   Loss :  0.139425073608
Iteration :  36   Loss :  0.136500083989
Iteration :  37   Loss :  0.133636457539
Iteration :  38   Loss :  0.130832906924
Iteration :  39   Loss :  0.128088171815
Iteration :  40   Loss :  0.125401018328
Iteration :  41   Loss :  0.122770238459
Iteration :  42   Loss :  0.120194649552
Iteration :  43   Loss :  0.117673093759
Iteration :  44   Loss :  0.115204437521
Iteration :  45   Loss :  0.112787571064
Iteration :  46   Loss :  0.110421407891
Iteration :  47   Loss :  0.108104884304
Iteration :  48   Loss :  0.105836958915
Iteration :  49   Loss :  0.103616612186
Iteration :  50   Loss :  0.101442845968
Iteration :  51   Loss :  0.0993146830511
Iteration :  52   Loss :  0.0972311667268
Iteration :  53   Loss :  0.0951913603569
Iteration :  54   Loss :  0.0931943469531
Iteration :  55   Loss :  0.0912392287646
Iteration :  56   Loss :  0.0893251268744
Iteration :  57   Loss :  0.0874511808042
Iteration :  58   Loss :  0.0856165481276
Iteration :  59   Loss :  0.0838204040915
Iteration :  60   Loss :  0.101493768167
Iteration :  61   Loss :  0.0993645369569
Iteration :  62   Loss :  0.0972799747507
Iteration :  63   Loss :  0.0952391444404
Iteration :  64   Loss :  0.0932411285774
Iteration :  65   Loss :  0.0912850289603
Iteration :  66   Loss :  0.0893699662307
Iteration :  67   Loss :  0.0874950794785
Iteration :  68   Loss :  0.0856595258545
Iteration :  69   Loss :  0.0838624801914
Iteration :  70   Loss :  0.0821031346333
Iteration :  71   Loss :  0.0803806982721
Iteration :  72   Loss :  0.0786943967921
Iteration :  73   Loss :  0.077043472122
Iteration :  74   Loss :  0.075427182094
Iteration :  75   Loss :  0.0738448001101
Iteration :  76   Loss :  0.0722956148157
Iteration :  77   Loss :  0.0707789297796
Iteration :  78   Loss :  0.0692940631809
Iteration :  79   Loss :  0.0678403475028
Iteration :  80   Loss :  0.0664171292321
Iteration :  81   Loss :  0.0650237685655
Iteration :  82   Loss :  0.0636596391225
Iteration :  83   Loss :  0.0623241276629
Iteration :  84   Loss :  0.0610166338121
Iteration :  85   Loss :  0.0597365697903
Iteration :  86   Loss :  0.058483360149
Iteration :  87   Loss :  0.0572564415119
Iteration :  88   Loss :  0.0560552623217
Iteration :  89   Loss :  0.0548792825922
Iteration :  90   Loss :  0.0537279736654
Iteration :  91   Loss :  0.0526008179744
Iteration :  92   Loss :  0.0514973088098
Iteration :  93   Loss :  0.0504169500927
Iteration :  94   Loss :  0.0493592561514
Iteration :  95   Loss :  0.0483237515031
Iteration :  96   Loss :  0.0473099706399
Iteration :  97   Loss :  0.04631745782
Iteration :  98   Loss :  0.0453457668624
Iteration :  99   Loss :  0.0443944609468
[ -9.37497666e-04  -4.40566229e-05  -2.29380638e-04 ...,   5.91316523e-04
   5.77299626e-04   6.55305881e-05]
CROSS VALIDATION 15
Iteration :  0   Loss :  21.0512628041
Iteration :  1   Loss :  1.63622530749
Iteration :  2   Loss :  1.09930400691
Iteration :  3   Loss :  0.308583386703
Iteration :  4   Loss :  0.302109635754
Iteration :  5   Loss :  0.295771697209
Iteration :  6   Loss :  0.289566721867
Iteration :  7   Loss :  0.283491920302
Iteration :  8   Loss :  0.277544561607
Iteration :  9   Loss :  0.271721972167
Iteration :  10   Loss :  0.266021534454
Iteration :  11   Loss :  0.260440685856
Iteration :  12   Loss :  0.25497691752
Iteration :  13   Loss :  0.249627773228
Iteration :  14   Loss :  0.244390848288
Iteration :  15   Loss :  0.239263788459
Iteration :  16   Loss :  0.234244288887
Iteration :  17   Loss :  0.229330093072
Iteration :  18   Loss :  0.224518991854
Iteration :  19   Loss :  0.219808822418
Iteration :  20   Loss :  0.215197467323
Iteration :  21   Loss :  0.210682853548
Iteration :  22   Loss :  0.206262951564
Iteration :  23   Loss :  0.201935774419
Iteration :  24   Loss :  0.197699376843
Iteration :  25   Loss :  0.193551854379
Iteration :  26   Loss :  0.189491342521
Iteration :  27   Loss :  0.185516015879
Iteration :  28   Loss :  0.181624087358
Iteration :  29   Loss :  0.177813807354
Iteration :  30   Loss :  0.174083462969
Iteration :  31   Loss :  0.170431377237
Iteration :  32   Loss :  0.166855908377
Iteration :  33   Loss :  0.163355449046
Iteration :  34   Loss :  0.159928425626
Iteration :  35   Loss :  0.15657329751
Iteration :  36   Loss :  0.153288556409
Iteration :  37   Loss :  0.15007272568
Iteration :  38   Loss :  0.146924359656
Iteration :  39   Loss :  0.143842042999
Iteration :  40   Loss :  0.140824390065
Iteration :  41   Loss :  0.137870044277
Iteration :  42   Loss :  0.134977677518
Iteration :  43   Loss :  0.132145989535
Iteration :  44   Loss :  0.129373707352
Iteration :  45   Loss :  0.126659584697
Iteration :  46   Loss :  0.124002401446
Iteration :  47   Loss :  0.121400963072
Iteration :  48   Loss :  0.118854100104
Iteration :  49   Loss :  0.116360667611
Iteration :  50   Loss :  0.113919544677
Iteration :  51   Loss :  0.111529633903
Iteration :  52   Loss :  0.109189860914
Iteration :  53   Loss :  0.10689917387
Iteration :  54   Loss :  0.104656543003
Iteration :  55   Loss :  0.102460960143
Iteration :  56   Loss :  0.100311438275
Iteration :  57   Loss :  0.0982070110871
Iteration :  58   Loss :  0.0961467325413
Iteration :  59   Loss :  0.0941296764462
Iteration :  60   Loss :  0.0921549360407
Iteration :  61   Loss :  0.0902216235866
Iteration :  62   Loss :  0.0883288699697
Iteration :  63   Loss :  0.0864758243088
Iteration :  64   Loss :  0.0846616535732
Iteration :  65   Loss :  0.0828855422083
Iteration :  66   Loss :  0.081146691769
Iteration :  67   Loss :  0.0794443205609
Iteration :  68   Loss :  0.0777776632885
Iteration :  69   Loss :  0.0761459707114
Iteration :  70   Loss :  0.0745485093075
Iteration :  71   Loss :  0.0729845609433
Iteration :  72   Loss :  0.0714534225508
Iteration :  73   Loss :  0.0699544058119
Iteration :  74   Loss :  0.0684868368483
Iteration :  75   Loss :  0.0670500559193
Iteration :  76   Loss :  0.0656434171247
Iteration :  77   Loss :  0.0642662881145
Iteration :  78   Loss :  0.0629180498049
Iteration :  79   Loss :  0.0615980960998
Iteration :  80   Loss :  0.0603058336182
Iteration :  81   Loss :  0.0590406814279
Iteration :  82   Loss :  0.0578020707836
Iteration :  83   Loss :  0.0565894448721
Iteration :  84   Loss :  0.0554022585613
Iteration :  85   Loss :  0.0542399781555
Iteration :  86   Loss :  0.0531020811553
Iteration :  87   Loss :  0.0519880560228
Iteration :  88   Loss :  0.0508974019517
Iteration :  89   Loss :  0.0498296286418
Iteration :  90   Loss :  0.0487842560794
Iteration :  91   Loss :  0.0477608143204
Iteration :  92   Loss :  0.04675884328
Iteration :  93   Loss :  0.0457778925254
Iteration :  94   Loss :  0.0448175210734
Iteration :  95   Loss :  0.043877297192
Iteration :  96   Loss :  0.0429567982067
Iteration :  97   Loss :  0.0420556103103
Iteration :  98   Loss :  0.0411733283765
Iteration :  99   Loss :  0.0403095557785
[ -1.02913461e-03  -1.03766262e-04  -1.84294264e-04 ...,   7.82768197e-04
   4.72414297e-04   7.07220057e-05]
CROSS VALIDATION 16
Iteration :  0   Loss :  21.0512628041
Iteration :  1   Loss :  0.271939680794
Iteration :  2   Loss :  0.266234675786
Iteration :  3   Loss :  0.260649355709
Iteration :  4   Loss :  0.255181209703
Iteration :  5   Loss :  0.249827779578
Iteration :  6   Loss :  0.244586658718
Iteration :  7   Loss :  0.239455490994
Iteration :  8   Loss :  0.234431969707
Iteration :  9   Loss :  0.229513836548
Iteration :  10   Loss :  0.224698880588
Iteration :  11   Loss :  0.219984937278
Iteration :  12   Loss :  0.21536988748
Iteration :  13   Loss :  0.210851656514
Iteration :  14   Loss :  0.206428213224
Iteration :  15   Loss :  0.202097569065
Iteration :  16   Loss :  0.19785777721
Iteration :  17   Loss :  0.193706931675
Iteration :  18   Loss :  0.18964316646
Iteration :  19   Loss :  0.185664654713
Iteration :  20   Loss :  0.181769607908
Iteration :  21   Loss :  0.177956275038
Iteration :  22   Loss :  0.174222941832
Iteration :  23   Loss :  0.170567929982
Iteration :  24   Loss :  0.166989596391
Iteration :  25   Loss :  0.163486332428
Iteration :  26   Loss :  0.160056563214
Iteration :  27   Loss :  0.156698746907
Iteration :  28   Loss :  0.153411374011
Iteration :  29   Loss :  0.1501929667
Iteration :  30   Loss :  0.147042078147
Iteration :  31   Loss :  0.143957291882
Iteration :  32   Loss :  0.140937221149
Iteration :  33   Loss :  0.137980508285
Iteration :  34   Loss :  0.135085824109
Iteration :  35   Loss :  0.132251867326
Iteration :  36   Loss :  0.129477363939
Iteration :  37   Loss :  0.126761066679
Iteration :  38   Loss :  0.124101754444
Iteration :  39   Loss :  0.12149823175
Iteration :  40   Loss :  0.118949328189
Iteration :  41   Loss :  0.116453897912
Iteration :  42   Loss :  0.114010819105
Iteration :  43   Loss :  0.111618993491
Iteration :  44   Loss :  0.109277345833
Iteration :  45   Loss :  0.106984823449
Iteration :  46   Loss :  0.104740395744
Iteration :  47   Loss :  0.102543053744
Iteration :  48   Loss :  0.100391809639
Iteration :  49   Loss :  0.0982856963454
Iteration :  50   Loss :  0.0962237670667
Iteration :  51   Loss :  0.0942050948692
Iteration :  52   Loss :  0.0922287722655
Iteration :  53   Loss :  0.090293910806
Iteration :  54   Loss :  0.0883996406802
Iteration :  55   Loss :  0.0865451103251
Iteration :  56   Loss :  0.0847294860426
Iteration :  57   Loss :  0.0829519516247
Iteration :  58   Loss :  0.0812117079866
Iteration :  59   Loss :  0.0795079728075
Iteration :  60   Loss :  0.0778399801787
Iteration :  61   Loss :  0.0762069802595
Iteration :  62   Loss :  0.0746082389403
Iteration :  63   Loss :  0.073043037512
Iteration :  64   Loss :  0.0715106723434
Iteration :  65   Loss :  0.0700104545647
Iteration :  66   Loss :  0.068541709758
Iteration :  67   Loss :  0.0671037776538
Iteration :  68   Loss :  0.0656960118343
Iteration :  69   Loss :  0.0643177794431
Iteration :  70   Loss :  0.0629684609003
Iteration :  71   Loss :  0.061647449624
Iteration :  72   Loss :  0.0603541517581
Iteration :  73   Loss :  0.0590879859046
Iteration :  74   Loss :  0.0578483828628
Iteration :  75   Loss :  0.0566347853732
Iteration :  76   Loss :  0.0554466478669
Iteration :  77   Loss :  0.0542834362207
Iteration :  78   Loss :  0.0531446275166
Iteration :  79   Loss :  0.0520297098067
Iteration :  80   Loss :  0.0509381818835
Iteration :  81   Loss :  0.0498695530541
Iteration :  82   Loss :  0.04882334292
Iteration :  83   Loss :  0.0477990811607
Iteration :  84   Loss :  0.0467963073228
Iteration :  85   Loss :  0.0458145706124
Iteration :  86   Loss :  0.0448534296932
Iteration :  87   Loss :  0.0439124524872
Iteration :  88   Loss :  0.0429912159813
Iteration :  89   Loss :  0.0420893060365
Iteration :  90   Loss :  0.0412063172023
Iteration :  91   Loss :  0.0403418525338
Iteration :  92   Loss :  0.0394955234138
Iteration :  93   Loss :  0.0386669493778
Iteration :  94   Loss :  0.037855757943
Iteration :  95   Loss :  0.0370615844409
Iteration :  96   Loss :  0.0362840718535
Iteration :  97   Loss :  0.0418025675746
Iteration :  98   Loss :  2.64988514013
Iteration :  99   Loss :  1.8214293918
[-0.00227171 -0.00044884 -0.00125121 ...,  0.00169382  0.00097931
  0.00023254]
CROSS VALIDATION 17
Iteration :  0   Loss :  21.0512628041
Iteration :  1   Loss :  1.63685133326
Iteration :  2   Loss :  1.10026049156
Iteration :  3   Loss :  0.308588931541
Iteration :  4   Loss :  0.302115064268
Iteration :  5   Loss :  0.295777011838
Iteration :  6   Loss :  0.289571925
Iteration :  7   Loss :  0.28349701428
Iteration :  8   Loss :  0.277549548718
Iteration :  9   Loss :  0.271726854653
Iteration :  10   Loss :  0.266026314511
Iteration :  11   Loss :  0.260445365633
Iteration :  12   Loss :  0.25498149912
Iteration :  13   Loss :  0.249632258711
Iteration :  14   Loss :  0.244395239671
Iteration :  15   Loss :  0.239268087715
Iteration :  16   Loss :  0.234248497949
Iteration :  17   Loss :  0.229334213832
Iteration :  18   Loss :  0.224523026165
Iteration :  19   Loss :  0.219812772094
Iteration :  20   Loss :  0.215201334138
Iteration :  21   Loss :  0.210686639242
Iteration :  22   Loss :  0.206266657838
Iteration :  23   Loss :  0.201939402939
Iteration :  24   Loss :  0.197702929241
Iteration :  25   Loss :  0.193555332252
Iteration :  26   Loss :  0.189494747432
Iteration :  27   Loss :  0.185519349358
Iteration :  28   Loss :  0.181627350904
Iteration :  29   Loss :  0.177817002435
Iteration :  30   Loss :  0.17408659102
Iteration :  31   Loss :  0.170434439665
Iteration :  32   Loss :  0.166858906558
Iteration :  33   Loss :  0.163358384329
Iteration :  34   Loss :  0.15993129933
Iteration :  35   Loss :  0.156576110926
Iteration :  36   Loss :  0.153291310803
Iteration :  37   Loss :  0.150075422289
Iteration :  38   Loss :  0.146926999693
Iteration :  39   Loss :  0.143844627652
Iteration :  40   Loss :  0.140826920494
Iteration :  41   Loss :  0.137872521621
Iteration :  42   Loss :  0.13498010289
Iteration :  43   Loss :  0.132148364025
Iteration :  44   Loss :  0.129376032027
Iteration :  45   Loss :  0.126661860603
Iteration :  46   Loss :  0.124004629607
Iteration :  47   Loss :  0.121403144487
Iteration :  48   Loss :  0.118856235756
Iteration :  49   Loss :  0.116362758459
Iteration :  50   Loss :  0.113921591662
Iteration :  51   Loss :  0.111531637944
Iteration :  52   Loss :  0.109191822912
Iteration :  53   Loss :  0.106901094708
Iteration :  54   Loss :  0.104658423543
Iteration :  55   Loss :  0.102462801232
Iteration :  56   Loss :  0.10031324074
Iteration :  57   Loss :  0.098208775738
Iteration :  58   Loss :  0.0961484601718
Iteration :  59   Loss :  0.0941313678328
Iteration :  60   Loss :  0.0921565919438
Iteration :  61   Loss :  0.0902232447506
Iteration :  62   Loss :  0.0883304571235
Iteration :  63   Loss :  0.0864773781657
Iteration :  64   Loss :  0.0846631748318
Iteration :  65   Loss :  0.0828870315525
Iteration :  66   Loss :  0.0811481498684
Iteration :  67   Loss :  0.079445748071
Iteration :  68   Loss :  0.0777790608509
Iteration :  69   Loss :  0.0761473389544
Iteration :  70   Loss :  0.0745498488462
Iteration :  71   Loss :  0.0729858723799
Iteration :  72   Loss :  0.0714547064749
Iteration :  73   Loss :  0.0699556628006
Iteration :  74   Loss :  0.0684880674668
Iteration :  75   Loss :  0.0670512607207
Iteration :  76   Loss :  0.0656445966506
Iteration :  77   Loss :  0.0642674428952
Iteration :  78   Loss :  0.0629191803596
Iteration :  79   Loss :  0.0615992029366
Iteration :  80   Loss :  0.0603069172348
Iteration :  81   Loss :  0.0590417423113
Iteration :  82   Loss :  0.0578031094109
Iteration :  83   Loss :  0.0565904617101
Iteration :  84   Loss :  0.0554032540671
Iteration :  85   Loss :  0.0542409527767
Iteration :  86   Loss :  0.0531030353299
Iteration :  87   Loss :  0.0519889901799
Iteration :  88   Loss :  0.0508983165111
Iteration :  89   Loss :  0.0498305240148
Iteration :  90   Loss :  0.0487851326683
Iteration :  91   Loss :  0.0477616725195
Iteration :  92   Loss :  0.046759683475
Iteration :  93   Loss :  0.045778715094
Iteration :  94   Loss :  0.0448183263853
Iteration :  95   Loss :  0.0438780856094
Iteration :  96   Loss :  0.042957570084
Iteration :  97   Loss :  0.0420563659943
Iteration :  98   Loss :  0.0411740682071
Iteration :  99   Loss :  0.0403102800882
[ -1.02916377e-03  -1.03767962e-04  -1.84303540e-04 ...,   7.82769239e-04
   4.72413996e-04   7.07149796e-05]
CROSS VALIDATION 18
Iteration :  0   Loss :  24.4844742436
Iteration :  1   Loss :  15.4597548949
Iteration :  2   Loss :  0.32933012132
Iteration :  3   Loss :  0.322421125965
Iteration :  4   Loss :  0.315657073978
Iteration :  5   Loss :  0.3090349246
Iteration :  6   Loss :  0.302551700867
Iteration :  7   Loss :  0.296204488266
Iteration :  8   Loss :  0.289990433428
Iteration :  9   Loss :  0.283906742846
Iteration :  10   Loss :  0.277950681616
Iteration :  11   Loss :  0.27211957221
Iteration :  12   Loss :  0.266410793271
Iteration :  13   Loss :  0.260821778438
Iteration :  14   Loss :  0.255350015185
Iteration :  15   Loss :  0.249993043701
Iteration :  16   Loss :  0.244748455776
Iteration :  17   Loss :  0.239613893722
Iteration :  18   Loss :  0.234587049314
Iteration :  19   Loss :  0.229665662751
Iteration :  20   Loss :  0.224847521638
Iteration :  21   Loss :  0.220130459997
Iteration :  22   Loss :  0.215512357287
Iteration :  23   Loss :  0.210991137456
Iteration :  24   Loss :  0.206564768005
Iteration :  25   Loss :  0.202231259072
Iteration :  26   Loss :  0.197988662543
Iteration :  27   Loss :  0.193835071173
Iteration :  28   Loss :  0.189768617728
Iteration :  29   Loss :  0.185787474147
Iteration :  30   Loss :  0.181889850721
Iteration :  31   Loss :  0.178073995284
Iteration :  32   Loss :  0.174338192433
Iteration :  33   Loss :  0.170680762749
Iteration :  34   Loss :  0.167100062045
Iteration :  35   Loss :  0.163594480631
Iteration :  36   Loss :  0.160162442583
Iteration :  37   Loss :  0.156802405039
Iteration :  38   Loss :  0.153512857507
Iteration :  39   Loss :  0.150292321179
Iteration :  40   Loss :  0.147139348275
Iteration :  41   Loss :  0.144052521387
Iteration :  42   Loss :  0.14103045284
Iteration :  43   Loss :  0.138071784074
Iteration :  44   Loss :  0.135175185029
Iteration :  45   Loss :  0.132339353549
Iteration :  46   Loss :  0.129563014794
Iteration :  47   Loss :  0.126844920671
Iteration :  48   Loss :  0.124183849269
Iteration :  49   Loss :  0.121578604312
Iteration :  50   Loss :  0.119028014621
Iteration :  51   Loss :  0.116530933586
Iteration :  52   Loss :  0.114086238653
Iteration :  53   Loss :  0.111692830817
Iteration :  54   Loss :  0.10934963413
Iteration :  55   Loss :  0.107055595215
Iteration :  56   Loss :  0.104809682794
Iteration :  57   Loss :  0.102610887224
Iteration :  58   Loss :  0.100458220046
Iteration :  59   Loss :  0.0983507135324
Iteration :  60   Loss :  0.0962874202623
Iteration :  61   Loss :  0.0942674126886
Iteration :  62   Loss :  0.0922897827233
Iteration :  63   Loss :  0.0903536413294
Iteration :  64   Loss :  0.0884581181208
Iteration :  65   Loss :  0.0866023609713
Iteration :  66   Loss :  0.0847855356311
Iteration :  67   Loss :  0.0830068253525
Iteration :  68   Loss :  0.0812654305221
Iteration :  69   Loss :  0.0795605683013
Iteration :  70   Loss :  0.077891472275
Iteration :  71   Loss :  0.0762573921065
Iteration :  72   Loss :  0.0746575932004
Iteration :  73   Loss :  0.0730913563722
Iteration :  74   Loss :  0.0715579775254
Iteration :  75   Loss :  0.0700567673344
Iteration :  76   Loss :  0.0685870509351
Iteration :  77   Loss :  0.0671481676212
Iteration :  78   Loss :  0.0657394705475
Iteration :  79   Loss :  0.0643603264387
Iteration :  80   Loss :  0.0630101153052
Iteration :  81   Loss :  0.0616882301639
Iteration :  82   Loss :  0.0603940767657
Iteration :  83   Loss :  0.059127073328
Iteration :  84   Loss :  0.0578866502737
Iteration :  85   Loss :  0.0566722499745
Iteration :  86   Loss :  0.0554833265008
Iteration :  87   Loss :  0.0543193453759
Iteration :  88   Loss :  0.0531797833359
Iteration :  89   Loss :  0.0520641280944
Iteration :  90   Loss :  0.0509718781122
Iteration :  91   Loss :  0.0499025423718
Iteration :  92   Loss :  0.0488556401569
Iteration :  93   Loss :  0.0478307008359
Iteration :  94   Loss :  0.0468272636509
Iteration :  95   Loss :  0.0458448775097
Iteration :  96   Loss :  0.044883100784
Iteration :  97   Loss :  0.0439415011101
Iteration :  98   Loss :  0.043019655195
Iteration :  99   Loss :  0.0421171486259
[ -1.37217881e-03  -2.23605239e-04  -8.05497092e-04 ...,   9.49550960e-04
   4.60272709e-04   6.94953685e-05]
CROSS VALIDATION 19
Iteration :  0   Loss :  30.4709198108
Iteration :  1   Loss :  14.0082559695
Iteration :  2   Loss :  0.267203233493
Iteration :  3   Loss :  0.261597594107
Iteration :  4   Loss :  0.256109555068
Iteration :  5   Loss :  0.250736649245
Iteration :  6   Loss :  0.245476461268
Iteration :  7   Loss :  0.240326626434
Iteration :  8   Loss :  0.235284829652
Iteration :  9   Loss :  0.2303488044
Iteration :  10   Loss :  0.225516331703
Iteration :  11   Loss :  0.220785239138
Iteration :  12   Loss :  0.21615339986
Iteration :  13   Loss :  0.211618731638
Iteration :  14   Loss :  0.207179195928
Iteration :  15   Loss :  0.202832796951
Iteration :  16   Loss :  0.198577580798
Iteration :  17   Loss :  0.19441163455
Iteration :  18   Loss :  0.190333085419
Iteration :  19   Loss :  0.186340099907
Iteration :  20   Loss :  0.182430882981
Iteration :  21   Loss :  0.178603677264
Iteration :  22   Loss :  0.174856762248
Iteration :  23   Loss :  0.171188453521
Iteration :  24   Loss :  0.167597102006
Iteration :  25   Loss :  0.164081093222
Iteration :  26   Loss :  0.16063884656
Iteration :  27   Loss :  0.157268814568
Iteration :  28   Loss :  0.15396948226
Iteration :  29   Loss :  0.150739366431
Iteration :  30   Loss :  0.147577014994
Iteration :  31   Loss :  0.144481006324
Iteration :  32   Loss :  0.141449948619
Iteration :  33   Loss :  0.138482479278
Iteration :  34   Loss :  0.135577264286
Iteration :  35   Loss :  0.13273299761
Iteration :  36   Loss :  0.129948400622
Iteration :  37   Loss :  0.127222221515
Iteration :  38   Loss :  0.124553234742
Iteration :  39   Loss :  0.121940240471
Iteration :  40   Loss :  0.119382064038
Iteration :  41   Loss :  0.116877555424
Iteration :  42   Loss :  0.114425588734
Iteration :  43   Loss :  0.112025061695
Iteration :  44   Loss :  0.109674895157
Iteration :  45   Loss :  0.107374032612
Iteration :  46   Loss :  0.105121439713
Iteration :  47   Loss :  0.102916103816
Iteration :  48   Loss :  0.100757033518
Iteration :  49   Loss :  0.098643258216
Iteration :  50   Loss :  0.0965738276699
Iteration :  51   Loss :  0.0945478115736
Iteration :  52   Loss :  0.0925642991382
Iteration :  53   Loss :  0.0906223986822
Iteration :  54   Loss :  0.0887212372305
Iteration :  55   Loss :  0.0868599601221
Iteration :  56   Loss :  0.085037730626
Iteration :  57   Loss :  0.0832537295649
Iteration :  58   Loss :  0.0815071549469
Iteration :  59   Loss :  0.079797221605
Iteration :  60   Loss :  0.0781231608444
Iteration :  61   Loss :  0.0764842200963
Iteration :  62   Loss :  0.0748796625804
Iteration :  63   Loss :  0.0733087669731
Iteration :  64   Loss :  0.0717708270833
Iteration :  65   Loss :  0.0702651515351
Iteration :  66   Loss :  0.0687910634571
Iteration :  67   Loss :  0.0673479001777
Iteration :  68   Loss :  0.0659350129276
Iteration :  69   Loss :  0.0645517665479
Iteration :  70   Loss :  0.0631975392047
Iteration :  71   Loss :  0.0618717221095
Iteration :  72   Loss :  0.0605737192456
Iteration :  73   Loss :  0.0593029470999
Iteration :  74   Loss :  0.0580588344011
Iteration :  75   Loss :  0.0568408218622
Iteration :  76   Loss :  0.0556483619298
Iteration :  77   Loss :  0.0544809185373
Iteration :  78   Loss :  0.0533379668644
Iteration :  79   Loss :  0.0522189931008
Iteration :  80   Loss :  0.0511234942156
Iteration :  81   Loss :  0.0500509777308
Iteration :  82   Loss :  0.0490009615
Iteration :  83   Loss :  0.047972973492
Iteration :  84   Loss :  0.0469665515781
Iteration :  85   Loss :  0.0459812433245
Iteration :  86   Loss :  0.0450166057892
Iteration :  87   Loss :  0.0440722053224
Iteration :  88   Loss :  0.0431476173721
Iteration :  89   Loss :  0.0422424262927
Iteration :  90   Loss :  0.0413562251585
Iteration :  91   Loss :  0.0404886155807
Iteration :  92   Loss :  0.0396392075282
Iteration :  93   Loss :  0.0388076191524
Iteration :  94   Loss :  0.0379934766154
Iteration :  95   Loss :  0.0371964139222
Iteration :  96   Loss :  0.0364160727558
Iteration :  97   Loss :  0.0356521023163
Iteration :  98   Loss :  0.0349041591634
Iteration :  99   Loss :  0.0341719070616
[ -7.38725931e-04  -2.15720788e-04  -5.60145378e-04 ...,   3.83434016e-04
   1.30854810e-04   6.93243957e-05]
Accuracy (Hinge Loss):	0.9
lmda : 0.3  eta : 0.0001
Logistic Loss
CROSS VALIDATION 0
Iteration :  0   Loss :  5.91796378255
Iteration :  1   Loss :  0.067543321922
Iteration :  2   Loss :  0.0651755499715
Iteration :  3   Loss :  0.0641187012722
Iteration :  4   Loss :  0.06335723809
Iteration :  5   Loss :  0.0627250942134
Iteration :  6   Loss :  0.0621645817001
Iteration :  7   Loss :  0.0616493779881
Iteration :  8   Loss :  0.0611654470942
Iteration :  9   Loss :  0.0607044920378
Iteration :  10   Loss :  0.0602612327184
Iteration :  11   Loss :  0.0598321168361
Iteration :  12   Loss :  0.0594146483518
Iteration :  13   Loss :  0.0590070110393
Iteration :  14   Loss :  0.058607844793
Iteration :  15   Loss :  0.0582161062357
Iteration :  16   Loss :  0.0578309783586
Iteration :  17   Loss :  0.057451809959
Iteration :  18   Loss :  0.0570780738746
Iteration :  19   Loss :  0.0567093374603
Iteration :  20   Loss :  0.056345241265
Iteration :  21   Loss :  0.0559854833342
Iteration :  22   Loss :  0.0556298074598
Iteration :  23   Loss :  0.0552779942492
Iteration :  24   Loss :  0.0549298542467
Iteration :  25   Loss :  0.0545852225691
Iteration :  26   Loss :  0.0542439546765
Iteration :  27   Loss :  0.0539059230033
Iteration :  28   Loss :  0.0535710142508
Iteration :  29   Loss :  0.0532391271914
Iteration :  30   Loss :  0.0529101708745
Iteration :  31   Loss :  0.0525840631501
Iteration :  32   Loss :  0.052260729445
Iteration :  33   Loss :  0.0519401017424
Iteration :  34   Loss :  0.0516221177259
Iteration :  35   Loss :  0.0513067200577
Iteration :  36   Loss :  0.0509938557662
Iteration :  37   Loss :  0.0506834757248
Iteration :  38   Loss :  0.0503755342057
Iteration :  39   Loss :  0.0500699884957
Iteration :  40   Loss :  0.0497667985659
Iteration :  41   Loss :  0.0494659267839
Iteration :  42   Loss :  0.0491673376651
Iteration :  43   Loss :  0.0488709976543
Iteration :  44   Loss :  0.0485768749351
Iteration :  45   Loss :  0.0482849392627
Iteration :  46   Loss :  0.047995161816
Iteration :  47   Loss :  0.0477075150668
Iteration :  48   Loss :  0.0474219726648
Iteration :  49   Loss :  0.0471385093343
Iteration :  50   Loss :  0.0468571007828
Iteration :  51   Loss :  0.0465777236192
Iteration :  52   Loss :  0.0463003552807
Iteration :  53   Loss :  0.0460249739665
Iteration :  54   Loss :  0.0457515585797
Iteration :  55   Loss :  0.045480088673
Iteration :  56   Loss :  0.0452105444011
Iteration :  57   Loss :  0.0449429064773
Iteration :  58   Loss :  0.0446771561335
Iteration :  59   Loss :  0.0444132750849
Iteration :  60   Loss :  0.044151245497
Iteration :  61   Loss :  0.0438910499559
Iteration :  62   Loss :  0.0436326714415
Iteration :  63   Loss :  0.043376093302
Iteration :  64   Loss :  0.0431212992319
Iteration :  65   Loss :  0.0428682732506
Iteration :  66   Loss :  0.0426169996836
Iteration :  67   Loss :  0.0423674631445
Iteration :  68   Loss :  0.0421196485193
Iteration :  69   Loss :  0.0418735409507
Iteration :  70   Loss :  0.0416291258251
Iteration :  71   Loss :  0.041386388759
Iteration :  72   Loss :  0.0411453155879
Iteration :  73   Loss :  0.0409058923548
Iteration :  74   Loss :  0.0406681053003
Iteration :  75   Loss :  0.040431940853
Iteration :  76   Loss :  0.040197385621
Iteration :  77   Loss :  0.0399644263836
Iteration :  78   Loss :  0.0397330500835
Iteration :  79   Loss :  0.0395032438202
Iteration :  80   Loss :  0.039274994843
Iteration :  81   Loss :  0.0390482905454
Iteration :  82   Loss :  0.038823118459
Iteration :  83   Loss :  0.0385994662484
Iteration :  84   Loss :  0.0383773217064
Iteration :  85   Loss :  0.0381566727493
Iteration :  86   Loss :  0.0379375074131
Iteration :  87   Loss :  0.037719813849
Iteration :  88   Loss :  0.0375035803205
Iteration :  89   Loss :  0.0372887951998
Iteration :  90   Loss :  0.0370754469648
Iteration :  91   Loss :  0.0368635241968
Iteration :  92   Loss :  0.0366530155775
Iteration :  93   Loss :  0.0364439098872
Iteration :  94   Loss :  0.0362361960028
Iteration :  95   Loss :  0.0360298628959
Iteration :  96   Loss :  0.0358248996318
Iteration :  97   Loss :  0.0356212953677
Iteration :  98   Loss :  0.0354190393521
Iteration :  99   Loss :  0.0352181209235
[ -1.71654170e-04   6.56685465e-04   2.59922089e-04 ...,   3.83543216e-04
  -1.20577890e-05   2.33649388e-04]
CROSS VALIDATION 1
Iteration :  0   Loss :  9.11320823913
Iteration :  1   Loss :  0.0764046255333
Iteration :  2   Loss :  0.0684680056582
Iteration :  3   Loss :  0.0662143333673
Iteration :  4   Loss :  0.0647748754496
Iteration :  5   Loss :  0.0636902390881
Iteration :  6   Loss :  0.0628017886955
Iteration :  7   Loss :  0.0620364221501
Iteration :  8   Loss :  0.0613547696387
Iteration :  9   Loss :  0.0607333903227
Iteration :  10   Loss :  0.0601573038797
Iteration :  11   Loss :  0.0596164165745
Iteration :  12   Loss :  0.0591036410371
Iteration :  13   Loss :  0.0586138333034
Iteration :  14   Loss :  0.0581431568205
Iteration :  15   Loss :  0.0576886840123
Iteration :  16   Loss :  0.0572481369818
Iteration :  17   Loss :  0.0568197132562
Iteration :  18   Loss :  0.0564019654241
Iteration :  19   Loss :  0.0559937159958
Iteration :  20   Loss :  0.0555939959167
Iteration :  21   Loss :  0.0552019993379
Iteration :  22   Loss :  0.0548170498
Iteration :  23   Loss :  0.0544385745778
Iteration :  24   Loss :  0.0540660849578
Iteration :  25   Loss :  0.0536991608954
Iteration :  26   Loss :  0.0533374389491
Iteration :  27   Loss :  0.0529806026965
Iteration :  28   Loss :  0.0526283750524
Iteration :  29   Loss :  0.0522805120594
Iteration :  30   Loss :  0.0519367978298
Iteration :  31   Loss :  0.0515970403957
Iteration :  32   Loss :  0.0512610682806
Iteration :  33   Loss :  0.0509287276505
Iteration :  34   Loss :  0.0505998799312
Iteration :  35   Loss :  0.0502743998065
Iteration :  36   Loss :  0.0499521735257
Iteration :  37   Loss :  0.0496330974669
Iteration :  38   Loss :  0.049317076912
Iteration :  39   Loss :  0.0490040249968
Iteration :  40   Loss :  0.0486938618082
Iteration :  41   Loss :  0.0483865136046
Iteration :  42   Loss :  0.0480819121405
Iteration :  43   Loss :  0.0477799940789
Iteration :  44   Loss :  0.0474807004787
Iteration :  45   Loss :  0.047183976346
Iteration :  46   Loss :  0.0468897702407
Iteration :  47   Loss :  0.0465980339289
Iteration :  48   Loss :  0.0463087220775
Iteration :  49   Loss :  0.0460217919831
Iteration :  50   Loss :  0.0457372033309
Iteration :  51   Loss :  0.0454549179818
Iteration :  52   Loss :  0.0451748997809
Iteration :  53   Loss :  0.0448971143876
Iteration :  54   Loss :  0.0446215291226
Iteration :  55   Loss :  0.0443481128313
Iteration :  56   Loss :  0.0440768357605
Iteration :  57   Loss :  0.0438076694478
Iteration :  58   Loss :  0.043540586621
Iteration :  59   Loss :  0.043275561108
Iteration :  60   Loss :  0.0430125677547
Iteration :  61   Loss :  0.0427515823508
Iteration :  62   Loss :  0.0424925815619
Iteration :  63   Loss :  0.0422355428682
Iteration :  64   Loss :  0.041980444508
Iteration :  65   Loss :  0.041727265427
Iteration :  66   Loss :  0.0414759852309
Iteration :  67   Loss :  0.0412265841427
Iteration :  68   Loss :  0.0409790429627
Iteration :  69   Loss :  0.0407333430329
Iteration :  70   Loss :  0.0404894662028
Iteration :  71   Loss :  0.040247394799
Iteration :  72   Loss :  0.0400071115962
Iteration :  73   Loss :  0.0397685997911
Iteration :  74   Loss :  0.0395318429773
Iteration :  75   Loss :  0.0392968251228
Iteration :  76   Loss :  0.0390635305484
Iteration :  77   Loss :  0.0388319439079
Iteration :  78   Loss :  0.0386020501693
Iteration :  79   Loss :  0.0383738345971
Iteration :  80   Loss :  0.0381472827362
Iteration :  81   Loss :  0.0379223803961
Iteration :  82   Loss :  0.0376991136362
Iteration :  83   Loss :  0.0374774687521
Iteration :  84   Loss :  0.0372574322624
Iteration :  85   Loss :  0.0370389908962
Iteration :  86   Loss :  0.0368221315815
Iteration :  87   Loss :  0.0366068414337
Iteration :  88   Loss :  0.0363931077451
Iteration :  89   Loss :  0.0361809179747
Iteration :  90   Loss :  0.0359702597386
Iteration :  91   Loss :  0.0357611208009
Iteration :  92   Loss :  0.0355534890645
Iteration :  93   Loss :  0.0353473525636
Iteration :  94   Loss :  0.0351426994552
Iteration :  95   Loss :  0.0349395180121
Iteration :  96   Loss :  0.0347377966154
Iteration :  97   Loss :  0.0345375237485
Iteration :  98   Loss :  0.0343386879906
Iteration :  99   Loss :  0.0341412780109
[-0.00073073  0.00040558 -0.00031539 ...,  0.00019841 -0.00015625
  0.00028769]
CROSS VALIDATION 2
Iteration :  0   Loss :  2.62520205364
Iteration :  1   Loss :  0.066234467482
Iteration :  2   Loss :  0.0648007874606
Iteration :  3   Loss :  0.0638825586856
Iteration :  4   Loss :  0.0631694875855
Iteration :  5   Loss :  0.0625621600604
Iteration :  6   Loss :  0.0620176769398
Iteration :  7   Loss :  0.061514221268
Iteration :  8   Loss :  0.0610394405834
Iteration :  9   Loss :  0.0605857842847
Iteration :  10   Loss :  0.0601483621681
Iteration :  11   Loss :  0.0597238598453
Iteration :  12   Loss :  0.0593099468112
Iteration :  13   Loss :  0.0589049336687
Iteration :  14   Loss :  0.0585075638234
Iteration :  15   Loss :  0.0581168817104
Iteration :  16   Loss :  0.0577321465573
Iteration :  17   Loss :  0.0573527742779
Iteration :  18   Loss :  0.0569782973179
Iteration :  19   Loss :  0.056608336284
Iteration :  20   Loss :  0.0562425795017
Iteration :  21   Loss :  0.0558807680261
Iteration :  22   Loss :  0.0555226844779
Iteration :  23   Loss :  0.0551681446093
Iteration :  24   Loss :  0.0548169908505
Iteration :  25   Loss :  0.0544690873136
Iteration :  26   Loss :  0.0541243158831
Iteration :  27   Loss :  0.053782573126
Iteration :  28   Loss :  0.0534437678279
Iteration :  29   Loss :  0.0531078190117
Iteration :  30   Loss :  0.0527746543318
Iteration :  31   Loss :  0.0524442087625
Iteration :  32   Loss :  0.0521164235208
Iteration :  33   Loss :  0.0517912451745
Iteration :  34   Loss :  0.0514686249007
Iteration :  35   Loss :  0.0511485178648
Iteration :  36   Loss :  0.050830882698
Iteration :  37   Loss :  0.0505156810561
Iteration :  38   Loss :  0.0502028772444
Iteration :  39   Loss :  0.0498924378968
Iteration :  40   Loss :  0.0495843317024
Iteration :  41   Loss :  0.0492785291689
Iteration :  42   Loss :  0.0489750024185
Iteration :  43   Loss :  0.0486737250115
Iteration :  44   Loss :  0.0483746717921
Iteration :  45   Loss :  0.0480778187542
Iteration :  46   Loss :  0.0477831429235
Iteration :  47   Loss :  0.0474906222541
Iteration :  48   Loss :  0.0472002355376
Iteration :  49   Loss :  0.0469119623221
Iteration :  50   Loss :  0.0466257828409
Iteration :  51   Loss :  0.0463416779491
Iteration :  52   Loss :  0.0460596290666
Iteration :  53   Loss :  0.0457796181278
Iteration :  54   Loss :  0.0455016275364
Iteration :  55   Loss :  0.0452256401242
Iteration :  56   Loss :  0.0449516391149
Iteration :  57   Loss :  0.044679608091
Iteration :  58   Loss :  0.0444095309638
Iteration :  59   Loss :  0.0441413919464
Iteration :  60   Loss :  0.0438751755294
Iteration :  61   Loss :  0.0436108664584
Iteration :  62   Loss :  0.0433484497138
Iteration :  63   Loss :  0.0430879104925
Iteration :  64   Loss :  0.0428292341907
Iteration :  65   Loss :  0.042572406389
Iteration :  66   Loss :  0.042317412838
Iteration :  67   Loss :  0.0420642394456
Iteration :  68   Loss :  0.0418128722651
Iteration :  69   Loss :  0.0415632974847
Iteration :  70   Loss :  0.0413155014176
Iteration :  71   Loss :  0.0410694704929
Iteration :  72   Loss :  0.0408251912476
Iteration :  73   Loss :  0.0405826503191
Iteration :  74   Loss :  0.0403418344384
Iteration :  75   Loss :  0.0401027304238
Iteration :  76   Loss :  0.0398653251756
Iteration :  77   Loss :  0.0396296056711
Iteration :  78   Loss :  0.03939555896
Iteration :  79   Loss :  0.0391631721602
Iteration :  80   Loss :  0.0389324324548
Iteration :  81   Loss :  0.0387033270886
Iteration :  82   Loss :  0.0384758433655
Iteration :  83   Loss :  0.0382499686465
Iteration :  84   Loss :  0.0380256903475
Iteration :  85   Loss :  0.0378029959377
Iteration :  86   Loss :  0.0375818729387
Iteration :  87   Loss :  0.0373623089237
Iteration :  88   Loss :  0.0371442915163
Iteration :  89   Loss :  0.0369278083908
Iteration :  90   Loss :  0.0367128472721
Iteration :  91   Loss :  0.0364993959358
Iteration :  92   Loss :  0.0362874422085
Iteration :  93   Loss :  0.0360769739692
Iteration :  94   Loss :  0.0358679791494
Iteration :  95   Loss :  0.0356604457347
Iteration :  96   Loss :  0.0354543617662
Iteration :  97   Loss :  0.0352497153414
Iteration :  98   Loss :  0.0350464946165
Iteration :  99   Loss :  0.0348446878079
[-0.0011281  -0.00040691 -0.00055763 ...,  0.00062165  0.00037108
  0.00021915]
CROSS VALIDATION 3
Iteration :  0   Loss :  8.03924449999
Iteration :  1   Loss :  0.0780982478121
Iteration :  2   Loss :  0.0636816149442
Iteration :  3   Loss :  0.0626607553337
Iteration :  4   Loss :  0.061848717507
Iteration :  5   Loss :  0.0611573786155
Iteration :  6   Loss :  0.0605435646677
Iteration :  7   Loss :  0.0599832577459
Iteration :  8   Loss :  0.0594618880936
Iteration :  9   Loss :  0.0589700398519
Iteration :  10   Loss :  0.0585013253184
Iteration :  11   Loss :  0.0580512408719
Iteration :  12   Loss :  0.0576165099121
Iteration :  13   Loss :  0.0571946853369
Iteration :  14   Loss :  0.0567838985369
Iteration :  15   Loss :  0.0563826951309
Iteration :  16   Loss :  0.0559899241469
Iteration :  17   Loss :  0.0556046612738
Iteration :  18   Loss :  0.0552261544828
Iteration :  19   Loss :  0.0548537847162
Iteration :  20   Loss :  0.05448703696
Iteration :  21   Loss :  0.0541254786154
Iteration :  22   Loss :  0.0537687430958
Iteration :  23   Loss :  0.0534165172255
Iteration :  24   Loss :  0.0530685314438
Iteration :  25   Loss :  0.0527245521093
Iteration :  26   Loss :  0.0523843753936
Iteration :  27   Loss :  0.0520478223929
Iteration :  28   Loss :  0.0517147351814
Iteration :  29   Loss :  0.0513849736003
Iteration :  30   Loss :  0.0510584126275
Iteration :  31   Loss :  0.0507349402064
Iteration :  32   Loss :  0.0504144554451
Iteration :  33   Loss :  0.0500968671117
Iteration :  34   Loss :  0.0497820923713
Iteration :  35   Loss :  0.04947005572
Iteration :  36   Loss :  0.0491606880807
Iteration :  37   Loss :  0.0488539260329
Iteration :  38   Loss :  0.0485497111528
Iteration :  39   Loss :  0.0482479894468
Iteration :  40   Loss :  0.0479487108622
Iteration :  41   Loss :  0.0476518288631
Iteration :  42   Loss :  0.0473573000621
Iteration :  43   Loss :  0.0470650838986
Iteration :  44   Loss :  0.0467751423566
Iteration :  45   Loss :  0.0464874397176
Iteration :  46   Loss :  0.046201942342
Iteration :  47   Loss :  0.0459186184764
Iteration :  48   Loss :  0.0456374380817
Iteration :  49   Loss :  0.0453583726814
Iteration :  50   Loss :  0.045081395225
Iteration :  51   Loss :  0.0448064799667
Iteration :  52   Loss :  0.0445336023559
Iteration :  53   Loss :  0.0442627389389
Iteration :  54   Loss :  0.0439938672703
Iteration :  55   Loss :  0.0437269658324
Iteration :  56   Loss :  0.0434620139626
Iteration :  57   Loss :  0.0431989917871
Iteration :  58   Loss :  0.0429378801607
Iteration :  59   Loss :  0.0426786606112
Iteration :  60   Loss :  0.0424213152892
Iteration :  61   Loss :  0.0421658269217
Iteration :  62   Loss :  0.0419121787694
Iteration :  63   Loss :  0.0416603545873
Iteration :  64   Loss :  0.0414103385888
Iteration :  65   Loss :  0.0411621154116
Iteration :  66   Loss :  0.040915670087
Iteration :  67   Loss :  0.0406709880109
Iteration :  68   Loss :  0.0404280549174
Iteration :  69   Loss :  0.0401868568532
Iteration :  70   Loss :  0.0399473801552
Iteration :  71   Loss :  0.0397096114283
Iteration :  72   Loss :  0.0394735375257
Iteration :  73   Loss :  0.0392391455301
Iteration :  74   Loss :  0.0390064227358
Iteration :  75   Loss :  0.0387753566331
Iteration :  76   Loss :  0.0385459348924
Iteration :  77   Loss :  0.0383181453505
Iteration :  78   Loss :  0.0380919759974
Iteration :  79   Loss :  0.0378674149637
Iteration :  80   Loss :  0.0376444505097
Iteration :  81   Loss :  0.0374230710144
Iteration :  82   Loss :  0.0372032649662
Iteration :  83   Loss :  0.0369850209535
Iteration :  84   Loss :  0.0367683276568
Iteration :  85   Loss :  0.036553173841
Iteration :  86   Loss :  0.0363395483483
Iteration :  87   Loss :  0.0361274400923
Iteration :  88   Loss :  0.0359168380523
Iteration :  89   Loss :  0.0357077312678
Iteration :  90   Loss :  0.0355001088348
Iteration :  91   Loss :  0.0352939599011
Iteration :  92   Loss :  0.0350892736634
Iteration :  93   Loss :  0.0348860393643
Iteration :  94   Loss :  0.0346842462898
Iteration :  95   Loss :  0.0344838837674
Iteration :  96   Loss :  0.0342849411644
Iteration :  97   Loss :  0.0340874078872
Iteration :  98   Loss :  0.0338912733802
Iteration :  99   Loss :  0.0336965271259
[-0.00072517  0.00045773 -0.00029296 ...,  0.0002082  -0.00013761
  0.0002996 ]
CROSS VALIDATION 4
Iteration :  0   Loss :  7.89334121332
Iteration :  1   Loss :  0.0769184169017
Iteration :  2   Loss :  0.0636810994791
Iteration :  3   Loss :  0.0626101136016
Iteration :  4   Loss :  0.0617719122463
Iteration :  5   Loss :  0.0610650890035
Iteration :  6   Loss :  0.0604413898018
Iteration :  7   Loss :  0.0598744587233
Iteration :  8   Loss :  0.059348509622
Iteration :  9   Loss :  0.0588534356458
Iteration :  10   Loss :  0.0583824296532
Iteration :  11   Loss :  0.057930719871
Iteration :  12   Loss :  0.0574948509745
Iteration :  13   Loss :  0.0570722525989
Iteration :  14   Loss :  0.0566609686631
Iteration :  15   Loss :  0.0562594811968
Iteration :  16   Loss :  0.055866592036
Iteration :  17   Loss :  0.0554813412174
Iteration :  18   Loss :  0.0551029493562
Iteration :  19   Loss :  0.0547307761184
Iteration :  20   Loss :  0.0543642897432
Iteration :  21   Loss :  0.054003044311
Iteration :  22   Loss :  0.0536466625406
Iteration :  23   Loss :  0.0532948225988
Iteration :  24   Loss :  0.0529472478655
Iteration :  25   Loss :  0.0526036989058
Iteration :  26   Loss :  0.0522639671092
Iteration :  27   Loss :  0.051927869605
Iteration :  28   Loss :  0.0515952451623
Iteration :  29   Loss :  0.0512659508577
Iteration :  30   Loss :  0.0509398593487
Iteration :  31   Loss :  0.050616856626
Iteration :  32   Loss :  0.0502968401495
Iteration :  33   Loss :  0.0499797172935
Iteration :  34   Loss :  0.0496654040418
Iteration :  35   Loss :  0.0493538238877
Iteration :  36   Loss :  0.0490449069017
Iteration :  37   Loss :  0.0487385889366
Iteration :  38   Loss :  0.0484348109493
Iteration :  39   Loss :  0.048133518416
Iteration :  40   Loss :  0.0478346608302
Iteration :  41   Loss :  0.0475381912658
Iteration :  42   Loss :  0.0472440659989
Iteration :  43   Loss :  0.0469522441771
Iteration :  44   Loss :  0.0466626875304
Iteration :  45   Loss :  0.0463753601175
Iteration :  46   Loss :  0.0460902281014
Iteration :  47   Loss :  0.0458072595523
Iteration :  48   Loss :  0.0455264242722
Iteration :  49   Loss :  0.0452476936387
Iteration :  50   Loss :  0.0449710404664
Iteration :  51   Loss :  0.0446964388828
Iteration :  52   Loss :  0.0444238642169
Iteration :  53   Loss :  0.0441532928987
Iteration :  54   Loss :  0.04388470237
Iteration :  55   Loss :  0.0436180710022
Iteration :  56   Loss :  0.0433533780228
Iteration :  57   Loss :  0.043090603449
Iteration :  58   Loss :  0.0428297280261
Iteration :  59   Loss :  0.042570733173
Iteration :  60   Loss :  0.042313600931
Iteration :  61   Loss :  0.0420583139176
Iteration :  62   Loss :  0.0418048552845
Iteration :  63   Loss :  0.0415532086782
Iteration :  64   Loss :  0.0413033582048
Iteration :  65   Loss :  0.0410552883964
Iteration :  66   Loss :  0.0408089841812
Iteration :  67   Loss :  0.0405644308554
Iteration :  68   Loss :  0.0403216140572
Iteration :  69   Loss :  0.0400805197429
Iteration :  70   Loss :  0.0398411341648
Iteration :  71   Loss :  0.0396034438505
Iteration :  72   Loss :  0.0393674355842
Iteration :  73   Loss :  0.0391330963887
Iteration :  74   Loss :  0.0389004135094
Iteration :  75   Loss :  0.0386693743991
Iteration :  76   Loss :  0.0384399667038
Iteration :  77   Loss :  0.0382121782499
Iteration :  78   Loss :  0.0379859970323
Iteration :  79   Loss :  0.037761411203
Iteration :  80   Loss :  0.0375384090612
Iteration :  81   Loss :  0.0373169790437
Iteration :  82   Loss :  0.0370971097162
Iteration :  83   Loss :  0.0368787897656
Iteration :  84   Loss :  0.0366620079924
Iteration :  85   Loss :  0.0364467533044
Iteration :  86   Loss :  0.0362330147101
Iteration :  87   Loss :  0.0360207813141
Iteration :  88   Loss :  0.0358100423113
Iteration :  89   Loss :  0.0356007869833
Iteration :  90   Loss :  0.0353930046936
Iteration :  91   Loss :  0.0351866848849
Iteration :  92   Loss :  0.0349818170757
Iteration :  93   Loss :  0.0347783908579
Iteration :  94   Loss :  0.0345763958939
Iteration :  95   Loss :  0.0343758219159
Iteration :  96   Loss :  0.034176658723
Iteration :  97   Loss :  0.0339788961811
Iteration :  98   Loss :  0.0337825242213
Iteration :  99   Loss :  0.0335875328394
[-0.00071109  0.00046967 -0.00028137 ...,  0.00019742 -0.00013764
  0.00030023]
CROSS VALIDATION 5
Iteration :  0   Loss :  6.68745163129
Iteration :  1   Loss :  0.0643948459395
Iteration :  2   Loss :  0.063542893551
Iteration :  3   Loss :  0.0628547642151
Iteration :  4   Loss :  0.0622561101218
Iteration :  5   Loss :  0.0617132620769
Iteration :  6   Loss :  0.0612083413556
Iteration :  7   Loss :  0.0607308312238
Iteration :  8   Loss :  0.0602740821539
Iteration :  9   Loss :  0.059833655921
Iteration :  10   Loss :  0.0594064621534
Iteration :  11   Loss :  0.0589902742073
Iteration :  12   Loss :  0.0585834417591
Iteration :  13   Loss :  0.0581847120917
Iteration :  14   Loss :  0.0577931146194
Iteration :  15   Loss :  0.0574078838182
Iteration :  16   Loss :  0.0570284063395
Iteration :  17   Loss :  0.0566541838369
Iteration :  18   Loss :  0.0562848062806
Iteration :  19   Loss :  0.0559199324401
Iteration :  20   Loss :  0.0555592753711
Iteration :  21   Loss :  0.0552025914615
Iteration :  22   Loss :  0.0548496720521
Iteration :  23   Loss :  0.0545003369479
Iteration :  24   Loss :  0.0541544293381
Iteration :  25   Loss :  0.0538118117769
Iteration :  26   Loss :  0.0534723629756
Iteration :  27   Loss :  0.0531359752186
Iteration :  28   Loss :  0.0528025522661
Iteration :  29   Loss :  0.0524720076384
Iteration :  30   Loss :  0.0521442632044
Iteration :  31   Loss :  0.0518192480106
Iteration :  32   Loss :  0.051496897306
Iteration :  33   Loss :  0.0511771517234
Iteration :  34   Loss :  0.0508599565909
Iteration :  35   Loss :  0.0505452613475
Iteration :  36   Loss :  0.0502330190467
Iteration :  37   Loss :  0.049923185932
Iteration :  38   Loss :  0.0496157210724
Iteration :  39   Loss :  0.0493105860491
Iteration :  40   Loss :  0.0490077446838
Iteration :  41   Loss :  0.0487071628037
Iteration :  42   Loss :  0.0484088080361
Iteration :  43   Loss :  0.04811264963
Iteration :  44   Loss :  0.0478186582992
Iteration :  45   Loss :  0.0475268060845
Iteration :  46   Loss :  0.0472370662329
Iteration :  47   Loss :  0.04694941309
Iteration :  48   Loss :  0.0466638220057
Iteration :  49   Loss :  0.0463802692499
Iteration :  50   Loss :  0.0460987319376
Iteration :  51   Loss :  0.0458191879625
Iteration :  52   Loss :  0.0455416159375
Iteration :  53   Loss :  0.0452659951408
Iteration :  54   Loss :  0.0449923054686
Iteration :  55   Loss :  0.0447205273921
Iteration :  56   Loss :  0.0444506419184
Iteration :  57   Loss :  0.0441826305557
Iteration :  58   Loss :  0.0439164752817
Iteration :  59   Loss :  0.0436521585151
Iteration :  60   Loss :  0.0433896630891
Iteration :  61   Loss :  0.0431289722283
Iteration :  62   Loss :  0.0428700695268
Iteration :  63   Loss :  0.0426129389285
Iteration :  64   Loss :  0.0423575647092
Iteration :  65   Loss :  0.0421039314601
Iteration :  66   Loss :  0.0418520240725
Iteration :  67   Loss :  0.041601827724
Iteration :  68   Loss :  0.0413533278655
Iteration :  69   Loss :  0.0411065102095
Iteration :  70   Loss :  0.0408613607188
Iteration :  71   Loss :  0.040617865597
Iteration :  72   Loss :  0.0403760112784
Iteration :  73   Loss :  0.0401357844195
Iteration :  74   Loss :  0.0398971718911
Iteration :  75   Loss :  0.0396601607703
Iteration :  76   Loss :  0.0394247383339
Iteration :  77   Loss :  0.0391908920515
Iteration :  78   Loss :  0.0389586095796
Iteration :  79   Loss :  0.0387278787555
Iteration :  80   Loss :  0.0384986875924
Iteration :  81   Loss :  0.0382710242743
Iteration :  82   Loss :  0.0380448771511
Iteration :  83   Loss :  0.0378202347343
Iteration :  84   Loss :  0.0375970856932
Iteration :  85   Loss :  0.0373754188507
Iteration :  86   Loss :  0.0371552231802
Iteration :  87   Loss :  0.0369364878017
Iteration :  88   Loss :  0.0367192019791
Iteration :  89   Loss :  0.0365033551173
Iteration :  90   Loss :  0.0362889367595
Iteration :  91   Loss :  0.0360759365842
Iteration :  92   Loss :  0.0358643444038
Iteration :  93   Loss :  0.0356541501613
Iteration :  94   Loss :  0.0354453439294
Iteration :  95   Loss :  0.0352379159076
Iteration :  96   Loss :  0.0350318564211
Iteration :  97   Loss :  0.0348271559187
Iteration :  98   Loss :  0.0346238049718
Iteration :  99   Loss :  0.0344217942722
[ -4.92676196e-04   5.71615241e-04  -2.95210888e-04 ...,   6.45900602e-05
  -1.06430194e-04   3.77868650e-04]
CROSS VALIDATION 6
Iteration :  0   Loss :  7.88757219792
Iteration :  1   Loss :  0.076858512448
Iteration :  2   Loss :  0.0636799861314
Iteration :  3   Loss :  0.0626069945415
Iteration :  4   Loss :  0.0617678689885
Iteration :  5   Loss :  0.0610605670514
Iteration :  6   Loss :  0.0604366111548
Iteration :  7   Loss :  0.0598695485948
Iteration :  8   Loss :  0.059343545196
Iteration :  9   Loss :  0.0588484678528
Iteration :  10   Loss :  0.0583774939994
Iteration :  11   Loss :  0.0579258422816
Iteration :  12   Loss :  0.0574900511573
Iteration :  13   Loss :  0.0570675460836
Iteration :  14   Loss :  0.0566563680927
Iteration :  15   Loss :  0.0562549971757
Iteration :  16   Loss :  0.0558622337045
Iteration :  17   Loss :  0.0554771166513
Iteration :  18   Loss :  0.0550988658516
Iteration :  19   Loss :  0.0547268403988
Iteration :  20   Loss :  0.0543605081131
Iteration :  21   Loss :  0.0539994227712
Iteration :  22   Loss :  0.0536432068765
Iteration :  23   Loss :  0.053291538449
Iteration :  24   Loss :  0.0529441407753
Iteration :  25   Loss :  0.0526007743697
Iteration :  26   Loss :  0.0522612306049
Iteration :  27   Loss :  0.0519253266208
Iteration :  28   Loss :  0.0515929012193
Iteration :  29   Loss :  0.0512638115281
Iteration :  30   Loss :  0.0509379302707
Iteration :  31   Loss :  0.0506151435162
Iteration :  32   Loss :  0.0502953488128
Iteration :  33   Loss :  0.0499784536316
Iteration :  34   Loss :  0.0496643740601
Iteration :  35   Loss :  0.0493530337007
Iteration :  36   Loss :  0.0490443627374
Iteration :  37   Loss :  0.0487382971403
Iteration :  38   Loss :  0.0484347779856
Iteration :  39   Loss :  0.0481337508713
Iteration :  40   Loss :  0.0478351654133
Iteration :  41   Loss :  0.0475389748089
Iteration :  42   Loss :  0.0472451354571
Iteration :  43   Loss :  0.0469536066281
Iteration :  44   Loss :  0.0466643501732
Iteration :  45   Loss :  0.0463773302704
Iteration :  46   Loss :  0.0460925132005
Iteration :  47   Loss :  0.0458098671483
Iteration :  48   Loss :  0.0455293620273
Iteration :  49   Loss :  0.0452509693233
Iteration :  50   Loss :  0.044974661955
Iteration :  51   Loss :  0.0447004141491
Iteration :  52   Loss :  0.044428201329
Iteration :  53   Loss :  0.0441580000142
Iteration :  54   Loss :  0.0438897877293
Iteration :  55   Loss :  0.0436235429226
Iteration :  56   Loss :  0.0433592448918
Iteration :  57   Loss :  0.0430968737165
Iteration :  58   Loss :  0.0428364101974
Iteration :  59   Loss :  0.0425778358001
Iteration :  60   Loss :  0.0423211326041
Iteration :  61   Loss :  0.042066283256
Iteration :  62   Loss :  0.0418132709272
Iteration :  63   Loss :  0.0415620792738
Iteration :  64   Loss :  0.0413126924008
Iteration :  65   Loss :  0.0410650948287
Iteration :  66   Loss :  0.0408192714627
Iteration :  67   Loss :  0.040575207564
Iteration :  68   Loss :  0.0403328887239
Iteration :  69   Loss :  0.0400923008391
Iteration :  70   Loss :  0.0398534300893
Iteration :  71   Loss :  0.0396162629165
Iteration :  72   Loss :  0.0393807860051
Iteration :  73   Loss :  0.0391469862646
Iteration :  74   Loss :  0.0389148508125
Iteration :  75   Loss :  0.0386843669589
Iteration :  76   Loss :  0.0384555221926
Iteration :  77   Loss :  0.0382283041675
Iteration :  78   Loss :  0.0380027006905
Iteration :  79   Loss :  0.0377786997105
Iteration :  80   Loss :  0.0375562893076
Iteration :  81   Loss :  0.0373354576839
Iteration :  82   Loss :  0.0371161931547
Iteration :  83   Loss :  0.0368984841404
Iteration :  84   Loss :  0.0366823191595
Iteration :  85   Loss :  0.0364676868217
Iteration :  86   Loss :  0.0362545758222
Iteration :  87   Loss :  0.0360429749363
Iteration :  88   Loss :  0.0358328730149
Iteration :  89   Loss :  0.0356242589799
Iteration :  90   Loss :  0.0354171218209
Iteration :  91   Loss :  0.0352114505921
Iteration :  92   Loss :  0.0350072344094
Iteration :  93   Loss :  0.0348044624486
Iteration :  94   Loss :  0.0346031239434
Iteration :  95   Loss :  0.0344032081839
Iteration :  96   Loss :  0.034204704516
Iteration :  97   Loss :  0.0340076023408
Iteration :  98   Loss :  0.0338118911141
Iteration :  99   Loss :  0.0336175603466
[-0.00071209  0.00046979 -0.00028093 ...,  0.00019677 -0.0001381
  0.00030011]
CROSS VALIDATION 7
Iteration :  0   Loss :  7.90782251533
Iteration :  1   Loss :  0.0770070546074
Iteration :  2   Loss :  0.0636187302182
Iteration :  3   Loss :  0.0625562010591
Iteration :  4   Loss :  0.0617210736258
Iteration :  5   Loss :  0.0610150978366
Iteration :  6   Loss :  0.0603911616818
Iteration :  7   Loss :  0.0598234067912
Iteration :  8   Loss :  0.0592962920962
Iteration :  9   Loss :  0.0587998426527
Iteration :  10   Loss :  0.0583273273106
Iteration :  11   Loss :  0.0578740205363
Iteration :  12   Loss :  0.0574364964642
Iteration :  13   Loss :  0.0570122042658
Iteration :  14   Loss :  0.0565992012965
Iteration :  15   Loss :  0.0561959791477
Iteration :  16   Loss :  0.0558013466869
Iteration :  17   Loss :  0.0554143492895
Iteration :  18   Loss :  0.0550342117535
Iteration :  19   Loss :  0.0546602971228
Iteration :  20   Loss :  0.0542920764454
Iteration :  21   Loss :  0.0539291062009
Iteration :  22   Loss :  0.0535710112086
Iteration :  23   Loss :  0.0532174715145
Iteration :  24   Loss :  0.0528682122108
Iteration :  25   Loss :  0.0525229954469
Iteration :  26   Loss :  0.0521816140976
Iteration :  27   Loss :  0.0518438866974
Iteration :  28   Loss :  0.0515096533558
Iteration :  29   Loss :  0.0511787724357
Iteration :  30   Loss :  0.050851117834
Iteration :  31   Loss :  0.0505265767392
Iteration :  32   Loss :  0.0502050477718
Iteration :  33   Loss :  0.0498864394313
Iteration :  34   Loss :  0.0495706687939
Iteration :  35   Loss :  0.0492576604133
Iteration :  36   Loss :  0.0489473453885
Iteration :  37   Loss :  0.0486396605702
Iteration :  38   Loss :  0.0483345478816
Iteration :  39   Loss :  0.0480319537346
Iteration :  40   Loss :  0.0477318285263
Iteration :  41   Loss :  0.047434126203
Iteration :  42   Loss :  0.0471388038806
Iteration :  43   Loss :  0.0468458215145
Iteration :  44   Loss :  0.0465551416091
Iteration :  45   Loss :  0.0462667289645
Iteration :  46   Loss :  0.0459805504513
Iteration :  47   Loss :  0.0456965748134
Iteration :  48   Loss :  0.0454147724919
Iteration :  49   Loss :  0.045135115469
Iteration :  50   Loss :  0.0448575771291
Iteration :  51   Loss :  0.044582132134
Iteration :  52   Loss :  0.0443087563115
Iteration :  53   Loss :  0.0440374265554
Iteration :  54   Loss :  0.0437681207346
Iteration :  55   Loss :  0.043500817612
Iteration :  56   Loss :  0.0432354967705
Iteration :  57   Loss :  0.0429721385461
Iteration :  58   Loss :  0.0427107239665
Iteration :  59   Loss :  0.0424512346964
Iteration :  60   Loss :  0.042193652986
Iteration :  61   Loss :  0.0419379616252
Iteration :  62   Loss :  0.0416841439007
Iteration :  63   Loss :  0.0414321835573
Iteration :  64   Loss :  0.0411820647616
Iteration :  65   Loss :  0.0409337720695
Iteration :  66   Loss :  0.0406872903955
Iteration :  67   Loss :  0.0404426049845
Iteration :  68   Loss :  0.0401997013859
Iteration :  69   Loss :  0.0399585654298
Iteration :  70   Loss :  0.0397191832045
Iteration :  71   Loss :  0.0394815410362
Iteration :  72   Loss :  0.03924562547
Iteration :  73   Loss :  0.0390114232523
Iteration :  74   Loss :  0.0387789213143
Iteration :  75   Loss :  0.0385481067574
Iteration :  76   Loss :  0.0383189668391
Iteration :  77   Loss :  0.0380914889602
Iteration :  78   Loss :  0.0378656606533
Iteration :  79   Loss :  0.0376414695717
Iteration :  80   Loss :  0.0374189034797
Iteration :  81   Loss :  0.0371979502438
Iteration :  82   Loss :  0.0369785978239
Iteration :  83   Loss :  0.0367608342666
Iteration :  84   Loss :  0.0365446476981
Iteration :  85   Loss :  0.0363300263185
Iteration :  86   Loss :  0.0361169583964
Iteration :  87   Loss :  0.0359054322642
Iteration :  88   Loss :  0.0356954363144
Iteration :  89   Loss :  0.035486958996
Iteration :  90   Loss :  0.0352799888116
Iteration :  91   Loss :  0.0350745143151
Iteration :  92   Loss :  0.0348705241101
Iteration :  93   Loss :  0.0346680068485
Iteration :  94   Loss :  0.0344669512295
Iteration :  95   Loss :  0.0342673459993
Iteration :  96   Loss :  0.0340691799508
Iteration :  97   Loss :  0.0338724419247
Iteration :  98   Loss :  0.0336771208092
Iteration :  99   Loss :  0.0334832055417
[-0.00071381  0.00046722 -0.00028523 ...,  0.0001949  -0.00014029
  0.00030093]
CROSS VALIDATION 8
Iteration :  0   Loss :  7.88957028887
Iteration :  1   Loss :  0.0773009550462
Iteration :  2   Loss :  0.0636814623484
Iteration :  3   Loss :  0.062620806153
Iteration :  4   Loss :  0.0617874155692
Iteration :  5   Loss :  0.061083137457
Iteration :  6   Loss :  0.0604608734826
Iteration :  7   Loss :  0.0598947680047
Iteration :  8   Loss :  0.0593692809558
Iteration :  9   Loss :  0.0588744380273
Iteration :  10   Loss :  0.0584035085397
Iteration :  11   Loss :  0.0579517672943
Iteration :  12   Loss :  0.0575157886343
Iteration :  13   Loss :  0.0570930218252
Iteration :  14   Loss :  0.0566815242135
Iteration :  15   Loss :  0.0562797872907
Iteration :  16   Loss :  0.0558866197444
Iteration :  17   Loss :  0.0555010666994
Iteration :  18   Loss :  0.0551223526413
Iteration :  19   Loss :  0.0547498402479
Iteration :  20   Loss :  0.0543830001533
Iteration :  21   Loss :  0.0540213883827
Iteration :  22   Loss :  0.0536646292654
Iteration :  23   Loss :  0.0533124023273
Iteration :  24   Loss :  0.0529644321151
Iteration :  25   Loss :  0.0526204802119
Iteration :  26   Loss :  0.0522803389088
Iteration :  27   Loss :  0.0519438261437
Iteration :  28   Loss :  0.0516107814197
Iteration :  29   Loss :  0.0512810624867
Iteration :  30   Loss :  0.0509545426249
Iteration :  31   Loss :  0.0506311084058
Iteration :  32   Loss :  0.0503106578342
Iteration :  33   Loss :  0.0499930987986
Iteration :  34   Loss :  0.0496783477704
Iteration :  35   Loss :  0.0493663287069
Iteration :  36   Loss :  0.0490569721215
Iteration :  37   Loss :  0.0487502142912
Iteration :  38   Loss :  0.0484459965794
Iteration :  39   Loss :  0.0481442648538
Iteration :  40   Loss :  0.0478449689838
Iteration :  41   Loss :  0.0475480624063
Iteration :  42   Loss :  0.0472535017469
Iteration :  43   Loss :  0.0469612464911
Iteration :  44   Loss :  0.046671258695
Iteration :  45   Loss :  0.0463835027326
Iteration :  46   Loss :  0.0460979450716
Iteration :  47   Loss :  0.0458145540771
Iteration :  48   Loss :  0.045533299836
Iteration :  49   Loss :  0.045254154002
Iteration :  50   Loss :  0.0449770896569
Iteration :  51   Loss :  0.0447020811864
Iteration :  52   Loss :  0.0444291041697
Iteration :  53   Loss :  0.0441581352793
Iteration :  54   Loss :  0.0438891521912
Iteration :  55   Loss :  0.0436221335037
Iteration :  56   Loss :  0.0433570586643
Iteration :  57   Loss :  0.0430939079024
Iteration :  58   Loss :  0.0428326621695
Iteration :  59   Loss :  0.0425733030832
Iteration :  60   Loss :  0.0423158128774
Iteration :  61   Loss :  0.0420601743555
Iteration :  62   Loss :  0.0418063708491
Iteration :  63   Loss :  0.0415543861781
Iteration :  64   Loss :  0.0413042046158
Iteration :  65   Loss :  0.0410558108557
Iteration :  66   Loss :  0.0408091899811
Iteration :  67   Loss :  0.0405643274374
Iteration :  68   Loss :  0.0403212090059
Iteration :  69   Loss :  0.0400798207798
Iteration :  70   Loss :  0.0398401491425
Iteration :  71   Loss :  0.0396021807464
Iteration :  72   Loss :  0.0393659024943
Iteration :  73   Loss :  0.0391313015214
Iteration :  74   Loss :  0.0388983651789
Iteration :  75   Loss :  0.0386670810191
Iteration :  76   Loss :  0.0384374367809
Iteration :  77   Loss :  0.0382094203767
Iteration :  78   Loss :  0.0379830198807
Iteration :  79   Loss :  0.0377582235172
Iteration :  80   Loss :  0.0375350196504
Iteration :  81   Loss :  0.0373133967747
Iteration :  82   Loss :  0.0370933435063
Iteration :  83   Loss :  0.0368748485746
Iteration :  84   Loss :  0.036657900815
Iteration :  85   Loss :  0.036442489162
Iteration :  86   Loss :  0.036228602643
Iteration :  87   Loss :  0.0360162303731
Iteration :  88   Loss :  0.0358053615493
Iteration :  89   Loss :  0.0355959854465
Iteration :  90   Loss :  0.0353880914134
Iteration :  91   Loss :  0.0351816688685
Iteration :  92   Loss :  0.0349767072976
Iteration :  93   Loss :  0.0347731962504
Iteration :  94   Loss :  0.0345711253386
Iteration :  95   Loss :  0.0343704842339
Iteration :  96   Loss :  0.0341712626661
Iteration :  97   Loss :  0.0339734504223
Iteration :  98   Loss :  0.0337770373453
Iteration :  99   Loss :  0.0335820133336
[-0.00071019  0.00047187 -0.00027479 ...,  0.00019889 -0.00014184
  0.00030016]
CROSS VALIDATION 9
Iteration :  0   Loss :  7.89852584459
Iteration :  1   Loss :  0.088551304983
Iteration :  2   Loss :  0.0736918999265
Iteration :  3   Loss :  0.0725366619117
Iteration :  4   Loss :  0.0716328552124
Iteration :  5   Loss :  0.0708656847704
Iteration :  6   Loss :  0.0701831814585
Iteration :  7   Loss :  0.0695577789818
Iteration :  8   Loss :  0.0689732862569
Iteration :  9   Loss :  0.0684194825185
Iteration :  10   Loss :  0.0678895571225
Iteration :  11   Loss :  0.0673787756211
Iteration :  12   Loss :  0.0668837325941
Iteration :  13   Loss :  0.066401908537
Iteration :  14   Loss :  0.0659313945988
Iteration :  15   Loss :  0.06547071488
Iteration :  16   Loss :  0.0650187079255
Iteration :  17   Loss :  0.0645744454618
Iteration :  18   Loss :  0.0641371753151
Iteration :  19   Loss :  0.0637062804565
Iteration :  20   Loss :  0.0632812490661
Iteration :  21   Loss :  0.062861652284
Iteration :  22   Loss :  0.0624471274266
Iteration :  23   Loss :  0.0620373651536
Iteration :  24   Loss :  0.0616320995373
Iteration :  25   Loss :  0.0612311002903
Iteration :  26   Loss :  0.0608341666217
Iteration :  27   Loss :  0.0604411223344
Iteration :  28   Loss :  0.0600518118793
Iteration :  29   Loss :  0.0596660971546
Iteration :  30   Loss :  0.0592838548895
Iteration :  31   Loss :  0.0589049744914
Iteration :  32   Loss :  0.0585293562629
Iteration :  33   Loss :  0.0581569099163
Iteration :  34   Loss :  0.0577875533292
Iteration :  35   Loss :  0.0574212114953
Iteration :  36   Loss :  0.0570578156372
Iteration :  37   Loss :  0.0566973024512
Iteration :  38   Loss :  0.056339613462
Iteration :  39   Loss :  0.0559846944695
Iteration :  40   Loss :  0.055632495071
Iteration :  41   Loss :  0.0552829682496
Iteration :  42   Loss :  0.0549360700152
Iteration :  43   Loss :  0.054591759093
Iteration :  44   Loss :  0.0542499966505
Iteration :  45   Loss :  0.0539107460583
Iteration :  46   Loss :  0.0535739726795
Iteration :  47   Loss :  0.053239643684
Iteration :  48   Loss :  0.052907727884
Iteration :  49   Loss :  0.0525781955881
Iteration :  50   Loss :  0.0522510184712
Iteration :  51   Loss :  0.0519261694588
Iteration :  52   Loss :  0.0516036226229
Iteration :  53   Loss :  0.0512833530891
Iteration :  54   Loss :  0.0509653369528
Iteration :  55   Loss :  0.0506495512036
Iteration :  56   Loss :  0.0503359736575
Iteration :  57   Loss :  0.0500245828946
Iteration :  58   Loss :  0.0497153582032
Iteration :  59   Loss :  0.0494082795292
Iteration :  60   Loss :  0.0491033274289
Iteration :  61   Loss :  0.0488004830274
Iteration :  62   Loss :  0.0484997279791
Iteration :  63   Loss :  0.0482010444323
Iteration :  64   Loss :  0.0479044149968
Iteration :  65   Loss :  0.0476098227137
Iteration :  66   Loss :  0.0473172510275
Iteration :  67   Loss :  0.047026683761
Iteration :  68   Loss :  0.0467381050915
Iteration :  69   Loss :  0.046451499529
Iteration :  70   Loss :  0.0461668518961
Iteration :  71   Loss :  0.0458841473096
Iteration :  72   Loss :  0.0456033711625
Iteration :  73   Loss :  0.0453245091087
Iteration :  74   Loss :  0.0450475470476
Iteration :  75   Loss :  0.0447724711101
Iteration :  76   Loss :  0.0444992676465
Iteration :  77   Loss :  0.0442279232135
Iteration :  78   Loss :  0.0439584245638
Iteration :  79   Loss :  0.0436907586354
Iteration :  80   Loss :  0.0434249125421
Iteration :  81   Loss :  0.0431608735645
Iteration :  82   Loss :  0.0428986291417
Iteration :  83   Loss :  0.0426381668636
Iteration :  84   Loss :  0.0423794744634
Iteration :  85   Loss :  0.0421225398117
Iteration :  86   Loss :  0.0418673509094
Iteration :  87   Loss :  0.0416138958828
Iteration :  88   Loss :  0.0413621629776
Iteration :  89   Loss :  0.0411121405545
Iteration :  90   Loss :  0.0408638170844
Iteration :  91   Loss :  0.0406171811443
Iteration :  92   Loss :  0.0403722214136
Iteration :  93   Loss :  0.0401289266699
Iteration :  94   Loss :  0.0398872857866
Iteration :  95   Loss :  0.0396472877292
Iteration :  96   Loss :  0.0394089215525
Iteration :  97   Loss :  0.0391721763985
Iteration :  98   Loss :  0.0389370414935
Iteration :  99   Loss :  0.0387035061463
[-0.00064306  0.00052601 -0.00019531 ...,  0.00046055 -0.000104    0.00030342]
CROSS VALIDATION 10
Iteration :  0   Loss :  7.2648143187
Iteration :  1   Loss :  0.0800817892407
Iteration :  2   Loss :  0.0791462624405
Iteration :  3   Loss :  0.0783662484712
Iteration :  4   Loss :  0.0776727041286
Iteration :  5   Loss :  0.0770337455341
Iteration :  6   Loss :  0.0764322566404
Iteration :  7   Loss :  0.0758581005515
Iteration :  8   Loss :  0.0753048381183
Iteration :  9   Loss :  0.0747681569173
Iteration :  10   Loss :  0.0742450461811
Iteration :  11   Loss :  0.0737333318097
Iteration :  12   Loss :  0.0732313992658
Iteration :  13   Loss :  0.0727380207636
Iteration :  14   Loss :  0.0722522433518
Iteration :  15   Loss :  0.0717733140802
Iteration :  16   Loss :  0.0713006285728
Iteration :  17   Loss :  0.070833694835
Iteration :  18   Loss :  0.0703721072452
Iteration :  19   Loss :  0.0699155275163
Iteration :  20   Loss :  0.0694636705281
Iteration :  21   Loss :  0.0690162936278
Iteration :  22   Loss :  0.068573188441
Iteration :  23   Loss :  0.0681341745265
Iteration :  24   Loss :  0.0676990944082
Iteration :  25   Loss :  0.067267809642
Iteration :  26   Loss :  0.0668401976763
Iteration :  27   Loss :  0.0664161493221
Iteration :  28   Loss :  0.0659955666993
Iteration :  29   Loss :  0.065578361557
Iteration :  30   Loss :  0.06516445389
Iteration :  31   Loss :  0.0647537707922
Iteration :  32   Loss :  0.0643462455008
Iteration :  33   Loss :  0.0639418165941
Iteration :  34   Loss :  0.0635404273166
Iteration :  35   Loss :  0.063142025006
Iteration :  36   Loss :  0.0627465606065
Iteration :  37   Loss :  0.062353988252
Iteration :  38   Loss :  0.0619642649089
Iteration :  39   Loss :  0.0615773500681
Iteration :  40   Loss :  0.0611932054782
Iteration :  41   Loss :  0.0608117949141
Iteration :  42   Loss :  0.0604330839757
Iteration :  43   Loss :  0.0600570399111
Iteration :  44   Loss :  0.0596836314627
Iteration :  45   Loss :  0.0593128287304
Iteration :  46   Loss :  0.0589446030522
Iteration :  47   Loss :  0.0585789268981
Iteration :  48   Loss :  0.0582157737753
Iteration :  49   Loss :  0.0578551181449
Iteration :  50   Loss :  0.0574969353468
Iteration :  51   Loss :  0.057141201533
Iteration :  52   Loss :  0.0567878936072
Iteration :  53   Loss :  0.0564369891712
Iteration :  54   Loss :  0.0560884664761
Iteration :  55   Loss :  0.055742304378
Iteration :  56   Loss :  0.0553984822984
Iteration :  57   Loss :  0.055056980188
Iteration :  58   Loss :  0.0547177784933
Iteration :  59   Loss :  0.054380858127
Iteration :  60   Loss :  0.0540462004399
Iteration :  61   Loss :  0.0537137871957
Iteration :  62   Loss :  0.0533836005479
Iteration :  63   Loss :  0.0530556230181
Iteration :  64   Loss :  0.0527298374762
Iteration :  65   Loss :  0.0524062271219
Iteration :  66   Loss :  0.0520847754678
Iteration :  67   Loss :  0.0517654663233
Iteration :  68   Loss :  0.05144828378
Iteration :  69   Loss :  0.0511332121976
Iteration :  70   Loss :  0.050820236191
Iteration :  71   Loss :  0.0505093406184
Iteration :  72   Loss :  0.0502005105694
Iteration :  73   Loss :  0.0498937313548
Iteration :  74   Loss :  0.049588988496
Iteration :  75   Loss :  0.0492862677159
Iteration :  76   Loss :  0.0489855549294
Iteration :  77   Loss :  0.0486868362354
Iteration :  78   Loss :  0.0483900979084
Iteration :  79   Loss :  0.0480953263911
Iteration :  80   Loss :  0.047802508287
Iteration :  81   Loss :  0.0475116303537
Iteration :  82   Loss :  0.0472226794963
Iteration :  83   Loss :  0.0469356427615
Iteration :  84   Loss :  0.0466505073315
Iteration :  85   Loss :  0.0463672605186
Iteration :  86   Loss :  0.0460858897601
Iteration :  87   Loss :  0.0458063826133
Iteration :  88   Loss :  0.0455287267504
Iteration :  89   Loss :  0.0452529099549
Iteration :  90   Loss :  0.0449789201166
Iteration :  91   Loss :  0.0447067452281
Iteration :  92   Loss :  0.0444363733808
Iteration :  93   Loss :  0.0441677927618
Iteration :  94   Loss :  0.0439009916499
Iteration :  95   Loss :  0.043635958413
Iteration :  96   Loss :  0.043372681505
Iteration :  97   Loss :  0.0431111494629
Iteration :  98   Loss :  0.0428513509044
Iteration :  99   Loss :  0.0425932745253
[ -6.65867862e-04   4.50054948e-04  -3.43359947e-04 ...,   4.31516617e-04
  -6.40818669e-05   3.20159730e-04]
CROSS VALIDATION 11
Iteration :  0   Loss :  7.88987107874
Iteration :  1   Loss :  0.0768811791455
Iteration :  2   Loss :  0.0636981411961
Iteration :  3   Loss :  0.0626263212869
Iteration :  4   Loss :  0.061787696481
Iteration :  5   Loss :  0.061080589419
Iteration :  6   Loss :  0.0604566643551
Iteration :  7   Loss :  0.0598895345772
Iteration :  8   Loss :  0.0593634005079
Iteration :  9   Loss :  0.0588681487856
Iteration :  10   Loss :  0.0583969688422
Iteration :  11   Loss :  0.0579450869711
Iteration :  12   Loss :  0.0575090466864
Iteration :  13   Loss :  0.0570862768831
Iteration :  14   Loss :  0.0566748209801
Iteration :  15   Loss :  0.0562731606487
Iteration :  16   Loss :  0.0558800974534
Iteration :  17   Loss :  0.0554946712128
Iteration :  18   Loss :  0.0551161023595
Iteration :  19   Loss :  0.0547437503987
Iteration :  20   Loss :  0.0543770834237
Iteration :  21   Loss :  0.054015655379
Iteration :  22   Loss :  0.0536590888535
Iteration :  23   Loss :  0.0533070618887
Iteration :  24   Loss :  0.0529592977417
Iteration :  25   Loss :  0.0526155568562
Iteration :  26   Loss :  0.0522756305013
Iteration :  27   Loss :  0.0519393356862
Iteration :  28   Loss :  0.0516065110595
Iteration :  29   Loss :  0.0512770135772
Iteration :  30   Loss :  0.0509507157756
Iteration :  31   Loss :  0.0506275035234
Iteration :  32   Loss :  0.0503072741579
Iteration :  33   Loss :  0.04998993493
Iteration :  34   Loss :  0.0496754016995
Iteration :  35   Loss :  0.0493635978349
Iteration :  36   Loss :  0.0490544532812
Iteration :  37   Loss :  0.0487479037655
Iteration :  38   Loss :  0.0484438901184
Iteration :  39   Loss :  0.04814235769
Iteration :  40   Loss :  0.047843255847
Iteration :  41   Loss :  0.0475465375373
Iteration :  42   Loss :  0.047252158911
Iteration :  43   Loss :  0.0469600789908
Iteration :  44   Loss :  0.0466702593823
Iteration :  45   Loss :  0.0463826640209
Iteration :  46   Loss :  0.0460972589484
Iteration :  47   Loss :  0.0458140121151
Iteration :  48   Loss :  0.0455328932052
Iteration :  49   Loss :  0.0452538734815
Iteration :  50   Loss :  0.0449769256464
Iteration :  51   Loss :  0.0447020237184
Iteration :  52   Loss :  0.0444291429213
Iteration :  53   Loss :  0.0441582595843
Iteration :  54   Loss :  0.0438893510525
Iteration :  55   Loss :  0.0436223956061
Iteration :  56   Loss :  0.0433573723868
Iteration :  57   Loss :  0.0430942613317
Iteration :  58   Loss :  0.0428330431129
Iteration :  59   Loss :  0.0425736990828
Iteration :  60   Loss :  0.0423162112237
Iteration :  61   Loss :  0.0420605621024
Iteration :  62   Loss :  0.0418067348279
Iteration :  63   Loss :  0.0415547130136
Iteration :  64   Loss :  0.0413044807414
Iteration :  65   Loss :  0.0410560225296
Iteration :  66   Loss :  0.0408093233032
Iteration :  67   Loss :  0.0405643683658
Iteration :  68   Loss :  0.0403211433747
Iteration :  69   Loss :  0.0400796343172
Iteration :  70   Loss :  0.0398398274888
Iteration :  71   Loss :  0.0396017094732
Iteration :  72   Loss :  0.0393652671237
Iteration :  73   Loss :  0.0391304875456
Iteration :  74   Loss :  0.0388973580808
Iteration :  75   Loss :  0.0386658662921
Iteration :  76   Loss :  0.0384359999503
Iteration :  77   Loss :  0.0382077470209
Iteration :  78   Loss :  0.0379810956523
Iteration :  79   Loss :  0.0377560341649
Iteration :  80   Loss :  0.0375325510409
Iteration :  81   Loss :  0.0373106349148
Iteration :  82   Loss :  0.0370902745647
Iteration :  83   Loss :  0.0368714589042
Iteration :  84   Loss :  0.0366541769749
Iteration :  85   Loss :  0.0364384179394
Iteration :  86   Loss :  0.0362241710753
Iteration :  87   Loss :  0.0360114257688
Iteration :  88   Loss :  0.0358001715097
Iteration :  89   Loss :  0.0355903978863
Iteration :  90   Loss :  0.0353820945808
Iteration :  91   Loss :  0.0351752513655
Iteration :  92   Loss :  0.0349698580985
Iteration :  93   Loss :  0.0347659047206
Iteration :  94   Loss :  0.0345633812522
Iteration :  95   Loss :  0.0343622777904
Iteration :  96   Loss :  0.0341625845064
Iteration :  97   Loss :  0.0339642916435
Iteration :  98   Loss :  0.0337673895148
Iteration :  99   Loss :  0.0335718685015
[-0.00071057  0.00047027 -0.00027911 ...,  0.0001973  -0.00013802
  0.00029985]
CROSS VALIDATION 12
Iteration :  0   Loss :  10.0299560515
Iteration :  1   Loss :  0.0628245863739
Iteration :  2   Loss :  0.0573655166049
Iteration :  3   Loss :  0.0561160264364
Iteration :  4   Loss :  0.0552719532969
Iteration :  5   Loss :  0.0546016216039
Iteration :  6   Loss :  0.0540270814478
Iteration :  7   Loss :  0.0535128768372
Iteration :  8   Loss :  0.0530400328433
Iteration :  9   Loss :  0.0525972805508
Iteration :  10   Loss :  0.0521774207271
Iteration :  11   Loss :  0.0517755975356
Iteration :  12   Loss :  0.051388394956
Iteration :  13   Loss :  0.0510133275379
Iteration :  14   Loss :  0.0506485362264
Iteration :  15   Loss :  0.0502925979441
Iteration :  16   Loss :  0.0499444016736
Iteration :  17   Loss :  0.0496030651436
Iteration :  18   Loss :  0.0492678772348
Iteration :  19   Loss :  0.0489382571967
Iteration :  20   Loss :  0.0486137251581
Iteration :  21   Loss :  0.0482938804041
Iteration :  22   Loss :  0.0479783851112
Iteration :  23   Loss :  0.0476669519881
Iteration :  24   Loss :  0.0473593347594
Iteration :  25   Loss :  0.0470553207504
Iteration :  26   Loss :  0.0467547250454
Iteration :  27   Loss :  0.0464573858403
Iteration :  28   Loss :  0.0461631607099
Iteration :  29   Loss :  0.0458719235863
Iteration :  30   Loss :  0.0455835622922
Iteration :  31   Loss :  0.0452979765134
Iteration :  32   Loss :  0.0450150761205
Iteration :  33   Loss :  0.0447347797715
Iteration :  34   Loss :  0.0444570137405
Iteration :  35   Loss :  0.0441817109309
Iteration :  36   Loss :  0.0439088100403
Iteration :  37   Loss :  0.0436382548486
Iteration :  38   Loss :  0.0433699936103
Iteration :  39   Loss :  0.0431039785317
Iteration :  40   Loss :  0.0428401653204
Iteration :  41   Loss :  0.0425785127951
Iteration :  42   Loss :  0.0423189825461
Iteration :  43   Loss :  0.0420615386394
Iteration :  44   Loss :  0.0418061473579
Iteration :  45   Loss :  0.0415527769731
Iteration :  46   Loss :  0.0413013975452
Iteration :  47   Loss :  0.0410519807454
Iteration :  48   Loss :  0.0408044996991
Iteration :  49   Loss :  0.0405589288461
Iteration :  50   Loss :  0.0403152438167
Iteration :  51   Loss :  0.0400734213201
Iteration :  52   Loss :  0.0398334390454
Iteration :  53   Loss :  0.0395952755725
Iteration :  54   Loss :  0.0393589102919
Iteration :  55   Loss :  0.0391243233326
Iteration :  56   Loss :  0.0388914954973
Iteration :  57   Loss :  0.0386604082034
Iteration :  58   Loss :  0.0384310434301
Iteration :  59   Loss :  0.0382033836704
Iteration :  60   Loss :  0.0379774118875
Iteration :  61   Loss :  0.037753111475
Iteration :  62   Loss :  0.0375304662217
Iteration :  63   Loss :  0.0373094602783
Iteration :  64   Loss :  0.0370900781284
Iteration :  65   Loss :  0.0368723045613
Iteration :  66   Loss :  0.0366561246476
Iteration :  67   Loss :  0.0364415237173
Iteration :  68   Loss :  0.0362284873394
Iteration :  69   Loss :  0.036017001304
Iteration :  70   Loss :  0.0358070516052
Iteration :  71   Loss :  0.035598624427
Iteration :  72   Loss :  0.0353917061292
Iteration :  73   Loss :  0.0351862832351
Iteration :  74   Loss :  0.0349823424215
Iteration :  75   Loss :  0.0347798705079
Iteration :  76   Loss :  0.0345788544483
Iteration :  77   Loss :  0.0343792813233
Iteration :  78   Loss :  0.0341811383335
Iteration :  79   Loss :  0.0339844127929
Iteration :  80   Loss :  0.0337890921241
Iteration :  81   Loss :  0.0335951638535
Iteration :  82   Loss :  0.0334026156074
Iteration :  83   Loss :  0.0332114351088
Iteration :  84   Loss :  0.0330216101743
Iteration :  85   Loss :  0.0328331287122
Iteration :  86   Loss :  0.0326459787204
Iteration :  87   Loss :  0.0324601482852
Iteration :  88   Loss :  0.0322756255802
Iteration :  89   Loss :  0.0320923988656
Iteration :  90   Loss :  0.0319104564882
Iteration :  91   Loss :  0.031729786881
Iteration :  92   Loss :  0.0315503785635
Iteration :  93   Loss :  0.0313722201424
Iteration :  94   Loss :  0.0311953003123
Iteration :  95   Loss :  0.0310196078562
Iteration :  96   Loss :  0.0308451316468
Iteration :  97   Loss :  0.030671860648
Iteration :  98   Loss :  0.0304997839154
Iteration :  99   Loss :  0.0303288905988
[ -1.37345008e-03   2.53638443e-05  -4.30910299e-04 ...,   6.33200650e-05
  -1.53597605e-04   1.72073530e-04]
CROSS VALIDATION 13
Iteration :  0   Loss :  7.99391818482
Iteration :  1   Loss :  0.0814325038111
Iteration :  2   Loss :  0.0772085009093
Iteration :  3   Loss :  0.0755072930162
Iteration :  4   Loss :  0.0743746067933
Iteration :  5   Loss :  0.0734859437201
Iteration :  6   Loss :  0.0727290578443
Iteration :  7   Loss :  0.0720531932805
Iteration :  8   Loss :  0.0714316083073
Iteration :  9   Loss :  0.0708487150484
Iteration :  10   Loss :  0.0702947444185
Iteration :  11   Loss :  0.0697632293552
Iteration :  12   Loss :  0.0692497006055
Iteration :  13   Loss :  0.0687509603373
Iteration :  14   Loss :  0.0682646537233
Iteration :  15   Loss :  0.0677890041861
Iteration :  16   Loss :  0.067322643301
Iteration :  17   Loss :  0.0668644978643
Iteration :  18   Loss :  0.0664137127697
Iteration :  19   Loss :  0.0659695970386
Iteration :  20   Loss :  0.0655315852333
Iteration :  21   Loss :  0.065099209343
Iteration :  22   Loss :  0.0646720779518
Iteration :  23   Loss :  0.0642498605677
Iteration :  24   Loss :  0.0638322756725
Iteration :  25   Loss :  0.0634190814961
Iteration :  26   Loss :  0.0630100688136
Iteration :  27   Loss :  0.0626050552631
Iteration :  28   Loss :  0.0622038808218
Iteration :  29   Loss :  0.0618064041712
Iteration :  30   Loss :  0.0614124997544
Iteration :  31   Loss :  0.0610220553757
Iteration :  32   Loss :  0.0606349702279
Iteration :  33   Loss :  0.0602511532619
Iteration :  34   Loss :  0.0598705218301
Iteration :  35   Loss :  0.0594930005518
Iteration :  36   Loss :  0.0591185203589
Iteration :  37   Loss :  0.0587470176893
Iteration :  38   Loss :  0.0583784338018
Iteration :  39   Loss :  0.0580127141921
Iteration :  40   Loss :  0.0576498080922
Iteration :  41   Loss :  0.0572896680405
Iteration :  42   Loss :  0.05693224951
Iteration :  43   Loss :  0.0565775105871
Iteration :  44   Loss :  0.0562254116919
Iteration :  45   Loss :  0.0558759153346
Iteration :  46   Loss :  0.0555289859026
Iteration :  47   Loss :  0.0551845894739
Iteration :  48   Loss :  0.0548426936528
Iteration :  49   Loss :  0.0545032674254
Iteration :  50   Loss :  0.0541662810319
Iteration :  51   Loss :  0.0538317058534
Iteration :  52   Loss :  0.053499514312
Iteration :  53   Loss :  0.0531696797814
Iteration :  54   Loss :  0.0528421765076
Iteration :  55   Loss :  0.0525169795383
Iteration :  56   Loss :  0.0521940646597
Iteration :  57   Loss :  0.0518734083397
Iteration :  58   Loss :  0.051554987678
Iteration :  59   Loss :  0.0512387803598
Iteration :  60   Loss :  0.0509247646157
Iteration :  61   Loss :  0.0506129191848
Iteration :  62   Loss :  0.0503032232816
Iteration :  63   Loss :  0.0499956565667
Iteration :  64   Loss :  0.0496901991196
Iteration :  65   Loss :  0.0493868314146
Iteration :  66   Loss :  0.049085534299
Iteration :  67   Loss :  0.0487862889734
Iteration :  68   Loss :  0.0484890769739
Iteration :  69   Loss :  0.0481938801555
Iteration :  70   Loss :  0.0479006806778
Iteration :  71   Loss :  0.0476094609915
Iteration :  72   Loss :  0.0473202038263
Iteration :  73   Loss :  0.0470328921798
Iteration :  74   Loss :  0.0467475093074
Iteration :  75   Loss :  0.0464640387135
Iteration :  76   Loss :  0.0461824641426
Iteration :  77   Loss :  0.0459027695718
Iteration :  78   Loss :  0.0456249392039
Iteration :  79   Loss :  0.0453489574607
Iteration :  80   Loss :  0.0450748089772
Iteration :  81   Loss :  0.0448024785961
Iteration :  82   Loss :  0.0445319513623
Iteration :  83   Loss :  0.0442632125188
Iteration :  84   Loss :  0.0439962475016
Iteration :  85   Loss :  0.0437310419362
Iteration :  86   Loss :  0.0434675816331
Iteration :  87   Loss :  0.0432058525847
Iteration :  88   Loss :  0.0429458409616
Iteration :  89   Loss :  0.0426875331093
Iteration :  90   Loss :  0.0424309155452
Iteration :  91   Loss :  0.0421759749556
Iteration :  92   Loss :  0.0419226981931
Iteration :  93   Loss :  0.0416710722733
Iteration :  94   Loss :  0.0414210843731
Iteration :  95   Loss :  0.0411727218274
Iteration :  96   Loss :  0.040925972127
Iteration :  97   Loss :  0.0406808229162
Iteration :  98   Loss :  0.0404372619908
Iteration :  99   Loss :  0.0401952772954
[ -1.06026328e-03  -1.35920977e-04  -5.23483788e-04 ...,   7.12942901e-05
   4.73845904e-05   2.76552746e-04]
CROSS VALIDATION 14
Iteration :  0   Loss :  7.87127765376
Iteration :  1   Loss :  0.0753811098737
Iteration :  2   Loss :  0.0622268111927
Iteration :  3   Loss :  0.0614330114812
Iteration :  4   Loss :  0.060775497055
Iteration :  5   Loss :  0.060196349508
Iteration :  6   Loss :  0.0596678501316
Iteration :  7   Loss :  0.0591747060331
Iteration :  8   Loss :  0.0587076510758
Iteration :  9   Loss :  0.0582606823688
Iteration :  10   Loss :  0.057829710007
Iteration :  11   Loss :  0.0574118351687
Iteration :  12   Loss :  0.0570049366335
Iteration :  13   Loss :  0.0566074206787
Iteration :  14   Loss :  0.0562180629396
Iteration :  15   Loss :  0.0558359046722
Iteration :  16   Loss :  0.0554601825552
Iteration :  17   Loss :  0.0550902799085
Iteration :  18   Loss :  0.0547256920032
Iteration :  19   Loss :  0.0543660008905
Iteration :  20   Loss :  0.0540108568094
Iteration :  21   Loss :  0.053659964233
Iteration :  22   Loss :  0.053313071248
Iteration :  23   Loss :  0.0529699613655
Iteration :  24   Loss :  0.0526304471329
Iteration :  25   Loss :  0.0522943650984
Iteration :  26   Loss :  0.0519615718022
Iteration :  27   Loss :  0.051631940557
Iteration :  28   Loss :  0.0513053588408
Iteration :  29   Loss :  0.050981726169
Iteration :  30   Loss :  0.0506609523463
Iteration :  31   Loss :  0.0503429560204
Iteration :  32   Loss :  0.0500276634773
Iteration :  33   Loss :  0.0497150076338
Iteration :  34   Loss :  0.0494049271884
Iteration :  35   Loss :  0.0490973659034
Iteration :  36   Loss :  0.0487922719936
Iteration :  37   Loss :  0.0484895976048
Iteration :  38   Loss :  0.0481892983659
Iteration :  39   Loss :  0.0478913330026
Iteration :  40   Loss :  0.0475956630037
Iteration :  41   Loss :  0.0473022523306
Iteration :  42   Loss :  0.0470110671642
Iteration :  43   Loss :  0.0467220756836
Iteration :  44   Loss :  0.0464352478712
Iteration :  45   Loss :  0.0461505553412
Iteration :  46   Loss :  0.0458679711882
Iteration :  47   Loss :  0.0455874698521
Iteration :  48   Loss :  0.0453090269989
Iteration :  49   Loss :  0.0450326194131
Iteration :  50   Loss :  0.0447582249026
Iteration :  51   Loss :  0.0444858222121
Iteration :  52   Loss :  0.0442153909458
Iteration :  53   Loss :  0.0439469114976
Iteration :  54   Loss :  0.0436803649876
Iteration :  55   Loss :  0.0434157332044
Iteration :  56   Loss :  0.043152998553
Iteration :  57   Loss :  0.0428921440067
Iteration :  58   Loss :  0.0426331530638
Iteration :  59   Loss :  0.042376009707
Iteration :  60   Loss :  0.0421206983671
Iteration :  61   Loss :  0.0418672038889
Iteration :  62   Loss :  0.0416155115
Iteration :  63   Loss :  0.041365606782
Iteration :  64   Loss :  0.0411174756438
Iteration :  65   Loss :  0.040871104297
Iteration :  66   Loss :  0.0406264792331
Iteration :  67   Loss :  0.0403835872022
Iteration :  68   Loss :  0.040142415193
Iteration :  69   Loss :  0.0399029504151
Iteration :  70   Loss :  0.0396651802816
Iteration :  71   Loss :  0.0394290923934
Iteration :  72   Loss :  0.0391946745245
Iteration :  73   Loss :  0.0389619146083
Iteration :  74   Loss :  0.0387308007253
Iteration :  75   Loss :  0.0385013210911
Iteration :  76   Loss :  0.0382734640456
Iteration :  77   Loss :  0.0380472180431
Iteration :  78   Loss :  0.037822571643
Iteration :  79   Loss :  0.0375995135015
Iteration :  80   Loss :  0.0373780323637
Iteration :  81   Loss :  0.0371581170566
Iteration :  82   Loss :  0.0369397564827
Iteration :  83   Loss :  0.0367229396143
Iteration :  84   Loss :  0.0365076554882
Iteration :  85   Loss :  0.0362938932012
Iteration :  86   Loss :  0.036081641906
Iteration :  87   Loss :  0.0358708908076
Iteration :  88   Loss :  0.0356616291604
Iteration :  89   Loss :  0.0354538462659
Iteration :  90   Loss :  0.03524753147
Iteration :  91   Loss :  0.0350426741621
Iteration :  92   Loss :  0.0348392637735
Iteration :  93   Loss :  0.0346372897764
Iteration :  94   Loss :  0.0344367416842
Iteration :  95   Loss :  0.0342376090507
Iteration :  96   Loss :  0.0340398814707
Iteration :  97   Loss :  0.0338435485806
Iteration :  98   Loss :  0.0336486000592
Iteration :  99   Loss :  0.0334550256288
[-0.00068874  0.00048161 -0.0002709  ...,  0.00018159 -0.00015351
  0.00029687]
CROSS VALIDATION 15
Iteration :  0   Loss :  7.91952724579
Iteration :  1   Loss :  0.0772355149449
Iteration :  2   Loss :  0.0636731434058
Iteration :  3   Loss :  0.0626199620782
Iteration :  4   Loss :  0.0617904537573
Iteration :  5   Loss :  0.0610884467632
Iteration :  6   Loss :  0.0604676100911
Iteration :  7   Loss :  0.0599024425894
Iteration :  8   Loss :  0.059377588758
Iteration :  9   Loss :  0.0588831790507
Iteration :  10   Loss :  0.0584125462349
Iteration :  11   Loss :  0.057961005504
Iteration :  12   Loss :  0.0575251579401
Iteration :  13   Loss :  0.0571024710631
Iteration :  14   Loss :  0.0566910149879
Iteration :  15   Loss :  0.0562892903079
Iteration :  16   Loss :  0.0558961122921
Iteration :  17   Loss :  0.0555105308724
Iteration :  18   Loss :  0.0551317740673
Iteration :  19   Loss :  0.0547592071554
Iteration :  20   Loss :  0.0543923026801
Iteration :  21   Loss :  0.054030618056
Iteration :  22   Loss :  0.0536737786072
Iteration :  23   Loss :  0.0533214645512
Iteration :  24   Loss :  0.0529734008933
Iteration :  25   Loss :  0.0526293494936
Iteration :  26   Loss :  0.0522891027799
Iteration :  27   Loss :  0.0519524787173
Iteration :  28   Loss :  0.0516193167513
Iteration :  29   Loss :  0.0512894745092
Iteration :  30   Loss :  0.0509628250991
Iteration :  31   Loss :  0.0506392548835
Iteration :  32   Loss :  0.0503186616322
Iteration :  33   Loss :  0.0500009529806
Iteration :  34   Loss :  0.0496860451363
Iteration :  35   Loss :  0.049373861788
Iteration :  36   Loss :  0.0490643331805
Iteration :  37   Loss :  0.048757395327
Iteration :  38   Loss :  0.0484529893348
Iteration :  39   Loss :  0.0481510608264
Iteration :  40   Loss :  0.0478515594403
Iteration :  41   Loss :  0.0475544383977
Iteration :  42   Loss :  0.0472596541274
Iteration :  43   Loss :  0.0469671659372
Iteration :  44   Loss :  0.046676935727
Iteration :  45   Loss :  0.0463889277365
Iteration :  46   Loss :  0.0461031083236
Iteration :  47   Loss :  0.0458194457674
Iteration :  48   Loss :  0.0455379100944
Iteration :  49   Loss :  0.0452584729238
Iteration :  50   Loss :  0.0449811073294
Iteration :  51   Loss :  0.0447057877159
Iteration :  52   Loss :  0.0444324897089
Iteration :  53   Loss :  0.0441611900545
Iteration :  54   Loss :  0.0438918665303
Iteration :  55   Loss :  0.0436244978638
Iteration :  56   Loss :  0.0433590636592
Iteration :  57   Loss :  0.0430955443304
Iteration :  58   Loss :  0.0428339210403
Iteration :  59   Loss :  0.0425741756453
Iteration :  60   Loss :  0.0423162906444
Iteration :  61   Loss :  0.0420602491328
Iteration :  62   Loss :  0.0418060347588
Iteration :  63   Loss :  0.0415536316845
Iteration :  64   Loss :  0.0413030245498
Iteration :  65   Loss :  0.0410541984378
Iteration :  66   Loss :  0.0408071388448
Iteration :  67   Loss :  0.0405618316503
Iteration :  68   Loss :  0.0403182630909
Iteration :  69   Loss :  0.0400764197347
Iteration :  70   Loss :  0.0398362884583
Iteration :  71   Loss :  0.0395978564249
Iteration :  72   Loss :  0.0393611110639
Iteration :  73   Loss :  0.0391260400518
Iteration :  74   Loss :  0.0388926312945
Iteration :  75   Loss :  0.0386608729105
Iteration :  76   Loss :  0.0384307532152
Iteration :  77   Loss :  0.0382022607064
Iteration :  78   Loss :  0.0379753840503
Iteration :  79   Loss :  0.0377501120687
Iteration :  80   Loss :  0.0375264337269
Iteration :  81   Loss :  0.0373043381227
Iteration :  82   Loss :  0.0370838144751
Iteration :  83   Loss :  0.0368648521152
Iteration :  84   Loss :  0.0366474404766
Iteration :  85   Loss :  0.0364315690868
Iteration :  86   Loss :  0.0362172275594
Iteration :  87   Loss :  0.0360044055866
Iteration :  88   Loss :  0.0357930929328
Iteration :  89   Loss :  0.0355832794278
Iteration :  90   Loss :  0.0353749549615
Iteration :  91   Loss :  0.0351681094785
Iteration :  92   Loss :  0.0349627329738
Iteration :  93   Loss :  0.0347588154882
Iteration :  94   Loss :  0.0345563471048
Iteration :  95   Loss :  0.0343553179457
Iteration :  96   Loss :  0.0341557181692
Iteration :  97   Loss :  0.0339575379675
Iteration :  98   Loss :  0.0337607675645
Iteration :  99   Loss :  0.0335653972146
[-0.00071213  0.00047094 -0.00028203 ...,  0.0001992  -0.00013947
  0.00029935]
CROSS VALIDATION 16
Iteration :  0   Loss :  7.89629288223
Iteration :  1   Loss :  0.0629258391266
Iteration :  2   Loss :  0.0619872779553
Iteration :  3   Loss :  0.0612547414719
Iteration :  4   Loss :  0.0606302389484
Iteration :  5   Loss :  0.0600716313668
Iteration :  6   Loss :  0.0595571575615
Iteration :  7   Loss :  0.0590742298414
Iteration :  8   Loss :  0.0586149743976
Iteration :  9   Loss :  0.0581741737564
Iteration :  10   Loss :  0.0577482133659
Iteration :  11   Loss :  0.0573344986115
Iteration :  12   Loss :  0.0569311119868
Iteration :  13   Loss :  0.0565366014103
Iteration :  14   Loss :  0.0561498441627
Iteration :  15   Loss :  0.0557699564351
Iteration :  16   Loss :  0.0553962314475
Iteration :  17   Loss :  0.0550280960528
Iteration :  18   Loss :  0.0546650796396
Iteration :  19   Loss :  0.0543067914199
Iteration :  20   Loss :  0.0539529035592
Iteration :  21   Loss :  0.0536031384547
Iteration :  22   Loss :  0.0532572590112
Iteration :  23   Loss :  0.0529150611151
Iteration :  24   Loss :  0.0525763677457
Iteration :  25   Loss :  0.0522410243187
Iteration :  26   Loss :  0.0519088949708
Iteration :  27   Loss :  0.0515798595677
Iteration :  28   Loss :  0.0512538112775
Iteration :  29   Loss :  0.0509306545855
Iteration :  30   Loss :  0.0506103036612
Iteration :  31   Loss :  0.0502926810046
Iteration :  32   Loss :  0.0499777163183
Iteration :  33   Loss :  0.0496653455615
Iteration :  34   Loss :  0.0493555101529
Iteration :  35   Loss :  0.0490481562959
Iteration :  36   Loss :  0.0487432344036
Iteration :  37   Loss :  0.0484406986075
Iteration :  38   Loss :  0.0481405063357
Iteration :  39   Loss :  0.047842617949
Iteration :  40   Loss :  0.0475469964258
Iteration :  41   Loss :  0.0472536070882
Iteration :  42   Loss :  0.0469624173637
Iteration :  43   Loss :  0.0466733965758
Iteration :  44   Loss :  0.0463865157604
Iteration :  45   Loss :  0.0461017475041
Iteration :  46   Loss :  0.0458190658013
Iteration :  47   Loss :  0.045538445927
Iteration :  48   Loss :  0.0452598643244
Iteration :  49   Loss :  0.0449832985038
Iteration :  50   Loss :  0.0447087269525
Iteration :  51   Loss :  0.0444361290544
Iteration :  52   Loss :  0.0441654850166
Iteration :  53   Loss :  0.0438967758045
Iteration :  54   Loss :  0.0436299830815
Iteration :  55   Loss :  0.0433650891561
Iteration :  56   Loss :  0.0431020769324
Iteration :  57   Loss :  0.0428409298655
Iteration :  58   Loss :  0.0425816319209
Iteration :  59   Loss :  0.0423241675371
Iteration :  60   Loss :  0.0420685215916
Iteration :  61   Loss :  0.0418146793692
Iteration :  62   Loss :  0.0415626265331
Iteration :  63   Loss :  0.0413123490983
Iteration :  64   Loss :  0.0410638334069
Iteration :  65   Loss :  0.0408170661051
Iteration :  66   Loss :  0.0405720341221
Iteration :  67   Loss :  0.0403287246506
Iteration :  68   Loss :  0.0400871251282
Iteration :  69   Loss :  0.0398472232209
Iteration :  70   Loss :  0.0396090068067
Iteration :  71   Loss :  0.0393724639616
Iteration :  72   Loss :  0.0391375829451
Iteration :  73   Loss :  0.0389043521878
Iteration :  74   Loss :  0.0386727602796
Iteration :  75   Loss :  0.0384427959581
Iteration :  76   Loss :  0.0382144480987
Iteration :  77   Loss :  0.0379877057045
Iteration :  78   Loss :  0.0377625578973
Iteration :  79   Loss :  0.0375389939096
Iteration :  80   Loss :  0.037317003076
Iteration :  81   Loss :  0.0370965748267
Iteration :  82   Loss :  0.03687769868
Iteration :  83   Loss :  0.0366603642364
Iteration :  84   Loss :  0.0364445611729
Iteration :  85   Loss :  0.036230279237
Iteration :  86   Loss :  0.0360175082423
Iteration :  87   Loss :  0.0358062380639
Iteration :  88   Loss :  0.0355964586336
Iteration :  89   Loss :  0.0353881599368
Iteration :  90   Loss :  0.0351813320087
Iteration :  91   Loss :  0.0349759649309
Iteration :  92   Loss :  0.0347720488291
Iteration :  93   Loss :  0.03456957387
Iteration :  94   Loss :  0.0343685302595
Iteration :  95   Loss :  0.0341689082403
Iteration :  96   Loss :  0.0339706980908
Iteration :  97   Loss :  0.0337738901231
Iteration :  98   Loss :  0.0335784746818
Iteration :  99   Loss :  0.0333844421436
[-0.00069647  0.00046996 -0.00027662 ...,  0.00019569 -0.00014058
  0.0003042 ]
CROSS VALIDATION 17
Iteration :  0   Loss :  7.88731373611
Iteration :  1   Loss :  0.0768516723967
Iteration :  2   Loss :  0.0636809363684
Iteration :  3   Loss :  0.0626076876445
Iteration :  4   Loss :  0.0617684530752
Iteration :  5   Loss :  0.0610610897219
Iteration :  6   Loss :  0.0604370879013
Iteration :  7   Loss :  0.05986998293
Iteration :  8   Loss :  0.0593439357952
Iteration :  9   Loss :  0.0588488113972
Iteration :  10   Loss :  0.0583777863902
Iteration :  11   Loss :  0.0579260791775
Iteration :  12   Loss :  0.0574902282127
Iteration :  13   Loss :  0.0570676590478
Iteration :  14   Loss :  0.0566564128439
Iteration :  15   Loss :  0.0562549697241
Iteration :  16   Loss :  0.0558621301817
Iteration :  17   Loss :  0.0554769332934
Iteration :  18   Loss :  0.0550985989812
Iteration :  19   Loss :  0.0547264864074
Iteration :  20   Loss :  0.0543600634442
Iteration :  21   Loss :  0.0539988839054
Iteration :  22   Loss :  0.0536425703186
Iteration :  23   Loss :  0.0532908007159
Iteration :  24   Loss :  0.0529432983857
Iteration :  25   Loss :  0.0525998238351
Iteration :  26   Loss :  0.0522601684218
Iteration :  27   Loss :  0.0519241492639
Iteration :  28   Loss :  0.0515916051351
Iteration :  29   Loss :  0.0512623931299
Iteration :  30   Loss :  0.0509363859342
Iteration :  31   Loss :  0.0506134695749
Iteration :  32   Loss :  0.0502935415554
Iteration :  33   Loss :  0.0499765092983
Iteration :  34   Loss :  0.0496622888405
Iteration :  35   Loss :  0.0493508037316
Iteration :  36   Loss :  0.0490419841009
Iteration :  37   Loss :  0.0487357658626
Iteration :  38   Loss :  0.0484320900359
Iteration :  39   Loss :  0.0481309021612
Iteration :  40   Loss :  0.047832151796
Iteration :  41   Loss :  0.0475357920792
Iteration :  42   Loss :  0.0472417793518
Iteration :  43   Loss :  0.0469500728261
Iteration :  44   Loss :  0.0466606342964
Iteration :  45   Loss :  0.0463734278848
Iteration :  46   Loss :  0.0460884198172
Iteration :  47   Loss :  0.0458055782252
Iteration :  48   Loss :  0.0455248729711
Iteration :  49   Loss :  0.0452462754915
Iteration :  50   Loss :  0.0449697586582
Iteration :  51   Loss :  0.0446952966542
Iteration :  52   Loss :  0.044422864862
Iteration :  53   Loss :  0.0441524397634
Iteration :  54   Loss :  0.0438839988492
Iteration :  55   Loss :  0.0436175205379
Iteration :  56   Loss :  0.0433529841017
Iteration :  57   Loss :  0.0430903695994
Iteration :  58   Loss :  0.0428296578159
Iteration :  59   Loss :  0.0425708302063
Iteration :  60   Loss :  0.0423138688452
Iteration :  61   Loss :  0.0420587563808
Iteration :  62   Loss :  0.041805475992
Iteration :  63   Loss :  0.0415540113493
Iteration :  64   Loss :  0.0413043465794
Iteration :  65   Loss :  0.0410564662317
Iteration :  66   Loss :  0.0408103552482
Iteration :  67   Loss :  0.0405659989354
Iteration :  68   Loss :  0.040323382938
Iteration :  69   Loss :  0.0400824932152
Iteration :  70   Loss :  0.0398433160185
Iteration :  71   Loss :  0.0396058378711
Iteration :  72   Loss :  0.0393700455487
Iteration :  73   Loss :  0.0391359260623
Iteration :  74   Loss :  0.0389034666412
Iteration :  75   Loss :  0.0386726547186
Iteration :  76   Loss :  0.0384434779171
Iteration :  77   Loss :  0.0382159240362
Iteration :  78   Loss :  0.0379899810398
Iteration :  79   Loss :  0.0377656370456
Iteration :  80   Loss :  0.0375428803151
Iteration :  81   Loss :  0.0373216992437
Iteration :  82   Loss :  0.0371020823525
Iteration :  83   Loss :  0.0368840182804
Iteration :  84   Loss :  0.0366674957772
Iteration :  85   Loss :  0.0364525036967
Iteration :  86   Loss :  0.0362390309908
Iteration :  87   Loss :  0.0360270667047
Iteration :  88   Loss :  0.0358165999717
Iteration :  89   Loss :  0.0356076200093
Iteration :  90   Loss :  0.035400116115
Iteration :  91   Loss :  0.0351940776635
Iteration :  92   Loss :  0.0349894941039
Iteration :  93   Loss :  0.0347863549568
Iteration :  94   Loss :  0.0345846498128
Iteration :  95   Loss :  0.0343843683304
Iteration :  96   Loss :  0.0341855002353
Iteration :  97   Loss :  0.0339880353186
Iteration :  98   Loss :  0.0337919634368
Iteration :  99   Loss :  0.0335972745109
[-0.00071182  0.00047016 -0.00028054 ...,  0.0001971  -0.0001384
  0.00030006]
CROSS VALIDATION 18
Iteration :  0   Loss :  7.00193745681
Iteration :  1   Loss :  0.067061277934
Iteration :  2   Loss :  0.0655919698171
Iteration :  3   Loss :  0.0644948669955
Iteration :  4   Loss :  0.0635992947961
Iteration :  5   Loss :  0.062829480048
Iteration :  6   Loss :  0.0621450150719
Iteration :  7   Loss :  0.061521894383
Iteration :  8   Loss :  0.0609447799833
Iteration :  9   Loss :  0.0604033371357
Iteration :  10   Loss :  0.0598903117724
Iteration :  11   Loss :  0.0594004432113
Iteration :  12   Loss :  0.0589298127008
Iteration :  13   Loss :  0.0584754346668
Iteration :  14   Loss :  0.0580349903499
Iteration :  15   Loss :  0.0576066486365
Iteration :  16   Loss :  0.0571889422392
Iteration :  17   Loss :  0.0567806801061
Iteration :  18   Loss :  0.0563808841793
Iteration :  19   Loss :  0.0559887429047
Iteration :  20   Loss :  0.0556035765017
Iteration :  21   Loss :  0.0552248106433
Iteration :  22   Loss :  0.054851956249
Iteration :  23   Loss :  0.0544845937863
Iteration :  24   Loss :  0.0541223609436
Iteration :  25   Loss :  0.0537649428551
Iteration :  26   Loss :  0.0534120642781
Iteration :  27   Loss :  0.0530634832799
Iteration :  28   Loss :  0.052718986105
Iteration :  29   Loss :  0.0523783829702
Iteration :  30   Loss :  0.0520415045968
Iteration :  31   Loss :  0.0517081993343
Iteration :  32   Loss :  0.0513783307581
Iteration :  33   Loss :  0.0510517756548
Iteration :  34   Loss :  0.0507284223213
Iteration :  35   Loss :  0.0504081691241
Iteration :  36   Loss :  0.0500909232708
Iteration :  37   Loss :  0.0497765997609
Iteration :  38   Loss :  0.0494651204837
Iteration :  39   Loss :  0.0491564134404
Iteration :  40   Loss :  0.0488504120714
Iteration :  41   Loss :  0.0485470546722
Iteration :  42   Loss :  0.0482462838833
Iteration :  43   Loss :  0.0479480462458
Iteration :  44   Loss :  0.0476522918107
Iteration :  45   Loss :  0.0473589737961
Iteration :  46   Loss :  0.047068048284
Iteration :  47   Loss :  0.0467794739534
Iteration :  48   Loss :  0.0464932118429
Iteration :  49   Loss :  0.0462092251404
Iteration :  50   Loss :  0.0459274789955
Iteration :  51   Loss :  0.0456479403521
Iteration :  52   Loss :  0.0453705777991
Iteration :  53   Loss :  0.0450953614364
Iteration :  54   Loss :  0.0448222627544
Iteration :  55   Loss :  0.0445512545262
Iteration :  56   Loss :  0.0442823107101
Iteration :  57   Loss :  0.0440154063614
Iteration :  58   Loss :  0.0437505175535
Iteration :  59   Loss :  0.0434876213057
Iteration :  60   Loss :  0.0432266955178
Iteration :  61   Loss :  0.0429677189112
Iteration :  62   Loss :  0.042710670975
Iteration :  63   Loss :  0.0424555319164
Iteration :  64   Loss :  0.0422022826167
Iteration :  65   Loss :  0.0419509045896
Iteration :  66   Loss :  0.041701379944
Iteration :  67   Loss :  0.0414536913496
Iteration :  68   Loss :  0.0412078220051
Iteration :  69   Loss :  0.040963755609
Iteration :  70   Loss :  0.0407214763329
Iteration :  71   Loss :  0.0404809687961
Iteration :  72   Loss :  0.040242218043
Iteration :  73   Loss :  0.0400052095212
Iteration :  74   Loss :  0.0397699290614
Iteration :  75   Loss :  0.0395363628588
Iteration :  76   Loss :  0.0393044974554
Iteration :  77   Loss :  0.0390743197231
Iteration :  78   Loss :  0.0388458168485
Iteration :  79   Loss :  0.0386189763174
Iteration :  80   Loss :  0.0383937859011
Iteration :  81   Loss :  0.0381702336425
Iteration :  82   Loss :  0.0379483078433
Iteration :  83   Loss :  0.0377279970516
Iteration :  84   Loss :  0.0375092900496
Iteration :  85   Loss :  0.0372921758421
Iteration :  86   Loss :  0.0370766436451
Iteration :  87   Loss :  0.0368626828751
Iteration :  88   Loss :  0.0366502831378
Iteration :  89   Loss :  0.0364394342179
Iteration :  90   Loss :  0.0362301260689
Iteration :  91   Loss :  0.0360223488029
Iteration :  92   Loss :  0.0358160926803
Iteration :  93   Loss :  0.0356113481008
Iteration :  94   Loss :  0.0354081055931
Iteration :  95   Loss :  0.0352063558057
Iteration :  96   Loss :  0.0350060894977
Iteration :  97   Loss :  0.0348072975294
Iteration :  98   Loss :  0.0346099708533
Iteration :  99   Loss :  0.0344141005054
[-0.00065124 -0.00017791 -0.0011938  ...,  0.00070459 -0.00012505
  0.00025022]
CROSS VALIDATION 19
Iteration :  0   Loss :  7.72063487598
Iteration :  1   Loss :  0.0758672266969
Iteration :  2   Loss :  0.0618752289918
Iteration :  3   Loss :  0.0608536410177
Iteration :  4   Loss :  0.0600412751389
Iteration :  5   Loss :  0.0593515901349
Iteration :  6   Loss :  0.0587414046957
Iteration :  7   Loss :  0.058186384727
Iteration :  8   Loss :  0.0576716292081
Iteration :  9   Loss :  0.0571874410445
Iteration :  10   Loss :  0.0567272077371
Iteration :  11   Loss :  0.0562862493898
Iteration :  12   Loss :  0.0558611517153
Iteration :  13   Loss :  0.0554493597972
Iteration :  14   Loss :  0.0550489201596
Iteration :  15   Loss :  0.0546583111957
Iteration :  16   Loss :  0.0542763283305
Iteration :  17   Loss :  0.0539020042385
Iteration :  18   Loss :  0.0535345521656
Iteration :  19   Loss :  0.053173324867
Iteration :  20   Loss :  0.0528177843357
Iteration :  21   Loss :  0.052467479136
Iteration :  22   Loss :  0.0521220271926
Iteration :  23   Loss :  0.0517811025546
Iteration :  24   Loss :  0.0514444250979
Iteration :  25   Loss :  0.0511117524308
Iteration :  26   Loss :  0.0507828734667
Iteration :  27   Loss :  0.0504576032773
Iteration :  28   Loss :  0.0501357789355
Iteration :  29   Loss :  0.0498172561327
Iteration :  30   Loss :  0.0495019064056
Iteration :  31   Loss :  0.0491896148486
Iteration :  32   Loss :  0.0488802782143
Iteration :  33   Loss :  0.0485738033272
Iteration :  34   Loss :  0.048270105751
Iteration :  35   Loss :  0.0479691086646
Iteration :  36   Loss :  0.0476707419074
Iteration :  37   Loss :  0.0473749411669
Iteration :  38   Loss :  0.0470816472818
Iteration :  39   Loss :  0.0467908056444
Iteration :  40   Loss :  0.0465023656834
Iteration :  41   Loss :  0.0462162804169
Iteration :  42   Loss :  0.0459325060631
Iteration :  43   Loss :  0.0456510017004
Iteration :  44   Loss :  0.0453717289708
Iteration :  45   Loss :  0.0450946518181
Iteration :  46   Loss :  0.0448197362584
Iteration :  47   Loss :  0.0445469501769
Iteration :  48   Loss :  0.0442762631477
Iteration :  49   Loss :  0.0440076462741
Iteration :  50   Loss :  0.0437410720466
Iteration :  51   Loss :  0.0434765142155
Iteration :  52   Loss :  0.0432139476778
Iteration :  53   Loss :  0.0429533483753
Iteration :  54   Loss :  0.0426946932035
Iteration :  55   Loss :  0.0424379599296
Iteration :  56   Loss :  0.0421831271184
Iteration :  57   Loss :  0.0419301740659
Iteration :  58   Loss :  0.041679080739
Iteration :  59   Loss :  0.0414298277216
Iteration :  60   Loss :  0.041182396165
Iteration :  61   Loss :  0.0409367677436
Iteration :  62   Loss :  0.0406929246148
Iteration :  63   Loss :  0.0404508493826
Iteration :  64   Loss :  0.040210525064
Iteration :  65   Loss :  0.03997193506
Iteration :  66   Loss :  0.0397350631276
Iteration :  67   Loss :  0.039499893356
Iteration :  68   Loss :  0.039266410144
Iteration :  69   Loss :  0.0390345981801
Iteration :  70   Loss :  0.0388044424243
Iteration :  71   Loss :  0.0385759280919
Iteration :  72   Loss :  0.0383490406386
Iteration :  73   Loss :  0.0381237657473
Iteration :  74   Loss :  0.0379000893164
Iteration :  75   Loss :  0.0376779974488
Iteration :  76   Loss :  0.0374574764427
Iteration :  77   Loss :  0.0372385127828
Iteration :  78   Loss :  0.0370210931328
Iteration :  79   Loss :  0.0368052043287
Iteration :  80   Loss :  0.0365908333726
Iteration :  81   Loss :  0.036377967428
Iteration :  82   Loss :  0.0361665938142
Iteration :  83   Loss :  0.035956700003
Iteration :  84   Loss :  0.0357482736147
Iteration :  85   Loss :  0.0355413024149
Iteration :  86   Loss :  0.035335774312
Iteration :  87   Loss :  0.0351316773545
Iteration :  88   Loss :  0.0349289997289
Iteration :  89   Loss :  0.034727729758
Iteration :  90   Loss :  0.0345278558991
Iteration :  91   Loss :  0.0343293667427
Iteration :  92   Loss :  0.0341322510112
Iteration :  93   Loss :  0.0339364975576
Iteration :  94   Loss :  0.0337420953647
Iteration :  95   Loss :  0.0335490335439
Iteration :  96   Loss :  0.0333573013346
Iteration :  97   Loss :  0.0331668881032
Iteration :  98   Loss :  0.0329777833425
Iteration :  99   Loss :  0.0327899766707
[-0.00084464  0.00047144 -0.0003545  ...,  0.00017396 -0.00015344
  0.0003167 ]
Accuracy (Logistic Loss):	0.95
Hinge Loss
CROSS VALIDATION 0
Iteration :  0   Loss :  24.7320428826
Iteration :  1   Loss :  0.0865175179658
Iteration :  2   Loss :  0.0859689962438
Iteration :  3   Loss :  0.0854239521537
Iteration :  4   Loss :  0.0848823636473
Iteration :  5   Loss :  0.0843442088161
Iteration :  6   Loss :  0.0838094658907
Iteration :  7   Loss :  0.0832781132394
Iteration :  8   Loss :  0.0827501293681
Iteration :  9   Loss :  0.0822254929184
Iteration :  10   Loss :  0.0817041826679
Iteration :  11   Loss :  0.0811861775283
Iteration :  12   Loss :  0.0806714565452
Iteration :  13   Loss :  0.0801599988971
Iteration :  14   Loss :  0.0796517838944
Iteration :  15   Loss :  0.0791467909786
Iteration :  16   Loss :  0.0786449997218
Iteration :  17   Loss :  0.0781463898254
Iteration :  18   Loss :  0.0776509411194
Iteration :  19   Loss :  0.0771586335621
Iteration :  20   Loss :  0.0766694472384
Iteration :  21   Loss :  0.0761833623597
Iteration :  22   Loss :  0.0757003592629
Iteration :  23   Loss :  0.0752204184094
Iteration :  24   Loss :  0.0747435203845
Iteration :  25   Loss :  0.0742696458967
Iteration :  26   Loss :  0.0737987757768
Iteration :  27   Loss :  0.073330890977
Iteration :  28   Loss :  0.0728659725705
Iteration :  29   Loss :  0.0724040017501
Iteration :  30   Loss :  0.0719449598283
Iteration :  31   Loss :  0.0714888282357
Iteration :  32   Loss :  0.0710355885209
Iteration :  33   Loss :  0.0705852223494
Iteration :  34   Loss :  0.0701377115028
Iteration :  35   Loss :  0.0696930378784
Iteration :  36   Loss :  0.0692511834881
Iteration :  37   Loss :  0.068812130458
Iteration :  38   Loss :  0.0683758610275
Iteration :  39   Loss :  0.0679423575485
Iteration :  40   Loss :  0.0675116024849
Iteration :  41   Loss :  0.0670835784116
Iteration :  42   Loss :  0.0666582680142
Iteration :  43   Loss :  0.066235654088
Iteration :  44   Loss :  0.0658157195372
Iteration :  45   Loss :  0.0653984473747
Iteration :  46   Loss :  0.0649838207209
Iteration :  47   Loss :  0.0645718228033
Iteration :  48   Loss :  0.0641624369555
Iteration :  49   Loss :  0.0637556466172
Iteration :  50   Loss :  0.0633514353327
Iteration :  51   Loss :  0.0629497867508
Iteration :  52   Loss :  0.062550684624
Iteration :  53   Loss :  0.0621541128077
Iteration :  54   Loss :  0.0617600552597
Iteration :  55   Loss :  0.0613684960396
Iteration :  56   Loss :  0.0609794193079
Iteration :  57   Loss :  0.0605928093256
Iteration :  58   Loss :  0.0602086504536
Iteration :  59   Loss :  0.0598269271518
Iteration :  60   Loss :  0.0594476239786
Iteration :  61   Loss :  0.0590707255904
Iteration :  62   Loss :  0.0586962167408
Iteration :  63   Loss :  0.0583240822802
Iteration :  64   Loss :  0.0579543071549
Iteration :  65   Loss :  0.0575868764067
Iteration :  66   Loss :  0.0572217751723
Iteration :  67   Loss :  0.0568589886824
Iteration :  68   Loss :  0.0564985022617
Iteration :  69   Loss :  0.0561403013276
Iteration :  70   Loss :  0.0557843713902
Iteration :  71   Loss :  0.0554306980512
Iteration :  72   Loss :  0.0550792670039
Iteration :  73   Loss :  0.0547300640322
Iteration :  74   Loss :  0.0543830750099
Iteration :  75   Loss :  0.0540382859007
Iteration :  76   Loss :  0.053695682757
Iteration :  77   Loss :  0.0533552517199
Iteration :  78   Loss :  0.0530169790182
Iteration :  79   Loss :  0.0526808509679
Iteration :  80   Loss :  0.0523468539721
Iteration :  81   Loss :  0.0520149745197
Iteration :  82   Loss :  0.0516851991856
Iteration :  83   Loss :  0.0513575146297
Iteration :  84   Loss :  0.0510319075963
Iteration :  85   Loss :  0.0507083649139
Iteration :  86   Loss :  0.0503868734947
Iteration :  87   Loss :  0.0500674203334
Iteration :  88   Loss :  0.0497499925077
Iteration :  89   Loss :  0.0494345771768
Iteration :  90   Loss :  0.0491211615815
Iteration :  91   Loss :  0.0488097330434
Iteration :  92   Loss :  0.0485002789646
Iteration :  93   Loss :  0.0481927868271
Iteration :  94   Loss :  0.047887244192
Iteration :  95   Loss :  0.0475836386996
Iteration :  96   Loss :  0.0472819580683
Iteration :  97   Loss :  0.0469821900945
Iteration :  98   Loss :  0.0466843226519
Iteration :  99   Loss :  0.0463883436912
[ -1.50183517e-03   1.13756814e-04   1.68698234e-04 ...,   4.62444242e-04
   6.11638494e-04  -7.31229587e-05]
CROSS VALIDATION 1
Iteration :  0   Loss :  14.7420738478
Iteration :  1   Loss :  0.347856826135
Iteration :  2   Loss :  0.0925213634815
Iteration :  3   Loss :  0.0919347773333
Iteration :  4   Loss :  0.0913519101457
Iteration :  5   Loss :  0.0907727383405
Iteration :  6   Loss :  0.090197238489
Iteration :  7   Loss :  0.0896253873109
Iteration :  8   Loss :  0.0890571616736
Iteration :  9   Loss :  0.0884925385912
Iteration :  10   Loss :  0.0879314952234
Iteration :  11   Loss :  0.0873740088749
Iteration :  12   Loss :  0.086820056994
Iteration :  13   Loss :  0.0862696171723
Iteration :  14   Loss :  0.0857226671432
Iteration :  15   Loss :  0.0851791847815
Iteration :  16   Loss :  0.084639148102
Iteration :  17   Loss :  0.0841025352593
Iteration :  18   Loss :  0.083569324546
Iteration :  19   Loss :  0.0830394943927
Iteration :  20   Loss :  0.0825130233666
Iteration :  21   Loss :  0.0819898901709
Iteration :  22   Loss :  0.0814700736435
Iteration :  23   Loss :  0.0809535527569
Iteration :  24   Loss :  0.0804403066167
Iteration :  25   Loss :  0.0799303144609
Iteration :  26   Loss :  0.0794235556592
Iteration :  27   Loss :  0.0789200097122
Iteration :  28   Loss :  0.0784196562503
Iteration :  29   Loss :  0.0779224750332
Iteration :  30   Loss :  0.0774284459488
Iteration :  31   Loss :  0.0769375490125
Iteration :  32   Loss :  0.0764497643666
Iteration :  33   Loss :  0.075965072279
Iteration :  34   Loss :  0.0754834531429
Iteration :  35   Loss :  0.0750048874758
Iteration :  36   Loss :  0.0745293559187
Iteration :  37   Loss :  0.0740568392353
Iteration :  38   Loss :  0.0735873183112
Iteration :  39   Loss :  0.0731207741534
Iteration :  40   Loss :  0.0726571878891
Iteration :  41   Loss :  0.0721965407652
Iteration :  42   Loss :  0.0717388141476
Iteration :  43   Loss :  0.0712839895202
Iteration :  44   Loss :  0.0708320484844
Iteration :  45   Loss :  0.0703829727583
Iteration :  46   Loss :  0.0699367441756
Iteration :  47   Loss :  0.0694933446856
Iteration :  48   Loss :  0.0690527563517
Iteration :  49   Loss :  0.0686149613512
Iteration :  50   Loss :  0.0681799419743
Iteration :  51   Loss :  0.0677476806236
Iteration :  52   Loss :  0.0673181598131
Iteration :  53   Loss :  0.0668913621678
Iteration :  54   Loss :  0.0664672704229
Iteration :  55   Loss :  0.0660458674228
Iteration :  56   Loss :  0.065627136121
Iteration :  57   Loss :  0.0652110595788
Iteration :  58   Loss :  0.064797620965
Iteration :  59   Loss :  0.0643868035552
Iteration :  60   Loss :  0.0639785907309
Iteration :  61   Loss :  0.0635729659791
Iteration :  62   Loss :  0.0631699128912
Iteration :  63   Loss :  0.062769415163
Iteration :  64   Loss :  0.0623714565934
Iteration :  65   Loss :  0.0619760210842
Iteration :  66   Loss :  0.0615830926391
Iteration :  67   Loss :  0.0611926553633
Iteration :  68   Loss :  0.0608046934628
Iteration :  69   Loss :  0.0604191912438
Iteration :  70   Loss :  0.0600361331117
Iteration :  71   Loss :  0.0596555035711
Iteration :  72   Loss :  0.0592772872247
Iteration :  73   Loss :  0.0589014687728
Iteration :  74   Loss :  0.0585280330128
Iteration :  75   Loss :  0.0581569648383
Iteration :  76   Loss :  0.0577882492389
Iteration :  77   Loss :  0.0574218712991
Iteration :  78   Loss :  0.0570578161983
Iteration :  79   Loss :  0.0566960692096
Iteration :  80   Loss :  0.0563366156996
Iteration :  81   Loss :  0.0559794411275
Iteration :  82   Loss :  0.055624531045
Iteration :  83   Loss :  0.055271871095
Iteration :  84   Loss :  0.0549214470119
Iteration :  85   Loss :  0.0545732446201
Iteration :  86   Loss :  0.0542272498341
Iteration :  87   Loss :  0.0538834486576
Iteration :  88   Loss :  0.0535418271833
Iteration :  89   Loss :  0.0532023715917
Iteration :  90   Loss :  0.0528650681511
Iteration :  91   Loss :  0.0525299032169
Iteration :  92   Loss :  0.052196863231
Iteration :  93   Loss :  0.051865934721
Iteration :  94   Loss :  0.0515371043004
Iteration :  95   Loss :  0.0512103586671
Iteration :  96   Loss :  0.0508856846036
Iteration :  97   Loss :  0.0505630689762
Iteration :  98   Loss :  0.0502424987343
Iteration :  99   Loss :  0.0499239609102
[-0.00173941 -0.00023584 -0.00039395 ...,  0.00091122  0.00082632
  0.0002188 ]
CROSS VALIDATION 2
Iteration :  0   Loss :  10.4466236847
Iteration :  1   Loss :  2.52046633957
Iteration :  2   Loss :  0.0927953452058
Iteration :  3   Loss :  0.0922070220114
Iteration :  4   Loss :  0.0916224287904
Iteration :  5   Loss :  0.091041541895
Iteration :  6   Loss :  0.0904643378269
Iteration :  7   Loss :  0.0898907932369
Iteration :  8   Loss :  0.0893208849241
Iteration :  9   Loss :  0.0887545898342
Iteration :  10   Loss :  0.0881918850596
Iteration :  11   Loss :  0.0876327478375
Iteration :  12   Loss :  0.0870771555496
Iteration :  13   Loss :  0.0865250857211
Iteration :  14   Loss :  0.0859765160195
Iteration :  15   Loss :  0.0854314242539
Iteration :  16   Loss :  0.0848897883744
Iteration :  17   Loss :  0.0843515864704
Iteration :  18   Loss :  0.0838167967705
Iteration :  19   Loss :  0.0832853976414
Iteration :  20   Loss :  0.0827573675869
Iteration :  21   Loss :  0.0822326852469
Iteration :  22   Loss :  0.0817113293969
Iteration :  23   Loss :  0.081193278947
Iteration :  24   Loss :  0.0806785129409
Iteration :  25   Loss :  0.0801670105552
Iteration :  26   Loss :  0.0796587510985
Iteration :  27   Loss :  0.0791537140106
Iteration :  28   Loss :  0.0786518788617
Iteration :  29   Loss :  0.0781532253515
Iteration :  30   Loss :  0.0776577333083
Iteration :  31   Loss :  0.0771653826884
Iteration :  32   Loss :  0.0766761535752
Iteration :  33   Loss :  0.0761900261783
Iteration :  34   Loss :  0.0757069808328
Iteration :  35   Loss :  0.0752269979985
Iteration :  36   Loss :  0.074750058259
Iteration :  37   Loss :  0.074276142321
Iteration :  38   Loss :  0.0738052310137
Iteration :  39   Loss :  0.0733373052877
Iteration :  40   Loss :  0.0728723462143
Iteration :  41   Loss :  0.072410334985
Iteration :  42   Loss :  0.0719512529105
Iteration :  43   Loss :  0.0714950814197
Iteration :  44   Loss :  0.0710418020597
Iteration :  45   Loss :  0.0705913964943
Iteration :  46   Loss :  0.0701438465035
Iteration :  47   Loss :  0.0696991339832
Iteration :  48   Loss :  0.0692572409435
Iteration :  49   Loss :  0.0688181495092
Iteration :  50   Loss :  0.0683818419178
Iteration :  51   Loss :  0.0679483005199
Iteration :  52   Loss :  0.0675175077778
Iteration :  53   Loss :  0.0670894462649
Iteration :  54   Loss :  0.0666640986653
Iteration :  55   Loss :  0.0662414477727
Iteration :  56   Loss :  0.0658214764899
Iteration :  57   Loss :  0.0654041678283
Iteration :  58   Loss :  0.0649895049068
Iteration :  59   Loss :  0.0645774709513
Iteration :  60   Loss :  0.0641680492943
Iteration :  61   Loss :  0.0637612233737
Iteration :  62   Loss :  0.0633569767325
Iteration :  63   Loss :  0.0629552930181
Iteration :  64   Loss :  0.0625561559815
Iteration :  65   Loss :  0.0621595494768
Iteration :  66   Loss :  0.0617654574603
Iteration :  67   Loss :  0.0613738639901
Iteration :  68   Loss :  0.0609847532256
Iteration :  69   Loss :  0.0605981094263
Iteration :  70   Loss :  0.0602139169516
Iteration :  71   Loss :  0.0598321602601
Iteration :  72   Loss :  0.0594528239089
Iteration :  73   Loss :  0.0590758925531
Iteration :  74   Loss :  0.058701350945
Iteration :  75   Loss :  0.0583291839335
Iteration :  76   Loss :  0.0579593764637
Iteration :  77   Loss :  0.057591913576
Iteration :  78   Loss :  0.0572267804059
Iteration :  79   Loss :  0.0568639621828
Iteration :  80   Loss :  0.0565034442301
Iteration :  81   Loss :  0.0561452119639
Iteration :  82   Loss :  0.0557892508929
Iteration :  83   Loss :  0.0554355466179
Iteration :  84   Loss :  0.0550840848307
Iteration :  85   Loss :  0.0547348513139
Iteration :  86   Loss :  0.0543878319402
Iteration :  87   Loss :  0.054043012672
Iteration :  88   Loss :  0.0537003795606
Iteration :  89   Loss :  0.0533599187457
Iteration :  90   Loss :  0.053021616455
Iteration :  91   Loss :  0.0526854590034
Iteration :  92   Loss :  0.0523514327925
Iteration :  93   Loss :  0.0520195243104
Iteration :  94   Loss :  0.0516897201306
Iteration :  95   Loss :  0.0513620069118
Iteration :  96   Loss :  0.0510363713973
Iteration :  97   Loss :  0.0507128004145
Iteration :  98   Loss :  0.0503912808741
Iteration :  99   Loss :  0.05007179977
[ -1.46639605e-03  -6.12804493e-04  -6.35017043e-04 ...,   1.55085220e-03
   6.90455010e-04   7.29525279e-05]
CROSS VALIDATION 3
Iteration :  0   Loss :  20.8463771658
Iteration :  1   Loss :  1.39754560939
Iteration :  2   Loss :  0.757329894395
Iteration :  3   Loss :  0.0970737250747
Iteration :  4   Loss :  0.0964582769194
Iteration :  5   Loss :  0.09584673071
Iteration :  6   Loss :  0.0952390617083
Iteration :  7   Loss :  0.0946352453326
Iteration :  8   Loss :  0.0940352571574
Iteration :  9   Loss :  0.0934390729117
Iteration :  10   Loss :  0.0928466684787
Iteration :  11   Loss :  0.0922580198943
Iteration :  12   Loss :  0.0916731033464
Iteration :  13   Loss :  0.0910918951739
Iteration :  14   Loss :  0.0905143718656
Iteration :  15   Loss :  0.0899405100595
Iteration :  16   Loss :  0.0893702865416
Iteration :  17   Loss :  0.0888036782452
Iteration :  18   Loss :  0.0882406622496
Iteration :  19   Loss :  0.0876812157797
Iteration :  20   Loss :  0.0871253162048
Iteration :  21   Loss :  0.0865729410373
Iteration :  22   Loss :  0.0860240679326
Iteration :  23   Loss :  0.0854786746876
Iteration :  24   Loss :  0.0849367392399
Iteration :  25   Loss :  0.084398239667
Iteration :  26   Loss :  0.0838631541856
Iteration :  27   Loss :  0.0833314611502
Iteration :  28   Loss :  0.0828031390526
Iteration :  29   Loss :  0.0822781665212
Iteration :  30   Loss :  0.0817565223196
Iteration :  31   Loss :  0.0812381853462
Iteration :  32   Loss :  0.0807231346332
Iteration :  33   Loss :  0.0802113493456
Iteration :  34   Loss :  0.0797028087806
Iteration :  35   Loss :  0.0791974923667
Iteration :  36   Loss :  0.0786953796627
Iteration :  37   Loss :  0.078196450357
Iteration :  38   Loss :  0.077700684267
Iteration :  39   Loss :  0.0772080613377
Iteration :  40   Loss :  0.0767185616415
Iteration :  41   Loss :  0.0762321653772
Iteration :  42   Loss :  0.0757488528688
Iteration :  43   Loss :  0.0752686045655
Iteration :  44   Loss :  0.07479140104
Iteration :  45   Loss :  0.0743172229885
Iteration :  46   Loss :  0.0738460512295
Iteration :  47   Loss :  0.073377866703
Iteration :  48   Loss :  0.0729126504699
Iteration :  49   Loss :  0.0724503837114
Iteration :  50   Loss :  0.0719910477275
Iteration :  51   Loss :  0.0715346239374
Iteration :  52   Loss :  0.0710810938775
Iteration :  53   Loss :  0.0706304392016
Iteration :  54   Loss :  0.0701826416798
Iteration :  55   Loss :  0.0697376831977
Iteration :  56   Loss :  0.0692955457558
Iteration :  57   Loss :  0.0688562114686
Iteration :  58   Loss :  0.0684196625642
Iteration :  59   Loss :  0.0679858813831
Iteration :  60   Loss :  0.067554850378
Iteration :  61   Loss :  0.0671265521127
Iteration :  62   Loss :  0.0667009692618
Iteration :  63   Loss :  0.0662780846093
Iteration :  64   Loss :  0.0658578810487
Iteration :  65   Loss :  0.0654403415819
Iteration :  66   Loss :  0.0650254493185
Iteration :  67   Loss :  0.0646131874752
Iteration :  68   Loss :  0.0642035393752
Iteration :  69   Loss :  0.0637964884472
Iteration :  70   Loss :  0.0633920182252
Iteration :  71   Loss :  0.0629901123474
Iteration :  72   Loss :  0.062590754556
Iteration :  73   Loss :  0.062193928696
Iteration :  74   Loss :  0.061799618715
Iteration :  75   Loss :  0.0614078086623
Iteration :  76   Loss :  0.0610184826883
Iteration :  77   Loss :  0.0606316250439
Iteration :  78   Loss :  0.0602472200799
Iteration :  79   Loss :  0.0598652522464
Iteration :  80   Loss :  0.0594857060918
Iteration :  81   Loss :  0.0591085662627
Iteration :  82   Loss :  0.058733817503
Iteration :  83   Loss :  0.0583614446532
Iteration :  84   Loss :  0.0579914326502
Iteration :  85   Loss :  0.0576237665261
Iteration :  86   Loss :  0.057258431408
Iteration :  87   Loss :  0.0568954125174
Iteration :  88   Loss :  0.0565346951692
Iteration :  89   Loss :  0.0561762647718
Iteration :  90   Loss :  0.0558201068259
Iteration :  91   Loss :  0.055466206924
Iteration :  92   Loss :  0.0551145507502
Iteration :  93   Loss :  0.0547651240792
Iteration :  94   Loss :  0.054417912776
Iteration :  95   Loss :  0.0540729027951
Iteration :  96   Loss :  0.0537300801801
Iteration :  97   Loss :  0.053389431063
Iteration :  98   Loss :  0.053050941664
Iteration :  99   Loss :  0.0527145982903
[-0.00214857 -0.00021613 -0.00038711 ...,  0.00162776  0.00098527
  0.00014641]
CROSS VALIDATION 4
Iteration :  0   Loss :  20.8463771658
Iteration :  1   Loss :  1.39754560939
Iteration :  2   Loss :  0.757329894395
Iteration :  3   Loss :  0.0970737250747
Iteration :  4   Loss :  0.0964582769194
Iteration :  5   Loss :  0.09584673071
Iteration :  6   Loss :  0.0952390617083
Iteration :  7   Loss :  0.0946352453326
Iteration :  8   Loss :  0.0940352571574
Iteration :  9   Loss :  0.0934390729117
Iteration :  10   Loss :  0.0928466684787
Iteration :  11   Loss :  0.0922580198943
Iteration :  12   Loss :  0.0916731033464
Iteration :  13   Loss :  0.0910918951739
Iteration :  14   Loss :  0.0905143718656
Iteration :  15   Loss :  0.0899405100595
Iteration :  16   Loss :  0.0893702865416
Iteration :  17   Loss :  0.0888036782452
Iteration :  18   Loss :  0.0882406622496
Iteration :  19   Loss :  0.0876812157797
Iteration :  20   Loss :  0.0871253162048
Iteration :  21   Loss :  0.0865729410373
Iteration :  22   Loss :  0.0860240679326
Iteration :  23   Loss :  0.0854786746876
Iteration :  24   Loss :  0.0849367392399
Iteration :  25   Loss :  0.084398239667
Iteration :  26   Loss :  0.0838631541856
Iteration :  27   Loss :  0.0833314611502
Iteration :  28   Loss :  0.0828031390526
Iteration :  29   Loss :  0.0822781665212
Iteration :  30   Loss :  0.0817565223196
Iteration :  31   Loss :  0.0812381853462
Iteration :  32   Loss :  0.0807231346332
Iteration :  33   Loss :  0.0802113493456
Iteration :  34   Loss :  0.0797028087806
Iteration :  35   Loss :  0.0791974923667
Iteration :  36   Loss :  0.0786953796627
Iteration :  37   Loss :  0.078196450357
Iteration :  38   Loss :  0.077700684267
Iteration :  39   Loss :  0.0772080613377
Iteration :  40   Loss :  0.0767185616415
Iteration :  41   Loss :  0.0762321653772
Iteration :  42   Loss :  0.0757488528688
Iteration :  43   Loss :  0.0752686045655
Iteration :  44   Loss :  0.07479140104
Iteration :  45   Loss :  0.0743172229885
Iteration :  46   Loss :  0.0738460512295
Iteration :  47   Loss :  0.073377866703
Iteration :  48   Loss :  0.0729126504699
Iteration :  49   Loss :  0.0724503837114
Iteration :  50   Loss :  0.0719910477275
Iteration :  51   Loss :  0.0715346239374
Iteration :  52   Loss :  0.0710810938775
Iteration :  53   Loss :  0.0706304392016
Iteration :  54   Loss :  0.0701826416798
Iteration :  55   Loss :  0.0697376831977
Iteration :  56   Loss :  0.0692955457558
Iteration :  57   Loss :  0.0688562114686
Iteration :  58   Loss :  0.0684196625642
Iteration :  59   Loss :  0.0679858813831
Iteration :  60   Loss :  0.067554850378
Iteration :  61   Loss :  0.0671265521127
Iteration :  62   Loss :  0.0667009692618
Iteration :  63   Loss :  0.0662780846093
Iteration :  64   Loss :  0.0658578810487
Iteration :  65   Loss :  0.0654403415819
Iteration :  66   Loss :  0.0650254493185
Iteration :  67   Loss :  0.0646131874752
Iteration :  68   Loss :  0.0642035393752
Iteration :  69   Loss :  0.0637964884472
Iteration :  70   Loss :  0.0633920182252
Iteration :  71   Loss :  0.0629901123474
Iteration :  72   Loss :  0.062590754556
Iteration :  73   Loss :  0.062193928696
Iteration :  74   Loss :  0.061799618715
Iteration :  75   Loss :  0.0614078086623
Iteration :  76   Loss :  0.0610184826883
Iteration :  77   Loss :  0.0606316250439
Iteration :  78   Loss :  0.0602472200799
Iteration :  79   Loss :  0.0598652522464
Iteration :  80   Loss :  0.0594857060918
Iteration :  81   Loss :  0.0591085662627
Iteration :  82   Loss :  0.058733817503
Iteration :  83   Loss :  0.0583614446532
Iteration :  84   Loss :  0.0579914326502
Iteration :  85   Loss :  0.0576237665261
Iteration :  86   Loss :  0.057258431408
Iteration :  87   Loss :  0.0568954125174
Iteration :  88   Loss :  0.0565346951692
Iteration :  89   Loss :  0.0561762647718
Iteration :  90   Loss :  0.0558201068259
Iteration :  91   Loss :  0.055466206924
Iteration :  92   Loss :  0.0551145507502
Iteration :  93   Loss :  0.0547651240792
Iteration :  94   Loss :  0.054417912776
Iteration :  95   Loss :  0.0540729027951
Iteration :  96   Loss :  0.0537300801801
Iteration :  97   Loss :  0.053389431063
Iteration :  98   Loss :  0.053050941664
Iteration :  99   Loss :  0.0527145982903
[-0.00214857 -0.00021613 -0.00038711 ...,  0.00162776  0.00098527
  0.00014641]
CROSS VALIDATION 5
Iteration :  0   Loss :  9.69912319892
Iteration :  1   Loss :  0.0896449575092
Iteration :  2   Loss :  0.0890766077967
Iteration :  3   Loss :  0.0885118614257
Iteration :  4   Loss :  0.087950695551
Iteration :  5   Loss :  0.0873930874723
Iteration :  6   Loss :  0.0868390146329
Iteration :  7   Loss :  0.0862884546196
Iteration :  8   Loss :  0.085741385161
Iteration :  9   Loss :  0.0851977841269
Iteration :  10   Loss :  0.0846576295275
Iteration :  11   Loss :  0.0841208995123
Iteration :  12   Loss :  0.0835875723696
Iteration :  13   Loss :  0.0830576265249
Iteration :  14   Loss :  0.082531040541
Iteration :  15   Loss :  0.0820077931162
Iteration :  16   Loss :  0.0814878630841
Iteration :  17   Loss :  0.0809712294123
Iteration :  18   Loss :  0.0804578712019
Iteration :  19   Loss :  0.0799477676865
Iteration :  20   Loss :  0.0794408982312
Iteration :  21   Loss :  0.0789372423322
Iteration :  22   Loss :  0.0784367796154
Iteration :  23   Loss :  0.077939489836
Iteration :  24   Loss :  0.0774453528776
Iteration :  25   Loss :  0.0769543487513
Iteration :  26   Loss :  0.0764664575948
Iteration :  27   Loss :  0.075981659672
Iteration :  28   Loss :  0.0754999353718
Iteration :  29   Loss :  0.0750212652072
Iteration :  30   Loss :  0.0745456298152
Iteration :  31   Loss :  0.0740730099551
Iteration :  32   Loss :  0.0736033865086
Iteration :  33   Loss :  0.0731367404783
Iteration :  34   Loss :  0.0726730529873
Iteration :  35   Loss :  0.0722123052786
Iteration :  36   Loss :  0.0717544787138
Iteration :  37   Loss :  0.071299554773
Iteration :  38   Loss :  0.0708475150534
Iteration :  39   Loss :  0.0703983412691
Iteration :  40   Loss :  0.06995201525
Iteration :  41   Loss :  0.0695085189412
Iteration :  42   Loss :  0.0690678344024
Iteration :  43   Loss :  0.0686299438069
Iteration :  44   Loss :  0.0681948294412
Iteration :  45   Loss :  0.0677624737038
Iteration :  46   Loss :  0.0673328591051
Iteration :  47   Loss :  0.0669059682662
Iteration :  48   Loss :  0.0664817839185
Iteration :  49   Loss :  0.0660602889027
Iteration :  50   Loss :  0.0656414661686
Iteration :  51   Loss :  0.0652252987738
Iteration :  52   Loss :  0.0648117698834
Iteration :  53   Loss :  0.0644008627694
Iteration :  54   Loss :  0.0639925608096
Iteration :  55   Loss :  0.0635868474873
Iteration :  56   Loss :  0.0631837063905
Iteration :  57   Loss :  0.0627831212114
Iteration :  58   Loss :  0.0623850757454
Iteration :  59   Loss :  0.0619895538906
Iteration :  60   Loss :  0.0615965396475
Iteration :  61   Loss :  0.0612060171176
Iteration :  62   Loss :  0.0608179705035
Iteration :  63   Loss :  0.0604323841079
Iteration :  64   Loss :  0.060049242333
Iteration :  65   Loss :  0.0596685296798
Iteration :  66   Loss :  0.0592902307478
Iteration :  67   Loss :  0.0589143302339
Iteration :  68   Loss :  0.058540812932
Iteration :  69   Loss :  0.0581696637328
Iteration :  70   Loss :  0.0578008676222
Iteration :  71   Loss :  0.0574344096819
Iteration :  72   Loss :  0.0570702750876
Iteration :  73   Loss :  0.0567084491094
Iteration :  74   Loss :  0.0563489171107
Iteration :  75   Loss :  0.0559916645477
Iteration :  76   Loss :  0.0556366769685
Iteration :  77   Loss :  0.0552839400133
Iteration :  78   Loss :  0.0549334394131
Iteration :  79   Loss :  0.0545851609894
Iteration :  80   Loss :  0.0542390906536
Iteration :  81   Loss :  0.0538952144062
Iteration :  82   Loss :  0.053553518337
Iteration :  83   Loss :  0.0532139886234
Iteration :  84   Loss :  0.0528766115307
Iteration :  85   Loss :  0.0525413734114
Iteration :  86   Loss :  0.0522082607043
Iteration :  87   Loss :  0.0518772599343
Iteration :  88   Loss :  0.0515483577118
Iteration :  89   Loss :  0.0512215407318
Iteration :  90   Loss :  0.0508967957739
Iteration :  91   Loss :  0.0505741097016
Iteration :  92   Loss :  0.0502534694614
Iteration :  93   Loss :  0.0499348620827
Iteration :  94   Loss :  0.0496182746773
Iteration :  95   Loss :  0.0493036944384
Iteration :  96   Loss :  0.0489911086407
Iteration :  97   Loss :  0.0486805046392
Iteration :  98   Loss :  0.0483718698695
Iteration :  99   Loss :  0.0480651918466
[-0.00073898  0.00059591 -0.000702   ...,  0.0001487   0.00019698
  0.00043708]
CROSS VALIDATION 6
Iteration :  0   Loss :  20.8456121781
Iteration :  1   Loss :  1.3974967235
Iteration :  2   Loss :  0.0920998347197
Iteration :  3   Loss :  0.091515921067
Iteration :  4   Loss :  0.0909357094313
Iteration :  5   Loss :  0.0903591763418
Iteration :  6   Loss :  0.0897862984765
Iteration :  7   Loss :  0.0892170526612
Iteration :  8   Loss :  0.0886514158687
Iteration :  9   Loss :  0.0880893652178
Iteration :  10   Loss :  0.0875308779723
Iteration :  11   Loss :  0.0869759315401
Iteration :  12   Loss :  0.0864245034726
Iteration :  13   Loss :  0.0858765714632
Iteration :  14   Loss :  0.0853321133469
Iteration :  15   Loss :  0.0847911070992
Iteration :  16   Loss :  0.0842535308352
Iteration :  17   Loss :  0.0837193628089
Iteration :  18   Loss :  0.0831885814119
Iteration :  19   Loss :  0.0826611651729
Iteration :  20   Loss :  0.082137092757
Iteration :  21   Loss :  0.0816163429641
Iteration :  22   Loss :  0.0810988947289
Iteration :  23   Loss :  0.0805847271195
Iteration :  24   Loss :  0.0800738193366
Iteration :  25   Loss :  0.0795661507129
Iteration :  26   Loss :  0.0790617007122
Iteration :  27   Loss :  0.0785604489283
Iteration :  28   Loss :  0.0780623750845
Iteration :  29   Loss :  0.0775674590327
Iteration :  30   Loss :  0.0770756807524
Iteration :  31   Loss :  0.0765870203502
Iteration :  32   Loss :  0.0761014580586
Iteration :  33   Loss :  0.0756189742357
Iteration :  34   Loss :  0.0751395493638
Iteration :  35   Loss :  0.0746631640493
Iteration :  36   Loss :  0.0741897990213
Iteration :  37   Loss :  0.0737194351312
Iteration :  38   Loss :  0.0732520533517
Iteration :  39   Loss :  0.0727876347763
Iteration :  40   Loss :  0.0723261606182
Iteration :  41   Loss :  0.0718676122097
Iteration :  42   Loss :  0.0714119710016
Iteration :  43   Loss :  0.0709592185623
Iteration :  44   Loss :  0.0705093365769
Iteration :  45   Loss :  0.0700623068467
Iteration :  46   Loss :  0.0696181112883
Iteration :  47   Loss :  0.0691767319332
Iteration :  48   Loss :  0.0687381509266
Iteration :  49   Loss :  0.0683023505268
Iteration :  50   Loss :  0.067869313105
Iteration :  51   Loss :  0.0674390211436
Iteration :  52   Loss :  0.0670114572366
Iteration :  53   Loss :  0.066586604088
Iteration :  54   Loss :  0.0661644445116
Iteration :  55   Loss :  0.0657449614301
Iteration :  56   Loss :  0.0653281378746
Iteration :  57   Loss :  0.0649139569836
Iteration :  58   Loss :  0.0645024020025
Iteration :  59   Loss :  0.0640934562832
Iteration :  60   Loss :  0.0636871032828
Iteration :  61   Loss :  0.0632833265636
Iteration :  62   Loss :  0.0628821097918
Iteration :  63   Loss :  0.0624834367374
Iteration :  64   Loss :  0.0620872912732
Iteration :  65   Loss :  0.0616936573743
Iteration :  66   Loss :  0.0613025191173
Iteration :  67   Loss :  0.0609138606798
Iteration :  68   Loss :  0.0605276663397
Iteration :  69   Loss :  0.0601439204747
Iteration :  70   Loss :  0.0597626075615
Iteration :  71   Loss :  0.0593837121749
Iteration :  72   Loss :  0.059007218988
Iteration :  73   Loss :  0.0586331127708
Iteration :  74   Loss :  0.0582613783899
Iteration :  75   Loss :  0.0578920008077
Iteration :  76   Loss :  0.0575249650823
Iteration :  77   Loss :  0.0571602563661
Iteration :  78   Loss :  0.0567978599059
Iteration :  79   Loss :  0.0564377610421
Iteration :  80   Loss :  0.0560799452078
Iteration :  81   Loss :  0.0557243979286
Iteration :  82   Loss :  0.0553711048219
Iteration :  83   Loss :  0.0550200515961
Iteration :  84   Loss :  0.0546712240505
Iteration :  85   Loss :  0.0543246080742
Iteration :  86   Loss :  0.0539801896458
Iteration :  87   Loss :  0.0536379548328
Iteration :  88   Loss :  0.0532978897912
Iteration :  89   Loss :  0.0529599807645
Iteration :  90   Loss :  0.0526242140836
Iteration :  91   Loss :  0.0522905761661
Iteration :  92   Loss :  0.0519590535154
Iteration :  93   Loss :  0.0516296327209
Iteration :  94   Loss :  0.0513023004567
Iteration :  95   Loss :  0.0509770434816
Iteration :  96   Loss :  0.0506538486382
Iteration :  97   Loss :  0.0503327028525
Iteration :  98   Loss :  0.0500135931336
Iteration :  99   Loss :  0.0496965065727
[ -2.42304964e-03  -4.79734555e-04  -5.68865753e-04 ...,   1.52051085e-03
   8.88227527e-04   7.29177189e-05]
CROSS VALIDATION 7
Iteration :  0   Loss :  20.8456121781
Iteration :  1   Loss :  1.3974967235
Iteration :  2   Loss :  0.757179936644
Iteration :  3   Loss :  0.0970748812728
Iteration :  4   Loss :  0.0964594257872
Iteration :  5   Loss :  0.0958478722939
Iteration :  6   Loss :  0.0952401960545
Iteration :  7   Loss :  0.0946363724871
Iteration :  8   Loss :  0.0940363771657
Iteration :  9   Loss :  0.0934401858192
Iteration :  10   Loss :  0.0928477743304
Iteration :  11   Loss :  0.0922591187349
Iteration :  12   Loss :  0.0916741952203
Iteration :  13   Loss :  0.0910929801253
Iteration :  14   Loss :  0.0905154499384
Iteration :  15   Loss :  0.0899415812973
Iteration :  16   Loss :  0.0893713509878
Iteration :  17   Loss :  0.0888047359427
Iteration :  18   Loss :  0.0882417132413
Iteration :  19   Loss :  0.0876822601082
Iteration :  20   Loss :  0.0871263539122
Iteration :  21   Loss :  0.0865739721656
Iteration :  22   Loss :  0.0860250925236
Iteration :  23   Loss :  0.0854796927826
Iteration :  24   Loss :  0.0849377508802
Iteration :  25   Loss :  0.0843992448936
Iteration :  26   Loss :  0.083864153039
Iteration :  27   Loss :  0.0833324536708
Iteration :  28   Loss :  0.0828041252807
Iteration :  29   Loss :  0.0822791464965
Iteration :  30   Loss :  0.0817574960819
Iteration :  31   Loss :  0.0812391529349
Iteration :  32   Loss :  0.0807240960873
Iteration :  33   Loss :  0.0802123047041
Iteration :  34   Loss :  0.0797037580821
Iteration :  35   Loss :  0.0791984356496
Iteration :  36   Loss :  0.0786963169652
Iteration :  37   Loss :  0.078197381717
Iteration :  38   Loss :  0.0777016097222
Iteration :  39   Loss :  0.0772089809255
Iteration :  40   Loss :  0.0767194753991
Iteration :  41   Loss :  0.0762330733415
Iteration :  42   Loss :  0.0757497550767
Iteration :  43   Loss :  0.0752695010533
Iteration :  44   Loss :  0.0747922918441
Iteration :  45   Loss :  0.0743181081449
Iteration :  46   Loss :  0.073846930774
Iteration :  47   Loss :  0.0733787406712
Iteration :  48   Loss :  0.0729135188972
Iteration :  49   Loss :  0.0724512466328
Iteration :  50   Loss :  0.071991905178
Iteration :  51   Loss :  0.0715354759516
Iteration :  52   Loss :  0.0710819404899
Iteration :  53   Loss :  0.0706312804465
Iteration :  54   Loss :  0.0701834775912
Iteration :  55   Loss :  0.0697385138095
Iteration :  56   Loss :  0.0692963711015
Iteration :  57   Loss :  0.0688570315816
Iteration :  58   Loss :  0.0684204774776
Iteration :  59   Loss :  0.06798669113
Iteration :  60   Loss :  0.0675556549911
Iteration :  61   Loss :  0.0671273516246
Iteration :  62   Loss :  0.0667017637047
Iteration :  63   Loss :  0.0662788740154
Iteration :  64   Loss :  0.06585866545
Iteration :  65   Loss :  0.0654411210101
Iteration :  66   Loss :  0.0650262238051
Iteration :  67   Loss :  0.0646139570516
Iteration :  68   Loss :  0.0642043040724
Iteration :  69   Loss :  0.0637972482962
Iteration :  70   Loss :  0.0633927732568
Iteration :  71   Loss :  0.0629908625921
Iteration :  72   Loss :  0.0625915000441
Iteration :  73   Loss :  0.0621946694578
Iteration :  74   Loss :  0.0618003547803
Iteration :  75   Loss :  0.0614085400609
Iteration :  76   Loss :  0.0610192094499
Iteration :  77   Loss :  0.0606323471978
Iteration :  78   Loss :  0.0602479376554
Iteration :  79   Loss :  0.0598659652724
Iteration :  80   Loss :  0.0594864145972
Iteration :  81   Loss :  0.0591092702762
Iteration :  82   Loss :  0.058734517053
Iteration :  83   Loss :  0.0583621397681
Iteration :  84   Loss :  0.0579921233581
Iteration :  85   Loss :  0.0576244528549
Iteration :  86   Loss :  0.0572591133854
Iteration :  87   Loss :  0.056896090171
Iteration :  88   Loss :  0.0565353685266
Iteration :  89   Loss :  0.0561769338601
Iteration :  90   Loss :  0.0558207716721
Iteration :  91   Loss :  0.0554668675551
Iteration :  92   Loss :  0.0551152071929
Iteration :  93   Loss :  0.0547657763601
Iteration :  94   Loss :  0.0544185609214
Iteration :  95   Loss :  0.0540735468312
Iteration :  96   Loss :  0.053730720133
Iteration :  97   Loss :  0.0533900669587
Iteration :  98   Loss :  0.053051573528
Iteration :  99   Loss :  0.0527152261483
[-0.00214857 -0.00021612 -0.0003871  ...,  0.00162779  0.0009853
  0.00014641]
CROSS VALIDATION 8
Iteration :  0   Loss :  23.6961548099
Iteration :  1   Loss :  0.820864810626
Iteration :  2   Loss :  0.164921795898
Iteration :  3   Loss :  0.0970756688608
Iteration :  4   Loss :  0.0964602083818
Iteration :  5   Loss :  0.0958486499269
Iteration :  6   Loss :  0.0952409687573
Iteration :  7   Loss :  0.094637140291
Iteration :  8   Loss :  0.0940371401017
Iteration :  9   Loss :  0.0934409439181
Iteration :  10   Loss :  0.0928485276229
Iteration :  11   Loss :  0.0922598672516
Iteration :  12   Loss :  0.0916749389914
Iteration :  13   Loss :  0.0910937191809
Iteration :  14   Loss :  0.0905161843084
Iteration :  15   Loss :  0.0899423110114
Iteration :  16   Loss :  0.0893720760754
Iteration :  17   Loss :  0.0888054564333
Iteration :  18   Loss :  0.088242429164
Iteration :  19   Loss :  0.0876829714919
Iteration :  20   Loss :  0.0871270607857
Iteration :  21   Loss :  0.0865746745576
Iteration :  22   Loss :  0.0860257904624
Iteration :  23   Loss :  0.0854803862965
Iteration :  24   Loss :  0.0849384399972
Iteration :  25   Loss :  0.0843999296415
Iteration :  26   Loss :  0.0838648334456
Iteration :  27   Loss :  0.0833331297637
Iteration :  28   Loss :  0.0828047970871
Iteration :  29   Loss :  0.0822798140437
Iteration :  30   Loss :  0.0817581593968
Iteration :  31   Loss :  0.0812398120444
Iteration :  32   Loss :  0.0807247510181
Iteration :  33   Loss :  0.0802129554826
Iteration :  34   Loss :  0.0797044047347
Iteration :  35   Loss :  0.0791990782024
Iteration :  36   Loss :  0.0786969554442
Iteration :  37   Loss :  0.0781980161481
Iteration :  38   Loss :  0.0777022401309
Iteration :  39   Loss :  0.0772096073374
Iteration :  40   Loss :  0.0767200978396
Iteration :  41   Loss :  0.0762336918357
Iteration :  42   Loss :  0.0757503696496
Iteration :  43   Loss :  0.0752701117299
Iteration :  44   Loss :  0.074792898649
Iteration :  45   Loss :  0.0743187111026
Iteration :  46   Loss :  0.0738475299089
Iteration :  47   Loss :  0.0733793360076
Iteration :  48   Loss :  0.0729141104591
Iteration :  49   Loss :  0.0724518344442
Iteration :  50   Loss :  0.0719924892627
Iteration :  51   Loss :  0.0715360563332
Iteration :  52   Loss :  0.0710825171919
Iteration :  53   Loss :  0.0706318534922
Iteration :  54   Loss :  0.0701840470039
Iteration :  55   Loss :  0.069739079612
Iteration :  56   Loss :  0.0692969333168
Iteration :  57   Loss :  0.0688575902325
Iteration :  58   Loss :  0.0684210325866
Iteration :  59   Loss :  0.0679872427196
Iteration :  60   Loss :  0.0675562030836
Iteration :  61   Loss :  0.0671278962422
Iteration :  62   Loss :  0.0667023048694
Iteration :  63   Loss :  0.0662794117492
Iteration :  64   Loss :  0.0658591997745
Iteration :  65   Loss :  0.065441651947
Iteration :  66   Loss :  0.0650267513759
Iteration :  67   Loss :  0.0646144812776
Iteration :  68   Loss :  0.0642048249748
Iteration :  69   Loss :  0.0637977658961
Iteration :  70   Loss :  0.063393287575
Iteration :  71   Loss :  0.0629913736496
Iteration :  72   Loss :  0.0625920078615
Iteration :  73   Loss :  0.0621951740556
Iteration :  74   Loss :  0.061800856179
Iteration :  75   Loss :  0.0614090382807
Iteration :  76   Loss :  0.0610197045109
Iteration :  77   Loss :  0.0606328391202
Iteration :  78   Loss :  0.060248426459
Iteration :  79   Loss :  0.0598664509769
Iteration :  80   Loss :  0.0594868972224
Iteration :  81   Loss :  0.0591097498415
Iteration :  82   Loss :  0.0587349935779
Iteration :  83   Loss :  0.0583626132718
Iteration :  84   Loss :  0.0579925938598
Iteration :  85   Loss :  0.0576249203736
Iteration :  86   Loss :  0.0572595779401
Iteration :  87   Loss :  0.0568965517804
Iteration :  88   Loss :  0.0565358272093
Iteration :  89   Loss :  0.0561773896348
Iteration :  90   Loss :  0.0558212245572
Iteration :  91   Loss :  0.0554673175689
Iteration :  92   Loss :  0.0551156543536
Iteration :  93   Loss :  0.0547662206858
Iteration :  94   Loss :  0.0544190024301
Iteration :  95   Loss :  0.0540739855407
Iteration :  96   Loss :  0.0537311560611
Iteration :  97   Loss :  0.053390500123
Iteration :  98   Loss :  0.0530520039461
Iteration :  99   Loss :  0.0527156538375
[ -1.64029451e-03   4.04400244e-04  -1.06094424e-03 ...,   1.31091126e-03
  -1.34624900e-04   7.17501373e-05]
CROSS VALIDATION 9
Iteration :  0   Loss :  11.1064267457
Iteration :  1   Loss :  0.47265415871
Iteration :  2   Loss :  0.0958859713534
Iteration :  3   Loss :  0.0952780535656
Iteration :  4   Loss :  0.0946739899813
Iteration :  5   Loss :  0.0940737561647
Iteration :  6   Loss :  0.093477327835
Iteration :  7   Loss :  0.0928846808655
Iteration :  8   Loss :  0.0922957912823
Iteration :  9   Loss :  0.0917106352636
Iteration :  10   Loss :  0.0911291891384
Iteration :  11   Loss :  0.0905514293861
Iteration :  12   Loss :  0.089977332635
Iteration :  13   Loss :  0.0894068756617
Iteration :  14   Loss :  0.0888400353899
Iteration :  15   Loss :  0.0882767888898
Iteration :  16   Loss :  0.0877171133768
Iteration :  17   Loss :  0.0871609862108
Iteration :  18   Loss :  0.0866083848952
Iteration :  19   Loss :  0.0860592870762
Iteration :  20   Loss :  0.0855136705415
Iteration :  21   Loss :  0.0849715132198
Iteration :  22   Loss :  0.0844327931797
Iteration :  23   Loss :  0.0838974886287
Iteration :  24   Loss :  0.0833655779126
Iteration :  25   Loss :  0.0828370395146
Iteration :  26   Loss :  0.0823118520539
Iteration :  27   Loss :  0.0817899942858
Iteration :  28   Loss :  0.0812714450999
Iteration :  29   Loss :  0.0807561835198
Iteration :  30   Loss :  0.080244188702
Iteration :  31   Loss :  0.0797354399353
Iteration :  32   Loss :  0.0792299166396
Iteration :  33   Loss :  0.0787275983654
Iteration :  34   Loss :  0.078228464793
Iteration :  35   Loss :  0.0777324957311
Iteration :  36   Loss :  0.0772396711169
Iteration :  37   Loss :  0.0767499710145
Iteration :  38   Loss :  0.0762633756145
Iteration :  39   Loss :  0.075779865233
Iteration :  40   Loss :  0.075299420311
Iteration :  41   Loss :  0.0748220214135
Iteration :  42   Loss :  0.0743476492286
Iteration :  43   Loss :  0.073876284567
Iteration :  44   Loss :  0.0734079083609
Iteration :  45   Loss :  0.0729425016635
Iteration :  46   Loss :  0.0724800456481
Iteration :  47   Loss :  0.0720205216074
Iteration :  48   Loss :  0.0715639109526
Iteration :  49   Loss :  0.0711101952128
Iteration :  50   Loss :  0.0706593560343
Iteration :  51   Loss :  0.0702113751796
Iteration :  52   Loss :  0.0697662345269
Iteration :  53   Loss :  0.0693239160693
Iteration :  54   Loss :  0.0688844019141
Iteration :  55   Loss :  0.068447674282
Iteration :  56   Loss :  0.0680137155064
Iteration :  57   Loss :  0.0675825080328
Iteration :  58   Loss :  0.0671540344178
Iteration :  59   Loss :  0.0667282773288
Iteration :  60   Loss :  0.0663052195429
Iteration :  61   Loss :  0.0658848439467
Iteration :  62   Loss :  0.0654671335349
Iteration :  63   Loss :  0.0650520714103
Iteration :  64   Loss :  0.0646396407828
Iteration :  65   Loss :  0.0642298249685
Iteration :  66   Loss :  0.0638226073897
Iteration :  67   Loss :  0.0634179715734
Iteration :  68   Loss :  0.0630159011512
Iteration :  69   Loss :  0.0626163798586
Iteration :  70   Loss :  0.062219391534
Iteration :  71   Loss :  0.0618249201184
Iteration :  72   Loss :  0.0614329496546
Iteration :  73   Loss :  0.0610434642865
Iteration :  74   Loss :  0.0606564482586
Iteration :  75   Loss :  0.0602718859153
Iteration :  76   Loss :  0.0598897617001
Iteration :  77   Loss :  0.0595100601554
Iteration :  78   Loss :  0.0591327659213
Iteration :  79   Loss :  0.0587578637356
Iteration :  80   Loss :  0.0583853384326
Iteration :  81   Loss :  0.0580151749428
Iteration :  82   Loss :  0.0576473582924
Iteration :  83   Loss :  0.0572818736023
Iteration :  84   Loss :  0.056918706088
Iteration :  85   Loss :  0.0565578410584
Iteration :  86   Loss :  0.0561992639159
Iteration :  87   Loss :  0.0558429601552
Iteration :  88   Loss :  0.0554889153631
Iteration :  89   Loss :  0.0551371152176
Iteration :  90   Loss :  0.0547875454877
Iteration :  91   Loss :  0.0544401920326
Iteration :  92   Loss :  0.054095040801
Iteration :  93   Loss :  0.0537520778309
Iteration :  94   Loss :  0.0534112892486
Iteration :  95   Loss :  0.0530726612685
Iteration :  96   Loss :  0.0527361801923
Iteration :  97   Loss :  0.0524018324088
Iteration :  98   Loss :  0.0520696043927
Iteration :  99   Loss :  0.0517394827048
[-0.00218091 -0.0001951  -0.00061895 ...,  0.00149392  0.0011878
  0.00014581]
CROSS VALIDATION 10
Iteration :  0   Loss :  13.4187784348
Iteration :  1   Loss :  11.6358572136
Iteration :  2   Loss :  0.0976629729726
Iteration :  3   Loss :  0.0970437889811
Iteration :  4   Loss :  0.0964285306208
Iteration :  5   Loss :  0.0958171730032
Iteration :  6   Loss :  0.0952096913976
Iteration :  7   Loss :  0.0946060612299
Iteration :  8   Loss :  0.0940062580821
Iteration :  9   Loss :  0.0934102576908
Iteration :  10   Loss :  0.0928180359465
Iteration :  11   Loss :  0.0922295688926
Iteration :  12   Loss :  0.0916448327243
Iteration :  13   Loss :  0.0910638037877
Iteration :  14   Loss :  0.090486458579
Iteration :  15   Loss :  0.0899127737434
Iteration :  16   Loss :  0.0893427260739
Iteration :  17   Loss :  0.088776292511
Iteration :  18   Loss :  0.0882134501412
Iteration :  19   Loss :  0.0876541761964
Iteration :  20   Loss :  0.0870984480525
Iteration :  21   Loss :  0.0865462432294
Iteration :  22   Loss :  0.085997539389
Iteration :  23   Loss :  0.0854523143351
Iteration :  24   Loss :  0.0849105460123
Iteration :  25   Loss :  0.0843722125047
Iteration :  26   Loss :  0.0838372920356
Iteration :  27   Loss :  0.0833057629664
Iteration :  28   Loss :  0.0827776037956
Iteration :  29   Loss :  0.0822527931579
Iteration :  30   Loss :  0.0817313098236
Iteration :  31   Loss :  0.0812131326976
Iteration :  32   Loss :  0.0806982408186
Iteration :  33   Loss :  0.080186613358
Iteration :  34   Loss :  0.0796782296193
Iteration :  35   Loss :  0.0791730690375
Iteration :  36   Loss :  0.0786711111776
Iteration :  37   Loss :  0.0781723357343
Iteration :  38   Loss :  0.0776767225311
Iteration :  39   Loss :  0.0771842515194
Iteration :  40   Loss :  0.0766949027777
Iteration :  41   Loss :  0.0762086565107
Iteration :  42   Loss :  0.0757254930488
Iteration :  43   Loss :  0.0752453928469
Iteration :  44   Loss :  0.0747683364839
Iteration :  45   Loss :  0.0742943046618
Iteration :  46   Loss :  0.0738232782052
Iteration :  47   Loss :  0.0733552380598
Iteration :  48   Loss :  0.0728901652925
Iteration :  49   Loss :  0.0724280410902
Iteration :  50   Loss :  0.0719688467587
Iteration :  51   Loss :  0.0715125637228
Iteration :  52   Loss :  0.0710591735249
Iteration :  53   Loss :  0.0706086578242
Iteration :  54   Loss :  0.0701609983966
Iteration :  55   Loss :  0.069716177133
Iteration :  56   Loss :  0.0692741760397
Iteration :  57   Loss :  0.0688349772367
Iteration :  58   Loss :  0.0683985629574
Iteration :  59   Loss :  0.067964915548
Iteration :  60   Loss :  0.0675340174665
Iteration :  61   Loss :  0.067105851282
Iteration :  62   Loss :  0.0666803996745
Iteration :  63   Loss :  0.0662576454334
Iteration :  64   Loss :  0.0658375714573
Iteration :  65   Loss :  0.0654201607535
Iteration :  66   Loss :  0.0650053964367
Iteration :  67   Loss :  0.0645932617288
Iteration :  68   Loss :  0.0641837399582
Iteration :  69   Loss :  0.0637768145586
Iteration :  70   Loss :  0.0633724690692
Iteration :  71   Loss :  0.0629706871333
Iteration :  72   Loss :  0.0625714524979
Iteration :  73   Loss :  0.0621747490131
Iteration :  74   Loss :  0.0617805606314
Iteration :  75   Loss :  0.0613888714071
Iteration :  76   Loss :  0.0609996654954
Iteration :  77   Loss :  0.0606129271522
Iteration :  78   Loss :  0.060228640733
Iteration :  79   Loss :  0.0598467906926
Iteration :  80   Loss :  0.0594673615844
Iteration :  81   Loss :  0.0590903380596
Iteration :  82   Loss :  0.0587157048669
Iteration :  83   Loss :  0.0583434468514
Iteration :  84   Loss :  0.0579735489546
Iteration :  85   Loss :  0.0576059962132
Iteration :  86   Loss :  0.057240773759
Iteration :  87   Loss :  0.056877866818
Iteration :  88   Loss :  0.0565172607098
Iteration :  89   Loss :  0.056158940847
Iteration :  90   Loss :  0.0558028927348
Iteration :  91   Loss :  0.0554491019704
Iteration :  92   Loss :  0.0550975542422
Iteration :  93   Loss :  0.0547482353292
Iteration :  94   Loss :  0.0544011311007
Iteration :  95   Loss :  0.0540562275158
Iteration :  96   Loss :  0.0537135106221
Iteration :  97   Loss :  0.0533729665562
Iteration :  98   Loss :  0.0530345815423
Iteration :  99   Loss :  0.0526983418919
[ -1.44690747e-03  -9.18866210e-05  -1.03339046e-03 ...,   1.69332315e-03
   7.16957435e-04   2.91453590e-04]
CROSS VALIDATION 11
Iteration :  0   Loss :  20.8456252524
Iteration :  1   Loss :  1.39730777833
Iteration :  2   Loss :  0.756860539193
Iteration :  3   Loss :  0.097076265182
Iteration :  4   Loss :  0.0964608009223
Iteration :  5   Loss :  0.0958492387107
Iteration :  6   Loss :  0.0952415538083
Iteration :  7   Loss :  0.0946377216327
Iteration :  8   Loss :  0.0940377177577
Iteration :  9   Loss :  0.0934415179118
Iteration :  10   Loss :  0.0928490979775
Iteration :  11   Loss :  0.09226043399
Iteration :  12   Loss :  0.0916755021368
Iteration :  13   Loss :  0.0910942787559
Iteration :  14   Loss :  0.0905167403357
Iteration :  15   Loss :  0.0899428635135
Iteration :  16   Loss :  0.0893726250746
Iteration :  17   Loss :  0.0888060019519
Iteration :  18   Loss :  0.088242971224
Iteration :  19   Loss :  0.0876835101152
Iteration :  20   Loss :  0.0871275959941
Iteration :  21   Loss :  0.0865752063728
Iteration :  22   Loss :  0.0860263189059
Iteration :  23   Loss :  0.0854809113896
Iteration :  24   Loss :  0.0849389617612
Iteration :  25   Loss :  0.0844004480976
Iteration :  26   Loss :  0.0838653486147
Iteration :  27   Loss :  0.0833336416665
Iteration :  28   Loss :  0.0828053057445
Iteration :  29   Loss :  0.0822803194762
Iteration :  30   Loss :  0.0817586616249
Iteration :  31   Loss :  0.0812403110883
Iteration :  32   Loss :  0.080725246898
Iteration :  33   Loss :  0.0802134482187
Iteration :  34   Loss :  0.0797048943468
Iteration :  35   Loss :  0.0791995647104
Iteration :  36   Loss :  0.0786974388677
Iteration :  37   Loss :  0.0781984965067
Iteration :  38   Loss :  0.077702717444
Iteration :  39   Loss :  0.0772100816244
Iteration :  40   Loss :  0.0767205691196
Iteration :  41   Loss :  0.0762341601278
Iteration :  42   Loss :  0.0757508349727
Iteration :  43   Loss :  0.0752705741028
Iteration :  44   Loss :  0.0747933580905
Iteration :  45   Loss :  0.0743191676313
Iteration :  46   Loss :  0.0738479835432
Iteration :  47   Loss :  0.0733797867658
Iteration :  48   Loss :  0.0729145583595
Iteration :  49   Loss :  0.0724522795049
Iteration :  50   Loss :  0.0719929315018
Iteration :  51   Loss :  0.0715364957684
Iteration :  52   Loss :  0.0710829538411
Iteration :  53   Loss :  0.0706322873731
Iteration :  54   Loss :  0.0701844781339
Iteration :  55   Loss :  0.0697395080087
Iteration :  56   Loss :  0.0692973589974
Iteration :  57   Loss :  0.0688580132143
Iteration :  58   Loss :  0.0684214528867
Iteration :  59   Loss :  0.067987660355
Iteration :  60   Loss :  0.0675566180712
Iteration :  61   Loss :  0.0671283085988
Iteration :  62   Loss :  0.0667027146116
Iteration :  63   Loss :  0.0662798188936
Iteration :  64   Loss :  0.0658596043377
Iteration :  65   Loss :  0.0654420539452
Iteration :  66   Loss :  0.0650271508254
Iteration :  67   Loss :  0.0646148781946
Iteration :  68   Loss :  0.0642052193753
Iteration :  69   Loss :  0.0637981577961
Iteration :  70   Loss :  0.0633936769904
Iteration :  71   Loss :  0.0629917605961
Iteration :  72   Loss :  0.0625923923548
Iteration :  73   Loss :  0.0621955561112
Iteration :  74   Loss :  0.0618012358123
Iteration :  75   Loss :  0.0614094155072
Iteration :  76   Loss :  0.0610200793458
Iteration :  77   Loss :  0.0606332115786
Iteration :  78   Loss :  0.060248796556
Iteration :  79   Loss :  0.0598668187275
Iteration :  80   Loss :  0.0594872626414
Iteration :  81   Loss :  0.0591101129438
Iteration :  82   Loss :  0.0587353543781
Iteration :  83   Loss :  0.0583629717846
Iteration :  84   Loss :  0.0579929500995
Iteration :  85   Loss :  0.0576252743548
Iteration :  86   Loss :  0.0572599296771
Iteration :  87   Loss :  0.0568969012874
Iteration :  88   Loss :  0.0565361745004
Iteration :  89   Loss :  0.056177734724
Iteration :  90   Loss :  0.0558215674586
Iteration :  91   Loss :  0.0554676582963
Iteration :  92   Loss :  0.0551159929208
Iteration :  93   Loss :  0.0547665571064
Iteration :  94   Loss :  0.0544193367178
Iteration :  95   Loss :  0.0540743177091
Iteration :  96   Loss :  0.0537314861235
Iteration :  97   Loss :  0.0533908280928
Iteration :  98   Loss :  0.0530523298365
Iteration :  99   Loss :  0.0527159776618
[-0.0021486  -0.00021614 -0.00038713 ...,  0.00162779  0.00098532
  0.00014641]
CROSS VALIDATION 12
Iteration :  0   Loss :  19.2562364799
Iteration :  1   Loss :  4.52452054812
Iteration :  2   Loss :  0.104952278273
Iteration :  3   Loss :  0.104286880031
Iteration :  4   Loss :  0.103625700418
Iteration :  5   Loss :  0.102968712689
Iteration :  6   Loss :  0.102315890268
Iteration :  7   Loss :  0.101667206745
Iteration :  8   Loss :  0.101022635881
Iteration :  9   Loss :  0.100382151601
Iteration :  10   Loss :  0.0997457279964
Iteration :  11   Loss :  0.0991133393222
Iteration :  12   Loss :  0.098484959997
Iteration :  13   Loss :  0.0978605646015
Iteration :  14   Loss :  0.0972401278775
Iteration :  15   Loss :  0.0966236247271
Iteration :  16   Loss :  0.0960110302114
Iteration :  17   Loss :  0.0954023195496
Iteration :  18   Loss :  0.0947974681179
Iteration :  19   Loss :  0.094196451449
Iteration :  20   Loss :  0.0935992452303
Iteration :  21   Loss :  0.0930058253036
Iteration :  22   Loss :  0.0924161676637
Iteration :  23   Loss :  0.0918302484578
Iteration :  24   Loss :  0.0912480439841
Iteration :  25   Loss :  0.0906695306911
Iteration :  26   Loss :  0.0900946851769
Iteration :  27   Loss :  0.0895234841876
Iteration :  28   Loss :  0.0889559046169
Iteration :  29   Loss :  0.088391923505
Iteration :  30   Loss :  0.0878315180376
Iteration :  31   Loss :  0.0872746655452
Iteration :  32   Loss :  0.0867213435018
Iteration :  33   Loss :  0.0861715295243
Iteration :  34   Loss :  0.0856252013717
Iteration :  35   Loss :  0.0850823369437
Iteration :  36   Loss :  0.0845429142803
Iteration :  37   Loss :  0.0840069115607
Iteration :  38   Loss :  0.0834743071024
Iteration :  39   Loss :  0.0829450793604
Iteration :  40   Loss :  0.0824192069264
Iteration :  41   Loss :  0.0818966685275
Iteration :  42   Loss :  0.0813774430261
Iteration :  43   Loss :  0.0808615094183
Iteration :  44   Loss :  0.0803488468336
Iteration :  45   Loss :  0.0798394345335
Iteration :  46   Loss :  0.0793332519112
Iteration :  47   Loss :  0.0788302784907
Iteration :  48   Loss :  0.0783304939254
Iteration :  49   Loss :  0.0778338779981
Iteration :  50   Loss :  0.0773404106196
Iteration :  51   Loss :  0.076850071828
Iteration :  52   Loss :  0.076362841788
Iteration :  53   Loss :  0.0758787007903
Iteration :  54   Loss :  0.0753976292501
Iteration :  55   Loss :  0.0749196077071
Iteration :  56   Loss :  0.0744446168244
Iteration :  57   Loss :  0.0739726373875
Iteration :  58   Loss :  0.0735036503038
Iteration :  59   Loss :  0.0730376366017
Iteration :  60   Loss :  0.0725745774301
Iteration :  61   Loss :  0.0721144540572
Iteration :  62   Loss :  0.0716572478699
Iteration :  63   Loss :  0.0712029403734
Iteration :  64   Loss :  0.0707515131898
Iteration :  65   Loss :  0.070302948058
Iteration :  66   Loss :  0.0698572268326
Iteration :  67   Loss :  0.0694143314831
Iteration :  68   Loss :  0.0689742440935
Iteration :  69   Loss :  0.0685369468613
Iteration :  70   Loss :  0.0681024220969
Iteration :  71   Loss :  0.0676706522228
Iteration :  72   Loss :  0.067241619773
Iteration :  73   Loss :  0.0668153073921
Iteration :  74   Loss :  0.066391697835
Iteration :  75   Loss :  0.0659707739658
Iteration :  76   Loss :  0.065552518757
Iteration :  77   Loss :  0.0651369152895
Iteration :  78   Loss :  0.0647239467511
Iteration :  79   Loss :  0.0643135964364
Iteration :  80   Loss :  0.0639058477458
Iteration :  81   Loss :  0.063500684185
Iteration :  82   Loss :  0.0630980893642
Iteration :  83   Loss :  0.0626980469977
Iteration :  84   Loss :  0.0623005409028
Iteration :  85   Loss :  0.0619055549996
Iteration :  86   Loss :  0.06151307331
Iteration :  87   Loss :  0.0611230799574
Iteration :  88   Loss :  0.0607355591655
Iteration :  89   Loss :  0.0603504952584
Iteration :  90   Loss :  0.0599678726594
Iteration :  91   Loss :  0.0595876758906
Iteration :  92   Loss :  0.0592098895722
Iteration :  93   Loss :  0.058834498422
Iteration :  94   Loss :  0.0584614872544
Iteration :  95   Loss :  0.0580908409806
Iteration :  96   Loss :  0.0577225446069
Iteration :  97   Loss :  0.057356583235
Iteration :  98   Loss :  0.056992942061
Iteration :  99   Loss :  0.0566316063749
[ -3.23262221e-03  -4.12832276e-04  -1.33727833e-03 ...,   1.51706631e-03
   9.24030426e-04   7.31888217e-05]
CROSS VALIDATION 13
Iteration :  0   Loss :  20.8472010439
Iteration :  1   Loss :  1.39692338273
Iteration :  2   Loss :  0.757525429954
Iteration :  3   Loss :  0.0970767512413
Iteration :  4   Loss :  0.0964612839
Iteration :  5   Loss :  0.0958497186264
Iteration :  6   Loss :  0.0952420306812
Iteration :  7   Loss :  0.0946381954822
Iteration :  8   Loss :  0.094038188603
Iteration :  9   Loss :  0.093441985772
Iteration :  10   Loss :  0.0928495628714
Iteration :  11   Loss :  0.0922608959366
Iteration :  12   Loss :  0.0916759611546
Iteration :  13   Loss :  0.0910947348635
Iteration :  14   Loss :  0.0905171935516
Iteration :  15   Loss :  0.0899433138559
Iteration :  16   Loss :  0.089373072562
Iteration :  17   Loss :  0.0888064466021
Iteration :  18   Loss :  0.0882434130551
Iteration :  19   Loss :  0.0876839491452
Iteration :  20   Loss :  0.0871280322406
Iteration :  21   Loss :  0.0865756398535
Iteration :  22   Loss :  0.0860267496383
Iteration :  23   Loss :  0.0854813393912
Iteration :  24   Loss :  0.0849393870493
Iteration :  25   Loss :  0.0844008706893
Iteration :  26   Loss :  0.0838657685271
Iteration :  27   Loss :  0.0833340589168
Iteration :  28   Loss :  0.0828057203494
Iteration :  29   Loss :  0.0822807314525
Iteration :  30   Loss :  0.0817590709892
Iteration :  31   Loss :  0.0812407178572
Iteration :  32   Loss :  0.0807256510881
Iteration :  33   Loss :  0.0802138498461
Iteration :  34   Loss :  0.079705293428
Iteration :  35   Loss :  0.0791999612614
Iteration :  36   Loss :  0.0786978329046
Iteration :  37   Loss :  0.0781988880453
Iteration :  38   Loss :  0.0777031065003
Iteration :  39   Loss :  0.0772104682141
Iteration :  40   Loss :  0.0767209532583
Iteration :  41   Loss :  0.0762345418311
Iteration :  42   Loss :  0.075751214256
Iteration :  43   Loss :  0.0752709509814
Iteration :  44   Loss :  0.0747937325796
Iteration :  45   Loss :  0.0743195397462
Iteration :  46   Loss :  0.0738483532989
Iteration :  47   Loss :  0.0733801541772
Iteration :  48   Loss :  0.0729149234416
Iteration :  49   Loss :  0.0724526422723
Iteration :  50   Loss :  0.0719932919692
Iteration :  51   Loss :  0.0715368539505
Iteration :  52   Loss :  0.0710833097524
Iteration :  53   Loss :  0.0706326410278
Iteration :  54   Loss :  0.0701848295465
Iteration :  55   Loss :  0.0697398571933
Iteration :  56   Loss :  0.0692977059682
Iteration :  57   Loss :  0.0688583579853
Iteration :  58   Loss :  0.0684217954719
Iteration :  59   Loss :  0.0679880007681
Iteration :  60   Loss :  0.0675569563261
Iteration :  61   Loss :  0.0671286447091
Iteration :  62   Loss :  0.0667030485911
Iteration :  63   Loss :  0.0662801507556
Iteration :  64   Loss :  0.0658599340957
Iteration :  65   Loss :  0.0654423816126
Iteration :  66   Loss :  0.0650274764154
Iteration :  67   Loss :  0.0646152017203
Iteration :  68   Loss :  0.0642055408499
Iteration :  69   Loss :  0.0637984772325
Iteration :  70   Loss :  0.0633939944016
Iteration :  71   Loss :  0.0629920759949
Iteration :  72   Loss :  0.0625927057539
Iteration :  73   Loss :  0.0621958675233
Iteration :  74   Loss :  0.0618015452501
Iteration :  75   Loss :  0.0614097229832
Iteration :  76   Loss :  0.0610203848723
Iteration :  77   Loss :  0.0606335151681
Iteration :  78   Loss :  0.0602490982207
Iteration :  79   Loss :  0.0598671184797
Iteration :  80   Loss :  0.0594875604932
Iteration :  81   Loss :  0.0591104089072
Iteration :  82   Loss :  0.0587356484651
Iteration :  83   Loss :  0.0583632640071
Iteration :  84   Loss :  0.0579932404693
Iteration :  85   Loss :  0.0576255628836
Iteration :  86   Loss :  0.0572602163766
Iteration :  87   Loss :  0.0568971861692
Iteration :  88   Loss :  0.0565364575761
Iteration :  89   Loss :  0.0561780160051
Iteration :  90   Loss :  0.0558218469563
Iteration :  91   Loss :  0.055467936022
Iteration :  92   Loss :  0.0551162688857
Iteration :  93   Loss :  0.0547668313217
Iteration :  94   Loss :  0.0544196091946
Iteration :  95   Loss :  0.0540745884583
Iteration :  96   Loss :  0.0537317551562
Iteration :  97   Loss :  0.0533910954198
Iteration :  98   Loss :  0.0530525954687
Iteration :  99   Loss :  0.0527162416099
[-0.00214857 -0.00021613 -0.00038713 ...,  0.00162779  0.00098532
  0.00014641]
CROSS VALIDATION 14
Iteration :  0   Loss :  20.8472010439
Iteration :  1   Loss :  0.0878468279498
Iteration :  2   Loss :  0.0872898783924
Iteration :  3   Loss :  0.0867364598994
Iteration :  4   Loss :  0.0861865500839
Iteration :  5   Loss :  0.0856401267008
Iteration :  6   Loss :  0.0850971676461
Iteration :  7   Loss :  0.0845576509559
Iteration :  8   Loss :  0.0840215548057
Iteration :  9   Loss :  0.0834888575091
Iteration :  10   Loss :  0.0829595375175
Iteration :  11   Loss :  0.0824335734186
Iteration :  12   Loss :  0.0819109439361
Iteration :  13   Loss :  0.0813916279285
Iteration :  14   Loss :  0.0808756043883
Iteration :  15   Loss :  0.0803628524413
Iteration :  16   Loss :  0.0798533513456
Iteration :  17   Loss :  0.0793470804906
Iteration :  18   Loss :  0.0788440193968
Iteration :  19   Loss :  0.0783441477141
Iteration :  20   Loss :  0.0778474452217
Iteration :  21   Loss :  0.0773538918268
Iteration :  22   Loss :  0.0768634675643
Iteration :  23   Loss :  0.0763761525952
Iteration :  24   Loss :  0.0758919272068
Iteration :  25   Loss :  0.0754107718111
Iteration :  26   Loss :  0.0749326669442
Iteration :  27   Loss :  0.0744575932658
Iteration :  28   Loss :  0.0739855315581
Iteration :  29   Loss :  0.0735164627253
Iteration :  30   Loss :  0.0730503677924
Iteration :  31   Loss :  0.072587227905
Iteration :  32   Loss :  0.0721270243279
Iteration :  33   Loss :  0.071669738445
Iteration :  34   Loss :  0.0712153517582
Iteration :  35   Loss :  0.0707638458863
Iteration :  36   Loss :  0.0703152025651
Iteration :  37   Loss :  0.069869403646
Iteration :  38   Loss :  0.0694264310955
Iteration :  39   Loss :  0.0689862669942
Iteration :  40   Loss :  0.0685488935367
Iteration :  41   Loss :  0.0681142930303
Iteration :  42   Loss :  0.0676824478944
Iteration :  43   Loss :  0.0672533406599
Iteration :  44   Loss :  0.0668269539686
Iteration :  45   Loss :  0.0664032705721
Iteration :  46   Loss :  0.0659822733316
Iteration :  47   Loss :  0.0655639452168
Iteration :  48   Loss :  0.0651482693054
Iteration :  49   Loss :  0.0647352287825
Iteration :  50   Loss :  0.0643248069396
Iteration :  51   Loss :  0.0639169871743
Iteration :  52   Loss :  0.0635117529894
Iteration :  53   Loss :  0.0631090879924
Iteration :  54   Loss :  0.0627089758945
Iteration :  55   Loss :  0.0623114005103
Iteration :  56   Loss :  0.0619163457571
Iteration :  57   Loss :  0.061523795654
Iteration :  58   Loss :  0.0611337343216
Iteration :  59   Loss :  0.060746145981
Iteration :  60   Loss :  0.0603610149534
Iteration :  61   Loss :  0.0599783256595
Iteration :  62   Loss :  0.0595980626186
Iteration :  63   Loss :  0.0592202104483
Iteration :  64   Loss :  0.0588447538636
Iteration :  65   Loss :  0.0584716776765
Iteration :  66   Loss :  0.0581009667953
Iteration :  67   Loss :  0.0577326062238
Iteration :  68   Loss :  0.0573665810612
Iteration :  69   Loss :  0.0570028765009
Iteration :  70   Loss :  0.0566414778303
Iteration :  71   Loss :  0.0562823704301
Iteration :  72   Loss :  0.0559255397734
Iteration :  73   Loss :  0.0555709714259
Iteration :  74   Loss :  0.0552186510443
Iteration :  75   Loss :  0.0548685643767
Iteration :  76   Loss :  0.0545206972612
Iteration :  77   Loss :  0.0541750356259
Iteration :  78   Loss :  0.0538315654879
Iteration :  79   Loss :  0.0534902729532
Iteration :  80   Loss :  0.0531511442159
Iteration :  81   Loss :  0.0528141655572
Iteration :  82   Loss :  0.0524793233459
Iteration :  83   Loss :  0.0521466040367
Iteration :  84   Loss :  0.0518159941704
Iteration :  85   Loss :  0.0514874803733
Iteration :  86   Loss :  0.0511610493561
Iteration :  87   Loss :  0.050836687914
Iteration :  88   Loss :  0.0505143829259
Iteration :  89   Loss :  0.050194121354
Iteration :  90   Loss :  0.0498758902428
Iteration :  91   Loss :  0.0495596767193
Iteration :  92   Loss :  0.0492454679919
Iteration :  93   Loss :  0.0489332513502
Iteration :  94   Loss :  0.0486230141645
Iteration :  95   Loss :  0.0483147438848
Iteration :  96   Loss :  0.0480084280411
Iteration :  97   Loss :  0.0477040542421
Iteration :  98   Loss :  0.0474016101754
Iteration :  99   Loss :  0.0471010836063
[ -1.95778660e-03  -2.54399435e-04  -3.64684905e-04 ...,   1.20537471e-03
   5.73510801e-04  -3.63272671e-07]
CROSS VALIDATION 15
Iteration :  0   Loss :  20.8472010439
Iteration :  1   Loss :  1.39692338273
Iteration :  2   Loss :  0.75763283589
Iteration :  3   Loss :  0.0970773197831
Iteration :  4   Loss :  0.0964618488373
Iteration :  5   Loss :  0.0958502799819
Iteration :  6   Loss :  0.0952425884778
Iteration :  7   Loss :  0.0946387497424
Iteration :  8   Loss :  0.0940387393491
Iteration :  9   Loss :  0.0934425330264
Iteration :  10   Loss :  0.0928501066562
Iteration :  11   Loss :  0.0922614362738
Iteration :  12   Loss :  0.091676498066
Iteration :  13   Loss :  0.0910952683709
Iteration :  14   Loss :  0.0905177236766
Iteration :  15   Loss :  0.0899438406199
Iteration :  16   Loss :  0.0893735959862
Iteration :  17   Loss :  0.0888069667079
Iteration :  18   Loss :  0.0882439298635
Iteration :  19   Loss :  0.0876844626769
Iteration :  20   Loss :  0.0871285425166
Iteration :  21   Loss :  0.0865761468943
Iteration :  22   Loss :  0.0860272534644
Iteration :  23   Loss :  0.0854818400231
Iteration :  24   Loss :  0.0849398845071
Iteration :  25   Loss :  0.0844013649933
Iteration :  26   Loss :  0.0838662596972
Iteration :  27   Loss :  0.0833345469728
Iteration :  28   Loss :  0.0828062053111
Iteration :  29   Loss :  0.0822812133396
Iteration :  30   Loss :  0.0817595498212
Iteration :  31   Loss :  0.0812411936534
Iteration :  32   Loss :  0.0807261238677
Iteration :  33   Loss :  0.0802143196283
Iteration :  34   Loss :  0.0797057602317
Iteration :  35   Loss :  0.0792004251056
Iteration :  36   Loss :  0.078698293808
Iteration :  37   Loss :  0.0781993460266
Iteration :  38   Loss :  0.077703561578
Iteration :  39   Loss :  0.0772109204066
Iteration :  40   Loss :  0.0767214025839
Iteration :  41   Loss :  0.0762349883079
Iteration :  42   Loss :  0.0757516579022
Iteration :  43   Loss :  0.0752713918149
Iteration :  44   Loss :  0.0747941706182
Iteration :  45   Loss :  0.0743199750076
Iteration :  46   Loss :  0.0738487858007
Iteration :  47   Loss :  0.073380583937
Iteration :  48   Loss :  0.0729153504767
Iteration :  49   Loss :  0.0724530666001
Iteration :  50   Loss :  0.0719937136067
Iteration :  51   Loss :  0.0715372729148
Iteration :  52   Loss :  0.0710837260604
Iteration :  53   Loss :  0.0706330546965
Iteration :  54   Loss :  0.0701852405925
Iteration :  55   Loss :  0.0697402656333
Iteration :  56   Loss :  0.0692981118187
Iteration :  57   Loss :  0.0688587612626
Iteration :  58   Loss :  0.0684221961924
Iteration :  59   Loss :  0.0679883989481
Iteration :  60   Loss :  0.0675573519817
Iteration :  61   Loss :  0.0671290378562
Iteration :  62   Loss :  0.0667034392456
Iteration :  63   Loss :  0.0662805389334
Iteration :  64   Loss :  0.0658603198124
Iteration :  65   Loss :  0.0654427648838
Iteration :  66   Loss :  0.0650278572567
Iteration :  67   Loss :  0.0646155801471
Iteration :  68   Loss :  0.0642059168774
Iteration :  69   Loss :  0.0637988508761
Iteration :  70   Loss :  0.0633943656762
Iteration :  71   Loss :  0.0629924449156
Iteration :  72   Loss :  0.0625930723357
Iteration :  73   Loss :  0.062196231781
Iteration :  74   Loss :  0.0618019071984
Iteration :  75   Loss :  0.0614100826367
Iteration :  76   Loss :  0.0610207422457
Iteration :  77   Loss :  0.0606338702757
Iteration :  78   Loss :  0.0602494510769
Iteration :  79   Loss :  0.0598674690988
Iteration :  80   Loss :  0.0594879088893
Iteration :  81   Loss :  0.0591107550945
Iteration :  82   Loss :  0.0587359924576
Iteration :  83   Loss :  0.0583636058186
Iteration :  84   Loss :  0.0579935801138
Iteration :  85   Loss :  0.0576259003748
Iteration :  86   Loss :  0.0572605517281
Iteration :  87   Loss :  0.0568975193946
Iteration :  88   Loss :  0.0565367886888
Iteration :  89   Loss :  0.0561783450185
Iteration :  90   Loss :  0.0558221738837
Iteration :  91   Loss :  0.0554682608767
Iteration :  92   Loss :  0.0551165916808
Iteration :  93   Loss :  0.0547671520703
Iteration :  94   Loss :  0.0544199279096
Iteration :  95   Loss :  0.0540749051527
Iteration :  96   Loss :  0.0537320698428
Iteration :  97   Loss :  0.0533914081112
Iteration :  98   Loss :  0.0530529061777
Iteration :  99   Loss :  0.052716550349
[-0.0021486  -0.00021614 -0.00038714 ...,  0.00162781  0.00098534
  0.00014641]
CROSS VALIDATION 16
Iteration :  0   Loss :  20.8472010439
Iteration :  1   Loss :  0.0833755018971
Iteration :  2   Loss :  0.0828469005809
Iteration :  3   Loss :  0.082321650601
Iteration :  4   Loss :  0.08179973071
Iteration :  5   Loss :  0.0812811197951
Iteration :  6   Loss :  0.0807657968774
Iteration :  7   Loss :  0.0802537411108
Iteration :  8   Loss :  0.0797449317818
Iteration :  9   Loss :  0.0792393483077
Iteration :  10   Loss :  0.0787369702367
Iteration :  11   Loss :  0.0782377772465
Iteration :  12   Loss :  0.0777417491436
Iteration :  13   Loss :  0.0772488658627
Iteration :  14   Loss :  0.0767591074656
Iteration :  15   Loss :  0.0762724541404
Iteration :  16   Loss :  0.075788886201
Iteration :  17   Loss :  0.075308384086
Iteration :  18   Loss :  0.074830928358
Iteration :  19   Loss :  0.074356499703
Iteration :  20   Loss :  0.0738850789293
Iteration :  21   Loss :  0.0734166469669
Iteration :  22   Loss :  0.0729511848667
Iteration :  23   Loss :  0.0724886737998
Iteration :  24   Loss :  0.0720290950565
Iteration :  25   Loss :  0.071572430046
Iteration :  26   Loss :  0.0711186602951
Iteration :  27   Loss :  0.0706677674478
Iteration :  28   Loss :  0.0702197332647
Iteration :  29   Loss :  0.0697745396217
Iteration :  30   Loss :  0.0693321685097
Iteration :  31   Loss :  0.068892602034
Iteration :  32   Loss :  0.0684558224131
Iteration :  33   Loss :  0.0680218119783
Iteration :  34   Loss :  0.0675905531729
Iteration :  35   Loss :  0.0671620285517
Iteration :  36   Loss :  0.0667362207798
Iteration :  37   Loss :  0.0663131126324
Iteration :  38   Loss :  0.065892686994
Iteration :  39   Loss :  0.0654749268572
Iteration :  40   Loss :  0.0650598153229
Iteration :  41   Loss :  0.0646473355989
Iteration :  42   Loss :  0.0642374709994
Iteration :  43   Loss :  0.0638302049447
Iteration :  44   Loss :  0.0634255209598
Iteration :  45   Loss :  0.0630234026745
Iteration :  46   Loss :  0.0626238338222
Iteration :  47   Loss :  0.0622267982394
Iteration :  48   Loss :  0.0618322798652
Iteration :  49   Loss :  0.0614402627406
Iteration :  50   Loss :  0.0610507310074
Iteration :  51   Loss :  0.0606636689085
Iteration :  52   Loss :  0.0602790607862
Iteration :  53   Loss :  0.0598968910823
Iteration :  54   Loss :  0.0595171443372
Iteration :  55   Loss :  0.0591398051894
Iteration :  56   Loss :  0.0587648583747
Iteration :  57   Loss :  0.0583922887256
Iteration :  58   Loss :  0.0580220811709
Iteration :  59   Loss :  0.0576542207349
Iteration :  60   Loss :  0.0572886925369
Iteration :  61   Loss :  0.0569254817905
Iteration :  62   Loss :  0.0565645738029
Iteration :  63   Loss :  0.0562059539748
Iteration :  64   Loss :  0.055849607799
Iteration :  65   Loss :  0.0554955208608
Iteration :  66   Loss :  0.0551436788364
Iteration :  67   Loss :  0.0547940674931
Iteration :  68   Loss :  0.0544466726884
Iteration :  69   Loss :  0.0541014803695
Iteration :  70   Loss :  0.0537584765724
Iteration :  71   Loss :  0.053417647422
Iteration :  72   Loss :  0.053078979131
Iteration :  73   Loss :  0.0527424579996
Iteration :  74   Loss :  0.0524080704147
Iteration :  75   Loss :  0.0520758028496
Iteration :  76   Loss :  0.0517456418635
Iteration :  77   Loss :  0.0514175741005
Iteration :  78   Loss :  0.0510915862896
Iteration :  79   Loss :  0.0507676652439
Iteration :  80   Loss :  0.05044579786
Iteration :  81   Loss :  0.0501259711179
Iteration :  82   Loss :  0.0498081720797
Iteration :  83   Loss :  0.0494923878899
Iteration :  84   Loss :  0.0491786057742
Iteration :  85   Loss :  0.0488668130395
Iteration :  86   Loss :  0.0485569970732
Iteration :  87   Loss :  0.0482491453424
Iteration :  88   Loss :  0.0479432453939
Iteration :  89   Loss :  0.0476392848534
Iteration :  90   Loss :  0.0473372514251
Iteration :  91   Loss :  0.047037132891
Iteration :  92   Loss :  0.0467389171107
Iteration :  93   Loss :  0.0464425920207
Iteration :  94   Loss :  0.046148145634
Iteration :  95   Loss :  0.0458555660397
Iteration :  96   Loss :  0.0455648414022
Iteration :  97   Loss :  0.0452759599611
Iteration :  98   Loss :  0.0449889100306
Iteration :  99   Loss :  0.0447036799988
[ -1.65457163e-03  -2.36714617e-04  -2.68221464e-04 ...,   1.19454000e-03
   5.76643535e-04   7.26981702e-05]
CROSS VALIDATION 17
Iteration :  0   Loss :  20.8472010439
Iteration :  1   Loss :  1.39711085408
Iteration :  2   Loss :  0.757922141525
Iteration :  3   Loss :  0.0970778394312
Iteration :  4   Loss :  0.0964623651908
Iteration :  5   Loss :  0.0958507930617
Iteration :  6   Loss :  0.0952430983046
Iteration :  7   Loss :  0.0946392563369
Iteration :  8   Loss :  0.0940392427319
Iteration :  9   Loss :  0.0934430332177
Iteration :  10   Loss :  0.0928506036763
Iteration :  11   Loss :  0.0922619301427
Iteration :  12   Loss :  0.0916769888038
Iteration :  13   Loss :  0.0910957559975
Iteration :  14   Loss :  0.0905182082115
Iteration :  15   Loss :  0.089944322083
Iteration :  16   Loss :  0.0893740743968
Iteration :  17   Loss :  0.0888074420853
Iteration :  18   Loss :  0.088244402227
Iteration :  19   Loss :  0.0876849320456
Iteration :  20   Loss :  0.0871290089095
Iteration :  21   Loss :  0.0865766103303
Iteration :  22   Loss :  0.0860277139622
Iteration :  23   Loss :  0.0854822976014
Iteration :  24   Loss :  0.0849403391844
Iteration :  25   Loss :  0.0844018167878
Iteration :  26   Loss :  0.0838667086274
Iteration :  27   Loss :  0.0833349930568
Iteration :  28   Loss :  0.0828066485669
Iteration :  29   Loss :  0.0822816537851
Iteration :  30   Loss :  0.0817599874743
Iteration :  31   Loss :  0.0812416285318
Iteration :  32   Loss :  0.0807265559889
Iteration :  33   Loss :  0.0802147490099
Iteration :  34   Loss :  0.0797061868911
Iteration :  35   Loss :  0.0792008490599
Iteration :  36   Loss :  0.0786987150744
Iteration :  37   Loss :  0.0781997646222
Iteration :  38   Loss :  0.0777039775197
Iteration :  39   Loss :  0.0772113337112
Iteration :  40   Loss :  0.0767218132682
Iteration :  41   Loss :  0.0762353963885
Iteration :  42   Loss :  0.0757520633955
Iteration :  43   Loss :  0.0752717947373
Iteration :  44   Loss :  0.0747945709862
Iteration :  45   Loss :  0.0743203728372
Iteration :  46   Loss :  0.0738491811081
Iteration :  47   Loss :  0.0733809767381
Iteration :  48   Loss :  0.0729157407875
Iteration :  49   Loss :  0.0724534544362
Iteration :  50   Loss :  0.071994098984
Iteration :  51   Loss :  0.0715376558488
Iteration :  52   Loss :  0.0710841065666
Iteration :  53   Loss :  0.0706334327903
Iteration :  54   Loss :  0.0701856162891
Iteration :  55   Loss :  0.069740638948
Iteration :  56   Loss :  0.0692984827666
Iteration :  57   Loss :  0.0688591298587
Iteration :  58   Loss :  0.0684225624516
Iteration :  59   Loss :  0.0679887628852
Iteration :  60   Loss :  0.0675577136114
Iteration :  61   Loss :  0.0671293971932
Iteration :  62   Loss :  0.0667037963044
Iteration :  63   Loss :  0.0662808937285
Iteration :  64   Loss :  0.0658606723581
Iteration :  65   Loss :  0.0654431151944
Iteration :  66   Loss :  0.0650282053462
Iteration :  67   Loss :  0.0646159260297
Iteration :  68   Loss :  0.0642062605672
Iteration :  69   Loss :  0.0637991923868
Iteration :  70   Loss :  0.0633947050218
Iteration :  71   Loss :  0.0629927821097
Iteration :  72   Loss :  0.062593407392
Iteration :  73   Loss :  0.062196564713
Iteration :  74   Loss :  0.0618022380197
Iteration :  75   Loss :  0.0614104113605
Iteration :  76   Loss :  0.0610210688854
Iteration :  77   Loss :  0.0606341948445
Iteration :  78   Loss :  0.060249773588
Iteration :  79   Loss :  0.0598677895652
Iteration :  80   Loss :  0.0594882273239
Iteration :  81   Loss :  0.0591110715102
Iteration :  82   Loss :  0.0587363068672
Iteration :  83   Loss :  0.0583639182349
Iteration :  84   Loss :  0.0579938905493
Iteration :  85   Loss :  0.0576262088421
Iteration :  86   Loss :  0.0572608582397
Iteration :  87   Loss :  0.056897823963
Iteration :  88   Loss :  0.0565370913262
Iteration :  89   Loss :  0.0561786457372
Iteration :  90   Loss :  0.0558224726959
Iteration :  91   Loss :  0.0554685577944
Iteration :  92   Loss :  0.0551168867161
Iteration :  93   Loss :  0.054767445235
Iteration :  94   Loss :  0.0544202192157
Iteration :  95   Loss :  0.0540751946119
Iteration :  96   Loss :  0.0537323574667
Iteration :  97   Loss :  0.0533916939117
Iteration :  98   Loss :  0.0530531901662
Iteration :  99   Loss :  0.052716832537
[-0.00214862 -0.00021614 -0.00038714 ...,  0.00162781  0.00098534
  0.00014641]
CROSS VALIDATION 18
Iteration :  0   Loss :  24.2416957509
Iteration :  1   Loss :  15.1436193985
Iteration :  2   Loss :  0.101963039676
Iteration :  3   Loss :  0.101316593229
Iteration :  4   Loss :  0.100674245257
Iteration :  5   Loss :  0.100035969776
Iteration :  6   Loss :  0.0994017409669
Iteration :  7   Loss :  0.098771533173
Iteration :  8   Loss :  0.0981453209014
Iteration :  9   Loss :  0.0975230788204
Iteration :  10   Loss :  0.0969047817588
Iteration :  11   Loss :  0.0962904047054
Iteration :  12   Loss :  0.0956799228071
Iteration :  13   Loss :  0.0950733113688
Iteration :  14   Loss :  0.0944705458515
Iteration :  15   Loss :  0.0938716018722
Iteration :  16   Loss :  0.0932764552023
Iteration :  17   Loss :  0.0926850817669
Iteration :  18   Loss :  0.0920974576435
Iteration :  19   Loss :  0.0915135590615
Iteration :  20   Loss :  0.0909333624009
Iteration :  21   Loss :  0.0903568441916
Iteration :  22   Loss :  0.0897839811122
Iteration :  23   Loss :  0.089214749989
Iteration :  24   Loss :  0.0886491277954
Iteration :  25   Loss :  0.0880870916509
Iteration :  26   Loss :  0.0875286188198
Iteration :  27   Loss :  0.0869736867107
Iteration :  28   Loss :  0.0864222728754
Iteration :  29   Loss :  0.085874355008
Iteration :  30   Loss :  0.0853299109441
Iteration :  31   Loss :  0.0847889186596
Iteration :  32   Loss :  0.0842513562704
Iteration :  33   Loss :  0.0837172020308
Iteration :  34   Loss :  0.0831864343331
Iteration :  35   Loss :  0.0826590317067
Iteration :  36   Loss :  0.0821349728169
Iteration :  37   Loss :  0.0816142364645
Iteration :  38   Loss :  0.0810968015845
Iteration :  39   Loss :  0.0805826472456
Iteration :  40   Loss :  0.0800717526492
Iteration :  41   Loss :  0.0795640971283
Iteration :  42   Loss :  0.0790596601473
Iteration :  43   Loss :  0.0785584213006
Iteration :  44   Loss :  0.078060360312
Iteration :  45   Loss :  0.0775654570339
Iteration :  46   Loss :  0.0770736914463
Iteration :  47   Loss :  0.0765850436563
Iteration :  48   Loss :  0.076099493897
Iteration :  49   Loss :  0.0756170225268
Iteration :  50   Loss :  0.0751376100288
Iteration :  51   Loss :  0.0746612370097
Iteration :  52   Loss :  0.0741878841992
Iteration :  53   Loss :  0.073717532449
Iteration :  54   Loss :  0.0732501627326
Iteration :  55   Loss :  0.0727857561437
Iteration :  56   Loss :  0.0723242938961
Iteration :  57   Loss :  0.0718657573227
Iteration :  58   Loss :  0.0714101278746
Iteration :  59   Loss :  0.0709573871207
Iteration :  60   Loss :  0.0705075167466
Iteration :  61   Loss :  0.0700604985541
Iteration :  62   Loss :  0.0696163144604
Iteration :  63   Loss :  0.0691749464972
Iteration :  64   Loss :  0.0687363768102
Iteration :  65   Loss :  0.0683005876584
Iteration :  66   Loss :  0.0678675614131
Iteration :  67   Loss :  0.0674372805575
Iteration :  68   Loss :  0.0670097276858
Iteration :  69   Loss :  0.0665848855026
Iteration :  70   Loss :  0.0661627368221
Iteration :  71   Loss :  0.0657432645673
Iteration :  72   Loss :  0.0653264517699
Iteration :  73   Loss :  0.0649122815688
Iteration :  74   Loss :  0.0645007372099
Iteration :  75   Loss :  0.0640918020454
Iteration :  76   Loss :  0.0636854595329
Iteration :  77   Loss :  0.063281693235
Iteration :  78   Loss :  0.0628804868186
Iteration :  79   Loss :  0.0624818240539
Iteration :  80   Loss :  0.0620856888141
Iteration :  81   Loss :  0.0616920650748
Iteration :  82   Loss :  0.0613009369129
Iteration :  83   Loss :  0.0609122885066
Iteration :  84   Loss :  0.0605261041342
Iteration :  85   Loss :  0.0601423681736
Iteration :  86   Loss :  0.0597610651019
Iteration :  87   Loss :  0.0593821794946
Iteration :  88   Loss :  0.0590056960249
Iteration :  89   Loss :  0.0586315994633
Iteration :  90   Loss :  0.0582598746767
Iteration :  91   Loss :  0.0578905066281
Iteration :  92   Loss :  0.0575234803757
Iteration :  93   Loss :  0.0571587810726
Iteration :  94   Loss :  0.0567963939658
Iteration :  95   Loss :  0.0564363043961
Iteration :  96   Loss :  0.0560784977969
Iteration :  97   Loss :  0.0557229596943
Iteration :  98   Loss :  0.055369675706
Iteration :  99   Loss :  0.0550186315409
[-0.0028628  -0.00046027 -0.00168312 ...,  0.0019753   0.00096203
  0.00014565]
CROSS VALIDATION 19
Iteration :  0   Loss :  30.2905542004
Iteration :  1   Loss :  13.7863719212
Iteration :  2   Loss :  0.0830060878819
Iteration :  3   Loss :  0.0824798286533
Iteration :  4   Loss :  0.0819569059123
Iteration :  5   Loss :  0.0814372985054
Iteration :  6   Loss :  0.0809209854134
Iteration :  7   Loss :  0.0804079457504
Iteration :  8   Loss :  0.0798981587627
Iteration :  9   Loss :  0.0793916038284
Iteration :  10   Loss :  0.0788882604563
Iteration :  11   Loss :  0.0783881082849
Iteration :  12   Loss :  0.0778911270821
Iteration :  13   Loss :  0.077397296744
Iteration :  14   Loss :  0.0769065972939
Iteration :  15   Loss :  0.0764190088821
Iteration :  16   Loss :  0.0759345117845
Iteration :  17   Loss :  0.0754530864022
Iteration :  18   Loss :  0.0749747132605
Iteration :  19   Loss :  0.0744993730081
Iteration :  20   Loss :  0.0740270464166
Iteration :  21   Loss :  0.0735577143792
Iteration :  22   Loss :  0.0730913579105
Iteration :  23   Loss :  0.0726279581454
Iteration :  24   Loss :  0.0721674963383
Iteration :  25   Loss :  0.0717099538626
Iteration :  26   Loss :  0.0712553122097
Iteration :  27   Loss :  0.0708035529882
Iteration :  28   Loss :  0.0703546579237
Iteration :  29   Loss :  0.0699086088574
Iteration :  30   Loss :  0.0694653877454
Iteration :  31   Loss :  0.0690249766588
Iteration :  32   Loss :  0.0685873577817
Iteration :  33   Loss :  0.0681525134117
Iteration :  34   Loss :  0.0677204259582
Iteration :  35   Loss :  0.0672910779426
Iteration :  36   Loss :  0.0668644519966
Iteration :  37   Loss :  0.0664405308623
Iteration :  38   Loss :  0.0660192973913
Iteration :  39   Loss :  0.0656007345436
Iteration :  40   Loss :  0.0651848253876
Iteration :  41   Loss :  0.0647715530989
Iteration :  42   Loss :  0.0643609009595
Iteration :  43   Loss :  0.0639528523579
Iteration :  44   Loss :  0.0635473907875
Iteration :  45   Loss :  0.0631444998465
Iteration :  46   Loss :  0.0627441632372
Iteration :  47   Loss :  0.062346364765
Iteration :  48   Loss :  0.0619510883382
Iteration :  49   Loss :  0.0615583179669
Iteration :  50   Loss :  0.0611680377627
Iteration :  51   Loss :  0.060780231938
Iteration :  52   Loss :  0.0603948848052
Iteration :  53   Loss :  0.0600119807762
Iteration :  54   Loss :  0.0596315043616
Iteration :  55   Loss :  0.0592534401704
Iteration :  56   Loss :  0.058877772909
Iteration :  57   Loss :  0.0585044873809
Iteration :  58   Loss :  0.0581335684859
Iteration :  59   Loss :  0.0577650012195
Iteration :  60   Loss :  0.0573987706724
Iteration :  61   Loss :  0.0570348620297
Iteration :  62   Loss :  0.0566732605706
Iteration :  63   Loss :  0.0563139516675
Iteration :  64   Loss :  0.0559569207856
Iteration :  65   Loss :  0.0556021534822
Iteration :  66   Loss :  0.0552496354062
Iteration :  67   Loss :  0.0548993522974
Iteration :  68   Loss :  0.0545512899863
Iteration :  69   Loss :  0.0542054343928
Iteration :  70   Loss :  0.0538617715265
Iteration :  71   Loss :  0.0535202874853
Iteration :  72   Loss :  0.0531809684555
Iteration :  73   Loss :  0.052843800711
Iteration :  74   Loss :  0.0525087706126
Iteration :  75   Loss :  0.0521758646075
Iteration :  76   Loss :  0.051845069229
Iteration :  77   Loss :  0.0515163710957
Iteration :  78   Loss :  0.0511897569111
Iteration :  79   Loss :  0.0508652134629
Iteration :  80   Loss :  0.0505427276227
Iteration :  81   Loss :  0.0502222863451
Iteration :  82   Loss :  0.0499038766677
Iteration :  83   Loss :  0.04958748571
Iteration :  84   Loss :  0.0492731006734
Iteration :  85   Loss :  0.0489607088404
Iteration :  86   Loss :  0.0486502975739
Iteration :  87   Loss :  0.0483418543172
Iteration :  88   Loss :  0.0480353665931
Iteration :  89   Loss :  0.0477308220036
Iteration :  90   Loss :  0.047428208229
Iteration :  91   Loss :  0.0471275130281
Iteration :  92   Loss :  0.046828724237
Iteration :  93   Loss :  0.0465318297691
Iteration :  94   Loss :  0.0462368176145
Iteration :  95   Loss :  0.0459436758391
Iteration :  96   Loss :  0.0456523925848
Iteration :  97   Loss :  0.0453629560686
Iteration :  98   Loss :  0.045075354582
Iteration :  99   Loss :  0.0447895764911
[-0.0015467  -0.00045039 -0.00117274 ...,  0.0007986   0.00027718
  0.00014555]
Accuracy (Hinge Loss):	0.7
lmda : 0.2  eta : 0.0001
Logistic Loss
CROSS VALIDATION 0
Iteration :  0   Loss :  5.89047671795
Iteration :  1   Loss :  0.0470832531588
Iteration :  2   Loss :  0.044934456787
Iteration :  3   Loss :  0.044079582275
Iteration :  4   Loss :  0.0435188176983
Iteration :  5   Loss :  0.0430864962039
Iteration :  6   Loss :  0.0427248185779
Iteration :  7   Loss :  0.0424073293541
Iteration :  8   Loss :  0.0421198871464
Iteration :  9   Loss :  0.0418541168641
Iteration :  10   Loss :  0.0416046814839
Iteration :  11   Loss :  0.0413679874838
Iteration :  12   Loss :  0.0411415091864
Iteration :  13   Loss :  0.0409234093413
Iteration :  14   Loss :  0.0407123132994
Iteration :  15   Loss :  0.0405071680791
Iteration :  16   Loss :  0.0403071508766
Iteration :  17   Loss :  0.0401116076668
Iteration :  18   Loss :  0.0399200108087
Iteration :  19   Loss :  0.0397319290445
Iteration :  20   Loss :  0.0395470058082
Iteration :  21   Loss :  0.039364943242
Iteration :  22   Loss :  0.0391854902208
Iteration :  23   Loss :  0.0390084332427
Iteration :  24   Loss :  0.0388335894067
Iteration :  25   Loss :  0.0386608009327
Iteration :  26   Loss :  0.0384899308381
Iteration :  27   Loss :  0.038320859491
Iteration :  28   Loss :  0.0381534818392
Iteration :  29   Loss :  0.0379877051607
Iteration :  30   Loss :  0.0378234472259
Iteration :  31   Loss :  0.0376606347832
Iteration :  32   Loss :  0.037499202304
Iteration :  33   Loss :  0.0373390909351
Iteration :  34   Loss :  0.0371802476204
Iteration :  35   Loss :  0.0370226243594
Iteration :  36   Loss :  0.0368661775779
Iteration :  37   Loss :  0.0367108675928
Iteration :  38   Loss :  0.0365566581533
Iteration :  39   Loss :  0.036403516046
Iteration :  40   Loss :  0.0362514107555
Iteration :  41   Loss :  0.0361003141685
Iteration :  42   Loss :  0.0359502003181
Iteration :  43   Loss :  0.0358010451597
Iteration :  44   Loss :  0.0356528263751
Iteration :  45   Loss :  0.0355055232003
Iteration :  46   Loss :  0.0353591162743
Iteration :  47   Loss :  0.0352135875051
Iteration :  48   Loss :  0.0350689199508
Iteration :  49   Loss :  0.0349250977151
Iteration :  50   Loss :  0.0347821058525
Iteration :  51   Loss :  0.0346399302858
Iteration :  52   Loss :  0.0344985577304
Iteration :  53   Loss :  0.0343579756279
Iteration :  54   Loss :  0.0342181720854
Iteration :  55   Loss :  0.0340791358215
Iteration :  56   Loss :  0.0339408561177
Iteration :  57   Loss :  0.0338033227735
Iteration :  58   Loss :  0.0336665260673
Iteration :  59   Loss :  0.0335304567196
Iteration :  60   Loss :  0.0333951058602
Iteration :  61   Loss :  0.0332604649982
Iteration :  62   Loss :  0.0331265259951
Iteration :  63   Loss :  0.0329932810392
Iteration :  64   Loss :  0.0328607226235
Iteration :  65   Loss :  0.0327288435244
Iteration :  66   Loss :  0.0325976367831
Iteration :  67   Loss :  0.0324670956874
Iteration :  68   Loss :  0.0323372137562
Iteration :  69   Loss :  0.0322079847246
Iteration :  70   Loss :  0.0320794025298
Iteration :  71   Loss :  0.0319514612991
Iteration :  72   Loss :  0.031824155338
Iteration :  73   Loss :  0.0316974791193
Iteration :  74   Loss :  0.0315714272735
Iteration :  75   Loss :  0.0314459945794
Iteration :  76   Loss :  0.0313211759554
Iteration :  77   Loss :  0.0311969664521
Iteration :  78   Loss :  0.0310733612442
Iteration :  79   Loss :  0.0309503556242
Iteration :  80   Loss :  0.0308279449959
Iteration :  81   Loss :  0.0307061248683
Iteration :  82   Loss :  0.0305848908502
Iteration :  83   Loss :  0.0304642386449
Iteration :  84   Loss :  0.0303441640455
Iteration :  85   Loss :  0.0302246629302
Iteration :  86   Loss :  0.0301057312583
Iteration :  87   Loss :  0.029987365066
Iteration :  88   Loss :  0.0298695604627
Iteration :  89   Loss :  0.0297523136279
Iteration :  90   Loss :  0.0296356208073
Iteration :  91   Loss :  0.0295194783101
Iteration :  92   Loss :  0.0294038825061
Iteration :  93   Loss :  0.029288829823
Iteration :  94   Loss :  0.0291743167436
Iteration :  95   Loss :  0.0290603398036
Iteration :  96   Loss :  0.0289468955891
Iteration :  97   Loss :  0.028833980735
Iteration :  98   Loss :  0.0287215919222
Iteration :  99   Loss :  0.0286097258763
[ -1.78857647e-04   7.40265959e-04   2.95262446e-04 ...,   4.19963192e-04
  -1.81687642e-05   2.60276116e-04]
CROSS VALIDATION 1
Iteration :  0   Loss :  9.10338253806
Iteration :  1   Loss :  0.056437863699
Iteration :  2   Loss :  0.0487430655706
Iteration :  3   Loss :  0.0466695572571
Iteration :  4   Loss :  0.0454168297524
Iteration :  5   Loss :  0.0445216583573
Iteration :  6   Loss :  0.0438237895303
Iteration :  7   Loss :  0.0432492615083
Iteration :  8   Loss :  0.0427581903166
Iteration :  9   Loss :  0.042326804232
Iteration :  10   Loss :  0.0419398991334
Iteration :  11   Loss :  0.0415872244297
Iteration :  12   Loss :  0.0412615799058
Iteration :  13   Loss :  0.0409577388478
Iteration :  14   Loss :  0.0406718031935
Iteration :  15   Loss :  0.0404007992471
Iteration :  16   Loss :  0.0401424143893
Iteration :  17   Loss :  0.0398948200269
Iteration :  18   Loss :  0.0396565492256
Iteration :  19   Loss :  0.0394264101062
Iteration :  20   Loss :  0.0392034232679
Iteration :  21   Loss :  0.0389867757364
Iteration :  22   Loss :  0.0387757865175
Iteration :  23   Loss :  0.0385698804529
Iteration :  24   Loss :  0.0383685681147
Iteration :  25   Loss :  0.0381714301595
Iteration :  26   Loss :  0.0379781050193
Iteration :  27   Loss :  0.0377882791224
Iteration :  28   Loss :  0.0376016790521
Iteration :  29   Loss :  0.0374180652077
Iteration :  30   Loss :  0.0372372266385
Iteration :  31   Loss :  0.0370589768051
Iteration :  32   Loss :  0.036883150077
Iteration :  33   Loss :  0.0367095988212
Iteration :  34   Loss :  0.0365381909672
Iteration :  35   Loss :  0.0363688079605
Iteration :  36   Loss :  0.0362013430323
Iteration :  37   Loss :  0.036035699731
Iteration :  38   Loss :  0.0358717906687
Iteration :  39   Loss :  0.0357095364477
Iteration :  40   Loss :  0.0355488647372
Iteration :  41   Loss :  0.035389709476
Iteration :  42   Loss :  0.0352320101809
Iteration :  43   Loss :  0.0350757113458
Iteration :  44   Loss :  0.0349207619177
Iteration :  45   Loss :  0.0347671148368
Iteration :  46   Loss :  0.0346147266342
Iteration :  47   Loss :  0.0344635570765
Iteration :  48   Loss :  0.0343135688527
Iteration :  49   Loss :  0.0341647272969
Iteration :  50   Loss :  0.034017000142
Iteration :  51   Loss :  0.033870357301
Iteration :  52   Loss :  0.0337247706715
Iteration :  53   Loss :  0.0335802139614
Iteration :  54   Loss :  0.0334366625328
Iteration :  55   Loss :  0.0332940932616
Iteration :  56   Loss :  0.0331524844117
Iteration :  57   Loss :  0.0330118155212
Iteration :  58   Loss :  0.0328720673
Iteration :  59   Loss :  0.0327332215374
Iteration :  60   Loss :  0.0325952610178
Iteration :  61   Loss :  0.0324581694452
Iteration :  62   Loss :  0.0323219313732
Iteration :  63   Loss :  0.032186532143
Iteration :  64   Loss :  0.0320519578254
Iteration :  65   Loss :  0.0319181951686
Iteration :  66   Loss :  0.0317852315508
Iteration :  67   Loss :  0.0316530549358
Iteration :  68   Loss :  0.0315216538333
Iteration :  69   Loss :  0.0313910172621
Iteration :  70   Loss :  0.0312611347161
Iteration :  71   Loss :  0.0311319961333
Iteration :  72   Loss :  0.0310035918671
Iteration :  73   Loss :  0.0308759126601
Iteration :  74   Loss :  0.0307489496196
Iteration :  75   Loss :  0.030622694195
Iteration :  76   Loss :  0.0304971381572
Iteration :  77   Loss :  0.0303722735789
Iteration :  78   Loss :  0.0302480928174
Iteration :  79   Loss :  0.0301245884974
Iteration :  80   Loss :  0.0300017534958
Iteration :  81   Loss :  0.0298795809276
Iteration :  82   Loss :  0.0297580641322
Iteration :  83   Loss :  0.0296371966613
Iteration :  84   Loss :  0.0295169722672
Iteration :  85   Loss :  0.0293973848919
Iteration :  86   Loss :  0.0292784286574
Iteration :  87   Loss :  0.0291600978558
Iteration :  88   Loss :  0.0290423869406
Iteration :  89   Loss :  0.0289252905186
Iteration :  90   Loss :  0.0288088033422
Iteration :  91   Loss :  0.0286929203017
Iteration :  92   Loss :  0.0285776364189
Iteration :  93   Loss :  0.0284629468406
Iteration :  94   Loss :  0.0283488468325
Iteration :  95   Loss :  0.0282353317731
Iteration :  96   Loss :  0.0281223971493
Iteration :  97   Loss :  0.0280100385502
Iteration :  98   Loss :  0.0278982516632
Iteration :  99   Loss :  0.0277870322692
[-0.00080189  0.00045757 -0.00034794 ...,  0.00021618 -0.00017864
  0.00031801]
CROSS VALIDATION 2
Iteration :  0   Loss :  2.60372310036
Iteration :  1   Loss :  0.0454726551631
Iteration :  2   Loss :  0.044258607119
Iteration :  3   Loss :  0.0435551988067
Iteration :  4   Loss :  0.0430547815035
Iteration :  5   Loss :  0.0426583573929
Iteration :  6   Loss :  0.0423231123164
Iteration :  7   Loss :  0.0420272437393
Iteration :  8   Loss :  0.0417583977768
Iteration :  9   Loss :  0.0415090203054
Iteration :  10   Loss :  0.0412742190217
Iteration :  11   Loss :  0.0410506795313
Iteration :  12   Loss :  0.0408360732849
Iteration :  13   Loss :  0.0406287144826
Iteration :  14   Loss :  0.0404273514449
Iteration :  15   Loss :  0.0402310345642
Iteration :  16   Loss :  0.0400390298443
Iteration :  17   Loss :  0.0398507606186
Iteration :  18   Loss :  0.0396657672582
Iteration :  19   Loss :  0.0394836786936
Iteration :  20   Loss :  0.0393041918877
Iteration :  21   Loss :  0.0391270567799
Iteration :  22   Loss :  0.0389520650694
Iteration :  23   Loss :  0.0387790417388
Iteration :  24   Loss :  0.0386078385671
Iteration :  25   Loss :  0.0384383291059
Iteration :  26   Loss :  0.0382704047482
Iteration :  27   Loss :  0.0381039716205
Iteration :  28   Loss :  0.0379389481042
Iteration :  29   Loss :  0.0377752628428
Iteration :  30   Loss :  0.0376128531256
Iteration :  31   Loss :  0.037451663569
Iteration :  32   Loss :  0.037291645032
Iteration :  33   Loss :  0.0371327537196
Iteration :  34   Loss :  0.0369749504366
Iteration :  35   Loss :  0.0368181999634
Iteration :  36   Loss :  0.0366624705307
Iteration :  37   Loss :  0.0365077333757
Iteration :  38   Loss :  0.0363539623657
Iteration :  39   Loss :  0.0362011336764
Iteration :  40   Loss :  0.0360492255164
Iteration :  41   Loss :  0.035898217891
Iteration :  42   Loss :  0.0357480923976
Iteration :  43   Loss :  0.0355988320489
Iteration :  44   Loss :  0.0354504211191
Iteration :  45   Loss :  0.0353028450097
Iteration :  46   Loss :  0.0351560901324
Iteration :  47   Loss :  0.035010143806
Iteration :  48   Loss :  0.0348649941659
Iteration :  49   Loss :  0.0347206300844
Iteration :  50   Loss :  0.0345770410999
Iteration :  51   Loss :  0.0344342173543
Iteration :  52   Loss :  0.0342921495372
Iteration :  53   Loss :  0.0341508288365
Iteration :  54   Loss :  0.034010246894
Iteration :  55   Loss :  0.0338703957657
Iteration :  56   Loss :  0.0337312678863
Iteration :  57   Loss :  0.0335928560372
Iteration :  58   Loss :  0.0334551533178
Iteration :  59   Loss :  0.0333181531197
Iteration :  60   Loss :  0.0331818491027
Iteration :  61   Loss :  0.0330462351744
Iteration :  62   Loss :  0.03291130547
Iteration :  63   Loss :  0.0327770543358
Iteration :  64   Loss :  0.0326434763125
Iteration :  65   Loss :  0.0325105661211
Iteration :  66   Loss :  0.0323783186496
Iteration :  67   Loss :  0.0322467289408
Iteration :  68   Loss :  0.0321157921816
Iteration :  69   Loss :  0.0319855036921
Iteration :  70   Loss :  0.0318558589172
Iteration :  71   Loss :  0.0317268534174
Iteration :  72   Loss :  0.0315984828613
Iteration :  73   Loss :  0.031470743018
Iteration :  74   Loss :  0.0313436297507
Iteration :  75   Loss :  0.0312171390105
Iteration :  76   Loss :  0.0310912668305
Iteration :  77   Loss :  0.0309660093207
Iteration :  78   Loss :  0.0308413626631
Iteration :  79   Loss :  0.030717323107
Iteration :  80   Loss :  0.030593886965
Iteration :  81   Loss :  0.030471050609
Iteration :  82   Loss :  0.0303488104665
Iteration :  83   Loss :  0.0302271630174
Iteration :  84   Loss :  0.0301061047905
Iteration :  85   Loss :  0.0299856323612
Iteration :  86   Loss :  0.0298657423479
Iteration :  87   Loss :  0.0297464314103
Iteration :  88   Loss :  0.0296276962465
Iteration :  89   Loss :  0.029509533591
Iteration :  90   Loss :  0.0293919402126
Iteration :  91   Loss :  0.0292749129125
Iteration :  92   Loss :  0.0291584485221
Iteration :  93   Loss :  0.0290425439021
Iteration :  94   Loss :  0.0289271959403
Iteration :  95   Loss :  0.0288124015503
Iteration :  96   Loss :  0.0286981576702
Iteration :  97   Loss :  0.0285844612613
Iteration :  98   Loss :  0.0284713093068
Iteration :  99   Loss :  0.0283586988109
[-0.00124691 -0.00044787 -0.00061781 ...,  0.00068829  0.00040885
  0.00024248]
CROSS VALIDATION 3
Iteration :  0   Loss :  8.02200955038
Iteration :  1   Loss :  0.0583348032873
Iteration :  2   Loss :  0.0441167241754
Iteration :  3   Loss :  0.0432798131815
Iteration :  4   Loss :  0.0426572725548
Iteration :  5   Loss :  0.0421574163332
Iteration :  6   Loss :  0.0417355672927
Iteration :  7   Loss :  0.0413669563236
Iteration :  8   Loss :  0.04103659459
Iteration :  9   Loss :  0.0407348139461
Iteration :  10   Loss :  0.0404550663592
Iteration :  11   Loss :  0.0401927423033
Iteration :  12   Loss :  0.0399444933331
Iteration :  13   Loss :  0.0397078227966
Iteration :  14   Loss :  0.0394808276776
Iteration :  15   Loss :  0.0392620297955
Iteration :  16   Loss :  0.039050262004
Iteration :  17   Loss :  0.0388445894224
Iteration :  18   Loss :  0.0386442536481
Iteration :  19   Loss :  0.0384486324433
Iteration :  20   Loss :  0.0382572100773
Iteration :  21   Loss :  0.0380695551571
Iteration :  22   Loss :  0.0378853038169
Iteration :  23   Loss :  0.0377041468043
Iteration :  24   Loss :  0.0375258194436
Iteration :  25   Loss :  0.0373500937495
Iteration :  26   Loss :  0.0371767721708
Iteration :  27   Loss :  0.0370056825807
Iteration :  28   Loss :  0.0368366742328
Iteration :  29   Loss :  0.0366696144702
Iteration :  30   Loss :  0.0365043860293
Iteration :  31   Loss :  0.036340884815
Iteration :  32   Loss :  0.0361790180539
Iteration :  33   Loss :  0.0360187027518
Iteration :  34   Loss :  0.0358598643994
Iteration :  35   Loss :  0.035702435879
Iteration :  36   Loss :  0.0355463565376
Iteration :  37   Loss :  0.0353915713977
Iteration :  38   Loss :  0.0352380304809
Iteration :  39   Loss :  0.0350856882278
Iteration :  40   Loss :  0.0349345029968
Iteration :  41   Loss :  0.0347844366307
Iteration :  42   Loss :  0.0346354540799
Iteration :  43   Loss :  0.034487523074
Iteration :  44   Loss :  0.0343406138342
Iteration :  45   Loss :  0.0341946988219
Iteration :  46   Loss :  0.0340497525164
Iteration :  47   Loss :  0.0339057512198
Iteration :  48   Loss :  0.0337626728834
Iteration :  49   Loss :  0.0336204969546
Iteration :  50   Loss :  0.0334792042405
Iteration :  51   Loss :  0.0333387767859
Iteration :  52   Loss :  0.0331991977647
Iteration :  53   Loss :  0.0330604513827
Iteration :  54   Loss :  0.0329225227904
Iteration :  55   Loss :  0.0327853980042
Iteration :  56   Loss :  0.032649063836
Iteration :  57   Loss :  0.0325135078292
Iteration :  58   Loss :  0.0323787182009
Iteration :  59   Loss :  0.0322446837898
Iteration :  60   Loss :  0.0321113940083
Iteration :  61   Loss :  0.0319788388002
Iteration :  62   Loss :  0.0318470086004
Iteration :  63   Loss :  0.0317158942996
Iteration :  64   Loss :  0.0315854872117
Iteration :  65   Loss :  0.0314557790432
Iteration :  66   Loss :  0.0313267618666
Iteration :  67   Loss :  0.0311984280943
Iteration :  68   Loss :  0.0310707704561
Iteration :  69   Loss :  0.0309437819776
Iteration :  70   Loss :  0.0308174559609
Iteration :  71   Loss :  0.0306917859658
Iteration :  72   Loss :  0.0305667657938
Iteration :  73   Loss :  0.0304423894722
Iteration :  74   Loss :  0.0303186512399
Iteration :  75   Loss :  0.0301955455342
Iteration :  76   Loss :  0.0300730669782
Iteration :  77   Loss :  0.0299512103694
Iteration :  78   Loss :  0.0298299706694
Iteration :  79   Loss :  0.0297093429935
Iteration :  80   Loss :  0.0295893226017
Iteration :  81   Loss :  0.02946990489
Iteration :  82   Loss :  0.0293510853822
Iteration :  83   Loss :  0.0292328597227
Iteration :  84   Loss :  0.0291152236691
Iteration :  85   Loss :  0.0289981730857
Iteration :  86   Loss :  0.0288817039375
Iteration :  87   Loss :  0.0287658122841
Iteration :  88   Loss :  0.0286504942743
Iteration :  89   Loss :  0.0285357461412
Iteration :  90   Loss :  0.0284215641973
Iteration :  91   Loss :  0.0283079448299
Iteration :  92   Loss :  0.0281948844967
Iteration :  93   Loss :  0.0280823797223
Iteration :  94   Loss :  0.0279704270939
Iteration :  95   Loss :  0.027859023258
Iteration :  96   Loss :  0.027748164917
Iteration :  97   Loss :  0.0276378488263
Iteration :  98   Loss :  0.0275280717909
Iteration :  99   Loss :  0.0274188306628
[-0.00079459  0.00051701 -0.00032269 ...,  0.00022699 -0.00015606
  0.00033213]
CROSS VALIDATION 4
Iteration :  0   Loss :  7.87548503426
Iteration :  1   Loss :  0.0572424731572
Iteration :  2   Loss :  0.0441675538955
Iteration :  3   Loss :  0.0432811469128
Iteration :  4   Loss :  0.0426325386909
Iteration :  5   Loss :  0.0421170080091
Iteration :  6   Loss :  0.0416849220209
Iteration :  7   Loss :  0.0413092390599
Iteration :  8   Loss :  0.0409737908765
Iteration :  9   Loss :  0.040668240613
Iteration :  10   Loss :  0.0403856349704
Iteration :  11   Loss :  0.0401211056977
Iteration :  12   Loss :  0.0398711321324
Iteration :  13   Loss :  0.0396330989899
Iteration :  14   Loss :  0.0394050191607
Iteration :  15   Loss :  0.0391853533903
Iteration :  16   Loss :  0.0389728892443
Iteration :  17   Loss :  0.0387666576474
Iteration :  18   Loss :  0.0385658739687
Iteration :  19   Loss :  0.0383698955703
Iteration :  20   Loss :  0.0381781906569
Iteration :  21   Loss :  0.0379903150452
Iteration :  22   Loss :  0.0378058945842
Iteration :  23   Loss :  0.0376246116768
Iteration :  24   Loss :  0.0374461948217
Iteration :  25   Loss :  0.0372704104093
Iteration :  26   Loss :  0.0370970562224
Iteration :  27   Loss :  0.0369259562398
Iteration :  28   Loss :  0.0367569564454
Iteration :  29   Loss :  0.0365899214235
Iteration :  30   Loss :  0.036424731571
Iteration :  31   Loss :  0.0362612807996
Iteration :  32   Loss :  0.0360994746313
Iteration :  33   Loss :  0.0359392286078
Iteration :  34   Loss :  0.0357804669578
Iteration :  35   Loss :  0.0356231214718
Iteration :  36   Loss :  0.0354671305494
Iteration :  37   Loss :  0.0353124383879
Iteration :  38   Loss :  0.0351589942882
Iteration :  39   Loss :  0.0350067520595
Iteration :  40   Loss :  0.0348556695054
Iteration :  41   Loss :  0.0347057079798
Iteration :  42   Loss :  0.0345568320008
Iteration :  43   Loss :  0.0344090089144
Iteration :  44   Loss :  0.0342622086014
Iteration :  45   Loss :  0.0341164032188
Iteration :  46   Loss :  0.0339715669737
Iteration :  47   Loss :  0.0338276759237
Iteration :  48   Loss :  0.0336847077997
Iteration :  49   Loss :  0.0335426418498
Iteration :  50   Loss :  0.0334014586998
Iteration :  51   Loss :  0.0332611402293
Iteration :  52   Loss :  0.0331216694608
Iteration :  53   Loss :  0.032983030461
Iteration :  54   Loss :  0.0328452082515
Iteration :  55   Loss :  0.0327081887293
Iteration :  56   Loss :  0.0325719585945
Iteration :  57   Loss :  0.0324365052859
Iteration :  58   Loss :  0.0323018169219
Iteration :  59   Loss :  0.0321678822476
Iteration :  60   Loss :  0.0320346905867
Iteration :  61   Loss :  0.0319022317977
Iteration :  62   Loss :  0.0317704962339
Iteration :  63   Loss :  0.0316394747074
Iteration :  64   Loss :  0.0315091584555
Iteration :  65   Loss :  0.031379539111
Iteration :  66   Loss :  0.0312506086738
Iteration :  67   Loss :  0.031122359486
Iteration :  68   Loss :  0.030994784208
Iteration :  69   Loss :  0.0308678757976
Iteration :  70   Loss :  0.0307416274898
Iteration :  71   Loss :  0.0306160327787
Iteration :  72   Loss :  0.0304910854006
Iteration :  73   Loss :  0.0303667793187
Iteration :  74   Loss :  0.0302431087085
Iteration :  75   Loss :  0.0301200679442
Iteration :  76   Loss :  0.0299976515873
Iteration :  77   Loss :  0.0298758543739
Iteration :  78   Loss :  0.0297546712051
Iteration :  79   Loss :  0.0296340971365
Iteration :  80   Loss :  0.0295141273693
Iteration :  81   Loss :  0.0293947572414
Iteration :  82   Loss :  0.0292759822198
Iteration :  83   Loss :  0.0291577978928
Iteration :  84   Loss :  0.0290401999632
Iteration :  85   Loss :  0.0289231842417
Iteration :  86   Loss :  0.0288067466411
Iteration :  87   Loss :  0.02869088317
Iteration :  88   Loss :  0.0285755899279
Iteration :  89   Loss :  0.0284608631004
Iteration :  90   Loss :  0.0283466989538
Iteration :  91   Loss :  0.0282330938315
Iteration :  92   Loss :  0.0281200441491
Iteration :  93   Loss :  0.0280075463913
Iteration :  94   Loss :  0.0278955971076
Iteration :  95   Loss :  0.0277841929093
Iteration :  96   Loss :  0.027673330466
Iteration :  97   Loss :  0.027563006503
Iteration :  98   Loss :  0.0274532177978
Iteration :  99   Loss :  0.027343961178
[-0.00077921  0.00053045 -0.00030963 ...,  0.00021487 -0.00015627
  0.00033276]
CROSS VALIDATION 5
Iteration :  0   Loss :  6.66209913689
Iteration :  1   Loss :  0.0441543945318
Iteration :  2   Loss :  0.0434916214121
Iteration :  3   Loss :  0.0429994246992
Iteration :  4   Loss :  0.0425987929581
Iteration :  5   Loss :  0.0422543511167
Iteration :  6   Loss :  0.04194744074
Iteration :  7   Loss :  0.0416671360099
Iteration :  8   Loss :  0.0414065517642
Iteration :  9   Loss :  0.0411611048002
Iteration :  10   Loss :  0.0409276112517
Iteration :  11   Loss :  0.0407037821825
Iteration :  12   Loss :  0.0404879249063
Iteration :  13   Loss :  0.0402787576378
Iteration :  14   Loss :  0.0400752899191
Iteration :  15   Loss :  0.0398767429139
Iteration :  16   Loss :  0.0396824947666
Iteration :  17   Loss :  0.0394920422246
Iteration :  18   Loss :  0.0393049731034
Iteration :  19   Loss :  0.0391209461574
Iteration :  20   Loss :  0.0389396761156
Iteration :  21   Loss :  0.0387609223894
Iteration :  22   Loss :  0.0385844804348
Iteration :  23   Loss :  0.0384101750643
Iteration :  24   Loss :  0.0382378552092
Iteration :  25   Loss :  0.0380673897764
Iteration :  26   Loss :  0.0378986643396
Iteration :  27   Loss :  0.0377315784741
Iteration :  28   Loss :  0.0375660435922
Iteration :  29   Loss :  0.0374019811727
Iteration :  30   Loss :  0.0372393213027
Iteration :  31   Loss :  0.0370780014677
Iteration :  32   Loss :  0.0369179655435
Iteration :  33   Loss :  0.0367591629496
Iteration :  34   Loss :  0.0366015479359
Iteration :  35   Loss :  0.0364450789775
Iteration :  36   Loss :  0.0362897182603
Iteration :  37   Loss :  0.0361354312398
Iteration :  38   Loss :  0.035982186263
Iteration :  39   Loss :  0.0358299542422
Iteration :  40   Loss :  0.0356787083722
Iteration :  41   Loss :  0.0355284238852
Iteration :  42   Loss :  0.0353790778365
Iteration :  43   Loss :  0.035230648918
Iteration :  44   Loss :  0.0350831172934
Iteration :  45   Loss :  0.034936464454
Iteration :  46   Loss :  0.0347906730916
Iteration :  47   Loss :  0.0346457269855
Iteration :  48   Loss :  0.0345016109022
Iteration :  49   Loss :  0.0343583105073
Iteration :  50   Loss :  0.0342158122853
Iteration :  51   Loss :  0.0340741034696
Iteration :  52   Loss :  0.0339331719785
Iteration :  53   Loss :  0.0337930063588
Iteration :  54   Loss :  0.0336535957344
Iteration :  55   Loss :  0.0335149297602
Iteration :  56   Loss :  0.0333769985809
Iteration :  57   Loss :  0.0332397927931
Iteration :  58   Loss :  0.0331033034112
Iteration :  59   Loss :  0.0329675218371
Iteration :  60   Loss :  0.0328324398313
Iteration :  61   Loss :  0.0326980494881
Iteration :  62   Loss :  0.0325643432121
Iteration :  63   Loss :  0.0324313136967
Iteration :  64   Loss :  0.0322989539051
Iteration :  65   Loss :  0.0321672570522
Iteration :  66   Loss :  0.0320362165885
Iteration :  67   Loss :  0.0319058261849
Iteration :  68   Loss :  0.0317760797193
Iteration :  69   Loss :  0.0316469712635
Iteration :  70   Loss :  0.0315184950721
Iteration :  71   Loss :  0.0313906455713
Iteration :  72   Loss :  0.0312634173488
Iteration :  73   Loss :  0.0311368051453
Iteration :  74   Loss :  0.0310108038454
Iteration :  75   Loss :  0.0308854084697
Iteration :  76   Loss :  0.0307606141681
Iteration :  77   Loss :  0.0306364162122
Iteration :  78   Loss :  0.0305128099895
Iteration :  79   Loss :  0.0303897909973
Iteration :  80   Loss :  0.0302673548374
Iteration :  81   Loss :  0.0301454972107
Iteration :  82   Loss :  0.0300242139127
Iteration :  83   Loss :  0.029903500829
Iteration :  84   Loss :  0.029783353931
Iteration :  85   Loss :  0.029663769272
Iteration :  86   Loss :  0.0295447429837
Iteration :  87   Loss :  0.0294262712725
Iteration :  88   Loss :  0.0293083504168
Iteration :  89   Loss :  0.0291909767634
Iteration :  90   Loss :  0.0290741467248
Iteration :  91   Loss :  0.0289578567768
Iteration :  92   Loss :  0.0288421034559
Iteration :  93   Loss :  0.0287268833565
Iteration :  94   Loss :  0.0286121931293
Iteration :  95   Loss :  0.0284980294787
Iteration :  96   Loss :  0.0283843891613
Iteration :  97   Loss :  0.0282712689835
Iteration :  98   Loss :  0.0281586657999
Iteration :  99   Loss :  0.028046576512
[ -5.35169299e-04   6.44030327e-04  -3.25518067e-04 ...,   6.68324173e-05
  -1.18986917e-04   4.20378239e-04]
CROSS VALIDATION 6
Iteration :  0   Loss :  7.86959260678
Iteration :  1   Loss :  0.057184560788
Iteration :  2   Loss :  0.0441692288803
Iteration :  3   Loss :  0.0432807268193
Iteration :  4   Loss :  0.0426311272673
Iteration :  5   Loss :  0.0421150566288
Iteration :  6   Loss :  0.041682653925
Iteration :  7   Loss :  0.0413067790265
Iteration :  8   Loss :  0.0409712149442
Iteration :  9   Loss :  0.0406655982274
Iteration :  10   Loss :  0.040382959969
Iteration :  11   Loss :  0.0401184222231
Iteration :  12   Loss :  0.0398684580263
Iteration :  13   Loss :  0.0396304478462
Iteration :  14   Loss :  0.0394024016207
Iteration :  15   Loss :  0.0391827779909
Iteration :  16   Loss :  0.0389703629902
Iteration :  17   Loss :  0.0387641864076
Iteration :  18   Loss :  0.0385634627577
Iteration :  19   Loss :  0.0383675487514
Iteration :  20   Loss :  0.0381759120922
Iteration :  21   Loss :  0.0379881082075
Iteration :  22   Loss :  0.037803762642
Iteration :  23   Loss :  0.03762255756
Iteration :  24   Loss :  0.037444221272
Iteration :  25   Loss :  0.0372685200207
Iteration :  26   Loss :  0.0370952514725
Iteration :  27   Loss :  0.0369242395152
Iteration :  28   Loss :  0.0367553300623
Iteration :  29   Loss :  0.0365883876442
Iteration :  30   Loss :  0.0364232926173
Iteration :  31   Loss :  0.0362599388645
Iteration :  32   Loss :  0.0360982318877
Iteration :  33   Loss :  0.0359380872163
Iteration :  34   Loss :  0.0357794290732
Iteration :  35   Loss :  0.0356221892484
Iteration :  36   Loss :  0.0354663061455
Iteration :  37   Loss :  0.0353117239694
Iteration :  38   Loss :  0.0351583920321
Iteration :  39   Loss :  0.0350062641563
Iteration :  40   Loss :  0.0348552981616
Iteration :  41   Loss :  0.0347054554194
Iteration :  42   Loss :  0.0345567004672
Iteration :  43   Loss :  0.0344090006718
Iteration :  44   Loss :  0.0342623259355
Iteration :  45   Loss :  0.0341166484379
Iteration :  46   Loss :  0.0339719424095
Iteration :  47   Loss :  0.0338281839317
Iteration :  48   Loss :  0.0336853507598
Iteration :  49   Loss :  0.0335434221662
Iteration :  50   Loss :  0.0334023788016
Iteration :  51   Loss :  0.0332622025702
Iteration :  52   Loss :  0.0331228765195
Iteration :  53   Loss :  0.0329843847406
Iteration :  54   Loss :  0.0328467122797
Iteration :  55   Loss :  0.0327098450578
Iteration :  56   Loss :  0.0325737697991
Iteration :  57   Loss :  0.0324384739656
Iteration :  58   Loss :  0.0323039456988
Iteration :  59   Loss :  0.0321701737661
Iteration :  60   Loss :  0.0320371475132
Iteration :  61   Loss :  0.0319048568197
Iteration :  62   Loss :  0.0317732920593
Iteration :  63   Loss :  0.0316424440638
Iteration :  64   Loss :  0.0315123040895
Iteration :  65   Loss :  0.0313828637871
Iteration :  66   Loss :  0.0312541151737
Iteration :  67   Loss :  0.0311260506073
Iteration :  68   Loss :  0.0309986627637
Iteration :  69   Loss :  0.0308719446145
Iteration :  70   Loss :  0.0307458894075
Iteration :  71   Loss :  0.0306204906487
Iteration :  72   Loss :  0.0304957420848
Iteration :  73   Loss :  0.0303716376881
Iteration :  74   Loss :  0.030248171642
Iteration :  75   Loss :  0.0301253383273
Iteration :  76   Loss :  0.0300031323104
Iteration :  77   Loss :  0.029881548331
Iteration :  78   Loss :  0.0297605812924
Iteration :  79   Loss :  0.0296402262505
Iteration :  80   Loss :  0.0295204784053
Iteration :  81   Loss :  0.0294013330921
Iteration :  82   Loss :  0.0292827857733
Iteration :  83   Loss :  0.0291648320309
Iteration :  84   Loss :  0.0290474675596
Iteration :  85   Loss :  0.0289306881603
Iteration :  86   Loss :  0.0288144897337
Iteration :  87   Loss :  0.0286988682748
Iteration :  88   Loss :  0.0285838198674
Iteration :  89   Loss :  0.0284693406792
Iteration :  90   Loss :  0.0283554269568
Iteration :  91   Loss :  0.0282420750215
Iteration :  92   Loss :  0.0281292812651
Iteration :  93   Loss :  0.0280170421459
Iteration :  94   Loss :  0.0279053541851
Iteration :  95   Loss :  0.0277942139632
Iteration :  96   Loss :  0.027683618117
Iteration :  97   Loss :  0.0275735633364
Iteration :  98   Loss :  0.0274640463616
Iteration :  99   Loss :  0.0273550639803
[-0.00077986  0.0005308  -0.00030925 ...,  0.00021432 -0.00015652
  0.00033274]
CROSS VALIDATION 7
Iteration :  0   Loss :  7.8899922553
Iteration :  1   Loss :  0.0573211279287
Iteration :  2   Loss :  0.044100387122
Iteration :  3   Loss :  0.0432229598676
Iteration :  4   Loss :  0.0425780466639
Iteration :  5   Loss :  0.0420640487168
Iteration :  6   Loss :  0.0416324543233
Iteration :  7   Loss :  0.0412567072603
Iteration :  8   Loss :  0.0409208769502
Iteration :  9   Loss :  0.0406147535682
Iteration :  10   Loss :  0.0403314563515
Iteration :  11   Loss :  0.0400661606666
Iteration :  12   Loss :  0.0398153731942
Iteration :  13   Loss :  0.0395764963909
Iteration :  14   Loss :  0.0393475549958
Iteration :  15   Loss :  0.0391270178668
Iteration :  16   Loss :  0.0389136782461
Iteration :  17   Loss :  0.0387065711085
Iteration :  18   Loss :  0.038504914764
Iteration :  19   Loss :  0.0383080687461
Iteration :  20   Loss :  0.0381155028882
Iteration :  21   Loss :  0.0379267742473
Iteration :  22   Loss :  0.037741509632
Iteration :  23   Loss :  0.0375593921987
Iteration :  24   Loss :  0.0373801510472
Iteration :  25   Loss :  0.0372035530555
Iteration :  26   Loss :  0.0370293964085
Iteration :  27   Loss :  0.0368575054215
Iteration :  28   Loss :  0.0366877263656
Iteration :  29   Loss :  0.0365199240734
Iteration :  30   Loss :  0.0363539791601
Iteration :  31   Loss :  0.0361897857324
Iteration :  32   Loss :  0.0360272494882
Iteration :  33   Loss :  0.0358662861303
Iteration :  34   Loss :  0.0357068200361
Iteration :  35   Loss :  0.0355487831347
Iteration :  36   Loss :  0.0353921139558
Iteration :  37   Loss :  0.0352367568197
Iteration :  38   Loss :  0.035082661144
Iteration :  39   Loss :  0.0349297808489
Iteration :  40   Loss :  0.0347780738442
Iteration :  41   Loss :  0.0346275015853
Iteration :  42   Loss :  0.0344780286875
Iteration :  43   Loss :  0.0343296225905
Iteration :  44   Loss :  0.0341822532641
Iteration :  45   Loss :  0.0340358929513
Iteration :  46   Loss :  0.0338905159416
Iteration :  47   Loss :  0.0337460983709
Iteration :  48   Loss :  0.0336026180455
Iteration :  49   Loss :  0.0334600542852
Iteration :  50   Loss :  0.0333183877839
Iteration :  51   Loss :  0.0331776004861
Iteration :  52   Loss :  0.0330376754757
Iteration :  53   Loss :  0.0328985968775
Iteration :  54   Loss :  0.0327603497675
Iteration :  55   Loss :  0.0326229200938
Iteration :  56   Loss :  0.0324862946041
Iteration :  57   Loss :  0.0323504607814
Iteration :  58   Loss :  0.0322154067848
Iteration :  59   Loss :  0.0320811213967
Iteration :  60   Loss :  0.0319475939744
Iteration :  61   Loss :  0.0318148144065
Iteration :  62   Loss :  0.0316827730731
Iteration :  63   Loss :  0.0315514608092
Iteration :  64   Loss :  0.0314208688719
Iteration :  65   Loss :  0.0312909889097
Iteration :  66   Loss :  0.0311618129349
Iteration :  67   Loss :  0.0310333332983
Iteration :  68   Loss :  0.0309055426654
Iteration :  69   Loss :  0.0307784339954
Iteration :  70   Loss :  0.0306520005211
Iteration :  71   Loss :  0.0305262357304
Iteration :  72   Loss :  0.03040113335
Iteration :  73   Loss :  0.0302766873296
Iteration :  74   Loss :  0.0301528918272
Iteration :  75   Loss :  0.0300297411962
Iteration :  76   Loss :  0.0299072299729
Iteration :  77   Loss :  0.0297853528646
Iteration :  78   Loss :  0.0296641047396
Iteration :  79   Loss :  0.0295434806168
Iteration :  80   Loss :  0.0294234756565
Iteration :  81   Loss :  0.0293040851521
Iteration :  82   Loss :  0.0291853045216
Iteration :  83   Loss :  0.0290671293005
Iteration :  84   Loss :  0.0289495551345
Iteration :  85   Loss :  0.0288325777733
Iteration :  86   Loss :  0.028716193064
Iteration :  87   Loss :  0.0286003969458
Iteration :  88   Loss :  0.0284851854443
Iteration :  89   Loss :  0.0283705546667
Iteration :  90   Loss :  0.028256500797
Iteration :  91   Loss :  0.0281430200917
Iteration :  92   Loss :  0.0280301088751
Iteration :  93   Loss :  0.0279177635362
Iteration :  94   Loss :  0.0278059805245
Iteration :  95   Loss :  0.0276947563467
Iteration :  96   Loss :  0.0275840875635
Iteration :  97   Loss :  0.0274739707865
Iteration :  98   Loss :  0.0273644026754
Iteration :  99   Loss :  0.0272553799353
[-0.0007805   0.00052838 -0.00031282 ...,  0.00021297 -0.00015878
  0.00033327]
CROSS VALIDATION 8
Iteration :  0   Loss :  7.87108779722
Iteration :  1   Loss :  0.0576016039769
Iteration :  2   Loss :  0.0441607423741
Iteration :  3   Loss :  0.0432840997463
Iteration :  4   Loss :  0.0426400814198
Iteration :  5   Loss :  0.0421270292693
Iteration :  6   Loss :  0.04169639223
Iteration :  7   Loss :  0.0413215952073
Iteration :  8   Loss :  0.0409866988837
Iteration :  9   Loss :  0.0406814894955
Iteration :  10   Loss :  0.0403990846682
Iteration :  11   Loss :  0.0401346593161
Iteration :  12   Loss :  0.039884720253
Iteration :  13   Loss :  0.0396466703601
Iteration :  14   Loss :  0.0394185349375
Iteration :  15   Loss :  0.0391987834563
Iteration :  16   Loss :  0.0389862097781
Iteration :  17   Loss :  0.0387798494773
Iteration :  18   Loss :  0.0385789214317
Iteration :  19   Loss :  0.0383827857033
Iteration :  20   Loss :  0.0381909126131
Iteration :  21   Loss :  0.0380028596642
Iteration :  22   Loss :  0.0378182540715
Iteration :  23   Loss :  0.0376367793599
Iteration :  24   Loss :  0.0374581649622
Iteration :  25   Loss :  0.0372821780568
Iteration :  26   Loss :  0.0371086170985
Iteration :  27   Loss :  0.0369373066451
Iteration :  28   Loss :  0.036768093185
Iteration :  29   Loss :  0.0366008417451
Iteration :  30   Loss :  0.0364354331141
Iteration :  31   Loss :  0.0362717615535
Iteration :  32   Loss :  0.0361097328987
Iteration :  33   Loss :  0.0359492629749
Iteration :  34   Loss :  0.035790276268
Iteration :  35   Loss :  0.0356327048032
Iteration :  36   Loss :  0.0354764871953
Iteration :  37   Loss :  0.0353215678395
Iteration :  38   Loss :  0.0351678962195
Iteration :  39   Loss :  0.0350154263139
Iteration :  40   Loss :  0.0348641160837
Iteration :  41   Loss :  0.0347139270292
Iteration :  42   Loss :  0.0345648238056
Iteration :  43   Loss :  0.0344167738871
Iteration :  44   Loss :  0.0342697472746
Iteration :  45   Loss :  0.0341237162382
Iteration :  46   Loss :  0.0339786550912
Iteration :  47   Loss :  0.0338345399916
Iteration :  48   Loss :  0.0336913487649
Iteration :  49   Loss :  0.0335490607486
Iteration :  50   Loss :  0.0334076566534
Iteration :  51   Loss :  0.033267118439
Iteration :  52   Loss :  0.0331274292044
Iteration :  53   Loss :  0.0329885730886
Iteration :  54   Loss :  0.0328505351821
Iteration :  55   Loss :  0.0327133014476
Iteration :  56   Loss :  0.0325768586476
Iteration :  57   Loss :  0.0324411942807
Iteration :  58   Loss :  0.0323062965223
Iteration :  59   Loss :  0.0321721541721
Iteration :  60   Loss :  0.0320387566059
Iteration :  61   Loss :  0.0319060937324
Iteration :  62   Loss :  0.0317741559527
Iteration :  63   Loss :  0.0316429341251
Iteration :  64   Loss :  0.0315124195311
Iteration :  65   Loss :  0.0313826038462
Iteration :  66   Loss :  0.0312534791112
Iteration :  67   Loss :  0.0311250377079
Iteration :  68   Loss :  0.0309972723348
Iteration :  69   Loss :  0.0308701759866
Iteration :  70   Loss :  0.0307437419339
Iteration :  71   Loss :  0.0306179637053
Iteration :  72   Loss :  0.0304928350705
Iteration :  73   Loss :  0.030368350025
Iteration :  74   Loss :  0.0302445027756
Iteration :  75   Loss :  0.0301212877272
Iteration :  76   Loss :  0.0299986994705
Iteration :  77   Loss :  0.0298767327706
Iteration :  78   Loss :  0.0297553825564
Iteration :  79   Loss :  0.0296346439106
Iteration :  80   Loss :  0.0295145120607
Iteration :  81   Loss :  0.0293949823704
Iteration :  82   Loss :  0.0292760503316
Iteration :  83   Loss :  0.0291577115569
Iteration :  84   Loss :  0.0290399617725
Iteration :  85   Loss :  0.0289227968124
Iteration :  86   Loss :  0.0288062126112
Iteration :  87   Loss :  0.0286902051995
Iteration :  88   Loss :  0.0285747706978
Iteration :  89   Loss :  0.0284599053117
Iteration :  90   Loss :  0.0283456053276
Iteration :  91   Loss :  0.0282318671077
Iteration :  92   Loss :  0.0281186870863
Iteration :  93   Loss :  0.0280060617655
Iteration :  94   Loss :  0.0278939877121
Iteration :  95   Loss :  0.0277824615536
Iteration :  96   Loss :  0.0276714799756
Iteration :  97   Loss :  0.0275610397179
Iteration :  98   Loss :  0.0274511375727
Iteration :  99   Loss :  0.0273417703809
[-0.00077815  0.000533   -0.00030315 ...,  0.00021662 -0.0001601
  0.00033294]
CROSS VALIDATION 9
Iteration :  0   Loss :  7.87203937924
Iteration :  1   Loss :  0.0653329777952
Iteration :  2   Loss :  0.0507688574427
Iteration :  3   Loss :  0.0498419830354
Iteration :  4   Loss :  0.0491683463527
Iteration :  5   Loss :  0.0486316019064
Iteration :  6   Loss :  0.048178985607
Iteration :  7   Loss :  0.0477824921704
Iteration :  8   Loss :  0.0474256706311
Iteration :  9   Loss :  0.0470981380853
Iteration :  10   Loss :  0.0467929789043
Iteration :  11   Loss :  0.0465053890106
Iteration :  12   Loss :  0.0462319162527
Iteration :  13   Loss :  0.0459700097973
Iteration :  14   Loss :  0.0457177401561
Iteration :  15   Loss :  0.0454736184125
Iteration :  16   Loss :  0.0452364756471
Iteration :  17   Loss :  0.0450053802473
Iteration :  18   Loss :  0.0447795798116
Iteration :  19   Loss :  0.0445584594652
Iteration :  20   Loss :  0.0443415113836
Iteration :  21   Loss :  0.0441283121412
Iteration :  22   Loss :  0.0439185056198
Iteration :  23   Loss :  0.0437117899404
Iteration :  24   Loss :  0.0435079073473
Iteration :  25   Loss :  0.0433066362911
Iteration :  26   Loss :  0.0431077851686
Iteration :  27   Loss :  0.0429111873279
Iteration :  28   Loss :  0.0427166970466
Iteration :  29   Loss :  0.0425241862696
Iteration :  30   Loss :  0.0423335419424
Iteration :  31   Loss :  0.0421446638165
Iteration :  32   Loss :  0.0419574626314
Iteration :  33   Loss :  0.0417718586001
Iteration :  34   Loss :  0.0415877801402
Iteration :  35   Loss :  0.0414051628045
Iteration :  36   Loss :  0.0412239483759
Iteration :  37   Loss :  0.0410440840983
Iteration :  38   Loss :  0.0408655220186
Iteration :  39   Loss :  0.0406882184225
Iteration :  40   Loss :  0.0405121333491
Iteration :  41   Loss :  0.0403372301708
Iteration :  42   Loss :  0.0401634752289
Iteration :  43   Loss :  0.0399908375163
Iteration :  44   Loss :  0.0398192884008
Iteration :  45   Loss :  0.039648801382
Iteration :  46   Loss :  0.0394793518783
Iteration :  47   Loss :  0.0393109170391
Iteration :  48   Loss :  0.0391434755785
Iteration :  49   Loss :  0.0389770076283
Iteration :  50   Loss :  0.0388114946074
Iteration :  51   Loss :  0.0386469191058
Iteration :  52   Loss :  0.0384832647805
Iteration :  53   Loss :  0.0383205162627
Iteration :  54   Loss :  0.038158659075
Iteration :  55   Loss :  0.0379976795565
Iteration :  56   Loss :  0.0378375647958
Iteration :  57   Loss :  0.0376783025701
Iteration :  58   Loss :  0.0375198812911
Iteration :  59   Loss :  0.0373622899549
Iteration :  60   Loss :  0.0372055180973
Iteration :  61   Loss :  0.0370495557536
Iteration :  62   Loss :  0.0368943934208
Iteration :  63   Loss :  0.0367400220244
Iteration :  64   Loss :  0.0365864328872
Iteration :  65   Loss :  0.0364336177018
Iteration :  66   Loss :  0.0362815685042
Iteration :  67   Loss :  0.0361302776508
Iteration :  68   Loss :  0.0359797377962
Iteration :  69   Loss :  0.035829941874
Iteration :  70   Loss :  0.035680883078
Iteration :  71   Loss :  0.0355325548457
Iteration :  72   Loss :  0.0353849508425
Iteration :  73   Loss :  0.0352380649475
Iteration :  74   Loss :  0.0350918912405
Iteration :  75   Loss :  0.0349464239893
Iteration :  76   Loss :  0.0348016576387
Iteration :  77   Loss :  0.0346575867997
Iteration :  78   Loss :  0.0345142062399
Iteration :  79   Loss :  0.0343715108746
Iteration :  80   Loss :  0.0342294957578
Iteration :  81   Loss :  0.0340881560749
Iteration :  82   Loss :  0.0339474871352
Iteration :  83   Loss :  0.0338074843646
Iteration :  84   Loss :  0.0336681433002
Iteration :  85   Loss :  0.0335294595834
Iteration :  86   Loss :  0.0333914289547
Iteration :  87   Loss :  0.0332540472486
Iteration :  88   Loss :  0.0331173103885
Iteration :  89   Loss :  0.0329812143825
Iteration :  90   Loss :  0.0328457553183
Iteration :  91   Loss :  0.0327109293603
Iteration :  92   Loss :  0.0325767327448
Iteration :  93   Loss :  0.0324431617771
Iteration :  94   Loss :  0.0323102128278
Iteration :  95   Loss :  0.03217788233
Iteration :  96   Loss :  0.032046166776
Iteration :  97   Loss :  0.0319150627148
Iteration :  98   Loss :  0.0317845667492
Iteration :  99   Loss :  0.0316546755338
[-0.00070282  0.00059109 -0.00021505 ...,  0.0005059  -0.00011714
  0.00033744]
CROSS VALIDATION 10
Iteration :  0   Loss :  7.23399700406
Iteration :  1   Loss :  0.0545380971684
Iteration :  2   Loss :  0.0538623463085
Iteration :  3   Loss :  0.0533412569555
Iteration :  4   Loss :  0.052905537185
Iteration :  5   Loss :  0.0525230556287
Iteration :  6   Loss :  0.0521765198512
Iteration :  7   Loss :  0.0518556723564
Iteration :  8   Loss :  0.0515539915045
Iteration :  9   Loss :  0.0512671081247
Iteration :  10   Loss :  0.0509919723776
Iteration :  11   Loss :  0.0507263835069
Iteration :  12   Loss :  0.0504687092436
Iteration :  13   Loss :  0.0502177106259
Iteration :  14   Loss :  0.0499724284346
Iteration :  15   Loss :  0.0497321071905
Iteration :  16   Loss :  0.0494961428794
Iteration :  17   Loss :  0.049264046132
Iteration :  18   Loss :  0.049035415745
Iteration :  19   Loss :  0.0488099192831
Iteration :  20   Loss :  0.0485872786334
Iteration :  21   Loss :  0.0483672590877
Iteration :  22   Loss :  0.0481496609799
Iteration :  23   Loss :  0.0479343132039
Iteration :  24   Loss :  0.0477210681311
Iteration :  25   Loss :  0.0475097975876
Iteration :  26   Loss :  0.0473003896382
Iteration :  27   Loss :  0.0470927459941
Iteration :  28   Loss :  0.046886779907
Iteration :  29   Loss :  0.0466824144455
Iteration :  30   Loss :  0.046479581074
Iteration :  31   Loss :  0.0462782184747
Iteration :  32   Loss :  0.0460782715643
Iteration :  33   Loss :  0.0458796906691
Iteration :  34   Loss :  0.0456824308292
Iteration :  35   Loss :  0.0454864512083
Iteration :  36   Loss :  0.0452917145918
Iteration :  37   Loss :  0.0450981869567
Iteration :  38   Loss :  0.0449058371026
Iteration :  39   Loss :  0.044714636333
Iteration :  40   Loss :  0.0445245581803
Iteration :  41   Loss :  0.0443355781658
Iteration :  42   Loss :  0.0441476735912
Iteration :  43   Loss :  0.0439608233561
Iteration :  44   Loss :  0.043775007798
Iteration :  45   Loss :  0.043590208551
Iteration :  46   Loss :  0.043406408422
Iteration :  47   Loss :  0.0432235912806
Iteration :  48   Loss :  0.0430417419613
Iteration :  49   Loss :  0.0428608461771
Iteration :  50   Loss :  0.0426808904422
Iteration :  51   Loss :  0.0425018620027
Iteration :  52   Loss :  0.042323748775
Iteration :  53   Loss :  0.0421465392904
Iteration :  54   Loss :  0.041970222645
Iteration :  55   Loss :  0.0417947884549
Iteration :  56   Loss :  0.0416202268157
Iteration :  57   Loss :  0.0414465282659
Iteration :  58   Loss :  0.0412736837533
Iteration :  59   Loss :  0.0411016846055
Iteration :  60   Loss :  0.0409305225019
Iteration :  61   Loss :  0.0407601894493
Iteration :  62   Loss :  0.0405906777586
Iteration :  63   Loss :  0.0404219800245
Iteration :  64   Loss :  0.0402540891061
Iteration :  65   Loss :  0.04008699811
Iteration :  66   Loss :  0.0399207003738
Iteration :  67   Loss :  0.0397551894516
Iteration :  68   Loss :  0.0395904591006
Iteration :  69   Loss :  0.0394265032687
Iteration :  70   Loss :  0.0392633160827
Iteration :  71   Loss :  0.0391008918382
Iteration :  72   Loss :  0.0389392249892
Iteration :  73   Loss :  0.0387783101395
Iteration :  74   Loss :  0.038618142034
Iteration :  75   Loss :  0.0384587155512
Iteration :  76   Loss :  0.0383000256954
Iteration :  77   Loss :  0.0381420675904
Iteration :  78   Loss :  0.0379848364731
Iteration :  79   Loss :  0.0378283276876
Iteration :  80   Loss :  0.0376725366794
Iteration :  81   Loss :  0.037517458991
Iteration :  82   Loss :  0.0373630902561
Iteration :  83   Loss :  0.0372094261962
Iteration :  84   Loss :  0.0370564626155
Iteration :  85   Loss :  0.0369041953974
Iteration :  86   Loss :  0.0367526205006
Iteration :  87   Loss :  0.0366017339557
Iteration :  88   Loss :  0.0364515318619
Iteration :  89   Loss :  0.0363020103838
Iteration :  90   Loss :  0.0361531657486
Iteration :  91   Loss :  0.0360049942434
Iteration :  92   Loss :  0.0358574922123
Iteration :  93   Loss :  0.0357106560543
Iteration :  94   Loss :  0.0355644822205
Iteration :  95   Loss :  0.0354189672126
Iteration :  96   Loss :  0.0352741075801
Iteration :  97   Loss :  0.0351298999187
Iteration :  98   Loss :  0.0349863408685
Iteration :  99   Loss :  0.0348434271119
[ -7.28908981e-04   5.07254007e-04  -3.79647670e-04 ...,   4.73116975e-04
  -7.38453120e-05   3.55154694e-04]
CROSS VALIDATION 11
Iteration :  0   Loss :  7.87188841805
Iteration :  1   Loss :  0.0571987641432
Iteration :  2   Loss :  0.044179328153
Iteration :  3   Loss :  0.0432920901582
Iteration :  4   Loss :  0.0426430968104
Iteration :  5   Loss :  0.0421273353765
Iteration :  6   Loss :  0.0416950841783
Iteration :  7   Loss :  0.0413192676742
Iteration :  8   Loss :  0.0409837026792
Iteration :  9   Loss :  0.0406780450301
Iteration :  10   Loss :  0.0403953375584
Iteration :  11   Loss :  0.0401307098279
Iteration :  12   Loss :  0.0398806398726
Iteration :  13   Loss :  0.0396425115924
Iteration :  14   Loss :  0.0394143373446
Iteration :  15   Loss :  0.0391945775113
Iteration :  16   Loss :  0.0389820194008
Iteration :  17   Loss :  0.0387756937484
Iteration :  18   Loss :  0.0385748157784
Iteration :  19   Loss :  0.0383787427386
Iteration :  20   Loss :  0.0381869427402
Iteration :  21   Loss :  0.0379989715211
Iteration :  22   Loss :  0.0378144548625
Iteration :  23   Loss :  0.0376330751069
Iteration :  24   Loss :  0.0374545606985
Iteration :  25   Loss :  0.0372786779777
Iteration :  26   Loss :  0.0371052246806
Iteration :  27   Loss :  0.0369340247419
Iteration :  28   Loss :  0.0367649241035
Iteration :  29   Loss :  0.0365977873092
Iteration :  30   Loss :  0.0364324947168
Iteration :  31   Loss :  0.0362689402
Iteration :  32   Loss :  0.0361070292435
Iteration :  33   Loss :  0.0359466773526
Iteration :  34   Loss :  0.0357878087199
Iteration :  35   Loss :  0.0356303551006
Iteration :  36   Loss :  0.0354742548592
Iteration :  37   Loss :  0.0353194521583
Iteration :  38   Loss :  0.0351658962645
Iteration :  39   Loss :  0.0350135409529
Iteration :  40   Loss :  0.0348623439932
Iteration :  41   Loss :  0.0347122667056
Iteration :  42   Loss :  0.0345632735749
Iteration :  43   Loss :  0.0344153319139
Iteration :  44   Loss :  0.0342684115702
Iteration :  45   Loss :  0.0341224846681
Iteration :  46   Loss :  0.0339775253822
Iteration :  47   Loss :  0.0338335097377
Iteration :  48   Loss :  0.0336904154335
Iteration :  49   Loss :  0.0335482216857
Iteration :  50   Loss :  0.0334069090887
Iteration :  51   Loss :  0.0332664594907
Iteration :  52   Loss :  0.0331268558836
Iteration :  53   Loss :  0.0329880823033
Iteration :  54   Loss :  0.0328501237414
Iteration :  55   Loss :  0.0327129660651
Iteration :  56   Loss :  0.0325765959455
Iteration :  57   Loss :  0.0324410007924
Iteration :  58   Loss :  0.0323061686961
Iteration :  59   Loss :  0.0321720883743
Iteration :  60   Loss :  0.0320387491237
Iteration :  61   Loss :  0.0319061407764
Iteration :  62   Loss :  0.0317742536603
Iteration :  63   Loss :  0.0316430785625
Iteration :  64   Loss :  0.0315126066966
Iteration :  65   Loss :  0.0313828296721
Iteration :  66   Loss :  0.0312537394668
Iteration :  67   Loss :  0.0311253284016
Iteration :  68   Loss :  0.0309975891168
Iteration :  69   Loss :  0.0308705145509
Iteration :  70   Loss :  0.0307440979211
Iteration :  71   Loss :  0.0306183327047
Iteration :  72   Loss :  0.0304932126226
Iteration :  73   Loss :  0.0303687316235
Iteration :  74   Loss :  0.0302448838701
Iteration :  75   Loss :  0.0301216637255
Iteration :  76   Loss :  0.0299990657407
Iteration :  77   Loss :  0.0298770846437
Iteration :  78   Loss :  0.0297557153287
Iteration :  79   Loss :  0.0296349528461
Iteration :  80   Loss :  0.0295147923935
Iteration :  81   Loss :  0.029395229307
Iteration :  82   Loss :  0.0292762590538
Iteration :  83   Loss :  0.029157877224
Iteration :  84   Loss :  0.0290400795243
Iteration :  85   Loss :  0.0289228617712
Iteration :  86   Loss :  0.0288062198851
Iteration :  87   Loss :  0.0286901498847
Iteration :  88   Loss :  0.0285746478814
Iteration :  89   Loss :  0.0284597100747
Iteration :  90   Loss :  0.0283453327476
Iteration :  91   Loss :  0.0282315122616
Iteration :  92   Loss :  0.0281182450535
Iteration :  93   Loss :  0.0280055276309
Iteration :  94   Loss :  0.0278933565688
Iteration :  95   Loss :  0.0277817285062
Iteration :  96   Loss :  0.0276706401431
Iteration :  97   Loss :  0.027560088237
Iteration :  98   Loss :  0.0274500696007
Iteration :  99   Loss :  0.0273405810993
[-0.00077823  0.00053099 -0.00030773 ...,  0.00021485 -0.00015646
  0.00033244]
CROSS VALIDATION 12
Iteration :  0   Loss :  10.0123542731
Iteration :  1   Loss :  0.0452296421484
Iteration :  2   Loss :  0.0399686763114
Iteration :  3   Loss :  0.0388837440533
Iteration :  4   Loss :  0.0382080881459
Iteration :  5   Loss :  0.0377072971487
Iteration :  6   Loss :  0.0373024025513
Iteration :  7   Loss :  0.0369574376842
Iteration :  8   Loss :  0.0366531334917
Iteration :  9   Loss :  0.0363780374318
Iteration :  10   Loss :  0.0361248295278
Iteration :  11   Loss :  0.0358885717441
Iteration :  12   Loss :  0.0356657908102
Iteration :  13   Loss :  0.0354539608642
Iteration :  14   Loss :  0.0352511941894
Iteration :  15   Loss :  0.0350560474705
Iteration :  16   Loss :  0.0348673956282
Iteration :  17   Loss :  0.0346843469441
Iteration :  18   Loss :  0.0345061843536
Iteration :  19   Loss :  0.0343323238562
Iteration :  20   Loss :  0.0341622844263
Iteration :  21   Loss :  0.0339956658427
Iteration :  22   Loss :  0.0338321320796
Iteration :  23   Loss :  0.0336713986841
Iteration :  24   Loss :  0.0335132230533
Iteration :  25   Loss :  0.0333573968572
Iteration :  26   Loss :  0.0332037400679
Iteration :  27   Loss :  0.0330520962091
Iteration :  28   Loss :  0.0329023285408
Iteration :  29   Loss :  0.0327543169706
Iteration :  30   Loss :  0.0326079555325
Iteration :  31   Loss :  0.0324631503151
Iteration :  32   Loss :  0.0323198177472
Iteration :  33   Loss :  0.032177883171
Iteration :  34   Loss :  0.0320372796461
Iteration :  35   Loss :  0.0318979469442
Iteration :  36   Loss :  0.0317598306965
Iteration :  37   Loss :  0.0316228816704
Iteration :  38   Loss :  0.0314870551499
Iteration :  39   Loss :  0.0313523104058
Iteration :  40   Loss :  0.0312186102379
Iteration :  41   Loss :  0.0310859205802
Iteration :  42   Loss :  0.0309542101574
Iteration :  43   Loss :  0.0308234501865
Iteration :  44   Loss :  0.0306936141157
Iteration :  45   Loss :  0.0305646773956
Iteration :  46   Loss :  0.0304366172781
Iteration :  47   Loss :  0.0303094126391
Iteration :  48   Loss :  0.0301830438219
Iteration :  49   Loss :  0.0300574924982
Iteration :  50   Loss :  0.0299327415446
Iteration :  51   Loss :  0.0298087749328
Iteration :  52   Loss :  0.0296855776315
Iteration :  53   Loss :  0.0295631355184
Iteration :  54   Loss :  0.0294414353016
Iteration :  55   Loss :  0.0293204644491
Iteration :  56   Loss :  0.0292002111247
Iteration :  57   Loss :  0.0290806641308
Iteration :  58   Loss :  0.0289618128568
Iteration :  59   Loss :  0.0288436472315
Iteration :  60   Loss :  0.0287261576813
Iteration :  61   Loss :  0.0286093350909
Iteration :  62   Loss :  0.0284931707685
Iteration :  63   Loss :  0.0283776564138
Iteration :  64   Loss :  0.0282627840885
Iteration :  65   Loss :  0.0281485461899
Iteration :  66   Loss :  0.0280349354266
Iteration :  67   Loss :  0.0279219447956
Iteration :  68   Loss :  0.0278095675624
Iteration :  69   Loss :  0.0276977972421
Iteration :  70   Loss :  0.0275866275818
Iteration :  71   Loss :  0.0274760525448
Iteration :  72   Loss :  0.0273660662964
Iteration :  73   Loss :  0.0272566631896
Iteration :  74   Loss :  0.0271478377533
Iteration :  75   Loss :  0.0270395846806
Iteration :  76   Loss :  0.0269318988181
Iteration :  77   Loss :  0.0268247751559
Iteration :  78   Loss :  0.026718208819
Iteration :  79   Loss :  0.0266121950583
Iteration :  80   Loss :  0.0265067292431
Iteration :  81   Loss :  0.0264018068536
Iteration :  82   Loss :  0.0262974234747
Iteration :  83   Loss :  0.0261935747889
Iteration :  84   Loss :  0.0260902565713
Iteration :  85   Loss :  0.0259874646836
Iteration :  86   Loss :  0.0258851950696
Iteration :  87   Loss :  0.0257834437499
Iteration :  88   Loss :  0.0256822068182
Iteration :  89   Loss :  0.0255814804369
Iteration :  90   Loss :  0.0254812608334
Iteration :  91   Loss :  0.0253815442964
[ -1.53806085e-03   3.82131604e-05  -4.83038500e-04 ...,   6.49520356e-05
  -1.74889120e-04   1.94836975e-04]
CROSS VALIDATION 13
Iteration :  0   Loss :  7.97744078796
Iteration :  1   Loss :  0.0573376509255
Iteration :  2   Loss :  0.0533906279173
Iteration :  3   Loss :  0.0519358719505
Iteration :  4   Loss :  0.0510470057702
Iteration :  5   Loss :  0.0504006182952
Iteration :  6   Loss :  0.0498844963516
Iteration :  7   Loss :  0.0494478060441
Iteration :  8   Loss :  0.0490637317729
Iteration :  9   Loss :  0.0487166301164
Iteration :  10   Loss :  0.0483966928666
Iteration :  11   Loss :  0.0480974263869
Iteration :  12   Loss :  0.0478143440956
Iteration :  13   Loss :  0.0475442376176
Iteration :  14   Loss :  0.0472847465974
Iteration :  15   Loss :  0.0470340926772
Iteration :  16   Loss :  0.0467909084859
Iteration :  17   Loss :  0.0465541240415
Iteration :  18   Loss :  0.0463228891378
Iteration :  19   Loss :  0.0460965190078
Iteration :  20   Loss :  0.0458744554606
Iteration :  21   Loss :  0.0456562385564
Iteration :  22   Loss :  0.0454414856109
Iteration :  23   Loss :  0.0452298753975
Iteration :  24   Loss :  0.0450211360969
Iteration :  25   Loss :  0.044815035992
Iteration :  26   Loss :  0.0446113762009
Iteration :  27   Loss :  0.0444099849445
Iteration :  28   Loss :  0.0442107129796
Iteration :  29   Loss :  0.0440134299312
Iteration :  30   Loss :  0.0438180213213
Iteration :  31   Loss :  0.0436243861455
Iteration :  32   Loss :  0.0434324348811
Iteration :  33   Loss :  0.0432420878405
Iteration :  34   Loss :  0.0430532738014
Iteration :  35   Loss :  0.0428659288605
Iteration :  36   Loss :  0.0426799954697
Iteration :  37   Loss :  0.0424954216206
Iteration :  38   Loss :  0.0423121601533
Iteration :  39   Loss :  0.0421301681651
Iteration :  40   Loss :  0.0419494065052
Iteration :  41   Loss :  0.0417698393395
Iteration :  42   Loss :  0.0415914337748
Iteration :  43   Loss :  0.0414141595335
Iteration :  44   Loss :  0.0412379886707
Iteration :  45   Loss :  0.0410628953271
Iteration :  46   Loss :  0.0408888555132
Iteration :  47   Loss :  0.0407158469197
Iteration :  48   Loss :  0.0405438487509
Iteration :  49   Loss :  0.040372841577
Iteration :  50   Loss :  0.0402028072044
Iteration :  51   Loss :  0.04003372856
Iteration :  52   Loss :  0.0398655895884
Iteration :  53   Loss :  0.0396983751607
Iteration :  54   Loss :  0.0395320709927
Iteration :  55   Loss :  0.0393666635714
Iteration :  56   Loss :  0.0392021400903
Iteration :  57   Loss :  0.0390384883896
Iteration :  58   Loss :  0.038875696904
Iteration :  59   Loss :  0.0387137546144
Iteration :  60   Loss :  0.038552651005
Iteration :  61   Loss :  0.038392376024
Iteration :  62   Loss :  0.0382329200486
Iteration :  63   Loss :  0.0380742738526
Iteration :  64   Loss :  0.0379164285776
Iteration :  65   Loss :  0.0377593757059
Iteration :  66   Loss :  0.0376031070367
Iteration :  67   Loss :  0.037447614664
Iteration :  68   Loss :  0.0372928909564
Iteration :  69   Loss :  0.0371389285385
Iteration :  70   Loss :  0.0369857202742
Iteration :  71   Loss :  0.0368332592509
Iteration :  72   Loss :  0.0366815387659
Iteration :  73   Loss :  0.0365305523124
Iteration :  74   Loss :  0.0363802935685
Iteration :  75   Loss :  0.0362307563852
Iteration :  76   Loss :  0.0360819347767
Iteration :  77   Loss :  0.035933822911
Iteration :  78   Loss :  0.0357864151011
Iteration :  79   Loss :  0.0356397057966
Iteration :  80   Loss :  0.035493689577
Iteration :  81   Loss :  0.0353483611442
Iteration :  82   Loss :  0.0352037153163
Iteration :  83   Loss :  0.0350597470218
Iteration :  84   Loss :  0.0349164512941
Iteration :  85   Loss :  0.0347738232659
Iteration :  86   Loss :  0.0346318581654
Iteration :  87   Loss :  0.0344905513108
Iteration :  88   Loss :  0.0343498981071
Iteration :  89   Loss :  0.0342098940416
Iteration :  90   Loss :  0.0340705346809
Iteration :  91   Loss :  0.0339318156669
Iteration :  92   Loss :  0.0337937327143
Iteration :  93   Loss :  0.0336562816074
Iteration :  94   Loss :  0.0335194581973
Iteration :  95   Loss :  0.0333832583994
Iteration :  96   Loss :  0.0332476781911
Iteration :  97   Loss :  0.0331127136096
Iteration :  98   Loss :  0.0329783607494
Iteration :  99   Loss :  0.0328446157608
[ -1.16756174e-03  -1.43311759e-04  -5.80520988e-04 ...,   7.39727525e-05
   5.10877906e-05   3.07567915e-04]
CROSS VALIDATION 14
Iteration :  0   Loss :  7.85348596815
Iteration :  1   Loss :  0.0556867958615
Iteration :  2   Loss :  0.0427211297161
Iteration :  3   Loss :  0.0421180554551
Iteration :  4   Loss :  0.0416536745491
Iteration :  5   Loss :  0.0412681619286
Iteration :  6   Loss :  0.0409329740602
Iteration :  7   Loss :  0.0406323912055
Iteration :  8   Loss :  0.0403569011081
Iteration :  9   Loss :  0.0401003486458
Iteration :  10   Loss :  0.0398585452403
Iteration :  11   Loss :  0.0396285261239
Iteration :  12   Loss :  0.039408125214
Iteration :  13   Loss :  0.0391957180881
Iteration :  14   Loss :  0.0389900595238
Iteration :  15   Loss :  0.0387901769534
Iteration :  16   Loss :  0.038595298377
Iteration :  17   Loss :  0.0384048022753
Iteration :  18   Loss :  0.038218181992
Iteration :  19   Loss :  0.0380350198912
Iteration :  20   Loss :  0.0378549682678
Iteration :  21   Loss :  0.0376777350209
Iteration :  22   Loss :  0.0375030727477
Iteration :  23   Loss :  0.0373307703332
Iteration :  24   Loss :  0.0371606463896
Iteration :  25   Loss :  0.0369925440829
Iteration :  26   Loss :  0.0368263270148
Iteration :  27   Loss :  0.0366618759136
Iteration :  28   Loss :  0.0364990859548
Iteration :  29   Loss :  0.0363378645741
Iteration :  30   Loss :  0.0361781296694
Iteration :  31   Loss :  0.0360198081141
Iteration :  32   Loss :  0.0358628345184
Iteration :  33   Loss :  0.0357071501929
Iteration :  34   Loss :  0.0355527022759
Iteration :  35   Loss :  0.0353994429946
Iteration :  36   Loss :  0.0352473290378
Iteration :  37   Loss :  0.0350963210197
Iteration :  38   Loss :  0.0349463830196
Iteration :  39   Loss :  0.0347974821866
Iteration :  40   Loss :  0.0346495883967
Iteration :  41   Loss :  0.0345026739562
Iteration :  42   Loss :  0.0343567133429
Iteration :  43   Loss :  0.0342116829804
Iteration :  44   Loss :  0.03406756104
Iteration :  45   Loss :  0.0339243272668
Iteration :  46   Loss :  0.0337819628262
Iteration :  47   Loss :  0.0336404501686
Iteration :  48   Loss :  0.0334997729092
Iteration :  49   Loss :  0.033359915721
Iteration :  50   Loss :  0.0332208642398
Iteration :  51   Loss :  0.0330826049797
Iteration :  52   Loss :  0.0329451252564
Iteration :  53   Loss :  0.0328084131197
Iteration :  54   Loss :  0.0326724572919
Iteration :  55   Loss :  0.0325372471127
Iteration :  56   Loss :  0.0324027724895
Iteration :  57   Loss :  0.0322690238524
Iteration :  58   Loss :  0.0321359921133
Iteration :  59   Loss :  0.0320036686292
Iteration :  60   Loss :  0.0318720451681
Iteration :  61   Loss :  0.0317411138788
Iteration :  62   Loss :  0.031610867263
Iteration :  63   Loss :  0.0314812981495
Iteration :  64   Loss :  0.0313523996708
Iteration :  65   Loss :  0.0312241652421
Iteration :  66   Loss :  0.0310965885416
Iteration :  67   Loss :  0.0309696634922
Iteration :  68   Loss :  0.030843384245
Iteration :  69   Loss :  0.030717745164
Iteration :  70   Loss :  0.0305927408124
Iteration :  71   Loss :  0.0304683659388
Iteration :  72   Loss :  0.0303446154657
Iteration :  73   Loss :  0.0302214844784
Iteration :  74   Loss :  0.030098968214
Iteration :  75   Loss :  0.0299770620527
Iteration :  76   Loss :  0.0298557615081
Iteration :  77   Loss :  0.0297350622193
Iteration :  78   Loss :  0.029614959943
Iteration :  79   Loss :  0.0294954505465
Iteration :  80   Loss :  0.0293765300009
Iteration :  81   Loss :  0.0292581943745
Iteration :  82   Loss :  0.0291404398275
Iteration :  83   Loss :  0.0290232626061
Iteration :  84   Loss :  0.0289066590376
Iteration :  85   Loss :  0.0287906255256
Iteration :  86   Loss :  0.0286751585452
Iteration :  87   Loss :  0.0285602546391
Iteration :  88   Loss :  0.0284459104138
Iteration :  89   Loss :  0.0283321225354
Iteration :  90   Loss :  0.0282188877264
Iteration :  91   Loss :  0.0281062027626
Iteration :  92   Loss :  0.0279940644695
Iteration :  93   Loss :  0.0278824697202
Iteration :  94   Loss :  0.0277714154319
Iteration :  95   Loss :  0.0276608985639
Iteration :  96   Loss :  0.027550916115
Iteration :  97   Loss :  0.0274414651211
Iteration :  98   Loss :  0.0273325426536
Iteration :  99   Loss :  0.0272241458168
[-0.00075673  0.00054246 -0.00029929 ...,  0.00019918 -0.00017196
  0.00032947]
CROSS VALIDATION 15
Iteration :  0   Loss :  7.90162014984
Iteration :  1   Loss :  0.0575339947373
Iteration :  2   Loss :  0.04414483331
Iteration :  3   Loss :  0.0432760399673
Iteration :  4   Loss :  0.0426361146387
Iteration :  5   Loss :  0.0421254902869
Iteration :  6   Loss :  0.0416964156318
Iteration :  7   Loss :  0.0413226818099
Iteration :  8   Loss :  0.0409885388383
Iteration :  9   Loss :  0.0406838797774
Iteration :  10   Loss :  0.0404018866166
Iteration :  11   Loss :  0.0401377750728
Iteration :  12   Loss :  0.0398880788873
Iteration :  13   Loss :  0.039650219295
Iteration :  14   Loss :  0.0394222344386
Iteration :  15   Loss :  0.0392026029709
Iteration :  16   Loss :  0.038990125435
Iteration :  17   Loss :  0.038783842339
Iteration :  18   Loss :  0.0385829762474
Iteration :  19   Loss :  0.0383868900034
Iteration :  20   Loss :  0.038195056042
Iteration :  21   Loss :  0.0380070334826
Iteration :  22   Loss :  0.0378224507797
Iteration :  23   Loss :  0.0376409924113
Iteration :  24   Loss :  0.0374623885426
Iteration :  25   Loss :  0.0372864069134
Iteration :  26   Loss :  0.037112846407
Iteration :  27   Loss :  0.0369415319054
Iteration :  28   Loss :  0.0367723101389
Iteration :  29   Loss :  0.0366050463122
Iteration :  30   Loss :  0.0364396213405
Iteration :  31   Loss :  0.0362759295719
Iteration :  32   Loss :  0.0361138768966
Iteration :  33   Loss :  0.0359533791702
Iteration :  34   Loss :  0.0357943608892
Iteration :  35   Loss :  0.0356367540749
Iteration :  36   Loss :  0.0354804973269
Iteration :  37   Loss :  0.0353255350166
Iteration :  38   Loss :  0.0351718165985
Iteration :  39   Loss :  0.0350192960173
Iteration :  40   Loss :  0.034867931198
Iteration :  41   Loss :  0.034717683604
Iteration :  42   Loss :  0.034568517853
Iteration :  43   Loss :  0.0344204013833
Iteration :  44   Loss :  0.0342733041607
Iteration :  45   Loss :  0.0341271984227
Iteration :  46   Loss :  0.0339820584532
Iteration :  47   Loss :  0.033837860383
Iteration :  48   Loss :  0.0336945820146
Iteration :  49   Loss :  0.0335522026659
Iteration :  50   Loss :  0.0334107030318
Iteration :  51   Loss :  0.0332700650607
Iteration :  52   Loss :  0.0331302718444
Iteration :  53   Loss :  0.0329913075189
Iteration :  54   Loss :  0.0328531571767
Iteration :  55   Loss :  0.0327158067863
Iteration :  56   Loss :  0.0325792431214
Iteration :  57   Loss :  0.0324434536959
Iteration :  58   Loss :  0.0323084267054
Iteration :  59   Loss :  0.0321741509742
Iteration :  60   Loss :  0.0320406159079
Iteration :  61   Loss :  0.0319078114488
Iteration :  62   Loss :  0.0317757280372
Iteration :  63   Loss :  0.0316443565743
Iteration :  64   Loss :  0.0315136883897
Iteration :  65   Loss :  0.0313837152111
Iteration :  66   Loss :  0.0312544291364
Iteration :  67   Loss :  0.0311258226084
Iteration :  68   Loss :  0.0309978883914
Iteration :  69   Loss :  0.03087061955
Iteration :  70   Loss :  0.030744009429
Iteration :  71   Loss :  0.0306180516352
Iteration :  72   Loss :  0.0304927400209
Iteration :  73   Loss :  0.030368068668
Iteration :  74   Loss :  0.0302440318736
Iteration :  75   Loss :  0.0301206241369
Iteration :  76   Loss :  0.0299978401466
Iteration :  77   Loss :  0.0298756747694
Iteration :  78   Loss :  0.0297541230394
Iteration :  79   Loss :  0.029633180148
Iteration :  80   Loss :  0.0295128414348
Iteration :  81   Loss :  0.0293931023786
Iteration :  82   Loss :  0.0292739585898
Iteration :  83   Loss :  0.0291554058023
Iteration :  84   Loss :  0.0290374398668
Iteration :  85   Loss :  0.0289200567441
Iteration :  86   Loss :  0.0288032524987
Iteration :  87   Loss :  0.0286870232931
Iteration :  88   Loss :  0.0285713653825
Iteration :  89   Loss :  0.0284562751092
Iteration :  90   Loss :  0.0283417488982
Iteration :  91   Loss :  0.0282277832525
Iteration :  92   Loss :  0.0281143747485
Iteration :  93   Loss :  0.0280015200327
Iteration :  94   Loss :  0.027889215817
Iteration :  95   Loss :  0.0277774588757
Iteration :  96   Loss :  0.0276662460418
Iteration :  97   Loss :  0.0275555742041
Iteration :  98   Loss :  0.0274454403038
Iteration :  99   Loss :  0.0273358413319
[-0.00078006  0.00053086 -0.00031043 ...,  0.00021696 -0.0001581
  0.00033188]
CROSS VALIDATION 16
Iteration :  0   Loss :  7.87855090048
Iteration :  1   Loss :  0.0432571958734
Iteration :  2   Loss :  0.0425006684111
Iteration :  3   Loss :  0.0419581501931
Iteration :  4   Loss :  0.0415261981504
Iteration :  5   Loss :  0.0411607600137
Iteration :  6   Loss :  0.0408392093225
Iteration :  7   Loss :  0.0405485046094
Iteration :  8   Loss :  0.0402805101597
Iteration :  9   Loss :  0.040029847132
Iteration :  10   Loss :  0.039792796958
Iteration :  11   Loss :  0.0395666959061
Iteration :  12   Loss :  0.0393495796998
Iteration :  13   Loss :  0.0391399643957
Iteration :  14   Loss :  0.0389367056999
Iteration :  15   Loss :  0.0387389055299
Iteration :  16   Loss :  0.0385458481379
Iteration :  17   Loss :  0.0383569553427
Iteration :  18   Loss :  0.0381717544646
Iteration :  19   Loss :  0.0379898549168
Iteration :  20   Loss :  0.0378109308228
Iteration :  21   Loss :  0.0376347079116
Iteration :  22   Loss :  0.0374609535028
Iteration :  23   Loss :  0.0372894687574
Iteration :  24   Loss :  0.0371200826134
Iteration :  25   Loss :  0.0369526469925
Iteration :  26   Loss :  0.0367870329744
Iteration :  27   Loss :  0.0366231277168
Iteration :  28   Loss :  0.0364608319567
Iteration :  29   Loss :  0.0363000579673
Iteration :  30   Loss :  0.0361407278762
Iteration :  31   Loss :  0.0359827722719
Iteration :  32   Loss :  0.0358261290425
Iteration :  33   Loss :  0.0356707424022
Iteration :  34   Loss :  0.0355165620709
Iteration :  35   Loss :  0.0353635425796
Iteration :  36   Loss :  0.0352116426788
Iteration :  37   Loss :  0.0350608248338
Iteration :  38   Loss :  0.0349110547908
Iteration :  39   Loss :  0.0347623012031
Iteration :  40   Loss :  0.0346145353081
Iteration :  41   Loss :  0.0344677306467
Iteration :  42   Loss :  0.0343218628191
Iteration :  43   Loss :  0.0341769092708
Iteration :  44   Loss :  0.0340328491058
Iteration :  45   Loss :  0.0338896629217
Iteration :  46   Loss :  0.0337473326649
Iteration :  47   Loss :  0.0336058415017
Iteration :  48   Loss :  0.0334651737055
Iteration :  49   Loss :  0.0333253145545
Iteration :  50   Loss :  0.0331862502426
Iteration :  51   Loss :  0.0330479677986
Iteration :  52   Loss :  0.032910455014
Iteration :  53   Loss :  0.0327737003791
Iteration :  54   Loss :  0.0326376930241
Iteration :  55   Loss :  0.0325024226676
Iteration :  56   Loss :  0.0323678795691
Iteration :  57   Loss :  0.0322340544863
Iteration :  58   Loss :  0.0321009386367
Iteration :  59   Loss :  0.0319685236625
Iteration :  60   Loss :  0.0318368015988
Iteration :  61   Loss :  0.0317057648443
Iteration :  62   Loss :  0.0315754061353
Iteration :  63   Loss :  0.0314457185213
Iteration :  64   Loss :  0.0313166953431
Iteration :  65   Loss :  0.0311883302122
Iteration :  66   Loss :  0.0310606169927
Iteration :  67   Loss :  0.0309335497841
Iteration :  68   Loss :  0.0308071229052
Iteration :  69   Loss :  0.0306813308803
Iteration :  70   Loss :  0.0305561684255
Iteration :  71   Loss :  0.0304316304364
Iteration :  72   Loss :  0.0303077119768
Iteration :  73   Loss :  0.0301844082682
Iteration :  74   Loss :  0.0300617146799
Iteration :  75   Loss :  0.02993962672
Iteration :  76   Loss :  0.029818140027
Iteration :  77   Loss :  0.0296972503619
Iteration :  78   Loss :  0.0295769536012
Iteration :  79   Loss :  0.0294572457297
Iteration :  80   Loss :  0.0293381228343
Iteration :  81   Loss :  0.0292195810983
Iteration :  82   Loss :  0.0291016167957
Iteration :  83   Loss :  0.0289842262861
Iteration :  84   Loss :  0.0288674060096
Iteration :  85   Loss :  0.0287511524829
Iteration :  86   Loss :  0.0286354622943
Iteration :  87   Loss :  0.0285203321004
Iteration :  88   Loss :  0.0284057586217
Iteration :  89   Loss :  0.0282917386398
Iteration :  90   Loss :  0.0281782689935
Iteration :  91   Loss :  0.0280653465761
Iteration :  92   Loss :  0.0279529683323
Iteration :  93   Loss :  0.0278411312555
Iteration :  94   Loss :  0.027729832385
Iteration :  95   Loss :  0.0276190688041
Iteration :  96   Loss :  0.0275088376373
Iteration :  97   Loss :  0.0273991360482
Iteration :  98   Loss :  0.0272899612377
Iteration :  99   Loss :  0.027181310442
[-0.00076445  0.00053075 -0.00030517 ...,  0.00021319 -0.00015934
  0.00033682]
CROSS VALIDATION 17
Iteration :  0   Loss :  7.8694184231
Iteration :  1   Loss :  0.0571790419562
Iteration :  2   Loss :  0.0441688749334
Iteration :  3   Loss :  0.0432802160075
Iteration :  4   Loss :  0.0426305568453
Iteration :  5   Loss :  0.0421144574076
Iteration :  6   Loss :  0.0416820358087
Iteration :  7   Loss :  0.0413061442508
Iteration :  8   Loss :  0.0409705627224
Iteration :  9   Loss :  0.0406649265786
Iteration :  10   Loss :  0.0403822664879
Iteration :  11   Loss :  0.0401177044147
Iteration :  12   Loss :  0.0398677134499
Iteration :  13   Loss :  0.0396296741719
Iteration :  14   Loss :  0.0394015966461
Iteration :  15   Loss :  0.0391819396395
Iteration :  16   Loss :  0.0389694893013
Iteration :  17   Loss :  0.0387632755236
Iteration :  18   Loss :  0.0385625129109
Iteration :  19   Loss :  0.0383665582514
Iteration :  20   Loss :  0.0381748793146
Iteration :  21   Loss :  0.0379870315836
Iteration :  22   Loss :  0.0378026406502
Iteration :  23   Loss :  0.0376213887176
Iteration :  24   Loss :  0.0374430041288
Iteration :  25   Loss :  0.0372672531527
Iteration :  26   Loss :  0.0370939334772
Iteration :  27   Loss :  0.0369228690069
Iteration :  28   Loss :  0.0367539056685
Iteration :  29   Loss :  0.036586908002
Iteration :  30   Loss :  0.0364217563707
Iteration :  31   Loss :  0.0362583446617
Iteration :  32   Loss :  0.0360965783787
Iteration :  33   Loss :  0.0359363730513
Iteration :  34   Loss :  0.0357776529006
Iteration :  35   Loss :  0.0356203497133
Iteration :  36   Loss :  0.0354644018886
Iteration :  37   Loss :  0.0353097536255
Iteration :  38   Loss :  0.0351563542294
Iteration :  39   Loss :  0.0350041575152
Iteration :  40   Loss :  0.034853121294
Iteration :  41   Loss :  0.0347032069283
Iteration :  42   Loss :  0.0345543789456
Iteration :  43   Loss :  0.0344066047026
Iteration :  44   Loss :  0.034259854091
Iteration :  45   Loss :  0.0341140992793
Iteration :  46   Loss :  0.0339693144867
Iteration :  47   Loss :  0.0338254757831
Iteration :  48   Loss :  0.0336825609119
Iteration :  49   Loss :  0.0335405491339
Iteration :  50   Loss :  0.0333994210878
Iteration :  51   Loss :  0.0332591586659
Iteration :  52   Loss :  0.0331197449037
Iteration :  53   Loss :  0.0329811638806
Iteration :  54   Loss :  0.032843400631
Iteration :  55   Loss :  0.0327064410644
Iteration :  56   Loss :  0.0325702718936
Iteration :  57   Loss :  0.0324348805695
Iteration :  58   Loss :  0.0323002552227
Iteration :  59   Loss :  0.0321663846103
Iteration :  60   Loss :  0.0320332580675
Iteration :  61   Loss :  0.0319008654643
Iteration :  62   Loss :  0.0317691971652
Iteration :  63   Loss :  0.031638243993
Iteration :  64   Loss :  0.0315079971956
Iteration :  65   Loss :  0.031378448416
Iteration :  66   Loss :  0.0312495896641
Iteration :  67   Loss :  0.0311214132913
Iteration :  68   Loss :  0.0309939119674
Iteration :  69   Loss :  0.0308670786589
Iteration :  70   Loss :  0.0307409066092
Iteration :  71   Loss :  0.0306153893204
Iteration :  72   Loss :  0.0304905205366
Iteration :  73   Loss :  0.0303662942278
Iteration :  74   Loss :  0.0302427045765
Iteration :  75   Loss :  0.0301197459634
Iteration :  76   Loss :  0.0299974129554
Iteration :  77   Loss :  0.0298757002944
Iteration :  78   Loss :  0.0297546028863
Iteration :  79   Loss :  0.0296341157912
Iteration :  80   Loss :  0.0295142342143
Iteration :  81   Loss :  0.0293949534969
Iteration :  82   Loss :  0.0292762691091
Iteration :  83   Loss :  0.0291581766417
Iteration :  84   Loss :  0.0290406717995
Iteration :  85   Loss :  0.0289237503947
Iteration :  86   Loss :  0.0288074083409
Iteration :  87   Loss :  0.0286916416473
Iteration :  88   Loss :  0.0285764464133
Iteration :  89   Loss :  0.0284618188238
Iteration :  90   Loss :  0.028347755144
Iteration :  91   Loss :  0.0282342517156
Iteration :  92   Loss :  0.0281213049521
Iteration :  93   Loss :  0.0280089113352
Iteration :  94   Loss :  0.0278970674114
Iteration :  95   Loss :  0.027785769788
Iteration :  96   Loss :  0.0276750151305
Iteration :  97   Loss :  0.0275648001589
Iteration :  98   Loss :  0.0274551216457
Iteration :  99   Loss :  0.0273459764126
[-0.00077972  0.00053098 -0.00030907 ...,  0.00021449 -0.00015665
  0.00033272]
CROSS VALIDATION 18
Iteration :  0   Loss :  6.98237470111
Iteration :  1   Loss :  0.0472176028268
Iteration :  2   Loss :  0.0459347525385
Iteration :  3   Loss :  0.0450269865863
Iteration :  4   Loss :  0.0443217662454
Iteration :  5   Loss :  0.0437424175996
Iteration :  6   Loss :  0.0432480405639
Iteration :  7   Loss :  0.0428143333925
Iteration :  8   Loss :  0.0424257668297
Iteration :  9   Loss :  0.0420718760845
Iteration :  10   Loss :  0.0417453151869
Iteration :  11   Loss :  0.0414407566475
Iteration :  12   Loss :  0.0411542321516
Iteration :  13   Loss :  0.0408827188456
Iteration :  14   Loss :  0.0406238697052
Iteration :  15   Loss :  0.0403758321362
Iteration :  16   Loss :  0.0401371225852
Iteration :  17   Loss :  0.0399065378151
Iteration :  18   Loss :  0.0396830908265
Iteration :  19   Loss :  0.0394659637354
Iteration :  20   Loss :  0.0392544725588
Iteration :  21   Loss :  0.0390480405177
Iteration :  22   Loss :  0.0388461775315
Iteration :  23   Loss :  0.0386484642813
Iteration :  24   Loss :  0.0384545396905
Iteration :  25   Loss :  0.0382640909916
Iteration :  26   Loss :  0.0380768457747
Iteration :  27   Loss :  0.0378925655667
Iteration :  28   Loss :  0.0377110406086
Iteration :  29   Loss :  0.0375320855745
Iteration :  30   Loss :  0.0373555360406
Iteration :  31   Loss :  0.0371812455533
Iteration :  32   Loss :  0.0370090831814
Iteration :  33   Loss :  0.03683893146
Iteration :  34   Loss :  0.036670684656
Iteration :  35   Loss :  0.0365042472965
Iteration :  36   Loss :  0.036339532915
Iteration :  37   Loss :  0.0361764629783
Iteration :  38   Loss :  0.0360149659651
Iteration :  39   Loss :  0.0358549765702
Iteration :  40   Loss :  0.0356964350164
Iteration :  41   Loss :  0.0355392864556
Iteration :  42   Loss :  0.035383480448
Iteration :  43   Loss :  0.0352289705051
Iteration :  44   Loss :  0.0350757136902
Iteration :  45   Loss :  0.0349236702661
Iteration :  46   Loss :  0.0347728033843
Iteration :  47   Loss :  0.0346230788104
Iteration :  48   Loss :  0.0344744646805
Iteration :  49   Loss :  0.0343269312844
Iteration :  50   Loss :  0.0341804508726
Iteration :  51   Loss :  0.0340349974838
Iteration :  52   Loss :  0.033890546791
Iteration :  53   Loss :  0.0337470759625
Iteration :  54   Loss :  0.0336045635377
Iteration :  55   Loss :  0.0334629893156
Iteration :  56   Loss :  0.0333223342529
Iteration :  57   Loss :  0.0331825803734
Iteration :  58   Loss :  0.0330437106851
Iteration :  59   Loss :  0.0329057091051
Iteration :  60   Loss :  0.0327685603919
Iteration :  61   Loss :  0.0326322500832
Iteration :  62   Loss :  0.0324967644393
Iteration :  63   Loss :  0.0323620903923
Iteration :  64   Loss :  0.0322282154986
Iteration :  65   Loss :  0.0320951278957
Iteration :  66   Loss :  0.0319628162636
Iteration :  67   Loss :  0.0318312697879
Iteration :  68   Loss :  0.0317004781268
Iteration :  69   Loss :  0.0315704313809
Iteration :  70   Loss :  0.0314411200648
Iteration :  71   Loss :  0.0313125350815
Iteration :  72   Loss :  0.0311846676981
Iteration :  73   Loss :  0.0310575095244
Iteration :  74   Loss :  0.0309310524917
Iteration :  75   Loss :  0.0308052888347
Iteration :  76   Loss :  0.0306802110735
Iteration :  77   Loss :  0.0305558119979
Iteration :  78   Loss :  0.0304320846516
Iteration :  79   Loss :  0.030309022319
Iteration :  80   Loss :  0.0301866185116
Iteration :  81   Loss :  0.0300648669565
Iteration :  82   Loss :  0.0299437615844
Iteration :  83   Loss :  0.0298232965196
Iteration :  84   Loss :  0.0297034660701
Iteration :  85   Loss :  0.0295842647182
Iteration :  86   Loss :  0.029465687112
Iteration :  87   Loss :  0.0293477280577
Iteration :  88   Loss :  0.0292303825113
Iteration :  89   Loss :  0.0291136455723
Iteration :  90   Loss :  0.0289975124766
Iteration :  91   Loss :  0.0288819785906
Iteration :  92   Loss :  0.0287670394047
Iteration :  93   Loss :  0.0286526905285
Iteration :  94   Loss :  0.0285389276851
Iteration :  95   Loss :  0.0284257467063
Iteration :  96   Loss :  0.028313143528
Iteration :  97   Loss :  0.0282011141856
Iteration :  98   Loss :  0.02808965481
Iteration :  99   Loss :  0.0279787616235
[-0.00072071 -0.00019338 -0.00133684 ...,  0.00078029 -0.00014592
  0.00027602]
CROSS VALIDATION 19
Iteration :  0   Loss :  7.7016191808
Iteration :  1   Loss :  0.056750557002
Iteration :  2   Loss :  0.0429752688191
Iteration :  3   Loss :  0.0421280645803
Iteration :  4   Loss :  0.0414965912179
Iteration :  5   Loss :  0.0409902525473
Iteration :  6   Loss :  0.0405641586391
Iteration :  7   Loss :  0.0401931276433
Iteration :  8   Loss :  0.0398617903686
Iteration :  9   Loss :  0.0395601710895
Iteration :  10   Loss :  0.0392814822824
Iteration :  11   Loss :  0.0390209297107
Iteration :  12   Loss :  0.0387750222856
Iteration :  13   Loss :  0.0385411525609
Iteration :  14   Loss :  0.0383173308098
Iteration :  15   Loss :  0.0381020104324
Iteration :  16   Loss :  0.0378939698522
Iteration :  17   Loss :  0.0376922305419
Iteration :  18   Loss :  0.0374959988368
Iteration :  19   Loss :  0.0373046238081
Iteration :  20   Loss :  0.0371175662268
Iteration :  21   Loss :  0.0369343753362
Iteration :  22   Loss :  0.0367546712229
Iteration :  23   Loss :  0.0365781312653
Iteration :  24   Loss :  0.0364044795949
Iteration :  25   Loss :  0.0362334788138
Iteration :  26   Loss :  0.0360649234216
Iteration :  27   Loss :  0.0358986345524
Iteration :  28   Loss :  0.0357344557256
Iteration :  29   Loss :  0.0355722493889
Iteration :  30   Loss :  0.0354118940861
Iteration :  31   Loss :  0.0352532821205
Iteration :  32   Loss :  0.0350963176162
Iteration :  33   Loss :  0.0349409148993
Iteration :  34   Loss :  0.0347869971396
Iteration :  35   Loss :  0.034634495204
Iteration :  36   Loss :  0.0344833466852
Iteration :  37   Loss :  0.034333495074
Iteration :  38   Loss :  0.0341848890518
Iteration :  39   Loss :  0.0340374818822
Iteration :  40   Loss :  0.0338912308874
Iteration :  41   Loss :  0.0337460969946
Iteration :  42   Loss :  0.033602044342
Iteration :  43   Loss :  0.0334590399356
Iteration :  44   Loss :  0.0333170533495
Iteration :  45   Loss :  0.033176056463
Iteration :  46   Loss :  0.0330360232287
Iteration :  47   Loss :  0.0328969294694
Iteration :  48   Loss :  0.0327587526972
Iteration :  49   Loss :  0.0326214719542
Iteration :  50   Loss :  0.0324850676699
Iteration :  51   Loss :  0.0323495215354
Iteration :  52   Loss :  0.0322148163899
Iteration :  53   Loss :  0.0320809361203
Iteration :  54   Loss :  0.03194786557
Iteration :  55   Loss :  0.0318155904581
Iteration :  56   Loss :  0.031684097306
Iteration :  57   Loss :  0.0315533733712
Iteration :  58   Loss :  0.031423406588
Iteration :  59   Loss :  0.0312941855134
Iteration :  60   Loss :  0.0311656992784
Iteration :  61   Loss :  0.0310379375429
Iteration :  62   Loss :  0.0309108904566
Iteration :  63   Loss :  0.0307845486208
Iteration :  64   Loss :  0.0306589030563
Iteration :  65   Loss :  0.0305339451716
Iteration :  66   Loss :  0.0304096667358
Iteration :  67   Loss :  0.0302860598526
Iteration :  68   Loss :  0.030163116937
Iteration :  69   Loss :  0.0300408306936
Iteration :  70   Loss :  0.029919194097
Iteration :  71   Loss :  0.0297982003737
Iteration :  72   Loss :  0.029677842985
Iteration :  73   Loss :  0.0295581156123
Iteration :  74   Loss :  0.0294390121421
Iteration :  75   Loss :  0.0293205266534
Iteration :  76   Loss :  0.0292026534056
Iteration :  77   Loss :  0.0290853868268
Iteration :  78   Loss :  0.0289687215044
Iteration :  79   Loss :  0.0288526521746
Iteration :  80   Loss :  0.0287371737141
Iteration :  81   Loss :  0.0286222811317
Iteration :  82   Loss :  0.0285079695607
Iteration :  83   Loss :  0.0283942342517
Iteration :  84   Loss :  0.0282810705667
Iteration :  85   Loss :  0.028168473972
Iteration :  86   Loss :  0.0280564400335
Iteration :  87   Loss :  0.0279449644109
Iteration :  88   Loss :  0.027834042853
Iteration :  89   Loss :  0.0277236711932
Iteration :  90   Loss :  0.0276138453453
Iteration :  91   Loss :  0.0275045612995
Iteration :  92   Loss :  0.0273958151189
Iteration :  93   Loss :  0.027287602936
Iteration :  94   Loss :  0.0271799209496
Iteration :  95   Loss :  0.0270727654219
Iteration :  96   Loss :  0.0269661326755
Iteration :  97   Loss :  0.0268600190916
Iteration :  98   Loss :  0.0267544211066
Iteration :  99   Loss :  0.0266493352107
[-0.00092742  0.00053291 -0.00039059 ...,  0.00018909 -0.0001739   0.000351  ]
Accuracy (Logistic Loss):	0.95
Hinge Loss
CROSS VALIDATION 0
Iteration :  0   Loss :  24.6952662485
Iteration :  1   Loss :  0.0578452888476
Iteration :  2   Loss :  0.0576005391639
Iteration :  3   Loss :  0.0573568250426
Iteration :  4   Loss :  0.0571141421021
Iteration :  5   Loss :  0.0568724859795
Iteration :  6   Loss :  0.05663185233
Iteration :  7   Loss :  0.0563922368277
Iteration :  8   Loss :  0.0561536351645
Iteration :  9   Loss :  0.0559160430507
Iteration :  10   Loss :  0.0556794562149
Iteration :  11   Loss :  0.0554438704037
Iteration :  12   Loss :  0.0552092813816
Iteration :  13   Loss :  0.054975684931
Iteration :  14   Loss :  0.0547430768523
Iteration :  15   Loss :  0.0545114529636
Iteration :  16   Loss :  0.0542808091007
Iteration :  17   Loss :  0.0540511411169
Iteration :  18   Loss :  0.0538224448832
Iteration :  19   Loss :  0.0535947162881
Iteration :  20   Loss :  0.0533679512374
Iteration :  21   Loss :  0.0531421456541
Iteration :  22   Loss :  0.0529172954787
Iteration :  23   Loss :  0.0526933966688
Iteration :  24   Loss :  0.052470445199
Iteration :  25   Loss :  0.052248437061
Iteration :  26   Loss :  0.0520273682635
Iteration :  27   Loss :  0.0518072348321
Iteration :  28   Loss :  0.051588032809
Iteration :  29   Loss :  0.0513697582534
Iteration :  30   Loss :  0.0511524072412
Iteration :  31   Loss :  0.0509359758646
Iteration :  32   Loss :  0.0507204602326
Iteration :  33   Loss :  0.0505058564706
Iteration :  34   Loss :  0.0502921607203
Iteration :  35   Loss :  0.05007936914
Iteration :  36   Loss :  0.0498674779038
Iteration :  37   Loss :  0.0496564832025
Iteration :  38   Loss :  0.0494463812426
Iteration :  39   Loss :  0.0492371682468
Iteration :  40   Loss :  0.0490288404539
Iteration :  41   Loss :  0.0488213941184
Iteration :  42   Loss :  0.0486148255109
Iteration :  43   Loss :  0.0484091309175
Iteration :  44   Loss :  0.0482043066402
Iteration :  45   Loss :  0.0480003489966
Iteration :  46   Loss :  0.04779725432
Iteration :  47   Loss :  0.0475950189589
Iteration :  48   Loss :  0.0473936392775
Iteration :  49   Loss :  0.0471931116553
Iteration :  50   Loss :  0.0469934324873
Iteration :  51   Loss :  0.0467945981834
Iteration :  52   Loss :  0.0465966051691
Iteration :  53   Loss :  0.0463994498846
Iteration :  54   Loss :  0.0462031287855
Iteration :  55   Loss :  0.0460076383422
Iteration :  56   Loss :  0.0458129750402
Iteration :  57   Loss :  0.0456191353797
Iteration :  58   Loss :  0.0454261158758
Iteration :  59   Loss :  0.0452339130583
Iteration :  60   Loss :  0.0450425234718
Iteration :  61   Loss :  0.0448519436753
Iteration :  62   Loss :  0.0446621702427
Iteration :  63   Loss :  0.0444731997619
Iteration :  64   Loss :  0.0442850288358
Iteration :  65   Loss :  0.0440976540812
Iteration :  66   Loss :  0.0439110721295
Iteration :  67   Loss :  0.0437252796262
Iteration :  68   Loss :  0.0435402732312
Iteration :  69   Loss :  0.0433560496182
Iteration :  70   Loss :  0.0431726054754
Iteration :  71   Loss :  0.0429899375045
Iteration :  72   Loss :  0.0428080424217
Iteration :  73   Loss :  0.0426269169566
Iteration :  74   Loss :  0.042446557853
Iteration :  75   Loss :  0.0422669618683
Iteration :  76   Loss :  0.0420881257737
Iteration :  77   Loss :  0.041910046354
Iteration :  78   Loss :  0.0417327204076
Iteration :  79   Loss :  0.0415561447464
Iteration :  80   Loss :  0.0413803161961
Iteration :  81   Loss :  0.0412052315953
Iteration :  82   Loss :  0.0410308877965
Iteration :  83   Loss :  0.0408572816652
Iteration :  84   Loss :  0.0406844100802
Iteration :  85   Loss :  0.0405122699337
Iteration :  86   Loss :  0.0403408581307
Iteration :  87   Loss :  0.0401701715897
Iteration :  88   Loss :  0.0400002072419
Iteration :  89   Loss :  0.0398309620317
Iteration :  90   Loss :  0.0396624329162
Iteration :  91   Loss :  0.0394946168658
Iteration :  92   Loss :  0.0393275108632
Iteration :  93   Loss :  0.0391611119042
Iteration :  94   Loss :  0.0389954169973
Iteration :  95   Loss :  0.0388304231634
Iteration :  96   Loss :  0.0386661274364
Iteration :  97   Loss :  0.0385025268623
Iteration :  98   Loss :  0.0383396185
Iteration :  99   Loss :  0.0381773994206
[ -1.66835289e-03   1.26903764e-04   1.87545653e-04 ...,   5.13568398e-04
   6.79350625e-04  -8.11652112e-05]
CROSS VALIDATION 1
Iteration :  0   Loss :  14.7123832064
Iteration :  1   Loss :  0.313272441955
Iteration :  2   Loss :  0.0619784313214
Iteration :  3   Loss :  0.0617161938641
Iteration :  4   Loss :  0.0614550659619
Iteration :  5   Loss :  0.0611950429201
Iteration :  6   Loss :  0.060936120064
Iteration :  7   Loss :  0.0606782927385
Iteration :  8   Loss :  0.0604215563084
Iteration :  9   Loss :  0.060165906158
Iteration :  10   Loss :  0.059911337691
Iteration :  11   Loss :  0.0596578463308
Iteration :  12   Loss :  0.05940542752
Iteration :  13   Loss :  0.0591540767205
Iteration :  14   Loss :  0.0589037894135
Iteration :  15   Loss :  0.0586545610992
Iteration :  16   Loss :  0.0584063872969
Iteration :  17   Loss :  0.0581592635448
Iteration :  18   Loss :  0.0579131854001
Iteration :  19   Loss :  0.0576681484387
Iteration :  20   Loss :  0.0574241482552
Iteration :  21   Loss :  0.057181180463
Iteration :  22   Loss :  0.0569392406937
Iteration :  23   Loss :  0.0566983245979
Iteration :  24   Loss :  0.0564584278442
Iteration :  25   Loss :  0.0562195461196
Iteration :  26   Loss :  0.0559816751295
Iteration :  27   Loss :  0.0557448105973
Iteration :  28   Loss :  0.0555089482645
Iteration :  29   Loss :  0.0552740838909
Iteration :  30   Loss :  0.0550402132539
Iteration :  31   Loss :  0.0548073321489
Iteration :  32   Loss :  0.0545754363891
Iteration :  33   Loss :  0.0543445218053
Iteration :  34   Loss :  0.0541145842462
Iteration :  35   Loss :  0.0538856195778
Iteration :  36   Loss :  0.0536576236837
Iteration :  37   Loss :  0.0534305924649
Iteration :  38   Loss :  0.0532045218399
Iteration :  39   Loss :  0.0529794077441
Iteration :  40   Loss :  0.0527552461304
Iteration :  41   Loss :  0.0525320329689
Iteration :  42   Loss :  0.0523097642464
Iteration :  43   Loss :  0.0520884359669
Iteration :  44   Loss :  0.0518680441514
Iteration :  45   Loss :  0.0516485848375
Iteration :  46   Loss :  0.0514300540797
Iteration :  47   Loss :  0.0512124479493
Iteration :  48   Loss :  0.050995762534
Iteration :  49   Loss :  0.0507799939381
Iteration :  50   Loss :  0.0505651382825
Iteration :  51   Loss :  0.0503511917045
Iteration :  52   Loss :  0.0501381503577
Iteration :  53   Loss :  0.0499260104118
Iteration :  54   Loss :  0.0497147680531
Iteration :  55   Loss :  0.0495044194836
Iteration :  56   Loss :  0.0492949609218
Iteration :  57   Loss :  0.0490863886018
Iteration :  58   Loss :  0.0488786987739
Iteration :  59   Loss :  0.0486718877041
Iteration :  60   Loss :  0.0484659516744
Iteration :  61   Loss :  0.0482608869824
Iteration :  62   Loss :  0.0480566899413
Iteration :  63   Loss :  0.0478533568799
Iteration :  64   Loss :  0.0476508841428
Iteration :  65   Loss :  0.0474492680898
Iteration :  66   Loss :  0.0472485050961
Iteration :  67   Loss :  0.0470485915524
Iteration :  68   Loss :  0.0468495238646
Iteration :  69   Loss :  0.0466512984537
Iteration :  70   Loss :  0.046453911756
Iteration :  71   Loss :  0.0462573602229
Iteration :  72   Loss :  0.0460616403205
Iteration :  73   Loss :  0.0458667485303
Iteration :  74   Loss :  0.0456726813483
Iteration :  75   Loss :  0.0454794352856
Iteration :  76   Loss :  0.0452870068679
Iteration :  77   Loss :  0.0450953926357
Iteration :  78   Loss :  0.0449045891441
Iteration :  79   Loss :  0.0447145929628
Iteration :  80   Loss :  0.0445254006758
Iteration :  81   Loss :  0.0443370088819
Iteration :  82   Loss :  0.0441494141941
Iteration :  83   Loss :  0.0439626132398
Iteration :  84   Loss :  0.0437766026605
Iteration :  85   Loss :  0.0435913791122
Iteration :  86   Loss :  0.0434069392647
Iteration :  87   Loss :  0.0432232798022
Iteration :  88   Loss :  0.0430403974228
Iteration :  89   Loss :  0.0428582888386
Iteration :  90   Loss :  0.0426769507755
Iteration :  91   Loss :  0.0424963799734
Iteration :  92   Loss :  0.0423165731859
Iteration :  93   Loss :  0.0421375271804
Iteration :  94   Loss :  0.041959238738
Iteration :  95   Loss :  0.0417817046533
Iteration :  96   Loss :  0.0416049217345
Iteration :  97   Loss :  0.0414288868034
Iteration :  98   Loss :  0.0412535966952
Iteration :  99   Loss :  0.0410790482584
[-0.00193153 -0.00026143 -0.00043747 ...,  0.00101168  0.00091813
  0.00024307]
CROSS VALIDATION 2
Iteration :  0   Loss :  10.4090860366
Iteration :  1   Loss :  2.45843976133
Iteration :  2   Loss :  0.0621739962598
Iteration :  3   Loss :  0.061910931346
Iteration :  4   Loss :  0.0616489794885
Iteration :  5   Loss :  0.0613881359775
Iteration :  6   Loss :  0.0611283961238
Iteration :  7   Loss :  0.0608697552575
Iteration :  8   Loss :  0.0606122087287
Iteration :  9   Loss :  0.0603557519072
Iteration :  10   Loss :  0.0601003801823
Iteration :  11   Loss :  0.0598460889628
Iteration :  12   Loss :  0.0595928736771
Iteration :  13   Loss :  0.0593407297726
Iteration :  14   Loss :  0.0590896527163
Iteration :  15   Loss :  0.0588396379942
Iteration :  16   Loss :  0.0585906811115
Iteration :  17   Loss :  0.0583427775924
Iteration :  18   Loss :  0.0580959229799
Iteration :  19   Loss :  0.057850112836
Iteration :  20   Loss :  0.0576053427414
Iteration :  21   Loss :  0.0573616082956
Iteration :  22   Loss :  0.0571189051167
Iteration :  23   Loss :  0.0568772288412
Iteration :  24   Loss :  0.0566365751243
Iteration :  25   Loss :  0.0563969396392
Iteration :  26   Loss :  0.0561583180779
Iteration :  27   Loss :  0.0559207061502
Iteration :  28   Loss :  0.0556840995844
Iteration :  29   Loss :  0.0554484941266
Iteration :  30   Loss :  0.055213885541
Iteration :  31   Loss :  0.0549802696097
Iteration :  32   Loss :  0.0547476421327
Iteration :  33   Loss :  0.0545159989278
Iteration :  34   Loss :  0.0542853358304
Iteration :  35   Loss :  0.0540556486936
Iteration :  36   Loss :  0.0538269333879
Iteration :  37   Loss :  0.0535991858014
Iteration :  38   Loss :  0.0533724018396
Iteration :  39   Loss :  0.0531465774254
Iteration :  40   Loss :  0.0529217084987
Iteration :  41   Loss :  0.0526977910168
Iteration :  42   Loss :  0.0524748209541
Iteration :  43   Loss :  0.0522527943018
Iteration :  44   Loss :  0.0520317070684
Iteration :  45   Loss :  0.0518115552789
Iteration :  46   Loss :  0.0515923349756
Iteration :  47   Loss :  0.0513740422171
Iteration :  48   Loss :  0.0511566730789
Iteration :  49   Loss :  0.0509402236531
Iteration :  50   Loss :  0.0507246900483
Iteration :  51   Loss :  0.0505100683894
Iteration :  52   Loss :  0.0502963548181
Iteration :  53   Loss :  0.0500835454921
Iteration :  54   Loss :  0.0498716365853
Iteration :  55   Loss :  0.0496606242881
Iteration :  56   Loss :  0.0494505048068
Iteration :  57   Loss :  0.0492412743638
Iteration :  58   Loss :  0.0490329291975
Iteration :  59   Loss :  0.0488254655621
Iteration :  60   Loss :  0.0486188797278
Iteration :  61   Loss :  0.0484131679806
Iteration :  62   Loss :  0.0482083266221
Iteration :  63   Loss :  0.0480043519695
Iteration :  64   Loss :  0.0478012403558
Iteration :  65   Loss :  0.0475989881294
Iteration :  66   Loss :  0.047397591654
Iteration :  67   Loss :  0.047197047309
Iteration :  68   Loss :  0.0469973514887
Iteration :  69   Loss :  0.0467985006032
Iteration :  70   Loss :  0.0466004910772
Iteration :  71   Loss :  0.0464033193511
Iteration :  72   Loss :  0.0462069818798
Iteration :  73   Loss :  0.0460114751337
Iteration :  74   Loss :  0.0458167955978
Iteration :  75   Loss :  0.0456229397721
Iteration :  76   Loss :  0.0454299041714
Iteration :  77   Loss :  0.0452376853252
Iteration :  78   Loss :  0.0450462797778
Iteration :  79   Loss :  0.044855684088
Iteration :  80   Loss :  0.0446658948293
Iteration :  81   Loss :  0.0444769085894
Iteration :  82   Loss :  0.0442887219708
Iteration :  83   Loss :  0.0441013315901
Iteration :  84   Loss :  0.0439147340785
Iteration :  85   Loss :  0.0437289260812
Iteration :  86   Loss :  0.0435439042576
Iteration :  87   Loss :  0.0433596652814
Iteration :  88   Loss :  0.0431762058402
Iteration :  89   Loss :  0.0429935226359
Iteration :  90   Loss :  0.0428116123839
Iteration :  91   Loss :  0.042630471814
Iteration :  92   Loss :  0.0424500976694
Iteration :  93   Loss :  0.0422704867073
Iteration :  94   Loss :  0.0420916356987
Iteration :  95   Loss :  0.0419135414281
Iteration :  96   Loss :  0.0417362006937
Iteration :  97   Loss :  0.0415596103071
Iteration :  98   Loss :  0.0413837670936
Iteration :  99   Loss :  0.0412086678917
[ -1.62923799e-03  -6.79582640e-04  -7.05774765e-04 ...,   1.72239154e-03
   7.67075490e-04   8.10389274e-05]
CROSS VALIDATION 3
Iteration :  0   Loss :  20.8172274791
Iteration :  1   Loss :  1.36239370084
Iteration :  2   Loss :  0.707064380293
Iteration :  3   Loss :  0.0651578269086
Iteration :  4   Loss :  0.0648821370841
Iteration :  5   Loss :  0.0646076137332
Iteration :  6   Loss :  0.0643342519202
Iteration :  7   Loss :  0.0640620467308
Iteration :  8   Loss :  0.0637909932709
Iteration :  9   Loss :  0.0635210866677
Iteration :  10   Loss :  0.0632523220685
Iteration :  11   Loss :  0.0629846946414
Iteration :  12   Loss :  0.062718199575
Iteration :  13   Loss :  0.0624528320781
Iteration :  14   Loss :  0.0621885873798
Iteration :  15   Loss :  0.0619254607294
Iteration :  16   Loss :  0.0616634473964
Iteration :  17   Loss :  0.0614025426702
Iteration :  18   Loss :  0.0611427418602
Iteration :  19   Loss :  0.0608840402955
Iteration :  20   Loss :  0.0606264333252
Iteration :  21   Loss :  0.0603699163179
Iteration :  22   Loss :  0.0601144846619
Iteration :  23   Loss :  0.0598601337648
Iteration :  24   Loss :  0.0596068590539
Iteration :  25   Loss :  0.0593546559758
Iteration :  26   Loss :  0.0591035199963
Iteration :  27   Loss :  0.0588534466002
Iteration :  28   Loss :  0.0586044312918
Iteration :  29   Loss :  0.0583564695942
Iteration :  30   Loss :  0.0581095570493
Iteration :  31   Loss :  0.0578636892182
Iteration :  32   Loss :  0.0576188616805
Iteration :  33   Loss :  0.0573750700346
Iteration :  34   Loss :  0.0571323098976
Iteration :  35   Loss :  0.0568905769051
Iteration :  36   Loss :  0.056649866711
Iteration :  37   Loss :  0.0564101749878
Iteration :  38   Loss :  0.0561714974263
Iteration :  39   Loss :  0.0559338297354
Iteration :  40   Loss :  0.0556971676422
Iteration :  41   Loss :  0.055461506892
Iteration :  42   Loss :  0.055226843248
Iteration :  43   Loss :  0.0549931724913
Iteration :  44   Loss :  0.0547604904208
Iteration :  45   Loss :  0.0545287928534
Iteration :  46   Loss :  0.0542980756235
Iteration :  47   Loss :  0.0540683345832
Iteration :  48   Loss :  0.0538395656021
Iteration :  49   Loss :  0.0536117645674
Iteration :  50   Loss :  0.0533849273835
Iteration :  51   Loss :  0.0531590499724
Iteration :  52   Loss :  0.052934128273
Iteration :  53   Loss :  0.0527101582417
Iteration :  54   Loss :  0.0524871358519
Iteration :  55   Loss :  0.0522650570939
Iteration :  56   Loss :  0.0520439179753
Iteration :  57   Loss :  0.0518237145202
Iteration :  58   Loss :  0.0516044427698
Iteration :  59   Loss :  0.0513860987819
Iteration :  60   Loss :  0.0511686786311
Iteration :  61   Loss :  0.0509521784085
Iteration :  62   Loss :  0.0507365942218
Iteration :  63   Loss :  0.0505219221951
Iteration :  64   Loss :  0.050308158469
Iteration :  65   Loss :  0.0500952992005
Iteration :  66   Loss :  0.0498833405625
Iteration :  67   Loss :  0.0496722787445
Iteration :  68   Loss :  0.0494621099519
Iteration :  69   Loss :  0.0492528304063
Iteration :  70   Loss :  0.0490444363451
Iteration :  71   Loss :  0.0488369240217
Iteration :  72   Loss :  0.0486302897055
Iteration :  73   Loss :  0.0484245296814
Iteration :  74   Loss :  0.0482196402503
Iteration :  75   Loss :  0.0480156177286
Iteration :  76   Loss :  0.0478124584482
Iteration :  77   Loss :  0.0476101587568
Iteration :  78   Loss :  0.0474087150173
Iteration :  79   Loss :  0.0472081236081
Iteration :  80   Loss :  0.0470083809228
Iteration :  81   Loss :  0.0468094833705
Iteration :  82   Loss :  0.0466114273753
Iteration :  83   Loss :  0.0464142093765
Iteration :  84   Loss :  0.0462178258284
Iteration :  85   Loss :  0.0460222732003
Iteration :  86   Loss :  0.0458275479766
Iteration :  87   Loss :  0.0456336466565
Iteration :  88   Loss :  0.0454405657538
Iteration :  89   Loss :  0.0452483017973
Iteration :  90   Loss :  0.0450568513305
Iteration :  91   Loss :  0.0448662109113
Iteration :  92   Loss :  0.0446763771124
Iteration :  93   Loss :  0.0444873465208
Iteration :  94   Loss :  0.0442991157382
Iteration :  95   Loss :  0.0441116813804
Iteration :  96   Loss :  0.0439250400777
Iteration :  97   Loss :  0.0437391884745
Iteration :  98   Loss :  0.0435541232296
Iteration :  99   Loss :  0.0433698410159
[-0.0023868  -0.00024002 -0.00043042 ...,  0.00180724  0.0010944
  0.00016245]
CROSS VALIDATION 4
Iteration :  0   Loss :  20.8172274791
Iteration :  1   Loss :  1.36239370084
Iteration :  2   Loss :  0.707064380293
Iteration :  3   Loss :  0.0651578269086
Iteration :  4   Loss :  0.0648821370841
Iteration :  5   Loss :  0.0646076137332
Iteration :  6   Loss :  0.0643342519202
Iteration :  7   Loss :  0.0640620467308
Iteration :  8   Loss :  0.0637909932709
Iteration :  9   Loss :  0.0635210866677
Iteration :  10   Loss :  0.0632523220685
Iteration :  11   Loss :  0.0629846946414
Iteration :  12   Loss :  0.062718199575
Iteration :  13   Loss :  0.0624528320781
Iteration :  14   Loss :  0.0621885873798
Iteration :  15   Loss :  0.0619254607294
Iteration :  16   Loss :  0.0616634473964
Iteration :  17   Loss :  0.0614025426702
Iteration :  18   Loss :  0.0611427418602
Iteration :  19   Loss :  0.0608840402955
Iteration :  20   Loss :  0.0606264333252
Iteration :  21   Loss :  0.0603699163179
Iteration :  22   Loss :  0.0601144846619
Iteration :  23   Loss :  0.0598601337648
Iteration :  24   Loss :  0.0596068590539
Iteration :  25   Loss :  0.0593546559758
Iteration :  26   Loss :  0.0591035199963
Iteration :  27   Loss :  0.0588534466002
Iteration :  28   Loss :  0.0586044312918
Iteration :  29   Loss :  0.0583564695942
Iteration :  30   Loss :  0.0581095570493
Iteration :  31   Loss :  0.0578636892182
Iteration :  32   Loss :  0.0576188616805
Iteration :  33   Loss :  0.0573750700346
Iteration :  34   Loss :  0.0571323098976
Iteration :  35   Loss :  0.0568905769051
Iteration :  36   Loss :  0.056649866711
Iteration :  37   Loss :  0.0564101749878
Iteration :  38   Loss :  0.0561714974263
Iteration :  39   Loss :  0.0559338297354
Iteration :  40   Loss :  0.0556971676422
Iteration :  41   Loss :  0.055461506892
Iteration :  42   Loss :  0.055226843248
Iteration :  43   Loss :  0.0549931724913
Iteration :  44   Loss :  0.0547604904208
Iteration :  45   Loss :  0.0545287928534
Iteration :  46   Loss :  0.0542980756235
Iteration :  47   Loss :  0.0540683345832
Iteration :  48   Loss :  0.0538395656021
Iteration :  49   Loss :  0.0536117645674
Iteration :  50   Loss :  0.0533849273835
Iteration :  51   Loss :  0.0531590499724
Iteration :  52   Loss :  0.052934128273
Iteration :  53   Loss :  0.0527101582417
Iteration :  54   Loss :  0.0524871358519
Iteration :  55   Loss :  0.0522650570939
Iteration :  56   Loss :  0.0520439179753
Iteration :  57   Loss :  0.0518237145202
Iteration :  58   Loss :  0.0516044427698
Iteration :  59   Loss :  0.0513860987819
Iteration :  60   Loss :  0.0511686786311
Iteration :  61   Loss :  0.0509521784085
Iteration :  62   Loss :  0.0507365942218
Iteration :  63   Loss :  0.0505219221951
Iteration :  64   Loss :  0.050308158469
Iteration :  65   Loss :  0.0500952992005
Iteration :  66   Loss :  0.0498833405625
Iteration :  67   Loss :  0.0496722787445
Iteration :  68   Loss :  0.0494621099519
Iteration :  69   Loss :  0.0492528304063
Iteration :  70   Loss :  0.0490444363451
Iteration :  71   Loss :  0.0488369240217
Iteration :  72   Loss :  0.0486302897055
Iteration :  73   Loss :  0.0484245296814
Iteration :  74   Loss :  0.0482196402503
Iteration :  75   Loss :  0.0480156177286
Iteration :  76   Loss :  0.0478124584482
Iteration :  77   Loss :  0.0476101587568
Iteration :  78   Loss :  0.0474087150173
Iteration :  79   Loss :  0.0472081236081
Iteration :  80   Loss :  0.0470083809228
Iteration :  81   Loss :  0.0468094833705
Iteration :  82   Loss :  0.0466114273753
Iteration :  83   Loss :  0.0464142093765
Iteration :  84   Loss :  0.0462178258284
Iteration :  85   Loss :  0.0460222732003
Iteration :  86   Loss :  0.0458275479766
Iteration :  87   Loss :  0.0456336466565
Iteration :  88   Loss :  0.0454405657538
Iteration :  89   Loss :  0.0452483017973
Iteration :  90   Loss :  0.0450568513305
Iteration :  91   Loss :  0.0448662109113
Iteration :  92   Loss :  0.0446763771124
Iteration :  93   Loss :  0.0444873465208
Iteration :  94   Loss :  0.0442991157382
Iteration :  95   Loss :  0.0441116813804
Iteration :  96   Loss :  0.0439250400777
Iteration :  97   Loss :  0.0437391884745
Iteration :  98   Loss :  0.0435541232296
Iteration :  99   Loss :  0.0433698410159
[-0.0023868  -0.00024002 -0.00043042 ...,  0.00180724  0.0010944
  0.00016245]
CROSS VALIDATION 5
Iteration :  0   Loss :  9.67112755394
Iteration :  1   Loss :  0.0599526814398
Iteration :  2   Loss :  0.0596990151497
Iteration :  3   Loss :  0.0594464221491
Iteration :  4   Loss :  0.0591948978969
Iteration :  5   Loss :  0.0589444378711
Iteration :  6   Loss :  0.0586950375688
Iteration :  7   Loss :  0.0584466925061
Iteration :  8   Loss :  0.0581993982184
Iteration :  9   Loss :  0.0579531502595
Iteration :  10   Loss :  0.0577079442025
Iteration :  11   Loss :  0.0574637756388
Iteration :  12   Loss :  0.0572206401788
Iteration :  13   Loss :  0.0569785334512
Iteration :  14   Loss :  0.0567374511035
Iteration :  15   Loss :  0.0564973888013
Iteration :  16   Loss :  0.0562583422286
Iteration :  17   Loss :  0.056020307088
Iteration :  18   Loss :  0.0557832790998
Iteration :  19   Loss :  0.0555472540027
Iteration :  20   Loss :  0.0553122275533
Iteration :  21   Loss :  0.0550781955263
Iteration :  22   Loss :  0.0548451537142
Iteration :  23   Loss :  0.0546130979272
Iteration :  24   Loss :  0.0543820239934
Iteration :  25   Loss :  0.0541519277585
Iteration :  26   Loss :  0.0539228050856
Iteration :  27   Loss :  0.0536946518556
Iteration :  28   Loss :  0.0534674639667
Iteration :  29   Loss :  0.0532412373343
Iteration :  30   Loss :  0.0530159678913
Iteration :  31   Loss :  0.0527916515877
Iteration :  32   Loss :  0.0525682843907
Iteration :  33   Loss :  0.0523458622845
Iteration :  34   Loss :  0.0521243812703
Iteration :  35   Loss :  0.0519038373663
Iteration :  36   Loss :  0.0516842266074
Iteration :  37   Loss :  0.0514655450455
Iteration :  38   Loss :  0.0512477887489
Iteration :  39   Loss :  0.0510309538029
Iteration :  40   Loss :  0.0508150363089
Iteration :  41   Loss :  0.0506000323853
Iteration :  42   Loss :  0.0503859381666
Iteration :  43   Loss :  0.0501727498037
Iteration :  44   Loss :  0.0499604634638
Iteration :  45   Loss :  0.0497490753305
Iteration :  46   Loss :  0.0495385816032
Iteration :  47   Loss :  0.0493289784977
Iteration :  48   Loss :  0.0491202622457
Iteration :  49   Loss :  0.0489124290947
Iteration :  50   Loss :  0.0487054753083
Iteration :  51   Loss :  0.0484993971659
Iteration :  52   Loss :  0.0482941909623
Iteration :  53   Loss :  0.0480898530084
Iteration :  54   Loss :  0.0478863796306
Iteration :  55   Loss :  0.0476837671706
Iteration :  56   Loss :  0.0474820119859
Iteration :  57   Loss :  0.0472811104493
Iteration :  58   Loss :  0.0470810589488
Iteration :  59   Loss :  0.0468818538879
Iteration :  60   Loss :  0.0466834916852
Iteration :  61   Loss :  0.0464859687745
Iteration :  62   Loss :  0.0462892816046
Iteration :  63   Loss :  0.0460934266394
Iteration :  64   Loss :  0.0458984003578
Iteration :  65   Loss :  0.0457041992535
Iteration :  66   Loss :  0.0455108198352
Iteration :  67   Loss :  0.0453182586261
Iteration :  68   Loss :  0.0451265121643
Iteration :  69   Loss :  0.0449355770026
Iteration :  70   Loss :  0.0447454497082
Iteration :  71   Loss :  0.0445561268631
Iteration :  72   Loss :  0.0443676050633
Iteration :  73   Loss :  0.0441798809198
Iteration :  74   Loss :  0.0439929510574
Iteration :  75   Loss :  0.0438068121156
Iteration :  76   Loss :  0.0436214607477
Iteration :  77   Loss :  0.0434368936216
Iteration :  78   Loss :  0.043253107419
Iteration :  79   Loss :  0.0430700988357
Iteration :  80   Loss :  0.0428878645816
Iteration :  81   Loss :  0.0427064013803
Iteration :  82   Loss :  0.0425257059695
Iteration :  83   Loss :  0.0423457751006
Iteration :  84   Loss :  0.0421666055386
Iteration :  85   Loss :  0.0419881940625
Iteration :  86   Loss :  0.0418105374647
Iteration :  87   Loss :  0.0416336325511
Iteration :  88   Loss :  0.0414574761415
Iteration :  89   Loss :  0.0412820650687
Iteration :  90   Loss :  0.0411073961791
Iteration :  91   Loss :  0.0409334663325
Iteration :  92   Loss :  0.040760272402
Iteration :  93   Loss :  0.0405878112737
Iteration :  94   Loss :  0.0404160798472
Iteration :  95   Loss :  0.0402450750349
Iteration :  96   Loss :  0.0400747937625
Iteration :  97   Loss :  0.0399052329686
Iteration :  98   Loss :  0.0397363896048
Iteration :  99   Loss :  0.0395682606356
[-0.00082086  0.00066262 -0.00078036 ...,  0.00016501  0.00021902
  0.00048577]
CROSS VALIDATION 6
Iteration :  0   Loss :  20.8167168977
Iteration :  1   Loss :  1.36236093756
Iteration :  2   Loss :  0.0617058365578
Iteration :  3   Loss :  0.0614447524785
Iteration :  4   Loss :  0.0611847730742
Iteration :  5   Loss :  0.0609258936709
Iteration :  6   Loss :  0.0606681096144
Iteration :  7   Loss :  0.0604114162702
Iteration :  8   Loss :  0.0601558090234
Iteration :  9   Loss :  0.0599012832785
Iteration :  10   Loss :  0.0596478344595
Iteration :  11   Loss :  0.05939545801
Iteration :  12   Loss :  0.0591441493927
Iteration :  13   Loss :  0.0588939040892
Iteration :  14   Loss :  0.0586447176008
Iteration :  15   Loss :  0.0583965854474
Iteration :  16   Loss :  0.058149503168
Iteration :  17   Loss :  0.0579034663206
Iteration :  18   Loss :  0.0576584704816
Iteration :  19   Loss :  0.0574145112466
Iteration :  20   Loss :  0.0571715842296
Iteration :  21   Loss :  0.0569296850631
Iteration :  22   Loss :  0.0566888093982
Iteration :  23   Loss :  0.0564489529043
Iteration :  24   Loss :  0.0562101112692
Iteration :  25   Loss :  0.0559722801989
Iteration :  26   Loss :  0.0557354554177
Iteration :  27   Loss :  0.0554996326678
Iteration :  28   Loss :  0.0552648077094
Iteration :  29   Loss :  0.055030976321
Iteration :  30   Loss :  0.0547981342984
Iteration :  31   Loss :  0.0545662774557
Iteration :  32   Loss :  0.0543354016244
Iteration :  33   Loss :  0.0541055026537
Iteration :  34   Loss :  0.0538765764105
Iteration :  35   Loss :  0.053648618779
Iteration :  36   Loss :  0.053421625661
Iteration :  37   Loss :  0.0531955929755
Iteration :  38   Loss :  0.0529705166587
Iteration :  39   Loss :  0.0527463926642
Iteration :  40   Loss :  0.0525232169626
Iteration :  41   Loss :  0.0523009855415
Iteration :  42   Loss :  0.0520796944057
Iteration :  43   Loss :  0.0518593395767
Iteration :  44   Loss :  0.0516399170928
Iteration :  45   Loss :  0.0514214230092
Iteration :  46   Loss :  0.0512038533978
Iteration :  47   Loss :  0.0509872043469
Iteration :  48   Loss :  0.0507714719617
Iteration :  49   Loss :  0.0505566523635
Iteration :  50   Loss :  0.0503427416903
Iteration :  51   Loss :  0.0501297360964
Iteration :  52   Loss :  0.0499176317522
Iteration :  53   Loss :  0.0497064248445
Iteration :  54   Loss :  0.0494961115761
Iteration :  55   Loss :  0.0492866881659
Iteration :  56   Loss :  0.0490781508488
Iteration :  57   Loss :  0.0488704958757
Iteration :  58   Loss :  0.0486637195133
Iteration :  59   Loss :  0.0484578180441
Iteration :  60   Loss :  0.0482527877664
Iteration :  61   Loss :  0.0480486249939
Iteration :  62   Loss :  0.0478453260562
Iteration :  63   Loss :  0.0476428872984
Iteration :  64   Loss :  0.0474413050809
Iteration :  65   Loss :  0.0472405757796
Iteration :  66   Loss :  0.0470406957857
Iteration :  67   Loss :  0.0468416615057
Iteration :  68   Loss :  0.0466434693613
Iteration :  69   Loss :  0.0464461157894
Iteration :  70   Loss :  0.0462495972418
Iteration :  71   Loss :  0.0460539101854
Iteration :  72   Loss :  0.0458590511022
Iteration :  73   Loss :  0.0456650164889
Iteration :  74   Loss :  0.0454718028571
Iteration :  75   Loss :  0.045279406733
Iteration :  76   Loss :  0.0450878246578
Iteration :  77   Loss :  0.0448970531871
Iteration :  78   Loss :  0.0447070888912
Iteration :  79   Loss :  0.0445179283548
Iteration :  80   Loss :  0.0443295681771
Iteration :  81   Loss :  0.0441420049718
Iteration :  82   Loss :  0.0439552353666
Iteration :  83   Loss :  0.043769256004
Iteration :  84   Loss :  0.0435840635401
Iteration :  85   Loss :  0.0433996546456
Iteration :  86   Loss :  0.0432160260051
Iteration :  87   Loss :  0.0430331743173
Iteration :  88   Loss :  0.0428510962948
Iteration :  89   Loss :  0.0426697886642
Iteration :  90   Loss :  0.0424892481657
Iteration :  91   Loss :  0.0423094715537
Iteration :  92   Loss :  0.042130455596
Iteration :  93   Loss :  0.0419521970742
Iteration :  94   Loss :  0.0417746927835
Iteration :  95   Loss :  0.0415979395327
Iteration :  96   Loss :  0.0414219341441
Iteration :  97   Loss :  0.0412466734533
Iteration :  98   Loss :  0.0410721543095
Iteration :  99   Loss :  0.0408983735751
[ -2.69096788e-03  -5.32129807e-04  -6.31832084e-04 ...,   1.68838535e-03
   9.86846778e-04   8.10130977e-05]
CROSS VALIDATION 7
Iteration :  0   Loss :  20.8167168977
Iteration :  1   Loss :  1.36236093756
Iteration :  2   Loss :  0.706963994768
Iteration :  3   Loss :  0.0651583431713
Iteration :  4   Loss :  0.0648826511625
Iteration :  5   Loss :  0.0646081256364
Iteration :  6   Loss :  0.0643347616576
Iteration :  7   Loss :  0.0640625543114
Iteration :  8   Loss :  0.0637914987039
Iteration :  9   Loss :  0.0635215899621
Iteration :  10   Loss :  0.0632528232334
Iteration :  11   Loss :  0.0629851936859
Iteration :  12   Loss :  0.0627186965079
Iteration :  13   Loss :  0.0624533269084
Iteration :  14   Loss :  0.0621890801165
Iteration :  15   Loss :  0.0619259513813
Iteration :  16   Loss :  0.0616639359723
Iteration :  17   Loss :  0.0614030291789
Iteration :  18   Loss :  0.0611432263104
Iteration :  19   Loss :  0.060884522696
Iteration :  20   Loss :  0.0606269136846
Iteration :  21   Loss :  0.0603703946448
Iteration :  22   Loss :  0.0601149609649
Iteration :  23   Loss :  0.0598606080525
Iteration :  24   Loss :  0.0596073313349
Iteration :  25   Loss :  0.0593551262585
Iteration :  26   Loss :  0.0591039882891
Iteration :  27   Loss :  0.0588539129117
Iteration :  28   Loss :  0.0586048956303
Iteration :  29   Loss :  0.058356931968
Iteration :  30   Loss :  0.0581100174667
Iteration :  31   Loss :  0.0578641476875
Iteration :  32   Loss :  0.05761931821
Iteration :  33   Loss :  0.0573755246325
Iteration :  34   Loss :  0.0571327625721
Iteration :  35   Loss :  0.0568910276642
Iteration :  36   Loss :  0.0566503155629
Iteration :  37   Loss :  0.0564106219406
Iteration :  38   Loss :  0.056171942488
Iteration :  39   Loss :  0.055934272914
Iteration :  40   Loss :  0.0556976089457
Iteration :  41   Loss :  0.0554619463283
Iteration :  42   Loss :  0.0552272808249
Iteration :  43   Loss :  0.0549936082168
Iteration :  44   Loss :  0.0547609243027
Iteration :  45   Loss :  0.0545292248995
Iteration :  46   Loss :  0.0542985058416
Iteration :  47   Loss :  0.054068762981
Iteration :  48   Loss :  0.0538399921873
Iteration :  49   Loss :  0.0536121893476
Iteration :  50   Loss :  0.0533853503665
Iteration :  51   Loss :  0.0531594711656
Iteration :  52   Loss :  0.0529345476841
Iteration :  53   Loss :  0.0527105758783
Iteration :  54   Loss :  0.0524875517214
Iteration :  55   Loss :  0.0522654712038
Iteration :  56   Loss :  0.052044330333
Iteration :  57   Loss :  0.0518241251332
Iteration :  58   Loss :  0.0516048516455
Iteration :  59   Loss :  0.0513865059276
Iteration :  60   Loss :  0.0511690840541
Iteration :  61   Loss :  0.0509525821161
Iteration :  62   Loss :  0.0507369962213
Iteration :  63   Loss :  0.0505223224937
Iteration :  64   Loss :  0.0503085570739
Iteration :  65   Loss :  0.0500956961188
Iteration :  66   Loss :  0.0498837358015
Iteration :  67   Loss :  0.0496726723112
Iteration :  68   Loss :  0.0494625018534
Iteration :  69   Loss :  0.0492532206495
Iteration :  70   Loss :  0.0490448249372
Iteration :  71   Loss :  0.0488373109696
Iteration :  72   Loss :  0.0486306750162
Iteration :  73   Loss :  0.0484249133618
Iteration :  74   Loss :  0.0482200223073
Iteration :  75   Loss :  0.048015998169
Iteration :  76   Loss :  0.047812837279
Iteration :  77   Loss :  0.0476105359847
Iteration :  78   Loss :  0.0474090906491
Iteration :  79   Loss :  0.0472084976506
Iteration :  80   Loss :  0.0470087533827
Iteration :  81   Loss :  0.0468098542545
Iteration :  82   Loss :  0.04661179669
Iteration :  83   Loss :  0.0464145771286
Iteration :  84   Loss :  0.0462181920245
Iteration :  85   Loss :  0.046022637847
Iteration :  86   Loss :  0.0458279110805
Iteration :  87   Loss :  0.0456340082239
Iteration :  88   Loss :  0.0454409257914
Iteration :  89   Loss :  0.0452486603116
Iteration :  90   Loss :  0.0450572083278
Iteration :  91   Loss :  0.0448665663982
Iteration :  92   Loss :  0.0446767310952
Iteration :  93   Loss :  0.0444876990059
Iteration :  94   Loss :  0.0442994667319
Iteration :  95   Loss :  0.044112030889
Iteration :  96   Loss :  0.0439253881074
Iteration :  97   Loss :  0.0437395350317
Iteration :  98   Loss :  0.0435544683205
Iteration :  99   Loss :  0.0433701846466
[-0.0023868  -0.00024001 -0.00043041 ...,  0.00180726  0.00109442
  0.00016245]
CROSS VALIDATION 8
Iteration :  0   Loss :  23.6696130447
Iteration :  1   Loss :  0.786119368179
Iteration :  2   Loss :  0.123439339785
Iteration :  3   Loss :  0.0651258507677
Iteration :  4   Loss :  0.0648502962378
Iteration :  5   Loss :  0.0645759076089
Iteration :  6   Loss :  0.064302679948
Iteration :  7   Loss :  0.064030608343
Iteration :  8   Loss :  0.0637596879024
Iteration :  9   Loss :  0.0634899137555
Iteration :  10   Loss :  0.0632212810523
Iteration :  11   Loss :  0.0629537849631
Iteration :  12   Loss :  0.0626874206789
Iteration :  13   Loss :  0.0624221834108
Iteration :  14   Loss :  0.0621580683904
Iteration :  15   Loss :  0.0618950708691
Iteration :  16   Loss :  0.061633186119
Iteration :  17   Loss :  0.0613724094315
Iteration :  18   Loss :  0.0611127361185
Iteration :  19   Loss :  0.0608541615114
Iteration :  20   Loss :  0.0605966809614
Iteration :  21   Loss :  0.0603402898396
Iteration :  22   Loss :  0.0600849835363
Iteration :  23   Loss :  0.0598307574617
Iteration :  24   Loss :  0.0595776070452
Iteration :  25   Loss :  0.0593255277355
Iteration :  26   Loss :  0.0590745150007
Iteration :  27   Loss :  0.0588245643279
Iteration :  28   Loss :  0.0585756712235
Iteration :  29   Loss :  0.0583278312128
Iteration :  30   Loss :  0.0580810398401
Iteration :  31   Loss :  0.0578352926684
Iteration :  32   Loss :  0.0575905852795
Iteration :  33   Loss :  0.0573469132742
Iteration :  34   Loss :  0.0571042722715
Iteration :  35   Loss :  0.0568626579092
Iteration :  36   Loss :  0.0566220658434
Iteration :  37   Loss :  0.0563824917487
Iteration :  38   Loss :  0.056143931318
Iteration :  39   Loss :  0.0559063802623
Iteration :  40   Loss :  0.0556698343108
Iteration :  41   Loss :  0.0554342892109
Iteration :  42   Loss :  0.0551997407278
Iteration :  43   Loss :  0.0549661846448
Iteration :  44   Loss :  0.0547336167628
Iteration :  45   Loss :  0.0545020329008
Iteration :  46   Loss :  0.0542714288951
Iteration :  47   Loss :  0.0540418006
Iteration :  48   Loss :  0.0538131438871
Iteration :  49   Loss :  0.0535854546455
Iteration :  50   Loss :  0.0533587287818
Iteration :  51   Loss :  0.0531329622197
Iteration :  52   Loss :  0.0529081509005
Iteration :  53   Loss :  0.0526842907823
Iteration :  54   Loss :  0.0524613778404
Iteration :  55   Loss :  0.0522394080674
Iteration :  56   Loss :  0.0520183774726
Iteration :  57   Loss :  0.0517982820821
Iteration :  58   Loss :  0.0515791179391
Iteration :  59   Loss :  0.0513608811033
Iteration :  60   Loss :  0.0511435676513
Iteration :  61   Loss :  0.0509271736759
Iteration :  62   Loss :  0.050711695287
Iteration :  63   Loss :  0.0504971286104
Iteration :  64   Loss :  0.0502834697887
Iteration :  65   Loss :  0.0500707149806
Iteration :  66   Loss :  0.0498588603611
Iteration :  67   Loss :  0.0496479021215
Iteration :  68   Loss :  0.0494378364691
Iteration :  69   Loss :  0.0492286596272
Iteration :  70   Loss :  0.0490203678351
Iteration :  71   Loss :  0.0488129573482
Iteration :  72   Loss :  0.0486064244376
Iteration :  73   Loss :  0.0484007653901
Iteration :  74   Loss :  0.0481959765082
Iteration :  75   Loss :  0.0479920541104
Iteration :  76   Loss :  0.0477889945302
Iteration :  77   Loss :  0.0475867941172
Iteration :  78   Loss :  0.047385449236
Iteration :  79   Loss :  0.0471849562668
Iteration :  80   Loss :  0.0469853116051
Iteration :  81   Loss :  0.0467865116616
Iteration :  82   Loss :  0.0465885528622
Iteration :  83   Loss :  0.0463914316479
Iteration :  84   Loss :  0.0461951444748
Iteration :  85   Loss :  0.045999687814
Iteration :  86   Loss :  0.0458050581516
Iteration :  87   Loss :  0.0456112519883
Iteration :  88   Loss :  0.0454182658398
Iteration :  89   Loss :  0.0452260962367
Iteration :  90   Loss :  0.045034739724
Iteration :  91   Loss :  0.0448441928614
Iteration :  92   Loss :  0.0446544522233
Iteration :  93   Loss :  0.0444655143983
Iteration :  94   Loss :  0.0442773759898
Iteration :  95   Loss :  0.0440900336152
Iteration :  96   Loss :  0.0439034839065
Iteration :  97   Loss :  0.0437177235099
Iteration :  98   Loss :  0.0435327490856
Iteration :  99   Loss :  0.0433485573081
[ -1.81912086e-03   4.49031090e-04  -1.17679318e-03 ...,   1.45396187e-03
  -1.48206389e-04   8.01492339e-05]
CROSS VALIDATION 9
Iteration :  0   Loss :  11.0776844606
Iteration :  1   Loss :  0.435480817026
Iteration :  2   Loss :  0.0642420474355
Iteration :  3   Loss :  0.0639702323732
Iteration :  4   Loss :  0.0636995673899
Iteration :  5   Loss :  0.0634300476194
Iteration :  6   Loss :  0.0631616682163
Iteration :  7   Loss :  0.0628944243555
Iteration :  8   Loss :  0.0626283112325
Iteration :  9   Loss :  0.0623633240629
Iteration :  10   Loss :  0.0620994580827
Iteration :  11   Loss :  0.061836708548
Iteration :  12   Loss :  0.0615750707351
Iteration :  13   Loss :  0.0613145399402
Iteration :  14   Loss :  0.0610551114792
Iteration :  15   Loss :  0.0607967806882
Iteration :  16   Loss :  0.0605395429227
Iteration :  17   Loss :  0.0602833935581
Iteration :  18   Loss :  0.0600283279892
Iteration :  19   Loss :  0.0597743416304
Iteration :  20   Loss :  0.0595214299153
Iteration :  21   Loss :  0.0592695882972
Iteration :  22   Loss :  0.0590188122482
Iteration :  23   Loss :  0.0587690972598
Iteration :  24   Loss :  0.0585204388426
Iteration :  25   Loss :  0.0582728325261
Iteration :  26   Loss :  0.0580262738587
Iteration :  27   Loss :  0.0577807584078
Iteration :  28   Loss :  0.0575362817593
Iteration :  29   Loss :  0.0572928395181
Iteration :  30   Loss :  0.0570504273073
Iteration :  31   Loss :  0.0568090407688
Iteration :  32   Loss :  0.056568675563
Iteration :  33   Loss :  0.0563293273684
Iteration :  34   Loss :  0.0560909918819
Iteration :  35   Loss :  0.0558536648187
Iteration :  36   Loss :  0.0556173419119
Iteration :  37   Loss :  0.0553820189131
Iteration :  38   Loss :  0.0551476915913
Iteration :  39   Loss :  0.0549143557338
Iteration :  40   Loss :  0.0546820071456
Iteration :  41   Loss :  0.0544506416494
Iteration :  42   Loss :  0.0542202550857
Iteration :  43   Loss :  0.0539908433125
Iteration :  44   Loss :  0.0537624022054
Iteration :  45   Loss :  0.0535349276574
Iteration :  46   Loss :  0.0533084155788
Iteration :  47   Loss :  0.0530828618974
Iteration :  48   Loss :  0.052858262558
Iteration :  49   Loss :  0.0526346135228
Iteration :  50   Loss :  0.0524119107708
Iteration :  51   Loss :  0.0521901502983
Iteration :  52   Loss :  0.0519693281185
Iteration :  53   Loss :  0.0517494402611
Iteration :  54   Loss :  0.0515304827732
Iteration :  55   Loss :  0.051312451718
Iteration :  56   Loss :  0.0510953431759
Iteration :  57   Loss :  0.0508791532436
Iteration :  58   Loss :  0.0506638780342
Iteration :  59   Loss :  0.0504495136777
Iteration :  60   Loss :  0.0502360563199
Iteration :  61   Loss :  0.0500235021233
Iteration :  62   Loss :  0.0498118472666
Iteration :  63   Loss :  0.0496010879445
Iteration :  64   Loss :  0.0493912203679
Iteration :  65   Loss :  0.0491822407637
Iteration :  66   Loss :  0.0489741453749
Iteration :  67   Loss :  0.0487669304603
Iteration :  68   Loss :  0.0485605922944
Iteration :  69   Loss :  0.0483551271676
Iteration :  70   Loss :  0.0481505313861
Iteration :  71   Loss :  0.0479468012714
Iteration :  72   Loss :  0.047743933161
Iteration :  73   Loss :  0.0475419234075
Iteration :  74   Loss :  0.0473407683791
Iteration :  75   Loss :  0.0471404644595
Iteration :  76   Loss :  0.0469410080474
Iteration :  77   Loss :  0.0467423955571
Iteration :  78   Loss :  0.0465446234177
Iteration :  79   Loss :  0.0463476880736
Iteration :  80   Loss :  0.0461515859843
Iteration :  81   Loss :  0.0459563136242
Iteration :  82   Loss :  0.0457618674826
Iteration :  83   Loss :  0.0455682440638
Iteration :  84   Loss :  0.0453754398865
Iteration :  85   Loss :  0.0451834514847
Iteration :  86   Loss :  0.0449922754065
Iteration :  87   Loss :  0.0448019082151
Iteration :  88   Loss :  0.0446123464878
Iteration :  89   Loss :  0.0444235868168
Iteration :  90   Loss :  0.0442356258084
Iteration :  91   Loss :  0.0440484600834
Iteration :  92   Loss :  0.0438620862768
Iteration :  93   Loss :  0.043676501038
Iteration :  94   Loss :  0.0434917010304
Iteration :  95   Loss :  0.0433076829317
Iteration :  96   Loss :  0.0431244434336
Iteration :  97   Loss :  0.0429419792415
Iteration :  98   Loss :  0.0427602870753
Iteration :  99   Loss :  0.0425793636683
[-0.00242164 -0.00021583 -0.0006875  ...,  0.00165889  0.00131961
  0.00016201]
CROSS VALIDATION 10
Iteration :  0   Loss :  13.3832067572
Iteration :  1   Loss :  11.5947309292
Iteration :  2   Loss :  0.065443860644
Iteration :  3   Loss :  0.0651669605798
Iteration :  4   Loss :  0.0648912321098
Iteration :  5   Loss :  0.0646166702768
Iteration :  6   Loss :  0.0643432701447
Iteration :  7   Loss :  0.0640710267981
Iteration :  8   Loss :  0.0637999353426
Iteration :  9   Loss :  0.0635299909045
Iteration :  10   Loss :  0.0632611886305
Iteration :  11   Loss :  0.0629935236881
Iteration :  12   Loss :  0.062726991265
Iteration :  13   Loss :  0.0624615865695
Iteration :  14   Loss :  0.06219730483
Iteration :  15   Loss :  0.0619341412951
Iteration :  16   Loss :  0.0616720912337
Iteration :  17   Loss :  0.0614111499345
Iteration :  18   Loss :  0.0611513127062
Iteration :  19   Loss :  0.0608925748774
Iteration :  20   Loss :  0.0606349317963
Iteration :  21   Loss :  0.0603783788311
Iteration :  22   Loss :  0.0601229113692
Iteration :  23   Loss :  0.0598685248178
Iteration :  24   Loss :  0.0596152146035
Iteration :  25   Loss :  0.0593629761722
Iteration :  26   Loss :  0.0591118049889
Iteration :  27   Loss :  0.0588616965382
Iteration :  28   Loss :  0.0586126463234
Iteration :  29   Loss :  0.0583646498671
Iteration :  30   Loss :  0.0581177027106
Iteration :  31   Loss :  0.0578718004143
Iteration :  32   Loss :  0.0576269385573
Iteration :  33   Loss :  0.0573831127373
Iteration :  34   Loss :  0.0571403185707
Iteration :  35   Loss :  0.0568985516926
Iteration :  36   Loss :  0.0566578077563
Iteration :  37   Loss :  0.0564180824337
Iteration :  38   Loss :  0.0561793714149
Iteration :  39   Loss :  0.0559416704083
Iteration :  40   Loss :  0.0557049751404
Iteration :  41   Loss :  0.0554692813559
Iteration :  42   Loss :  0.0552345848172
Iteration :  43   Loss :  0.0550008813051
Iteration :  44   Loss :  0.0547681666178
Iteration :  45   Loss :  0.0545364365716
Iteration :  46   Loss :  0.0543056870003
Iteration :  47   Loss :  0.0540759137554
Iteration :  48   Loss :  0.0538471127061
Iteration :  49   Loss :  0.0536192797387
Iteration :  50   Loss :  0.0533924107573
Iteration :  51   Loss :  0.0531665016832
Iteration :  52   Loss :  0.0529415484548
Iteration :  53   Loss :  0.0527175470279
Iteration :  54   Loss :  0.0524944933754
Iteration :  55   Loss :  0.0522723834869
Iteration :  56   Loss :  0.0520512133695
Iteration :  57   Loss :  0.0518309790468
Iteration :  58   Loss :  0.0516116765594
Iteration :  59   Loss :  0.0513933019646
Iteration :  60   Loss :  0.0511758513363
Iteration :  61   Loss :  0.0509593207652
Iteration :  62   Loss :  0.0507437063584
Iteration :  63   Loss :  0.0505290042396
Iteration :  64   Loss :  0.0503152105486
Iteration :  65   Loss :  0.0501023214419
Iteration :  66   Loss :  0.0498903330921
Iteration :  67   Loss :  0.049679241688
Iteration :  68   Loss :  0.0494690434344
Iteration :  69   Loss :  0.0492597345525
Iteration :  70   Loss :  0.049051311279
Iteration :  71   Loss :  0.0488437698671
Iteration :  72   Loss :  0.0486371065853
Iteration :  73   Loss :  0.0484313177183
Iteration :  74   Loss :  0.0482263995663
Iteration :  75   Loss :  0.0480223484451
Iteration :  76   Loss :  0.0478191606864
Iteration :  77   Loss :  0.0476168326371
Iteration :  78   Loss :  0.0474153606597
Iteration :  79   Loss :  0.047214741132
Iteration :  80   Loss :  0.0470149704473
Iteration :  81   Loss :  0.046816045014
Iteration :  82   Loss :  0.0466179612558
Iteration :  83   Loss :  0.0464207156114
Iteration :  84   Loss :  0.0462243045347
Iteration :  85   Loss :  0.0460287244946
Iteration :  86   Loss :  0.0458339719747
Iteration :  87   Loss :  0.0456400434739
Iteration :  88   Loss :  0.0454469355056
Iteration :  89   Loss :  0.045254644598
Iteration :  90   Loss :  0.0450631672941
Iteration :  91   Loss :  0.0448725001514
Iteration :  92   Loss :  0.044682639742
Iteration :  93   Loss :  0.0444935826526
Iteration :  94   Loss :  0.0443053254843
Iteration :  95   Loss :  0.0441178648524
Iteration :  96   Loss :  0.0439311973867
Iteration :  97   Loss :  0.0437453197313
Iteration :  98   Loss :  0.0435602285444
Iteration :  99   Loss :  0.0433759204984
[-0.00160632 -0.00010129 -0.00114782 ...,  0.0018808   0.00079674
  0.00032389]
CROSS VALIDATION 11
Iteration :  0   Loss :  20.8167253244
Iteration :  1   Loss :  1.36223442348
Iteration :  2   Loss :  0.706750117036
Iteration :  3   Loss :  0.0651589631844
Iteration :  4   Loss :  0.0648832685522
Iteration :  5   Loss :  0.0646087404139
Iteration :  6   Loss :  0.0643353738339
Iteration :  7   Loss :  0.0640631638974
Iteration :  8   Loss :  0.0637921057108
Iteration :  9   Loss :  0.0635221944006
Iteration :  10   Loss :  0.0632534251145
Iteration :  11   Loss :  0.0629857930203
Iteration :  12   Loss :  0.0627192933066
Iteration :  13   Loss :  0.0624539211819
Iteration :  14   Loss :  0.0621896718755
Iteration :  15   Loss :  0.0619265406365
Iteration :  16   Loss :  0.0616645227344
Iteration :  17   Loss :  0.0614036134583
Iteration :  18   Loss :  0.0611438081176
Iteration :  19   Loss :  0.0608851020415
Iteration :  20   Loss :  0.0606274905789
Iteration :  21   Loss :  0.0603709690982
Iteration :  22   Loss :  0.0601155329877
Iteration :  23   Loss :  0.0598611776551
Iteration :  24   Loss :  0.0596078985274
Iteration :  25   Loss :  0.0593556910511
Iteration :  26   Loss :  0.0591045506921
Iteration :  27   Loss :  0.058854472935
Iteration :  28   Loss :  0.0586054532841
Iteration :  29   Loss :  0.0583574872623
Iteration :  30   Loss :  0.0581105704116
Iteration :  31   Loss :  0.0578646982928
Iteration :  32   Loss :  0.0576198664856
Iteration :  33   Loss :  0.0573760705883
Iteration :  34   Loss :  0.0571333062178
Iteration :  35   Loss :  0.0568915690097
Iteration :  36   Loss :  0.056650854618
Iteration :  37   Loss :  0.0564111587148
Iteration :  38   Loss :  0.0561724769911
Iteration :  39   Loss :  0.0559348051555
Iteration :  40   Loss :  0.0556981389353
Iteration :  41   Loss :  0.0554624740754
Iteration :  42   Loss :  0.0552278063391
Iteration :  43   Loss :  0.0549941315074
Iteration :  44   Loss :  0.0547614453793
Iteration :  45   Loss :  0.0545297437713
Iteration :  46   Loss :  0.054299022518
Iteration :  47   Loss :  0.0540692774713
Iteration :  48   Loss :  0.0538405045008
Iteration :  49   Loss :  0.0536126994935
Iteration :  50   Loss :  0.0533858583538
Iteration :  51   Loss :  0.0531599770036
Iteration :  52   Loss :  0.0529350513819
Iteration :  53   Loss :  0.0527110774448
Iteration :  54   Loss :  0.0524880511657
Iteration :  55   Loss :  0.052265968535
Iteration :  56   Loss :  0.0520448255599
Iteration :  57   Loss :  0.0518246182647
Iteration :  58   Loss :  0.0516053426905
Iteration :  59   Loss :  0.0513869948949
Iteration :  60   Loss :  0.0511695709526
Iteration :  61   Loss :  0.0509530669545
Iteration :  62   Loss :  0.0507374790082
Iteration :  63   Loss :  0.0505228032379
Iteration :  64   Loss :  0.0503090357841
Iteration :  65   Loss :  0.0500961728035
Iteration :  66   Loss :  0.0498842104692
Iteration :  67   Loss :  0.0496731449706
Iteration :  68   Loss :  0.0494629725129
Iteration :  69   Loss :  0.0492536893177
Iteration :  70   Loss :  0.0490452916223
Iteration :  71   Loss :  0.0488377756802
Iteration :  72   Loss :  0.0486311377605
Iteration :  73   Loss :  0.0484253741482
Iteration :  74   Loss :  0.0482204811441
Iteration :  75   Loss :  0.0480164550644
Iteration :  76   Loss :  0.0478132922412
Iteration :  77   Loss :  0.0476109890219
Iteration :  78   Loss :  0.0474095417695
Iteration :  79   Loss :  0.0472089468622
Iteration :  80   Loss :  0.0470092006937
Iteration :  81   Loss :  0.0468102996728
Iteration :  82   Loss :  0.0466122402237
Iteration :  83   Loss :  0.0464150187857
Iteration :  84   Loss :  0.0462186318128
Iteration :  85   Loss :  0.0460230757746
Iteration :  86   Loss :  0.0458283471551
Iteration :  87   Loss :  0.0456344424535
Iteration :  88   Loss :  0.0454413581837
Iteration :  89   Loss :  0.0452490908744
Iteration :  90   Loss :  0.0450576370689
Iteration :  91   Loss :  0.0448669933252
Iteration :  92   Loss :  0.0446771562158
Iteration :  93   Loss :  0.0444881223278
Iteration :  94   Loss :  0.0442998882626
Iteration :  95   Loss :  0.0441124506362
Iteration :  96   Loss :  0.0439258060787
Iteration :  97   Loss :  0.0437399512345
Iteration :  98   Loss :  0.0435548827623
Iteration :  99   Loss :  0.0433705973348
[-0.00238682 -0.00024002 -0.00043043 ...,  0.00180726  0.00109443
  0.00016245]
CROSS VALIDATION 12
Iteration :  0   Loss :  19.1920966781
Iteration :  1   Loss :  4.4774459947
Iteration :  2   Loss :  0.0703145683251
Iteration :  3   Loss :  0.0700170597691
Iteration :  4   Loss :  0.069720810004
Iteration :  5   Loss :  0.0694258137037
Iteration :  6   Loss :  0.0691320655647
Iteration :  7   Loss :  0.0688395603059
Iteration :  8   Loss :  0.0685482926686
Iteration :  9   Loss :  0.0682582574162
Iteration :  10   Loss :  0.0679694493344
Iteration :  11   Loss :  0.0676818632308
Iteration :  12   Loss :  0.0673954939352
Iteration :  13   Loss :  0.0671103362992
Iteration :  14   Loss :  0.0668263851959
Iteration :  15   Loss :  0.0665436355206
Iteration :  16   Loss :  0.0662620821898
Iteration :  17   Loss :  0.0659817201416
Iteration :  18   Loss :  0.0657025443356
Iteration :  19   Loss :  0.0654245497527
Iteration :  20   Loss :  0.0651477313949
Iteration :  21   Loss :  0.0648720842857
Iteration :  22   Loss :  0.0645976034692
Iteration :  23   Loss :  0.0643242840108
Iteration :  24   Loss :  0.0640521209966
Iteration :  25   Loss :  0.0637811095337
Iteration :  26   Loss :  0.0635112447496
Iteration :  27   Loss :  0.0632425217926
Iteration :  28   Loss :  0.0629749358315
Iteration :  29   Loss :  0.0627084820557
Iteration :  30   Loss :  0.0624431556747
Iteration :  31   Loss :  0.0621789519183
Iteration :  32   Loss :  0.0619158660366
Iteration :  33   Loss :  0.0616538932998
Iteration :  34   Loss :  0.061393028998
Iteration :  35   Loss :  0.0611332684414
Iteration :  36   Loss :  0.0608746069598
Iteration :  37   Loss :  0.0606170399029
Iteration :  38   Loss :  0.0603605626402
Iteration :  39   Loss :  0.0601051705606
Iteration :  40   Loss :  0.0598508590725
Iteration :  41   Loss :  0.0595976236038
Iteration :  42   Loss :  0.0593454596019
Iteration :  43   Loss :  0.0590943625332
Iteration :  44   Loss :  0.0588443278834
Iteration :  45   Loss :  0.0585953511573
Iteration :  46   Loss :  0.0583474278786
Iteration :  47   Loss :  0.0581005535903
Iteration :  48   Loss :  0.0578547238537
Iteration :  49   Loss :  0.0576099342495
Iteration :  50   Loss :  0.0573661803765
Iteration :  51   Loss :  0.0571234578526
Iteration :  52   Loss :  0.056881762314
Iteration :  53   Loss :  0.0566410894155
Iteration :  54   Loss :  0.05640143483
Iteration :  55   Loss :  0.056162794249
Iteration :  56   Loss :  0.0559251633822
Iteration :  57   Loss :  0.0556885379573
Iteration :  58   Loss :  0.0554529137202
Iteration :  59   Loss :  0.0552182864348
Iteration :  60   Loss :  0.0549846518829
Iteration :  61   Loss :  0.0547520058641
Iteration :  62   Loss :  0.0545203441958
Iteration :  63   Loss :  0.054289662713
Iteration :  64   Loss :  0.0540599572687
Iteration :  65   Loss :  0.0538312237329
Iteration :  66   Loss :  0.0536034579936
Iteration :  67   Loss :  0.0533766559557
Iteration :  68   Loss :  0.0531508135418
Iteration :  69   Loss :  0.0529259266917
Iteration :  70   Loss :  0.0527019913622
Iteration :  71   Loss :  0.0524790035273
Iteration :  72   Loss :  0.0522569591781
Iteration :  73   Loss :  0.0520358543226
Iteration :  74   Loss :  0.0518156849857
Iteration :  75   Loss :  0.0515964472091
Iteration :  76   Loss :  0.0513781370513
Iteration :  77   Loss :  0.0511607505874
Iteration :  78   Loss :  0.0509442839093
Iteration :  79   Loss :  0.050728733125
Iteration :  80   Loss :  0.0505140943595
Iteration :  81   Loss :  0.0503003637539
Iteration :  82   Loss :  0.0500875374656
Iteration :  83   Loss :  0.0498756116684
Iteration :  84   Loss :  0.0496645825522
Iteration :  85   Loss :  0.049454446323
Iteration :  86   Loss :  0.049245199203
Iteration :  87   Loss :  0.0490368374303
Iteration :  88   Loss :  0.0488293572587
Iteration :  89   Loss :  0.0486227549583
Iteration :  90   Loss :  0.0484170268146
Iteration :  91   Loss :  0.0482121691289
Iteration :  92   Loss :  0.0480081782183
Iteration :  93   Loss :  0.0478050504153
Iteration :  94   Loss :  0.0476027820681
Iteration :  95   Loss :  0.0474013695401
Iteration :  96   Loss :  0.0472008092104
Iteration :  97   Loss :  0.0470010974732
Iteration :  98   Loss :  0.046802230738
Iteration :  99   Loss :  0.0466042054294
[ -3.59152110e-03  -4.58689807e-04  -1.48546928e-03 ...,   1.68474721e-03
   1.02684997e-03   8.12139361e-05]
CROSS VALIDATION 13
Iteration :  0   Loss :  20.8177765727
Iteration :  1   Loss :  1.36197756966
Iteration :  2   Loss :  0.707194556584
Iteration :  3   Loss :  0.065159180973
Iteration :  4   Loss :  0.0648834854193
Iteration :  5   Loss :  0.0646089563634
Iteration :  6   Loss :  0.0643355888697
Iteration :  7   Loss :  0.0640633780234
Iteration :  8   Loss :  0.0637923189308
Iteration :  9   Loss :  0.0635224067185
Iteration :  10   Loss :  0.063253636534
Iteration :  11   Loss :  0.0629860035453
Iteration :  12   Loss :  0.0627195029408
Iteration :  13   Loss :  0.0624541299291
Iteration :  14   Loss :  0.0621898797395
Iteration :  15   Loss :  0.061926747621
Iteration :  16   Loss :  0.0616647288431
Iteration :  17   Loss :  0.0614038186949
Iteration :  18   Loss :  0.0611440124859
Iteration :  19   Loss :  0.0608853055451
Iteration :  20   Loss :  0.0606276932214
Iteration :  21   Loss :  0.0603711708833
Iteration :  22   Loss :  0.060115733919
Iteration :  23   Loss :  0.0598613777362
Iteration :  24   Loss :  0.059608097762
Iteration :  25   Loss :  0.0593558894428
Iteration :  26   Loss :  0.0591047482443
Iteration :  27   Loss :  0.0588546696514
Iteration :  28   Loss :  0.0586056491681
Iteration :  29   Loss :  0.0583576823175
Iteration :  30   Loss :  0.0581107646415
Iteration :  31   Loss :  0.0578648917009
Iteration :  32   Loss :  0.0576200590754
Iteration :  33   Loss :  0.0573762623632
Iteration :  34   Loss :  0.0571334971813
Iteration :  35   Loss :  0.0568917591652
Iteration :  36   Loss :  0.0566510439689
Iteration :  37   Loss :  0.0564113472646
Iteration :  38   Loss :  0.0561726647431
Iteration :  39   Loss :  0.0559349921131
Iteration :  40   Loss :  0.0556983251018
Iteration :  41   Loss :  0.0554626594543
Iteration :  42   Loss :  0.0552279909336
Iteration :  43   Loss :  0.0549943153209
Iteration :  44   Loss :  0.054761628415
Iteration :  45   Loss :  0.0545299260326
Iteration :  46   Loss :  0.0542992040081
Iteration :  47   Loss :  0.0540694581935
Iteration :  48   Loss :  0.0538406844583
Iteration :  49   Loss :  0.0536128786896
Iteration :  50   Loss :  0.0533860367918
Iteration :  51   Loss :  0.0531601546866
Iteration :  52   Loss :  0.052935228313
Iteration :  53   Loss :  0.0527112536273
Iteration :  54   Loss :  0.0524882266028
Iteration :  55   Loss :  0.0522661432298
Iteration :  56   Loss :  0.0520449995156
Iteration :  57   Loss :  0.0518247914844
Iteration :  58   Loss :  0.0516055151772
Iteration :  59   Loss :  0.0513871666518
Iteration :  60   Loss :  0.0511697419828
Iteration :  61   Loss :  0.050953237261
Iteration :  62   Loss :  0.0507376485942
Iteration :  63   Loss :  0.0505229721063
Iteration :  64   Loss :  0.050309203938
Iteration :  65   Loss :  0.0500963402459
Iteration :  66   Loss :  0.0498843772032
Iteration :  67   Loss :  0.0496733109991
Iteration :  68   Loss :  0.0494631378389
Iteration :  69   Loss :  0.0492538539442
Iteration :  70   Loss :  0.0490454555523
Iteration :  71   Loss :  0.0488379389165
Iteration :  72   Loss :  0.0486313003061
Iteration :  73   Loss :  0.0484255360061
Iteration :  74   Loss :  0.0482206423171
Iteration :  75   Loss :  0.0480166155555
Iteration :  76   Loss :  0.0478134520533
Iteration :  77   Loss :  0.0476111481578
Iteration :  78   Loss :  0.0474097002321
Iteration :  79   Loss :  0.0472091046543
Iteration :  80   Loss :  0.0470093578181
Iteration :  81   Loss :  0.0468104561325
Iteration :  82   Loss :  0.0466123960214
Iteration :  83   Loss :  0.0464151739241
Iteration :  84   Loss :  0.0462187862949
Iteration :  85   Loss :  0.046023229603
Iteration :  86   Loss :  0.0458285003327
Iteration :  87   Loss :  0.045634594983
Iteration :  88   Loss :  0.0454415100678
Iteration :  89   Loss :  0.0452492421159
Iteration :  90   Loss :  0.0450577876704
Iteration :  91   Loss :  0.0448671432895
Iteration :  92   Loss :  0.0446773055456
Iteration :  93   Loss :  0.0444882710257
Iteration :  94   Loss :  0.0443000363314
Iteration :  95   Loss :  0.0441125980785
Iteration :  96   Loss :  0.0439259528971
Iteration :  97   Loss :  0.0437400974318
Iteration :  98   Loss :  0.043555028341
Iteration :  99   Loss :  0.0433707422976
[-0.00238681 -0.00024001 -0.00043043 ...,  0.00180726  0.00109443
  0.00016245]
CROSS VALIDATION 14
Iteration :  0   Loss :  20.8177765727
Iteration :  1   Loss :  0.0587420875204
Iteration :  2   Loss :  0.0584935433844
Iteration :  3   Loss :  0.0582460508654
Iteration :  4   Loss :  0.0579996055142
Iteration :  5   Loss :  0.0577542028999
Iteration :  6   Loss :  0.0575098386107
Iteration :  7   Loss :  0.0572665082533
Iteration :  8   Loss :  0.0570242074529
Iteration :  9   Loss :  0.0567829318535
Iteration :  10   Loss :  0.0565426771173
Iteration :  11   Loss :  0.0563034389249
Iteration :  12   Loss :  0.0560652129752
Iteration :  13   Loss :  0.0558279949854
Iteration :  14   Loss :  0.0555917806905
Iteration :  15   Loss :  0.0553565658439
Iteration :  16   Loss :  0.0551223462168
Iteration :  17   Loss :  0.0548891175983
Iteration :  18   Loss :  0.0546568757954
Iteration :  19   Loss :  0.0544256166327
Iteration :  20   Loss :  0.0541953359526
Iteration :  21   Loss :  0.053966029615
Iteration :  22   Loss :  0.0537376934973
Iteration :  23   Loss :  0.0535103234945
Iteration :  24   Loss :  0.0532839155188
Iteration :  25   Loss :  0.0530584654998
Iteration :  26   Loss :  0.0528339693842
Iteration :  27   Loss :  0.052610423136
Iteration :  28   Loss :  0.0523878227362
Iteration :  29   Loss :  0.0521661641828
Iteration :  30   Loss :  0.0519454434908
Iteration :  31   Loss :  0.0517256566919
Iteration :  32   Loss :  0.0515067998348
Iteration :  33   Loss :  0.0512888689847
Iteration :  34   Loss :  0.0510718602237
Iteration :  35   Loss :  0.0508557696503
Iteration :  36   Loss :  0.0506405933795
Iteration :  37   Loss :  0.0504263275428
Iteration :  38   Loss :  0.0502129682881
Iteration :  39   Loss :  0.0500005117794
Iteration :  40   Loss :  0.0497889541973
Iteration :  41   Loss :  0.0495782917383
Iteration :  42   Loss :  0.0493685206149
Iteration :  43   Loss :  0.0491596370559
Iteration :  44   Loss :  0.0489516373059
Iteration :  45   Loss :  0.0487445176253
Iteration :  46   Loss :  0.0485382742905
Iteration :  47   Loss :  0.0483329035936
Iteration :  48   Loss :  0.0481284018424
Iteration :  49   Loss :  0.0479247653603
Iteration :  50   Loss :  0.0477219904861
Iteration :  51   Loss :  0.0475200735745
Iteration :  52   Loss :  0.0473190109951
Iteration :  53   Loss :  0.0471187991333
Iteration :  54   Loss :  0.0469194343896
Iteration :  55   Loss :  0.0467209131798
Iteration :  56   Loss :  0.0465232319346
Iteration :  57   Loss :  0.0463263871003
Iteration :  58   Loss :  0.0461303751377
Iteration :  59   Loss :  0.045935192523
Iteration :  60   Loss :  0.0457408357471
Iteration :  61   Loss :  0.0455473013157
Iteration :  62   Loss :  0.0453545857495
Iteration :  63   Loss :  0.0451626855838
Iteration :  64   Loss :  0.0449715973684
Iteration :  65   Loss :  0.044781317668
Iteration :  66   Loss :  0.0445918430616
Iteration :  67   Loss :  0.0444031701428
Iteration :  68   Loss :  0.0442152955195
Iteration :  69   Loss :  0.0440282158142
Iteration :  70   Loss :  0.0438419276633
Iteration :  71   Loss :  0.0436564277178
Iteration :  72   Loss :  0.0434717126426
Iteration :  73   Loss :  0.0432877791169
Iteration :  74   Loss :  0.0431046238339
Iteration :  75   Loss :  0.0429222435008
Iteration :  76   Loss :  0.0427406348386
Iteration :  77   Loss :  0.0425597945823
Iteration :  78   Loss :  0.0423797194808
Iteration :  79   Loss :  0.0422004062965
Iteration :  80   Loss :  0.0420218518057
Iteration :  81   Loss :  0.0418440527983
Iteration :  82   Loss :  0.0416670060778
Iteration :  83   Loss :  0.0414907084611
Iteration :  84   Loss :  0.0413151567787
Iteration :  85   Loss :  0.0411403478746
Iteration :  86   Loss :  0.0409662786058
Iteration :  87   Loss :  0.040792945843
Iteration :  88   Loss :  0.0406203464698
Iteration :  89   Loss :  0.0404484773834
Iteration :  90   Loss :  0.0402773354936
Iteration :  91   Loss :  0.0401069177237
Iteration :  92   Loss :  0.0399372210099
Iteration :  93   Loss :  0.0397682423012
Iteration :  94   Loss :  0.0395999785598
Iteration :  95   Loss :  0.0394324267605
Iteration :  96   Loss :  0.039265583891
Iteration :  97   Loss :  0.0390994469517
Iteration :  98   Loss :  0.0389340129558
Iteration :  99   Loss :  0.0387692789291
[ -2.17491467e-03  -2.82193762e-04  -4.05352415e-04 ...,   1.33884692e-03
   6.37767144e-04  -2.69034439e-07]
CROSS VALIDATION 15
Iteration :  0   Loss :  20.8177765727
Iteration :  1   Loss :  1.36197756966
Iteration :  2   Loss :  0.707266086519
Iteration :  3   Loss :  0.065159434594
Iteration :  4   Loss :  0.0648837379672
Iteration :  5   Loss :  0.0646092078428
Iteration :  6   Loss :  0.064335839285
Iteration :  7   Loss :  0.0640636273792
Iteration :  8   Loss :  0.0637925672315
Iteration :  9   Loss :  0.0635226539686
Iteration :  10   Loss :  0.063253882738
Iteration :  11   Loss :  0.0629862487076
Iteration :  12   Loss :  0.0627197470657
Iteration :  13   Loss :  0.0624543730212
Iteration :  14   Loss :  0.062190121803
Iteration :  15   Loss :  0.0619269886603
Iteration :  16   Loss :  0.0616649688625
Iteration :  17   Loss :  0.0614040576988
Iteration :  18   Loss :  0.0611442504785
Iteration :  19   Loss :  0.0608855425308
Iteration :  20   Loss :  0.0606279292043
Iteration :  21   Loss :  0.0603714058678
Iteration :  22   Loss :  0.0601159679093
Iteration :  23   Loss :  0.0598616107364
Iteration :  24   Loss :  0.0596083297764
Iteration :  25   Loss :  0.0593561204754
Iteration :  26   Loss :  0.0591049782994
Iteration :  27   Loss :  0.0588548987331
Iteration :  28   Loss :  0.0586058772806
Iteration :  29   Loss :  0.0583579094648
Iteration :  30   Loss :  0.0581109908277
Iteration :  31   Loss :  0.0578651169301
Iteration :  32   Loss :  0.0576202833516
Iteration :  33   Loss :  0.0573764856905
Iteration :  34   Loss :  0.0571337195637
Iteration :  35   Loss :  0.0568919806067
Iteration :  36   Loss :  0.0566512644734
Iteration :  37   Loss :  0.0564115668362
Iteration :  38   Loss :  0.0561728833856
Iteration :  39   Loss :  0.0559352098305
Iteration :  40   Loss :  0.055698541898
Iteration :  41   Loss :  0.0554628753332
Iteration :  42   Loss :  0.0552282058992
Iteration :  43   Loss :  0.0549945293769
Iteration :  44   Loss :  0.0547618415653
Iteration :  45   Loss :  0.0545301382811
Iteration :  46   Loss :  0.0542994153585
Iteration :  47   Loss :  0.0540696686497
Iteration :  48   Loss :  0.053840894024
Iteration :  49   Loss :  0.0536130873686
Iteration :  50   Loss :  0.0533862445878
Iteration :  51   Loss :  0.0531603616034
Iteration :  52   Loss :  0.0529354343544
Iteration :  53   Loss :  0.0527114587969
Iteration :  54   Loss :  0.0524884309043
Iteration :  55   Loss :  0.0522663466668
Iteration :  56   Loss :  0.0520452020919
Iteration :  57   Loss :  0.0518249932035
Iteration :  58   Loss :  0.0516057160429
Iteration :  59   Loss :  0.0513873666676
Iteration :  60   Loss :  0.0511699411523
Iteration :  61   Loss :  0.0509534355878
Iteration :  62   Loss :  0.0507378460818
Iteration :  63   Loss :  0.0505231687584
Iteration :  64   Loss :  0.050309399758
Iteration :  65   Loss :  0.0500965352374
Iteration :  66   Loss :  0.0498845713696
Iteration :  67   Loss :  0.049673504344
Iteration :  68   Loss :  0.0494633303657
Iteration :  69   Loss :  0.0492540456564
Iteration :  70   Loss :  0.0490456464533
Iteration :  71   Loss :  0.0488381290099
Iteration :  72   Loss :  0.0486314895952
Iteration :  73   Loss :  0.0484257244943
Iteration :  74   Loss :  0.0482208300078
Iteration :  75   Loss :  0.048016802452
Iteration :  76   Loss :  0.047813638159
Iteration :  77   Loss :  0.0476113334761
Iteration :  78   Loss :  0.0474098847662
Iteration :  79   Loss :  0.0472092884077
Iteration :  80   Loss :  0.047009540794
Iteration :  81   Loss :  0.0468106383342
Iteration :  82   Loss :  0.0466125774522
Iteration :  83   Loss :  0.0464153545873
Iteration :  84   Loss :  0.0462189661937
Iteration :  85   Loss :  0.0460234087406
Iteration :  86   Loss :  0.0458286787123
Iteration :  87   Loss :  0.0456347726079
Iteration :  88   Loss :  0.0454416869412
Iteration :  89   Loss :  0.0452494182408
Iteration :  90   Loss :  0.0450579630502
Iteration :  91   Loss :  0.0448673179272
Iteration :  92   Loss :  0.0446774794444
Iteration :  93   Loss :  0.0444884441888
Iteration :  94   Loss :  0.0443002087618
Iteration :  95   Loss :  0.0441127697793
Iteration :  96   Loss :  0.0439261238714
Iteration :  97   Loss :  0.0437402676826
Iteration :  98   Loss :  0.0435551978715
Iteration :  99   Loss :  0.0433709111108
[-0.00238683 -0.00024002 -0.00043044 ...,  0.00180728  0.00109445
  0.00016245]
CROSS VALIDATION 16
Iteration :  0   Loss :  20.8177765727
Iteration :  1   Loss :  0.0557567576382
Iteration :  2   Loss :  0.0555208447563
Iteration :  3   Loss :  0.0552859300473
Iteration :  4   Loss :  0.0550520092879
Iteration :  5   Loss :  0.0548190782726
Iteration :  6   Loss :  0.0545871328136
Iteration :  7   Loss :  0.0543561687411
Iteration :  8   Loss :  0.0541261819025
Iteration :  9   Loss :  0.0538971681631
Iteration :  10   Loss :  0.0536691234057
Iteration :  11   Loss :  0.0534420435304
Iteration :  12   Loss :  0.0532159244546
Iteration :  13   Loss :  0.0529907621132
Iteration :  14   Loss :  0.0527665524579
Iteration :  15   Loss :  0.0525432914581
Iteration :  16   Loss :  0.0523209750997
Iteration :  17   Loss :  0.0520995993859
Iteration :  18   Loss :  0.0518791603367
Iteration :  19   Loss :  0.051659653989
Iteration :  20   Loss :  0.0514410763965
Iteration :  21   Loss :  0.0512234236295
Iteration :  22   Loss :  0.0510066917748
Iteration :  23   Loss :  0.0507908769362
Iteration :  24   Loss :  0.0505759752335
Iteration :  25   Loss :  0.0503619828031
Iteration :  26   Loss :  0.050148895798
Iteration :  27   Loss :  0.049936710387
Iteration :  28   Loss :  0.0497254227555
Iteration :  29   Loss :  0.0495150291049
Iteration :  30   Loss :  0.0493055256526
Iteration :  31   Loss :  0.0490969086321
Iteration :  32   Loss :  0.0488891742928
Iteration :  33   Loss :  0.0486823189
Iteration :  34   Loss :  0.0484763387347
Iteration :  35   Loss :  0.0482712300939
Iteration :  36   Loss :  0.04806698929
Iteration :  37   Loss :  0.047863612651
Iteration :  38   Loss :  0.0476610965206
Iteration :  39   Loss :  0.0474594372579
Iteration :  40   Loss :  0.0472586312374
Iteration :  41   Loss :  0.0470586748489
Iteration :  42   Loss :  0.0468595644975
Iteration :  43   Loss :  0.0466612966036
Iteration :  44   Loss :  0.0464638676027
Iteration :  45   Loss :  0.0462672739452
Iteration :  46   Loss :  0.0460715120969
Iteration :  47   Loss :  0.0458765785381
Iteration :  48   Loss :  0.0456824697643
Iteration :  49   Loss :  0.0454891822857
Iteration :  50   Loss :  0.0452967126274
Iteration :  51   Loss :  0.0451050573291
Iteration :  52   Loss :  0.0449142129451
Iteration :  53   Loss :  0.0447241760444
Iteration :  54   Loss :  0.0445349432104
Iteration :  55   Loss :  0.044346511041
Iteration :  56   Loss :  0.0441588761486
Iteration :  57   Loss :  0.0439720351597
Iteration :  58   Loss :  0.0437859847153
Iteration :  59   Loss :  0.0436007214704
Iteration :  60   Loss :  0.0434162420944
Iteration :  61   Loss :  0.0432325432707
Iteration :  62   Loss :  0.0430496216965
Iteration :  63   Loss :  0.0428674740834
Iteration :  64   Loss :  0.0426860971565
Iteration :  65   Loss :  0.0425054876551
Iteration :  66   Loss :  0.042325642332
Iteration :  67   Loss :  0.042146557954
Iteration :  68   Loss :  0.0419682313014
Iteration :  69   Loss :  0.0417906591681
Iteration :  70   Loss :  0.0416138383618
Iteration :  71   Loss :  0.0414377657035
Iteration :  72   Loss :  0.0412624380277
Iteration :  73   Loss :  0.0410878521823
Iteration :  74   Loss :  0.0409140050285
Iteration :  75   Loss :  0.0407408934408
Iteration :  76   Loss :  0.0405685143071
Iteration :  77   Loss :  0.0403968645281
Iteration :  78   Loss :  0.0402259410179
Iteration :  79   Loss :  0.0400557407036
Iteration :  80   Loss :  0.0398862605253
Iteration :  81   Loss :  0.0397174974359
Iteration :  82   Loss :  0.0395494484015
Iteration :  83   Loss :  0.0393821104008
Iteration :  84   Loss :  0.0392154804253
Iteration :  85   Loss :  0.0390495554792
Iteration :  86   Loss :  0.0388843325796
Iteration :  87   Loss :  0.0387198087559
Iteration :  88   Loss :  0.0385559810504
Iteration :  89   Loss :  0.0383928465176
Iteration :  90   Loss :  0.0382304022248
Iteration :  91   Loss :  0.0380686452513
Iteration :  92   Loss :  0.0379075726891
Iteration :  93   Loss :  0.0377471816424
Iteration :  94   Loss :  0.0375874692275
Iteration :  95   Loss :  0.0374284325732
Iteration :  96   Loss :  0.0372700688202
Iteration :  97   Loss :  0.0371123751214
Iteration :  98   Loss :  0.0369553486417
Iteration :  99   Loss :  0.0367989865581
[ -1.83825678e-03  -2.62558407e-04  -2.98249594e-04 ...,   1.32681721e-03
   6.41245401e-04   8.08506785e-05]
CROSS VALIDATION 17
Iteration :  0   Loss :  20.8177765727
Iteration :  1   Loss :  1.36210251736
Iteration :  2   Loss :  0.707459180732
Iteration :  3   Loss :  0.0651596668877
Iteration :  4   Loss :  0.0648839692781
Iteration :  5   Loss :  0.0646094381749
Iteration :  6   Loss :  0.0643360686426
Iteration :  7   Loss :  0.0640638557664
Iteration :  8   Loss :  0.0637927946523
Iteration :  9   Loss :  0.0635228804272
Iteration :  10   Loss :  0.0632541082384
Iteration :  11   Loss :  0.0629864732539
Iteration :  12   Loss :  0.0627199706619
Iteration :  13   Loss :  0.0624545956714
Iteration :  14   Loss :  0.0621903435111
Iteration :  15   Loss :  0.0619272094304
Iteration :  16   Loss :  0.0616651886984
Iteration :  17   Loss :  0.0614042766046
Iteration :  18   Loss :  0.0611444684581
Iteration :  19   Loss :  0.060885759588
Iteration :  20   Loss :  0.0606281453432
Iteration :  21   Loss :  0.0603716210922
Iteration :  22   Loss :  0.060116182223
Iteration :  23   Loss :  0.0598618241434
Iteration :  24   Loss :  0.0596085422803
Iteration :  25   Loss :  0.0593563320803
Iteration :  26   Loss :  0.059105189009
Iteration :  27   Loss :  0.0588551085512
Iteration :  28   Loss :  0.0586060862109
Iteration :  29   Loss :  0.0583581175111
Iteration :  30   Loss :  0.0581111979937
Iteration :  31   Loss :  0.0578653232195
Iteration :  32   Loss :  0.0576204887682
Iteration :  33   Loss :  0.057376690238
Iteration :  34   Loss :  0.0571339232457
Iteration :  35   Loss :  0.0568921834269
Iteration :  36   Loss :  0.0566514664355
Iteration :  37   Loss :  0.0564117679437
Iteration :  38   Loss :  0.0561730836422
Iteration :  39   Loss :  0.0559354092398
Iteration :  40   Loss :  0.0556987404636
Iteration :  41   Loss :  0.0554630730586
Iteration :  42   Loss :  0.055228402788
Iteration :  43   Loss :  0.0549947254327
Iteration :  44   Loss :  0.0547620367916
Iteration :  45   Loss :  0.0545303326813
Iteration :  46   Loss :  0.0542996089362
Iteration :  47   Loss :  0.0540698614083
Iteration :  48   Loss :  0.0538410859671
Iteration :  49   Loss :  0.0536132784995
Iteration :  50   Loss :  0.05338643491
Iteration :  51   Loss :  0.0531605511204
Iteration :  52   Loss :  0.0529356230695
Iteration :  53   Loss :  0.0527116467135
Iteration :  54   Loss :  0.0524886180258
Iteration :  55   Loss :  0.0522665329966
Iteration :  56   Loss :  0.0520453876333
Iteration :  57   Loss :  0.0518251779599
Iteration :  58   Loss :  0.0516059000175
Iteration :  59   Loss :  0.0513875498638
Iteration :  60   Loss :  0.0511701235733
Iteration :  61   Loss :  0.050953617237
Iteration :  62   Loss :  0.0507380269625
Iteration :  63   Loss :  0.0505233488737
Iteration :  64   Loss :  0.0503095791113
Iteration :  65   Loss :  0.0500967138318
Iteration :  66   Loss :  0.0498847492084
Iteration :  67   Loss :  0.0496736814303
Iteration :  68   Loss :  0.0494635067028
Iteration :  69   Loss :  0.0492542212473
Iteration :  70   Loss :  0.0490458213013
Iteration :  71   Loss :  0.048838303118
Iteration :  72   Loss :  0.0486316629667
Iteration :  73   Loss :  0.0484258971322
Iteration :  74   Loss :  0.0482210019153
Iteration :  75   Loss :  0.0480169736322
Iteration :  76   Loss :  0.0478138086149
Iteration :  77   Loss :  0.0476115032107
Iteration :  78   Loss :  0.0474100537827
Iteration :  79   Loss :  0.047209456709
Iteration :  80   Loss :  0.0470097083833
Iteration :  81   Loss :  0.0468108052144
Iteration :  82   Loss :  0.0466127436263
Iteration :  83   Loss :  0.0464155200583
Iteration :  84   Loss :  0.0462191309645
Iteration :  85   Loss :  0.0460235728143
Iteration :  86   Loss :  0.0458288420918
Iteration :  87   Loss :  0.0456349352961
Iteration :  88   Loss :  0.045441848941
Iteration :  89   Loss :  0.0452495795552
Iteration :  90   Loss :  0.045058123682
Iteration :  91   Loss :  0.0448674778794
Iteration :  92   Loss :  0.0446776387198
Iteration :  93   Loss :  0.0444886027903
Iteration :  94   Loss :  0.0443003666922
Iteration :  95   Loss :  0.0441129270415
Iteration :  96   Loss :  0.0439262804683
Iteration :  97   Loss :  0.0437404236169
Iteration :  98   Loss :  0.043555353146
Iteration :  99   Loss :  0.0433710657283
[-0.00238684 -0.00024002 -0.00043044 ...,  0.00180728  0.00109445
  0.00016245]
CROSS VALIDATION 18
Iteration :  0   Loss :  24.2067295622
Iteration :  1   Loss :  15.097618069
Iteration :  2   Loss :  0.0682826828285
Iteration :  3   Loss :  0.0679937714001
Iteration :  4   Loss :  0.0677060823872
Iteration :  5   Loss :  0.0674196106178
Iteration :  6   Loss :  0.0671343509414
Iteration :  7   Loss :  0.0668502982296
Iteration :  8   Loss :  0.0665674473756
Iteration :  9   Loss :  0.0662857932943
Iteration :  10   Loss :  0.0660053309218
Iteration :  11   Loss :  0.0657260552161
Iteration :  12   Loss :  0.0654479611561
Iteration :  13   Loss :  0.0651710437422
Iteration :  14   Loss :  0.0648952979958
Iteration :  15   Loss :  0.0646207189596
Iteration :  16   Loss :  0.0643473016971
Iteration :  17   Loss :  0.0640750412926
Iteration :  18   Loss :  0.0638039328514
Iteration :  19   Loss :  0.0635339714993
Iteration :  20   Loss :  0.063265152383
Iteration :  21   Loss :  0.0629974706695
Iteration :  22   Loss :  0.0627309215463
Iteration :  23   Loss :  0.0624655002214
Iteration :  24   Loss :  0.0622012019228
Iteration :  25   Loss :  0.0619380218989
Iteration :  26   Loss :  0.0616759554183
Iteration :  27   Loss :  0.0614149977693
Iteration :  28   Loss :  0.0611551442604
Iteration :  29   Loss :  0.0608963902198
Iteration :  30   Loss :  0.0606387309956
Iteration :  31   Loss :  0.0603821619555
Iteration :  32   Loss :  0.0601266784869
Iteration :  33   Loss :  0.0598722759964
Iteration :  34   Loss :  0.0596189499105
Iteration :  35   Loss :  0.0593666956746
Iteration :  36   Loss :  0.0591155087538
Iteration :  37   Loss :  0.058865384632
Iteration :  38   Loss :  0.0586163188125
Iteration :  39   Loss :  0.0583683068175
Iteration :  40   Loss :  0.058121344188
Iteration :  41   Loss :  0.0578754264842
Iteration :  42   Loss :  0.0576305492849
Iteration :  43   Loss :  0.0573867081875
Iteration :  44   Loss :  0.0571438988082
Iteration :  45   Loss :  0.0569021167817
Iteration :  46   Loss :  0.0566613577612
Iteration :  47   Loss :  0.0564216174181
Iteration :  48   Loss :  0.0561828914425
Iteration :  49   Loss :  0.0559451755422
Iteration :  50   Loss :  0.0557084654437
Iteration :  51   Loss :  0.0554727568913
Iteration :  52   Loss :  0.0552380456473
Iteration :  53   Loss :  0.055004327492
Iteration :  54   Loss :  0.0547715982235
Iteration :  55   Loss :  0.0545398536578
Iteration :  56   Loss :  0.0543090896285
Iteration :  57   Loss :  0.0540793019867
Iteration :  58   Loss :  0.0538504866014
Iteration :  59   Loss :  0.0536226393587
Iteration :  60   Loss :  0.0533957561624
Iteration :  61   Loss :  0.0531698329335
Iteration :  62   Loss :  0.0529448656103
Iteration :  63   Loss :  0.0527208501481
Iteration :  64   Loss :  0.0524977825197
Iteration :  65   Loss :  0.0522756587145
Iteration :  66   Loss :  0.0520544747393
Iteration :  67   Loss :  0.0518342266174
Iteration :  68   Loss :  0.0516149103891
Iteration :  69   Loss :  0.0513965221116
Iteration :  70   Loss :  0.0511790578586
Iteration :  71   Loss :  0.0509625137203
Iteration :  72   Loss :  0.0507468858038
Iteration :  73   Loss :  0.0505321702323
Iteration :  74   Loss :  0.0503183631457
Iteration :  75   Loss :  0.0501054607001
Iteration :  76   Loss :  0.0498934590677
Iteration :  77   Loss :  0.0496823544372
Iteration :  78   Loss :  0.0494721430133
Iteration :  79   Loss :  0.0492628210167
Iteration :  80   Loss :  0.0490543846841
Iteration :  81   Loss :  0.0488468302683
Iteration :  82   Loss :  0.0486401540376
Iteration :  83   Loss :  0.0484343522765
Iteration :  84   Loss :  0.0482294212849
Iteration :  85   Loss :  0.0480253573786
Iteration :  86   Loss :  0.0478221568887
Iteration :  87   Loss :  0.0476198161621
Iteration :  88   Loss :  0.0474183315611
Iteration :  89   Loss :  0.0472176994632
Iteration :  90   Loss :  0.0470179162615
Iteration :  91   Loss :  0.0468189783642
Iteration :  92   Loss :  0.0466208821946
Iteration :  93   Loss :  0.0464236241914
Iteration :  94   Loss :  0.0462272008082
Iteration :  95   Loss :  0.0460316085136
Iteration :  96   Loss :  0.0458368437912
Iteration :  97   Loss :  0.0456429031394
Iteration :  98   Loss :  0.0454497830715
Iteration :  99   Loss :  0.0452574801156
[-0.00317991 -0.00051026 -0.00186996 ...,  0.00219318  0.00106887
  0.00016189]
CROSS VALIDATION 19
Iteration :  0   Loss :  30.2645196903
Iteration :  1   Loss :  13.7539753047
Iteration :  2   Loss :  0.0556140529688
Iteration :  3   Loss :  0.0553787438857
Iteration :  4   Loss :  0.0551444304209
Iteration :  5   Loss :  0.0549111083618
Iteration :  6   Loss :  0.0546787735136
Iteration :  7   Loss :  0.0544474216993
Iteration :  8   Loss :  0.0542170487595
Iteration :  9   Loss :  0.0539876505527
Iteration :  10   Loss :  0.0537592229545
Iteration :  11   Loss :  0.0535317618582
Iteration :  12   Loss :  0.0533052631745
Iteration :  13   Loss :  0.0530797228312
Iteration :  14   Loss :  0.0528551367735
Iteration :  15   Loss :  0.0526315009638
Iteration :  16   Loss :  0.0524088113815
Iteration :  17   Loss :  0.0521870640228
Iteration :  18   Loss :  0.0519662549013
Iteration :  19   Loss :  0.0517463800471
Iteration :  20   Loss :  0.0515274355072
Iteration :  21   Loss :  0.0513094173454
Iteration :  22   Loss :  0.0510923216421
Iteration :  23   Loss :  0.0508761444942
Iteration :  24   Loss :  0.0506608820152
Iteration :  25   Loss :  0.0504465303351
Iteration :  26   Loss :  0.0502330856001
Iteration :  27   Loss :  0.050020543973
Iteration :  28   Loss :  0.0498089016325
Iteration :  29   Loss :  0.0495981547737
Iteration :  30   Loss :  0.0493882996077
Iteration :  31   Loss :  0.0491793323616
Iteration :  32   Loss :  0.0489712492786
Iteration :  33   Loss :  0.0487640466176
Iteration :  34   Loss :  0.0485577206536
Iteration :  35   Loss :  0.048352267677
Iteration :  36   Loss :  0.0481476839943
Iteration :  37   Loss :  0.0479439659273
Iteration :  38   Loss :  0.0477411098134
Iteration :  39   Loss :  0.0475391120058
Iteration :  40   Loss :  0.0473379688728
Iteration :  41   Loss :  0.0471376767982
Iteration :  42   Loss :  0.0469382321811
Iteration :  43   Loss :  0.0467396314357
Iteration :  44   Loss :  0.0465418709916
Iteration :  45   Loss :  0.0463449472933
Iteration :  46   Loss :  0.0461488568006
Iteration :  47   Loss :  0.045953595988
Iteration :  48   Loss :  0.045759161345
Iteration :  49   Loss :  0.0455655493761
Iteration :  50   Loss :  0.0453727566004
Iteration :  51   Loss :  0.0451807795518
Iteration :  52   Loss :  0.0449896147789
Iteration :  53   Loss :  0.0447992588448
Iteration :  54   Loss :  0.0446097083273
Iteration :  55   Loss :  0.0444209598187
Iteration :  56   Loss :  0.0442330099254
Iteration :  57   Loss :  0.0440458552684
Iteration :  58   Loss :  0.0438594924831
Iteration :  59   Loss :  0.0436739182189
Iteration :  60   Loss :  0.0434891291396
Iteration :  61   Loss :  0.0433051219228
Iteration :  62   Loss :  0.0431218932605
Iteration :  63   Loss :  0.0429394398586
Iteration :  64   Loss :  0.0427577584367
Iteration :  65   Loss :  0.0425768457287
Iteration :  66   Loss :  0.0423966984819
Iteration :  67   Loss :  0.0422173134576
Iteration :  68   Loss :  0.0420386874307
Iteration :  69   Loss :  0.04186081719
Iteration :  70   Loss :  0.0416836995375
Iteration :  71   Loss :  0.041507331289
Iteration :  72   Loss :  0.0413317092737
Iteration :  73   Loss :  0.0411568303341
Iteration :  74   Loss :  0.0409826913262
Iteration :  75   Loss :  0.0408092891194
Iteration :  76   Loss :  0.0406366205961
Iteration :  77   Loss :  0.040464682652
Iteration :  78   Loss :  0.040293472196
Iteration :  79   Loss :  0.04012298615
Iteration :  80   Loss :  0.0399532214489
Iteration :  81   Loss :  0.0397841750407
Iteration :  82   Loss :  0.0396158438861
Iteration :  83   Loss :  0.0394482249589
Iteration :  84   Loss :  0.0392813152455
Iteration :  85   Loss :  0.0391151117451
Iteration :  86   Loss :  0.0389496114699
Iteration :  87   Loss :  0.0387848114441
Iteration :  88   Loss :  0.0386207087052
Iteration :  89   Loss :  0.0384573003027
Iteration :  90   Loss :  0.0382945832989
Iteration :  91   Loss :  0.0381325547684
Iteration :  92   Loss :  0.0379712117981
Iteration :  93   Loss :  0.0378105514874
Iteration :  94   Loss :  0.0376505709479
Iteration :  95   Loss :  0.0374912673033
Iteration :  96   Loss :  0.0373326376898
Iteration :  97   Loss :  0.0371746792553
Iteration :  98   Loss :  0.0370173891601
Iteration :  99   Loss :  0.0368607645763
[-0.0017189  -0.00050033 -0.0013033  ...,  0.00088684  0.00030854
  0.00016181]
Accuracy (Hinge Loss):	0.7
lmda : 0.1  eta : 0.0001
Logistic Loss
CROSS VALIDATION 0
Iteration :  0   Loss :  5.86298330746
Iteration :  1   Loss :  0.0265021917269
Iteration :  2   Loss :  0.0244868242127
Iteration :  3   Loss :  0.0237498047161
Iteration :  4   Loss :  0.0233067843371
Iteration :  5   Loss :  0.0229924881833
Iteration :  6   Loss :  0.0227489920404
Iteration :  7   Loss :  0.0225496935935
Iteration :  8   Loss :  0.0223803327132
Iteration :  9   Loss :  0.0222324434412
Iteration :  10   Loss :  0.022100619536
Iteration :  11   Loss :  0.021981214321
Iteration :  12   Loss :  0.0218716608394
Iteration :  13   Loss :  0.0217700894196
Iteration :  14   Loss :  0.021675099684
[ -1.85132182e-04   9.26818680e-04   3.73272866e-04 ...,   4.89534350e-04
  -4.24159246e-05   3.19411756e-04]
CROSS VALIDATION 1
Iteration :  0   Loss :  9.09361796058
Iteration :  1   Loss :  0.0363564042581
Iteration :  2   Loss :  0.0288205529576
Iteration :  3   Loss :  0.0268456765263
Iteration :  4   Loss :  0.0256989989246
Iteration :  5   Loss :  0.0249136273199
Iteration :  6   Loss :  0.0243277137827
Iteration :  7   Loss :  0.0238664367582
Iteration :  8   Loss :  0.0234893921447
Iteration :  9   Loss :  0.0231724697047
Iteration :  10   Loss :  0.0229002333923
Iteration :  11   Loss :  0.0226622675036
Iteration :  12   Loss :  0.0224512507105
Iteration :  13   Loss :  0.022261865308
Iteration :  14   Loss :  0.0220901435242
Iteration :  15   Loss :  0.0219330573852
Iteration :  16   Loss :  0.021788251435
Iteration :  17   Loss :  0.0216538628889
Iteration :  18   Loss :  0.0215283972626
Iteration :  19   Loss :  0.0214106403044
Iteration :  20   Loss :  0.0212995943292
Iteration :  21   Loss :  0.0211944313446
Iteration :  22   Loss :  0.0210944579748
[-0.00093265  0.0005723  -0.00040855 ...,  0.00024942 -0.00023272
  0.00037661]
CROSS VALIDATION 2
Iteration :  0   Loss :  2.58219941905
Iteration :  1   Loss :  0.0245814657898
Iteration :  2   Loss :  0.0235002233336
Iteration :  3   Loss :  0.022925916018
Iteration :  4   Loss :  0.0225535534285
Iteration :  5   Loss :  0.0222845548721
Iteration :  6   Loss :  0.0220761858992
Iteration :  7   Loss :  0.0219066508376
Iteration :  8   Loss :  0.021763585936
Iteration :  9   Loss :  0.0216394247508
Iteration :  10   Loss :  0.0215292639495
Iteration :  11   Loss :  0.0214297801739
[-0.0015093  -0.0005385  -0.00074727 ...,  0.00083707  0.00048505
  0.00029186]
CROSS VALIDATION 3
Iteration :  0   Loss :  8.00484747411
Iteration :  1   Loss :  0.0384605697345
Iteration :  2   Loss :  0.024359941047
Iteration :  3   Loss :  0.0236254146303
Iteration :  4   Loss :  0.023111957315
Iteration :  5   Loss :  0.0227242667344
Iteration :  6   Loss :  0.0224161425731
Iteration :  7   Loss :  0.0221620517355
Iteration :  8   Loss :  0.0219465754112
Iteration :  9   Loss :  0.0217597828913
Iteration :  10   Loss :  0.0215949557903
Iteration :  11   Loss :  0.0214473687918
Iteration :  12   Loss :  0.0213135918189
Iteration :  13   Loss :  0.0211910689689
Iteration :  14   Loss :  0.0210778531988
Iteration :  15   Loss :  0.0209724329881
Iteration :  16   Loss :  0.020873615556
[-0.0009226   0.00065226 -0.00038165 ...,  0.00026069 -0.00019924
  0.00040005]
CROSS VALIDATION 4
Iteration :  0   Loss :  7.85769000192
Iteration :  1   Loss :  0.0374563739042
Iteration :  2   Loss :  0.0244622291694
Iteration :  3   Loss :  0.0236791167828
Iteration :  4   Loss :  0.0231399404422
Iteration :  5   Loss :  0.022736628209
Iteration :  6   Loss :  0.0224181534985
Iteration :  7   Loss :  0.0221567776233
Iteration :  8   Loss :  0.0219359397478
Iteration :  9   Loss :  0.0217450627288
Iteration :  10   Loss :  0.0215770369137
Iteration :  11   Loss :  0.0214268874485
Iteration :  12   Loss :  0.0212910182827
Iteration :  13   Loss :  0.0211667592468
Iteration :  14   Loss :  0.021052082328
Iteration :  15   Loss :  0.0209454172078
Iteration :  16   Loss :  0.0208455274985
[-0.00090386  0.00066933 -0.00036564 ...,  0.00024581 -0.00019969
  0.00040082]
CROSS VALIDATION 5
Iteration :  0   Loss :  6.63679171793
Iteration :  1   Loss :  0.0237997710277
Iteration :  2   Loss :  0.0232412297128
Iteration :  3   Loss :  0.022861323834
Iteration :  4   Loss :  0.0225762447295
Iteration :  5   Loss :  0.0223488776
Iteration :  6   Loss :  0.0221597667212
Iteration :  7   Loss :  0.0219975632204
Iteration :  8   Loss :  0.0218551334182
Iteration :  9   Loss :  0.0217277367607
Iteration :  10   Loss :  0.0216120838783
Iteration :  11   Loss :  0.0215058118513
Iteration :  12   Loss :  0.0214071742337
[ -6.17937732e-04   8.10348842e-04  -3.90494554e-04 ...,   6.57896974e-05
  -1.51828458e-04   5.13920976e-04]
CROSS VALIDATION 6
Iteration :  0   Loss :  7.85167283369
Iteration :  1   Loss :  0.0374005186864
Iteration :  2   Loss :  0.0244666617827
Iteration :  3   Loss :  0.0236813841142
Iteration :  4   Loss :  0.0231411668854
Iteration :  5   Loss :  0.022737270569
Iteration :  6   Loss :  0.0224184363269
Iteration :  7   Loss :  0.0221568255971
Iteration :  8   Loss :  0.0219358282548
Iteration :  9   Loss :  0.0217448403345
Iteration :  10   Loss :  0.0215767364774
Iteration :  11   Loss :  0.0214265320925
Iteration :  12   Loss :  0.0212906248105
Iteration :  13   Loss :  0.0211663402051
Iteration :  14   Loss :  0.0210516473054
Iteration :  15   Loss :  0.0209449736819
Iteration :  16   Loss :  0.0208450814066
[-0.00090425  0.00066994 -0.00036529 ...,  0.00024529 -0.00019974
  0.00040092]
CROSS VALIDATION 7
Iteration :  0   Loss :  7.87222247856
Iteration :  1   Loss :  0.037524983527
Iteration :  2   Loss :  0.0243901134446
Iteration :  3   Loss :  0.0236164413972
Iteration :  4   Loss :  0.0230815084365
Iteration :  5   Loss :  0.0226803303729
Iteration :  6   Loss :  0.0223629810363
Iteration :  7   Loss :  0.0221021954657
Iteration :  8   Loss :  0.0218816435281
Iteration :  9   Loss :  0.0216908706969
Iteration :  10   Loss :  0.0215228368814
Iteration :  11   Loss :  0.0213726087257
Iteration :  12   Loss :  0.0212366159357
Iteration :  13   Loss :  0.0211122048405
Iteration :  14   Loss :  0.020997358266
Iteration :  15   Loss :  0.0208905131579
Iteration :  16   Loss :  0.0207904380735
Iteration :  17   Loss :  0.0206961486466
[-0.0009017   0.00066709 -0.00036637 ...,  0.00024554 -0.00020178
  0.00040024]
CROSS VALIDATION 8
Iteration :  0   Loss :  7.85266698239
Iteration :  1   Loss :  0.0377919166897
Iteration :  2   Loss :  0.0244478994043
Iteration :  3   Loss :  0.0236739535788
Iteration :  4   Loss :  0.0231391134272
Iteration :  5   Loss :  0.02273817823
Iteration :  6   Loss :  0.0224211293481
Iteration :  7   Loss :  0.0221606611858
Iteration :  8   Loss :  0.0219404248404
Iteration :  9   Loss :  0.0217499568901
Iteration :  10   Loss :  0.0215822131029
Iteration :  11   Loss :  0.0214322583877
Iteration :  12   Loss :  0.0212965219753
Iteration :  13   Loss :  0.0211723503901
Iteration :  14   Loss :  0.021057727006
Iteration :  15   Loss :  0.0209510894946
Iteration :  16   Loss :  0.020851207217
[-0.00090252  0.00067254 -0.00035885 ...,  0.00024789 -0.00020319
  0.00040146]
CROSS VALIDATION 9
Iteration :  0   Loss :  7.84551792552
Iteration :  1   Loss :  0.0419736822558
Iteration :  2   Loss :  0.0276076848986
Iteration :  3   Loss :  0.026813448399
Iteration :  4   Loss :  0.0262755927322
Iteration :  5   Loss :  0.0258761480394
Iteration :  6   Loss :  0.0255615299189
Iteration :  7   Loss :  0.0253032792221
Iteration :  8   Loss :  0.0250846730682
Iteration :  9   Loss :  0.0248951552675
Iteration :  10   Loss :  0.0247276943629
Iteration :  11   Loss :  0.0245774059207
Iteration :  12   Loss :  0.0244407803877
Iteration :  13   Loss :  0.0243152249708
Iteration :  14   Loss :  0.024198778951
Iteration :  15   Loss :  0.0240899298426
Iteration :  16   Loss :  0.0239874907576
Iteration :  17   Loss :  0.0238905162911
[-0.00081241  0.0007358  -0.00025366 ...,  0.00059652 -0.00014973
  0.00040837]
CROSS VALIDATION 10
Iteration :  0   Loss :  7.20309593006
Iteration :  1   Loss :  0.0288433742401
Iteration :  2   Loss :  0.0283203078829
Iteration :  3   Loss :  0.0279525490131
Iteration :  4   Loss :  0.0276704977827
Iteration :  5   Loss :  0.027441743541
Iteration :  6   Loss :  0.0272487948909
Iteration :  7   Loss :  0.0270812551933
Iteration :  8   Loss :  0.0269325038904
Iteration :  9   Loss :  0.0267980999034
Iteration :  10   Loss :  0.0266749399978
Iteration :  11   Loss :  0.0265607830319
Iteration :  12   Loss :  0.0264539657224
Iteration :  13   Loss :  0.0263532249966
Iteration :  14   Loss :  0.0262575827158
[-0.00084998  0.00063935 -0.00045519 ...,  0.00055539 -0.00010087
  0.00042779]
CROSS VALIDATION 11
Iteration :  0   Loss :  7.85396597488
Iteration :  1   Loss :  0.037406272203
Iteration :  2   Loss :  0.0244686601843
Iteration :  3   Loss :  0.0236847023526
Iteration :  4   Loss :  0.0231451595356
Iteration :  5   Loss :  0.0227416487421
Iteration :  6   Loss :  0.0224230478544
Iteration :  7   Loss :  0.0221615814457
Iteration :  8   Loss :  0.0219406723445
Iteration :  9   Loss :  0.0217497353527
Iteration :  10   Loss :  0.0215816565207
Iteration :  11   Loss :  0.0214314585532
Iteration :  12   Loss :  0.0212955439388
Iteration :  13   Loss :  0.0211712415933
Iteration :  14   Loss :  0.0210565229088
Iteration :  15   Loss :  0.0209498171651
Iteration :  16   Loss :  0.0208498876949
[-0.0009023   0.00066985 -0.0003639  ...,  0.00024588 -0.00019969
  0.00040054]
CROSS VALIDATION 12
Iteration :  0   Loss :  9.99471570438
Iteration :  1   Loss :  0.0275255850732
Iteration :  2   Loss :  0.0223910372042
Iteration :  3   Loss :  0.0213984078276
Iteration :  4   Loss :  0.0208199071502
Iteration :  5   Loss :  0.0204183809433
Iteration :  6   Loss :  0.0201138290624
Iteration :  7   Loss :  0.0198697623372
Iteration :  8   Loss :  0.0196666068465
Iteration :  9   Loss :  0.0194927163605
Iteration :  10   Loss :  0.0193406405187
Iteration :  11   Loss :  0.0192053497242
Iteration :  12   Loss :  0.0190833042933
Iteration :  13   Loss :  0.0189719289453
Iteration :  14   Loss :  0.0188692984263
Iteration :  15   Loss :  0.0187739404267
[ -1.80598906e-03   6.52899703e-05  -5.70787174e-04 ...,   6.49928348e-05
  -2.05219260e-04   2.35703467e-04]
CROSS VALIDATION 13
Iteration :  0   Loss :  7.96093282922
Iteration :  1   Loss :  0.0331109088317
Iteration :  2   Loss :  0.0293385769698
Iteration :  3   Loss :  0.0280308553784
Iteration :  4   Loss :  0.0272876826145
Iteration :  5   Loss :  0.0267867370083
Iteration :  6   Loss :  0.0264158335987
Iteration :  7   Loss :  0.0261240515935
Iteration :  8   Loss :  0.0258844910399
Iteration :  9   Loss :  0.0256814419114
Iteration :  10   Loss :  0.0255050456212
Iteration :  11   Loss :  0.0253487706699
Iteration :  12   Loss :  0.0252081019002
Iteration :  13   Loss :  0.0250798092235
Iteration :  14   Loss :  0.0249615156796
Iteration :  15   Loss :  0.0248514301532
Iteration :  16   Loss :  0.0247481754518
Iteration :  17   Loss :  0.024650674041
[ -1.39017556e-03  -1.56664350e-04  -7.03054473e-04 ...,   7.16138767e-05
   5.29609458e-05   3.71684470e-04]
CROSS VALIDATION 14
Iteration :  0   Loss :  7.83575363254
Iteration :  1   Loss :  0.0358832536414
Iteration :  2   Loss :  0.0230243764175
Iteration :  3   Loss :  0.0225306524386
Iteration :  4   Loss :  0.0221791667273
Iteration :  5   Loss :  0.0219081497692
Iteration :  6   Loss :  0.0216882140464
Iteration :  7   Loss :  0.0215031989586
Iteration :  8   Loss :  0.0213433340696
Iteration :  9   Loss :  0.02120230089
Iteration :  10   Loss :  0.0210758015433
Iteration :  11   Loss :  0.020960795016
Iteration :  12   Loss :  0.0208550602928
Iteration :  13   Loss :  0.0207569323582
[-0.0008889   0.00068105 -0.00035907 ...,  0.0002347  -0.00021177
  0.00039969]
CROSS VALIDATION 15
Iteration :  0   Loss :  7.88377336725
Iteration :  1   Loss :  0.0377228577662
Iteration :  2   Loss :  0.0244252459041
Iteration :  3   Loss :  0.023659474883
Iteration :  4   Loss :  0.0231288936709
Iteration :  5   Loss :  0.0227305003572
Iteration :  6   Loss :  0.0224151075285
Iteration :  7   Loss :  0.0221557870896
Iteration :  8   Loss :  0.0219363840234
Iteration :  9   Loss :  0.0217465436983
Iteration :  10   Loss :  0.0215792871221
Iteration :  11   Loss :  0.0214297203997
Iteration :  12   Loss :  0.0212942998657
Iteration :  13   Loss :  0.0211703904803
Iteration :  14   Loss :  0.0210559885081
Iteration :  15   Loss :  0.0209495408435
Iteration :  16   Loss :  0.0208498235764
[-0.00090474  0.00066848 -0.00036665 ...,  0.00024849 -0.00020175
  0.00039982]
CROSS VALIDATION 16
Iteration :  0   Loss :  7.86086895294
Iteration :  1   Loss :  0.0234797775135
Iteration :  2   Loss :  0.0228228262937
Iteration :  3   Loss :  0.0223890939051
Iteration :  4   Loss :  0.0220695695575
Iteration :  5   Loss :  0.0218182619037
Iteration :  6   Loss :  0.0216116658214
Iteration :  7   Loss :  0.0214362750751
Iteration :  8   Loss :  0.0212836815876
Iteration :  9   Loss :  0.0211483348495
Iteration :  10   Loss :  0.0210264020579
Iteration :  11   Loss :  0.020915140227
Iteration :  12   Loss :  0.0208125282608
Iteration :  13   Loss :  0.0207170404021
[-0.00089415  0.00067292 -0.00036339 ...,  0.00024446 -0.0002044
  0.00040503]
CROSS VALIDATION 17
Iteration :  0   Loss :  7.85158290232
Iteration :  1   Loss :  0.0373968779169
Iteration :  2   Loss :  0.02446561417
Iteration :  3   Loss :  0.0236802634749
Iteration :  4   Loss :  0.0231400217696
Iteration :  5   Loss :  0.0227361161622
Iteration :  6   Loss :  0.0224172774616
Iteration :  7   Loss :  0.0221556633666
Iteration :  8   Loss :  0.0219346623099
Iteration :  9   Loss :  0.0217436697781
Iteration :  10   Loss :  0.021575560236
Iteration :  11   Loss :  0.0214253490747
Iteration :  12   Loss :  0.0212894339729
Iteration :  13   Loss :  0.021165140577
Iteration :  14   Loss :  0.0210504379942
Iteration :  15   Loss :  0.0209437538705
Iteration :  16   Loss :  0.0208438503469
[-0.00090422  0.00066996 -0.00036529 ...,  0.00024531 -0.00019972
  0.00040093]
CROSS VALIDATION 18
Iteration :  0   Loss :  6.96272905819
Iteration :  1   Loss :  0.0272570585857
Iteration :  2   Loss :  0.0260775508399
Iteration :  3   Loss :  0.0252773818923
Iteration :  4   Loss :  0.0246820081491
Iteration :  5   Loss :  0.0242137910492
Iteration :  6   Loss :  0.0238312995348
Iteration :  7   Loss :  0.0235099079187
Iteration :  8   Loss :  0.023233873641
Iteration :  9   Loss :  0.0229925831577
Iteration :  10   Loss :  0.0227785820968
Iteration :  11   Loss :  0.0225864612202
Iteration :  12   Loss :  0.0224121888724
Iteration :  13   Loss :  0.0222526920466
Iteration :  14   Loss :  0.0221055833054
Iteration :  15   Loss :  0.0219689770199
Iteration :  16   Loss :  0.0218413623146
Iteration :  17   Loss :  0.0217215131339
Iteration :  18   Loss :  0.021608423268
Iteration :  19   Loss :  0.0215012585525
Iteration :  20   Loss :  0.021399321133
Iteration :  21   Loss :  0.0213020223595
[-0.00086382 -0.00022298 -0.00165147 ...,  0.00093662 -0.00019793
  0.00032589]
CROSS VALIDATION 19
Iteration :  0   Loss :  7.68265316565
Iteration :  1   Loss :  0.0375257325322
Iteration :  2   Loss :  0.0238877708011
Iteration :  3   Loss :  0.0231362219481
Iteration :  4   Loss :  0.0226080420826
Iteration :  5   Loss :  0.0222085164333
Iteration :  6   Loss :  0.0218910272304
Iteration :  7   Loss :  0.0216295336627
Iteration :  8   Loss :  0.0214081886236
Iteration :  9   Loss :  0.0212167281446
Iteration :  10   Loss :  0.0210481799718
Iteration :  11   Loss :  0.0208976257447
Iteration :  12   Loss :  0.0207614877651
Iteration :  13   Loss :  0.0206370962739
Iteration :  14   Loss :  0.020522415571
Iteration :  15   Loss :  0.0204158644262
Iteration :  16   Loss :  0.0203161947253
[-0.00108423  0.00067399 -0.00046384 ...,  0.00021521 -0.00022197
  0.00042256]
Accuracy (Logistic Loss):	0.95
Hinge Loss
CROSS VALIDATION 0
Iteration :  0   Loss :  24.6584129106
Iteration :  1   Loss :  0.029006380026
Iteration :  2   Loss :  0.0289449510238
[ -2.05403283e-03   1.56897861e-04   2.31077276e-04 ...,   6.32107604e-04
   8.36270791e-04  -9.98481317e-05]
CROSS VALIDATION 1
Iteration :  0   Loss :  14.6826261172
Iteration :  1   Loss :  0.278500512547
Iteration :  2   Loss :  0.0311387536414
Iteration :  3   Loss :  0.0310728087505
[-0.00237463 -0.00032083 -0.00053783 ...,  0.00124354  0.00112943
  0.00029897]
CROSS VALIDATION 2
Iteration :  0   Loss :  10.3714683914
Iteration :  1   Loss :  2.39615863481
Iteration :  2   Loss :  0.0312430401989
Iteration :  3   Loss :  0.0311768744525
[ -2.00406289e-03  -8.34363503e-04  -8.68441222e-04 ...,   2.11780987e-03
   9.43484708e-04   9.96646512e-05]
CROSS VALIDATION 3
Iteration :  0   Loss :  20.7880090104
Iteration :  1   Loss :  1.32704784141
Iteration :  2   Loss :  0.656401381369
Iteration :  3   Loss :  0.0328015118959
Iteration :  4   Loss :  0.0327320456563
[-0.00293236 -0.00029478 -0.00052927 ...,  0.00221909  0.00134439
  0.00019935]
CROSS VALIDATION 4
Iteration :  0   Loss :  20.7880090104
Iteration :  1   Loss :  1.32704784141
Iteration :  2   Loss :  0.656401381369
Iteration :  3   Loss :  0.0328015118959
Iteration :  4   Loss :  0.0327320456563
[-0.00293236 -0.00029478 -0.00052927 ...,  0.00221909  0.00134439
  0.00019935]
CROSS VALIDATION 5
Iteration :  0   Loss :  9.64306726614
Iteration :  1   Loss :  0.0300713434002
Iteration :  2   Loss :  0.0300076590446
[-0.00101054  0.00081658 -0.00096141 ...,  0.00020295  0.00026991
  0.00059833]
CROSS VALIDATION 6
Iteration :  0   Loss :  20.7877534245
Iteration :  1   Loss :  1.32703137292
Iteration :  2   Loss :  0.031006693203
Iteration :  3   Loss :  0.0309410279864
[ -3.30863093e-03  -6.53472455e-04  -7.76940036e-04 ...,   2.07561653e-03
   1.21386147e-03   9.96487365e-05]
CROSS VALIDATION 7
Iteration :  0   Loss :  20.7877534245
Iteration :  1   Loss :  1.32703137292
Iteration :  2   Loss :  0.656350980619
Iteration :  3   Loss :  0.032801641565
Iteration :  4   Loss :  0.0327321750507
[-0.00293236 -0.00029477 -0.00052926 ...,  0.00221911  0.00134441
  0.00019935]
CROSS VALIDATION 8
Iteration :  0   Loss :  23.643014171
Iteration :  1   Loss :  0.75119012366
Iteration :  2   Loss :  0.0816401932472
Iteration :  3   Loss :  0.032768750307
Iteration :  4   Loss :  0.0326993534491
[ -2.23118044e-03   5.51413111e-04  -1.44358134e-03 ...,   1.78347212e-03
  -1.80431389e-04   9.90130150e-05]
CROSS VALIDATION 9
Iteration :  0   Loss :  11.0488762909
Iteration :  1   Loss :  0.398103347181
Iteration :  2   Loss :  0.0322808883424
Iteration :  3   Loss :  0.0322125246665
[-0.00297698 -0.00026434 -0.00084544 ...,  0.00203939  0.00162308
  0.00019929]
CROSS VALIDATION 10
Iteration :  0   Loss :  13.3475565125
Iteration :  1   Loss :  11.5533838163
Iteration :  2   Loss :  0.0328904256192
Iteration :  3   Loss :  0.0328207710803
[-0.00197432 -0.00012362 -0.0014115  ...,  0.00231281  0.00098025
  0.0003985 ]
CROSS VALIDATION 11
Iteration :  0   Loss :  20.7877574926
Iteration :  1   Loss :  1.32696783916
Iteration :  2   Loss :  0.656243565932
Iteration :  3   Loss :  0.0328017978137
Iteration :  4   Loss :  0.0327323309686
[-0.00293237 -0.00029478 -0.00052928 ...,  0.00221911  0.00134442
  0.00019935]
CROSS VALIDATION 12
Iteration :  0   Loss :  19.1278418894
Iteration :  1   Loss :  4.43012116538
Iteration :  2   Loss :  0.0353313715282
Iteration :  3   Loss :  0.0352565476137
[ -4.41769086e-03  -5.64231895e-04  -1.82683408e-03 ...,   2.07137311e-03
   1.26334305e-03   9.97722783e-05]
CROSS VALIDATION 13
Iteration :  0   Loss :  20.7882834772
Iteration :  1   Loss :  1.32683911634
Iteration :  2   Loss :  0.656466376429
Iteration :  3   Loss :  0.032801852705
Iteration :  4   Loss :  0.0327323857436
[-0.00293236 -0.00029477 -0.00052928 ...,  0.00221911  0.00134442
  0.00019935]
CROSS VALIDATION 14
Iteration :  0   Loss :  20.7882834772
Iteration :  1   Loss :  0.0294601016122
Iteration :  2   Loss :  0.0293977117295
[ -2.67776781e-03  -3.46922063e-04  -4.99346090e-04 ...,   1.64813764e-03
   7.86024720e-04  -1.65614373e-07]
CROSS VALIDATION 15
Iteration :  0   Loss :  20.7882834772
Iteration :  1   Loss :  1.32683911634
Iteration :  2   Loss :  0.656502104205
Iteration :  3   Loss :  0.0328019163451
Iteration :  4   Loss :  0.0327324492489
[-0.00293237 -0.00029478 -0.00052928 ...,  0.00221912  0.00134443
  0.00019935]
CROSS VALIDATION 16
Iteration :  0   Loss :  20.7882834772
Iteration :  1   Loss :  0.0279652104649
Iteration :  2   Loss :  0.0279059864261
[ -2.26350052e-03  -3.22760189e-04  -3.67552961e-04 ...,   1.63333473e-03
   7.90304815e-04   9.96545457e-05]
CROSS VALIDATION 17
Iteration :  0   Loss :  20.7882834772
Iteration :  1   Loss :  1.32690157344
Iteration :  2   Loss :  0.656598763077
Iteration :  3   Loss :  0.0328019747552
Iteration :  4   Loss :  0.0327325075353
[-0.00293238 -0.00029478 -0.00052928 ...,  0.00221912  0.00134443
  0.00019935]
CROSS VALIDATION 18
Iteration :  0   Loss :  24.1716920804
Iteration :  1   Loss :  15.0514043287
Iteration :  2   Loss :  0.0342957633387
Iteration :  3   Loss :  0.0342231326099
[-0.00391049 -0.00062628 -0.00230009 ...,  0.00269593  0.0013148
  0.00019921]
CROSS VALIDATION 19
Iteration :  0   Loss :  30.2384177576
Iteration :  1   Loss :  13.7214015244
Iteration :  2   Loss :  0.0279460807153
Iteration :  3   Loss :  0.027886897189
[-0.0021149  -0.00061535 -0.00160354 ...,  0.00109032  0.00038024
  0.00019917]
Accuracy (Hinge Loss):	0.7
lmda : 0.5  eta : 0.0001
Logistic Loss
CROSS VALIDATION 0
Iteration :  0   Loss :  5.97291845691
Iteration :  1   Loss :  0.108102839312
Iteration :  2   Loss :  0.105044627529
Iteration :  3   Loss :  0.103337749144
Iteration :  4   Loss :  0.101934383426
Iteration :  5   Loss :  0.100667599813
Iteration :  6   Loss :  0.0994798462872
Iteration :  7   Loss :  0.0983449788707
Iteration :  8   Loss :  0.0972490872185
Iteration :  9   Loss :  0.0961839474042
Iteration :  10   Loss :  0.0951443116404
Iteration :  11   Loss :  0.0941266298741
Iteration :  12   Loss :  0.0931283862861
Iteration :  13   Loss :  0.09214772866
Iteration :  14   Loss :  0.0911832488799
Iteration :  15   Loss :  0.0902338465791
Iteration :  16   Loss :  0.0892986410129
Iteration :  17   Loss :  0.0883769121555
Iteration :  18   Loss :  0.0874680601849
Iteration :  19   Loss :  0.0865715769128
Iteration :  20   Loss :  0.0856870251974
Iteration :  21   Loss :  0.0848140238206
Iteration :  22   Loss :  0.0839522361894
Iteration :  23   Loss :  0.0831013617662
Iteration :  24   Loss :  0.0822611294777
Iteration :  25   Loss :  0.0814312925839
Iteration :  26   Loss :  0.0806116246378
Iteration :  27   Loss :  0.0798019162703
Iteration :  28   Loss :  0.0790019726088
Iteration :  29   Loss :  0.0782116111836
Iteration :  30   Loss :  0.0774306602184
Iteration :  31   Loss :  0.0766589572218
Iteration :  32   Loss :  0.0758963478186
Iteration :  33   Loss :  0.0751426847736
Iteration :  34   Loss :  0.0743978271708
Iteration :  35   Loss :  0.0736616397184
Iteration :  36   Loss :  0.0729339921572
Iteration :  37   Loss :  0.0722147587538
Iteration :  38   Loss :  0.0715038178645
Iteration :  39   Loss :  0.0708010515576
Iteration :  40   Loss :  0.0701063452848
Iteration :  41   Loss :  0.0694195875942
Iteration :  42   Loss :  0.0687406698784
Iteration :  43   Loss :  0.068069486152
Iteration :  44   Loss :  0.0674059328554
Iteration :  45   Loss :  0.0667499086799
Iteration :  46   Loss :  0.0661013144127
Iteration :  47   Loss :  0.0654600527974
Iteration :  48   Loss :  0.0648260284098
Iteration :  49   Loss :  0.064199147546
Iteration :  50   Loss :  0.0635793181221
Iteration :  51   Loss :  0.0629664495832
Iteration :  52   Loss :  0.0623604528217
Iteration :  53   Loss :  0.0617612401035
Iteration :  54   Loss :  0.0611687250014
Iteration :  55   Loss :  0.0605828223348
Iteration :  56   Loss :  0.0600034481159
Iteration :  57   Loss :  0.0594305195004
Iteration :  58   Loss :  0.0588639547448
Iteration :  59   Loss :  0.0583036731676
Iteration :  60   Loss :  0.057749595115
Iteration :  61   Loss :  0.0572016419318
Iteration :  62   Loss :  0.0566597359362
Iteration :  63   Loss :  0.0561238003983
Iteration :  64   Loss :  0.0555937595234
Iteration :  65   Loss :  0.055069538439
Iteration :  66   Loss :  0.0545510631849
Iteration :  67   Loss :  0.0540382607085
Iteration :  68   Loss :  0.0535310588619
Iteration :  69   Loss :  0.053029386404
Iteration :  70   Loss :  0.0525331730048
Iteration :  71   Loss :  0.052042349253
Iteration :  72   Loss :  0.0515568466665
Iteration :  73   Loss :  0.0510765977049
Iteration :  74   Loss :  0.0506015357844
Iteration :  75   Loss :  0.0501315952944
Iteration :  76   Loss :  0.0496667116148
Iteration :  77   Loss :  0.0492068211352
Iteration :  78   Loss :  0.0487518612732
Iteration :  79   Loss :  0.0483017704936
Iteration :  80   Loss :  0.0478564883266
Iteration :  81   Loss :  0.0474159553852
Iteration :  82   Loss :  0.0469801133808
Iteration :  83   Loss :  0.0465489051378
Iteration :  84   Loss :  0.0461222746046
Iteration :  85   Loss :  0.045700166864
Iteration :  86   Loss :  0.0452825281389
Iteration :  87   Loss :  0.0448693057963
Iteration :  88   Loss :  0.044460448348
Iteration :  89   Loss :  0.0440559054472
Iteration :  90   Loss :  0.0436556278832
Iteration :  91   Loss :  0.043259567572
Iteration :  92   Loss :  0.0428676775437
Iteration :  93   Loss :  0.0424799119276
Iteration :  94   Loss :  0.042096225934
Iteration :  95   Loss :  0.0417165758338
Iteration :  96   Loss :  0.0413409189353
Iteration :  97   Loss :  0.0409692135595
Iteration :  98   Loss :  0.0406014190139
Iteration :  99   Loss :  0.0402374955644
[ -1.65610052e-04   5.09566096e-04   1.97036701e-04 ...,   3.23228382e-04
  -1.46828336e-06   1.88389502e-04]
CROSS VALIDATION 1
Iteration :  0   Loss :  9.13304477349
Iteration :  1   Loss :  0.115996283396
Iteration :  2   Loss :  0.107330964551
Iteration :  3   Loss :  0.104478277425
Iteration :  4   Loss :  0.102431263111
Iteration :  5   Loss :  0.100738629012
Iteration :  6   Loss :  0.099244993678
Iteration :  7   Loss :  0.0978789241438
Iteration :  8   Loss :  0.0966020135745
Iteration :  9   Loss :  0.0953914143156
Iteration :  10   Loss :  0.0942325249956
Iteration :  11   Loss :  0.0931154977551
Iteration :  12   Loss :  0.0920334040911
Iteration :  13   Loss :  0.0909811997378
Iteration :  14   Loss :  0.0899551063341
Iteration :  15   Loss :  0.0889522246418
Iteration :  16   Loss :  0.087970283185
Iteration :  17   Loss :  0.0870074695499
Iteration :  18   Loss :  0.0860623139973
Iteration :  19   Loss :  0.0851336072277
Iteration :  20   Loss :  0.08422034105
Iteration :  21   Loss :  0.0833216647789
Iteration :  22   Loss :  0.0824368526611
Iteration :  23   Loss :  0.0815652791804
Iteration :  24   Loss :  0.0807064000858
Iteration :  25   Loss :  0.079859737641
Iteration :  26   Loss :  0.0790248690288
Iteration :  27   Loss :  0.0782014171443
Iteration :  28   Loss :  0.0773890432179
Iteration :  29   Loss :  0.0765874408527
Iteration :  30   Loss :  0.0757963311677
Iteration :  31   Loss :  0.0750154588138
Iteration :  32   Loss :  0.0742445886813
Iteration :  33   Loss :  0.0734835031641
Iteration :  34   Loss :  0.0727319998705
Iteration :  35   Loss :  0.0719898896992
Iteration :  36   Loss :  0.0712569952124
Iteration :  37   Loss :  0.0705331492539
Iteration :  38   Loss :  0.0698181937696
Iteration :  39   Loss :  0.0691119787968
Iteration :  40   Loss :  0.0684143615939
Iteration :  41   Loss :  0.0677252058889
Iteration :  42   Loss :  0.0670443812268
Iteration :  43   Loss :  0.0663717624021
Iteration :  44   Loss :  0.0657072289628
Iteration :  45   Loss :  0.0650506647751
Iteration :  46   Loss :  0.064401957641
Iteration :  47   Loss :  0.0637609989601
Iteration :  48   Loss :  0.0631276834294
Iteration :  49   Loss :  0.0625019087771
Iteration :  50   Loss :  0.061883575523
Iteration :  51   Loss :  0.0612725867646
Iteration :  52   Loss :  0.0606688479829
Iteration :  53   Loss :  0.0600722668668
Iteration :  54   Loss :  0.0594827531519
Iteration :  55   Loss :  0.0589002184727
Iteration :  56   Loss :  0.0583245762264
Iteration :  57   Loss :  0.0577557414458
Iteration :  58   Loss :  0.0571936306804
Iteration :  59   Loss :  0.0566381618852
Iteration :  60   Loss :  0.0560892543155
Iteration :  61   Loss :  0.0555468284281
Iteration :  62   Loss :  0.0550108057873
Iteration :  63   Loss :  0.0544811089765
Iteration :  64   Loss :  0.0539576615148
Iteration :  65   Loss :  0.0534403877785
Iteration :  66   Loss :  0.0529292129286
Iteration :  67   Loss :  0.0524240628429
Iteration :  68   Loss :  0.0519248640548
Iteration :  69   Loss :  0.0514315436978
Iteration :  70   Loss :  0.0509440294569
Iteration :  71   Loss :  0.0504622495261
Iteration :  72   Loss :  0.0499861325741
Iteration :  73   Loss :  0.0495156077163
Iteration :  74   Loss :  0.049050604495
Iteration :  75   Loss :  0.0485910528662
Iteration :  76   Loss :  0.048136883195
Iteration :  77   Loss :  0.0476880262566
Iteration :  78   Loss :  0.0472444132454
Iteration :  79   Loss :  0.0468059757895
Iteration :  80   Loss :  0.0463726459721
Iteration :  81   Loss :  0.0459443563563
Iteration :  82   Loss :  0.0455210400166
Iteration :  83   Loss :  0.0451026305718
Iteration :  84   Loss :  0.0446890622218
Iteration :  85   Loss :  0.0442802697865
Iteration :  86   Loss :  0.043876188745
Iteration :  87   Loss :  0.0434767552752
Iteration :  88   Loss :  0.043081906293
Iteration :  89   Loss :  0.04269157949
Iteration :  90   Loss :  0.0423057133688
Iteration :  91   Loss :  0.0419242472763
Iteration :  92   Loss :  0.0415471214331
Iteration :  93   Loss :  0.0411742769601
Iteration :  94   Loss :  0.0408056559002
Iteration :  95   Loss :  0.0404412012371
Iteration :  96   Loss :  0.040080856909
Iteration :  97   Loss :  0.0397245678188
Iteration :  98   Loss :  0.0393722798393
Iteration :  99   Loss :  0.0390239398157
[-0.00061194  0.00031482 -0.00026108 ...,  0.00016956 -0.00011753
  0.00023581]
CROSS VALIDATION 2
Iteration :  0   Loss :  2.6680262051
Iteration :  1   Loss :  0.107372462523
Iteration :  2   Loss :  0.105243410023
Iteration :  3   Loss :  0.10364488744
Iteration :  4   Loss :  0.102261318717
Iteration :  5   Loss :  0.100992415926
Iteration :  6   Loss :  0.0997950489793
Iteration :  7   Loss :  0.0986473046138
Iteration :  8   Loss :  0.0975367636059
Iteration :  9   Loss :  0.0964558123206
Iteration :  10   Loss :  0.0953994949031
Iteration :  11   Loss :  0.0943644276644
Iteration :  12   Loss :  0.0933482075561
Iteration :  13   Loss :  0.0923490701008
Iteration :  14   Loss :  0.09136568176
Iteration :  15   Loss :  0.0903970087204
Iteration :  16   Loss :  0.0894422310981
Iteration :  17   Loss :  0.0885006851741
Iteration :  18   Loss :  0.0875718235014
Iteration :  19   Loss :  0.086655186734
Iteration :  20   Loss :  0.0857503833363
Iteration :  21   Loss :  0.0848570747088
Iteration :  22   Loss :  0.0839749641109
Iteration :  23   Loss :  0.083103788291
Iteration :  24   Loss :  0.0822433110803
Iteration :  25   Loss :  0.0813933184302
Iteration :  26   Loss :  0.0805536145244
Iteration :  27   Loss :  0.0797240187037
Iteration :  28   Loss :  0.0789043630078
Iteration :  29   Loss :  0.078094490195
Iteration :  30   Loss :  0.077294252132
Iteration :  31   Loss :  0.0765035084747
Iteration :  32   Loss :  0.0757221255791
Iteration :  33   Loss :  0.0749499755962
Iteration :  34   Loss :  0.0741869357133
Iteration :  35   Loss :  0.0734328875157
Iteration :  36   Loss :  0.0726877164447
Iteration :  37   Loss :  0.0719513113351
Iteration :  38   Loss :  0.0712235640189
Iteration :  39   Loss :  0.0705043689826
Iteration :  40   Loss :  0.0697936230701
Iteration :  41   Loss :  0.0690912252244
Iteration :  42   Loss :  0.0683970762604
Iteration :  43   Loss :  0.067711078666
Iteration :  44   Loss :  0.0670331364264
Iteration :  45   Loss :  0.0663631548685
Iteration :  46   Loss :  0.0657010405237
Iteration :  47   Loss :  0.0650467010055
Iteration :  48   Loss :  0.0644000449015
Iteration :  49   Loss :  0.063760981678
Iteration :  50   Loss :  0.0631294215948
Iteration :  51   Loss :  0.0625052756321
Iteration :  52   Loss :  0.0618884554254
Iteration :  53   Loss :  0.0612788732103
Iteration :  54   Loss :  0.0606764417753
Iteration :  55   Loss :  0.0600810744227
Iteration :  56   Loss :  0.0594926849374
Iteration :  57   Loss :  0.0589111875627
Iteration :  58   Loss :  0.058336496983
Iteration :  59   Loss :  0.0577685283136
Iteration :  60   Loss :  0.0572071970966
Iteration :  61   Loss :  0.0566524193029
Iteration :  62   Loss :  0.0561041113405
Iteration :  63   Loss :  0.0555621900673
Iteration :  64   Loss :  0.0550265728102
Iteration :  65   Loss :  0.0544971773882
Iteration :  66   Loss :  0.0539739221395
Iteration :  67   Loss :  0.053456725954
Iteration :  68   Loss :  0.0529455083071
Iteration :  69   Loss :  0.0524401892987
Iteration :  70   Loss :  0.0519406896928
Iteration :  71   Loss :  0.0514469309605
Iteration :  72   Loss :  0.0509588353235
Iteration :  73   Loss :  0.0504763257993
Iteration :  74   Loss :  0.0499993262461
Iteration :  75   Loss :  0.0495277614088
Iteration :  76   Loss :  0.049061556963
Iteration :  77   Loss :  0.0486006395599
Iteration :  78   Loss :  0.0481449368677
Iteration :  79   Loss :  0.0476943776135
Iteration :  80   Loss :  0.0472488916211
Iteration :  81   Loss :  0.0468084098477
Iteration :  82   Loss :  0.0463728644175
Iteration :  83   Loss :  0.0459421886519
Iteration :  84   Loss :  0.0455163170978
Iteration :  85   Loss :  0.045095185551
Iteration :  86   Loss :  0.0446787310782
Iteration :  87   Loss :  0.0442668920337
Iteration :  88   Loss :  0.0438596080747
Iteration :  89   Loss :  0.0434568201717
Iteration :  90   Loss :  0.0430584706166
Iteration :  91   Loss :  0.0426645030274
Iteration :  92   Loss :  0.0422748623504
Iteration :  93   Loss :  0.0418894948586
Iteration :  94   Loss :  0.0415083481489
Iteration :  95   Loss :  0.0411313711357
Iteration :  96   Loss :  0.0407585140432
Iteration :  97   Loss :  0.0403897283952
Iteration :  98   Loss :  0.0400249670032
Iteration :  99   Loss :  0.0396641839531
[-0.0009295  -0.00033886 -0.0004563  ...,  0.00051084  0.00030832
  0.00017961]
CROSS VALIDATION 3
Iteration :  0   Loss :  8.07393073841
Iteration :  1   Loss :  0.117294868426
Iteration :  2   Loss :  0.102241508404
Iteration :  3   Loss :  0.100613913914
Iteration :  4   Loss :  0.0991895041074
Iteration :  5   Loss :  0.0978870748936
Iteration :  6   Loss :  0.0966663537921
Iteration :  7   Loss :  0.0955047426049
Iteration :  8   Loss :  0.0943884333898
Iteration :  9   Loss :  0.0933084408465
Iteration :  10   Loss :  0.0922586250977
Iteration :  11   Loss :  0.0912346223151
Iteration :  12   Loss :  0.0902332282747
Iteration :  13   Loss :  0.0892520243271
Iteration :  14   Loss :  0.0882891406904
Iteration :  15   Loss :  0.0873431012651
Iteration :  16   Loss :  0.0864127187926
Iteration :  17   Loss :  0.0854970221689
Iteration :  18   Loss :  0.0845952049033
Iteration :  19   Loss :  0.0837065878398
Iteration :  20   Loss :  0.0828305917174
Iteration :  21   Loss :  0.0819667166554
Iteration :  22   Loss :  0.0811145265978
Iteration :  23   Loss :  0.080273637369
Iteration :  24   Loss :  0.0794437073953
Iteration :  25   Loss :  0.0786244304214
Iteration :  26   Loss :  0.0778155297374
Iteration :  27   Loss :  0.0770167535609
Iteration :  28   Loss :  0.0762278713131
Iteration :  29   Loss :  0.0754486705905
Iteration :  30   Loss :  0.0746789546842
Iteration :  31   Loss :  0.0739185405325
Iteration :  32   Loss :  0.0731672570189
Iteration :  33   Loss :  0.0724249435468
Iteration :  34   Loss :  0.0716914488378
Iteration :  35   Loss :  0.0709666299107
Iteration :  36   Loss :  0.0702503512078
Iteration :  37   Loss :  0.0695424838416
Iteration :  38   Loss :  0.0688429049404
Iteration :  39   Loss :  0.0681514970755
Iteration :  40   Loss :  0.0674681477561
Iteration :  41   Loss :  0.0667927489796
Iteration :  42   Loss :  0.0661251968298
Iteration :  43   Loss :  0.0654653911139
Iteration :  44   Loss :  0.0648132350337
Iteration :  45   Loss :  0.0641686348843
Iteration :  46   Loss :  0.0635314997784
Iteration :  47   Loss :  0.0629017413916
Iteration :  48   Loss :  0.0622792737265
Iteration :  49   Loss :  0.0616640128941
Iteration :  50   Loss :  0.0610558769099
Iteration :  51   Loss :  0.0604547855049
Iteration :  52   Loss :  0.0598606599488
Iteration :  53   Loss :  0.0592734228864
Iteration :  54   Loss :  0.0586929981858
Iteration :  55   Loss :  0.0581193107976
Iteration :  56   Loss :  0.0575522866263
Iteration :  57   Loss :  0.0569918524118
Iteration :  58   Loss :  0.0564379356229
Iteration :  59   Loss :  0.0558904643607
Iteration :  60   Loss :  0.0553493672735
Iteration :  61   Loss :  0.0548145734823
Iteration :  62   Loss :  0.0542860125168
Iteration :  63   Loss :  0.0537636142614
Iteration :  64   Loss :  0.0532473089124
Iteration :  65   Loss :  0.0527370269448
Iteration :  66   Loss :  0.0522326990881
Iteration :  67   Loss :  0.051734256313
Iteration :  68   Loss :  0.0512416298253
Iteration :  69   Loss :  0.0507547510697
Iteration :  70   Loss :  0.0502735517402
Iteration :  71   Loss :  0.0497979637986
Iteration :  72   Loss :  0.0493279194989
Iteration :  73   Loss :  0.0488633514177
Iteration :  74   Loss :  0.0484041924898
Iteration :  75   Loss :  0.047950376047
Iteration :  76   Loss :  0.0475018358611
Iteration :  77   Loss :  0.0470585061892
Iteration :  78   Loss :  0.0466203218196
Iteration :  79   Loss :  0.0461872181199
Iteration :  80   Loss :  0.0457591310833
Iteration :  81   Loss :  0.0453359973752
Iteration :  82   Loss :  0.0449177543776
Iteration :  83   Loss :  0.0445043402315
Iteration :  84   Loss :  0.0440956938755
Iteration :  85   Loss :  0.0436917550821
Iteration :  86   Loss :  0.0432924644891
Iteration :  87   Loss :  0.042897763627
Iteration :  88   Loss :  0.042507594942
Iteration :  89   Loss :  0.0421219018137
Iteration :  90   Loss :  0.0417406285684
Iteration :  91   Loss :  0.0413637204869
Iteration :  92   Loss :  0.0409911238081
Iteration :  93   Loss :  0.0406227857267
Iteration :  94   Loss :  0.0402586543877
Iteration :  95   Loss :  0.0398986788754
Iteration :  96   Loss :  0.0395428091985
Iteration :  97   Loss :  0.0391909962723
Iteration :  98   Loss :  0.0388431918966
Iteration :  99   Loss :  0.0384993487308
[-0.00060963  0.00035426 -0.00024393 ...,  0.00017715 -0.00010488
  0.00024425]
CROSS VALIDATION 4
Iteration :  0   Loss :  7.92923476406
Iteration :  1   Loss :  0.115941933607
Iteration :  2   Loss :  0.102138620176
Iteration :  3   Loss :  0.100460467905
Iteration :  4   Loss :  0.0990104461583
Iteration :  5   Loss :  0.0976936215944
Iteration :  6   Loss :  0.0964644072592
Iteration :  7   Loss :  0.0952977405203
Iteration :  8   Loss :  0.0941785241488
Iteration :  9   Loss :  0.0930970370212
Iteration :  10   Loss :  0.092046690646
Iteration :  11   Loss :  0.0910228331144
Iteration :  12   Loss :  0.0900220672942
Iteration :  13   Loss :  0.0890418408268
Iteration :  14   Loss :  0.088080188544
Iteration :  15   Loss :  0.0871355646238
Iteration :  16   Loss :  0.0862067297801
Iteration :  17   Loss :  0.0852926733962
Iteration :  18   Loss :  0.0843925585196
Iteration :  19   Loss :  0.0835056822109
Iteration :  20   Loss :  0.0826314464419
Iteration :  21   Loss :  0.0817693363923
Iteration :  22   Loss :  0.0809189040306
Iteration :  23   Loss :  0.0800797555292
Iteration :  24   Loss :  0.0792515415049
Iteration :  25   Loss :  0.0784339493689
Iteration :  26   Loss :  0.0776266972681
Iteration :  27   Loss :  0.0768295292454
Iteration :  28   Loss :  0.0760422113374
Iteration :  29   Loss :  0.0752645284036
Iteration :  30   Loss :  0.0744962815293
Iteration :  31   Loss :  0.0737372858825
Iteration :  32   Loss :  0.0729873689323
Iteration :  33   Loss :  0.0722463689578
Iteration :  34   Loss :  0.07151413379
Iteration :  35   Loss :  0.0707905197442
Iteration :  36   Loss :  0.0700753907065
Iteration :  37   Loss :  0.0693686173477
Iteration :  38   Loss :  0.0686700764414
Iteration :  39   Loss :  0.0679796502682
Iteration :  40   Loss :  0.0672972260926
Iteration :  41   Loss :  0.0666226956994
Iteration :  42   Loss :  0.0659559549817
Iteration :  43   Loss :  0.0652969035712
Iteration :  44   Loss :  0.0646454445053
Iteration :  45   Loss :  0.0640014839262
Iteration :  46   Loss :  0.0633649308067
Iteration :  47   Loss :  0.0627356967012
Iteration :  48   Loss :  0.0621136955162
Iteration :  49   Loss :  0.061498843302
Iteration :  50   Loss :  0.0608910580595
Iteration :  51   Loss :  0.0602902595643
Iteration :  52   Loss :  0.0596963692048
Iteration :  53   Loss :  0.0591093098335
Iteration :  54   Loss :  0.0585290056313
Iteration :  55   Loss :  0.0579553819848
Iteration :  56   Loss :  0.0573883653732
Iteration :  57   Loss :  0.0568278832682
Iteration :  58   Loss :  0.0562738640433
Iteration :  59   Loss :  0.0557262368937
Iteration :  60   Loss :  0.0551849317663
Iteration :  61   Loss :  0.0546498792983
Iteration :  62   Loss :  0.0541210107662
Iteration :  63   Loss :  0.0535982580423
Iteration :  64   Loss :  0.0530815535601
Iteration :  65   Loss :  0.0525708302875
Iteration :  66   Loss :  0.0520660217071
Iteration :  67   Loss :  0.0515670618038
Iteration :  68   Loss :  0.0510738850582
Iteration :  69   Loss :  0.0505864264465
Iteration :  70   Loss :  0.0501046214448
Iteration :  71   Loss :  0.0496284060382
Iteration :  72   Loss :  0.0491577167345
Iteration :  73   Loss :  0.0486924905799
Iteration :  74   Loss :  0.0482326651783
Iteration :  75   Loss :  0.0477781787121
Iteration :  76   Loss :  0.0473289699642
Iteration :  77   Loss :  0.046884978341
Iteration :  78   Loss :  0.0464461438944
Iteration :  79   Loss :  0.0460124073444
Iteration :  80   Loss :  0.0455837100994
Iteration :  81   Loss :  0.0451599942754
Iteration :  82   Loss :  0.0447412027127
Iteration :  83   Loss :  0.0443272789899
Iteration :  84   Loss :  0.0439181674351
Iteration :  85   Loss :  0.0435138131334
Iteration :  86   Loss :  0.0431141619315
Iteration :  87   Loss :  0.0427191604379
Iteration :  88   Loss :  0.0423287560194
Iteration :  89   Loss :  0.0419428967943
Iteration :  90   Loss :  0.0415615316211
Iteration :  91   Loss :  0.0411846100842
Iteration :  92   Loss :  0.040812082475
Iteration :  93   Loss :  0.040443899771
Iteration :  94   Loss :  0.0400800136106
Iteration :  95   Loss :  0.0397203762659
Iteration :  96   Loss :  0.0393649406125
Iteration :  97   Loss :  0.0390136600979
Iteration :  98   Loss :  0.0386664887074
Iteration :  99   Loss :  0.0383233809293
[-0.00059718  0.00036384 -0.00023519 ...,  0.00016899 -0.00010415
  0.00024507]
CROSS VALIDATION 5
Iteration :  0   Loss :  6.73828964839
Iteration :  1   Loss :  0.104535374468
Iteration :  2   Loss :  0.103054164179
Iteration :  3   Loss :  0.101729305682
Iteration :  4   Loss :  0.100495324642
Iteration :  5   Loss :  0.099321813272
Iteration :  6   Loss :  0.0981923535951
Iteration :  7   Loss :  0.0970971601578
Iteration :  8   Loss :  0.0960299714229
Iteration :  9   Loss :  0.094986557586
Iteration :  10   Loss :  0.0939639350366
Iteration :  11   Loss :  0.0929599226953
Iteration :  12   Loss :  0.0919728771162
Iteration :  13   Loss :  0.0910015270177
Iteration :  14   Loss :  0.0900448659685
Iteration :  15   Loss :  0.0891020805492
Iteration :  16   Loss :  0.0881725009342
Iteration :  17   Loss :  0.087255566086
Iteration :  18   Loss :  0.0863507987274
Iteration :  19   Loss :  0.0854577870115
Iteration :  20   Loss :  0.0845761708768
Iteration :  21   Loss :  0.0837056317397
Iteration :  22   Loss :  0.082845884604
Iteration :  23   Loss :  0.0819966719489
Iteration :  24   Loss :  0.0811577589415
Iteration :  25   Loss :  0.0803289296512
Iteration :  26   Loss :  0.0795099840273
Iteration :  27   Loss :  0.0787007354671
Iteration :  28   Loss :  0.0779010088441
Iteration :  29   Loss :  0.0771106388981
Iteration :  30   Loss :  0.076329468912
Iteration :  31   Loss :  0.0755573496191
Iteration :  32   Loss :  0.0747941382951
Iteration :  33   Loss :  0.0740396979999
Iteration :  34   Loss :  0.0732938969429
Iteration :  35   Loss :  0.0725566079477
Iteration :  36   Loss :  0.0718277080008
Iteration :  37   Loss :  0.0711070778689
Iteration :  38   Loss :  0.0703946017724
Iteration :  39   Loss :  0.0696901671083
Iteration :  40   Loss :  0.0689936642118
Iteration :  41   Loss :  0.0683049861517
Iteration :  42   Loss :  0.0676240285546
Iteration :  43   Loss :  0.0669506894517
Iteration :  44   Loss :  0.0662848691464
Iteration :  45   Loss :  0.0656264700985
Iteration :  46   Loss :  0.0649753968219
Iteration :  47   Loss :  0.0643315557954
Iteration :  48   Loss :  0.0636948553827
Iteration :  49   Loss :  0.0630652057607
Iteration :  50   Loss :  0.0624425188561
Iteration :  51   Loss :  0.0618267082872
Iteration :  52   Loss :  0.0612176893112
Iteration :  53   Loss :  0.0606153787762
Iteration :  54   Loss :  0.0600196950772
Iteration :  55   Loss :  0.0594305581154
Iteration :  56   Loss :  0.0588478892609
Iteration :  57   Loss :  0.0582716113179
Iteration :  58   Loss :  0.057701648493
Iteration :  59   Loss :  0.0571379263656
Iteration :  60   Loss :  0.0565803718607
Iteration :  61   Loss :  0.0560289132238
Iteration :  62   Loss :  0.0554834799982
Iteration :  63   Loss :  0.0549440030036
Iteration :  64   Loss :  0.0544104143168
Iteration :  65   Loss :  0.0538826472548
Iteration :  66   Loss :  0.0533606363578
Iteration :  67   Loss :  0.0528443173757
Iteration :  68   Loss :  0.0523336272537
Iteration :  69   Loss :  0.0518285041206
Iteration :  70   Loss :  0.0513288872771
Iteration :  71   Loss :  0.0508347171846
Iteration :  72   Loss :  0.0503459354552
Iteration :  73   Loss :  0.0498624848409
Iteration :  74   Loss :  0.0493843092234
Iteration :  75   Loss :  0.0489113536033
Iteration :  76   Loss :  0.0484435640887
Iteration :  77   Loss :  0.0479808878835
Iteration :  78   Loss :  0.0475232732742
Iteration :  79   Loss :  0.0470706696158
Iteration :  80   Loss :  0.0466230273167
Iteration :  81   Loss :  0.0461802978219
Iteration :  82   Loss :  0.0457424335945
Iteration :  83   Loss :  0.0453093880959
Iteration :  84   Loss :  0.0448811157645
Iteration :  85   Loss :  0.0444575719919
Iteration :  86   Loss :  0.0440387130987
Iteration :  87   Loss :  0.0436244963072
Iteration :  88   Loss :  0.0432148797133
Iteration :  89   Loss :  0.0428098222569
Iteration :  90   Loss :  0.0424092836907
Iteration :  91   Loss :  0.0420132245471
Iteration :  92   Loss :  0.0416216061047
Iteration :  93   Loss :  0.0412343903535
Iteration :  94   Loss :  0.0408515399584
Iteration :  95   Loss :  0.0404730182229
Iteration :  96   Loss :  0.0400987890517
Iteration :  97   Loss :  0.0397288169126
Iteration :  98   Loss :  0.0393630667984
Iteration :  99   Loss :  0.0390015041882
[ -4.24236300e-04   4.45269831e-04  -2.44950678e-04 ...,   6.20062849e-05
  -8.49802715e-05   3.04962144e-04]
CROSS VALIDATION 6
Iteration :  0   Loss :  7.92370855939
Iteration :  1   Loss :  0.115878235059
Iteration :  2   Loss :  0.102131839355
Iteration :  3   Loss :  0.100451918427
Iteration :  4   Loss :  0.0990011605419
Iteration :  5   Loss :  0.0976840325064
Iteration :  6   Loss :  0.0964547347494
Iteration :  7   Loss :  0.0952881121351
Iteration :  8   Loss :  0.0941690214548
Iteration :  9   Loss :  0.0930877164999
Iteration :  10   Loss :  0.0920375941442
Iteration :  11   Loss :  0.0910139935184
Iteration :  12   Loss :  0.0900135118207
Iteration :  13   Loss :  0.0890335930364
Iteration :  14   Loss :  0.0880722696314
Iteration :  15   Loss :  0.0871279942772
Iteration :  16   Loss :  0.0861995267737
Iteration :  17   Loss :  0.0852858560111
Iteration :  18   Loss :  0.0843861448522
Iteration :  19   Loss :  0.0834996904028
Iteration :  20   Loss :  0.0826258948568
Iteration :  21   Loss :  0.0817642437534
Iteration :  22   Loss :  0.0809142895295
Iteration :  23   Loss :  0.0800756389131
Iteration :  24   Loss :  0.0792479431474
Iteration :  25   Loss :  0.0784308903267
Iteration :  26   Loss :  0.0776241993271
Iteration :  27   Loss :  0.0768276149568
Iteration :  28   Loss :  0.0760409040458
Iteration :  29   Loss :  0.0752638522672
Iteration :  30   Loss :  0.0744962615332
Iteration :  31   Loss :  0.0737379478453
Iteration :  32   Loss :  0.0729887395061
Iteration :  33   Loss :  0.0722484756211
Iteration :  34   Loss :  0.0715170048349
Iteration :  35   Loss :  0.0707941842567
Iteration :  36   Loss :  0.0700798785397
Iteration :  37   Loss :  0.0693739590889
Iteration :  38   Loss :  0.0686763033713
Iteration :  39   Loss :  0.0679867943138
Iteration :  40   Loss :  0.0673053197723
Iteration :  41   Loss :  0.0666317720609
Iteration :  42   Loss :  0.0659660475323
Iteration :  43   Loss :  0.0653080462002
Iteration :  44   Loss :  0.0646576713993
Iteration :  45   Loss :  0.0640148294758
Iteration :  46   Loss :  0.0633794295066
Iteration :  47   Loss :  0.0627513830415
Iteration :  48   Loss :  0.0621306038673
Iteration :  49   Loss :  0.0615170077914
Iteration :  50   Loss :  0.0609105124424
Iteration :  51   Loss :  0.0603110370867
Iteration :  52   Loss :  0.0597185024613
Iteration :  53   Loss :  0.0591328306188
Iteration :  54   Loss :  0.0585539447875
Iteration :  55   Loss :  0.0579817692434
Iteration :  56   Loss :  0.0574162291943
Iteration :  57   Loss :  0.0568572506772
Iteration :  58   Loss :  0.0563047604652
Iteration :  59   Loss :  0.055758685988
Iteration :  60   Loss :  0.0552189552613
Iteration :  61   Loss :  0.0546854968284
Iteration :  62   Loss :  0.054158239711
Iteration :  63   Loss :  0.0536371133712
Iteration :  64   Loss :  0.053122047682
Iteration :  65   Loss :  0.0526129729075
Iteration :  66   Loss :  0.0521098196921
Iteration :  67   Loss :  0.0516125190572
Iteration :  68   Loss :  0.0511210024064
Iteration :  69   Loss :  0.050635201537
Iteration :  70   Loss :  0.0501550486582
Iteration :  71   Loss :  0.0496804764148
Iteration :  72   Loss :  0.049211417916
Iteration :  73   Loss :  0.0487478067682
Iteration :  74   Loss :  0.0482895771106
Iteration :  75   Loss :  0.0478366636547
Iteration :  76   Loss :  0.0473890017235
Iteration :  77   Loss :  0.0469465272931
Iteration :  78   Loss :  0.0465091770335
Iteration :  79   Loss :  0.0460768883488
Iteration :  80   Loss :  0.0456495994162
Iteration :  81   Loss :  0.0452272492222
Iteration :  82   Loss :  0.0448097775962
Iteration :  83   Loss :  0.0443971252409
Iteration :  84   Loss :  0.0439892337578
Iteration :  85   Loss :  0.0435860456699
Iteration :  86   Loss :  0.0431875044375
Iteration :  87   Loss :  0.042793554471
Iteration :  88   Loss :  0.0424041411369
Iteration :  89   Loss :  0.0420192107595
Iteration :  90   Loss :  0.0416387106162
Iteration :  91   Loss :  0.0412625889289
Iteration :  92   Loss :  0.0408907948486
Iteration :  93   Loss :  0.0405232784365
Iteration :  94   Loss :  0.0401599906395
Iteration :  95   Loss :  0.0398008832619
Iteration :  96   Loss :  0.0394459089329
Iteration :  97   Loss :  0.0390950210708
Iteration :  98   Loss :  0.038748173844
Iteration :  99   Loss :  0.0384053221299
[-0.00059977  0.00036309 -0.00023452 ...,  0.0001677  -0.00010547
  0.00024453]
CROSS VALIDATION 7
Iteration :  0   Loss :  7.94366221376
Iteration :  1   Loss :  0.116050342328
Iteration :  2   Loss :  0.102085390687
Iteration :  3   Loss :  0.10041445962
Iteration :  4   Loss :  0.0989660465387
Iteration :  5   Loss :  0.0976484389511
Iteration :  6   Loss :  0.0964172337209
Iteration :  7   Loss :  0.0952478907103
Iteration :  8   Loss :  0.094125573671
Iteration :  9   Loss :  0.0930407045014
Iteration :  10   Loss :  0.0919867792453
Iteration :  11   Loss :  0.0909591993936
Iteration :  12   Loss :  0.0899546037064
Iteration :  13   Loss :  0.0889704654452
Iteration :  14   Loss :  0.0880048388332
Iteration :  15   Loss :  0.0870561935671
Iteration :  16   Loss :  0.0861233034276
Iteration :  17   Loss :  0.0852051692918
Iteration :  18   Loss :  0.0843009646849
Iteration :  19   Loss :  0.0834099964845
Iteration :  20   Loss :  0.0825316760474
Iteration :  21   Loss :  0.0816654976531
Iteration :  22   Loss :  0.0808110221749
Iteration :  23   Loss :  0.0799678645499
Iteration :  24   Loss :  0.0791356840474
Iteration :  25   Loss :  0.0783141766293
Iteration :  26   Loss :  0.0775030688904
Iteration :  27   Loss :  0.0767021132077
Iteration :  28   Loss :  0.0759110838226
Iteration :  29   Loss :  0.0751297736497
Iteration :  30   Loss :  0.0743579916578
Iteration :  31   Loss :  0.0735955607027
Iteration :  32   Loss :  0.0728423157221
Iteration :  33   Loss :  0.0720981022199
Iteration :  34   Loss :  0.071362774985
Iteration :  35   Loss :  0.0706361970015
Iteration :  36   Loss :  0.069918238514
Iteration :  37   Loss :  0.0692087762212
Iteration :  38   Loss :  0.0685076925759
Iteration :  39   Loss :  0.0678148751725
Iteration :  40   Loss :  0.0671302162082
Iteration :  41   Loss :  0.0664536120057
Iteration :  42   Loss :  0.0657849625882
Iteration :  43   Loss :  0.065124171299
Iteration :  44   Loss :  0.0644711444586
Iteration :  45   Loss :  0.0638257910549
Iteration :  46   Loss :  0.063188022462
Iteration :  47   Loss :  0.0625577521841
Iteration :  48   Loss :  0.0619348956218
Iteration :  49   Loss :  0.0613193698585
Iteration :  50   Loss :  0.0607110934649
Iteration :  51   Loss :  0.0601099863209
Iteration :  52   Loss :  0.0595159694529
Iteration :  53   Loss :  0.0589289648856
Iteration :  54   Loss :  0.0583488955087
Iteration :  55   Loss :  0.0577756849565
Iteration :  56   Loss :  0.0572092575007
Iteration :  57   Loss :  0.0566495379562
Iteration :  58   Loss :  0.0560964515987
Iteration :  59   Loss :  0.0555499240955
Iteration :  60   Loss :  0.0550098814466
Iteration :  61   Loss :  0.0544762499387
Iteration :  62   Loss :  0.0539489561098
Iteration :  63   Loss :  0.0534279267244
Iteration :  64   Loss :  0.0529130887598
Iteration :  65   Loss :  0.0524043694014
Iteration :  66   Loss :  0.0519016960484
Iteration :  67   Loss :  0.0514049963275
Iteration :  68   Loss :  0.050914198115
Iteration :  69   Loss :  0.0504292295661
Iteration :  70   Loss :  0.0499500191507
Iteration :  71   Loss :  0.0494764956947
Iteration :  72   Loss :  0.0490085884264
Iteration :  73   Loss :  0.0485462270255
Iteration :  74   Loss :  0.0480893416765
Iteration :  75   Loss :  0.0476378631218
Iteration :  76   Loss :  0.0471917227172
Iteration :  77   Loss :  0.0467508524859
Iteration :  78   Loss :  0.046315185172
Iteration :  79   Loss :  0.0458846542913
Iteration :  80   Loss :  0.0454591941795
Iteration :  81   Loss :  0.0450387400367
Iteration :  82   Loss :  0.0446232279669
Iteration :  83   Loss :  0.044212595013
Iteration :  84   Loss :  0.0438067791866
Iteration :  85   Loss :  0.0434057194911
Iteration :  86   Loss :  0.0430093559394
Iteration :  87   Loss :  0.0426176295657
Iteration :  88   Loss :  0.0422304824295
Iteration :  89   Loss :  0.0418478576155
Iteration :  90   Loss :  0.0414696992258
Iteration :  91   Loss :  0.0410959523671
Iteration :  92   Loss :  0.0407265631321
Iteration :  93   Loss :  0.0403614785766
Iteration :  94   Loss :  0.0400006466906
Iteration :  95   Loss :  0.0396440163667
Iteration :  96   Loss :  0.0392915373644
Iteration :  97   Loss :  0.0389431602712
Iteration :  98   Loss :  0.0385988364616
Iteration :  99   Loss :  0.0382585180544
[-0.00060342  0.00036045 -0.00024005 ...,  0.000165   -0.00010767
  0.00024591]
CROSS VALIDATION 8
Iteration :  0   Loss :  7.92671803678
Iteration :  1   Loss :  0.116370758085
Iteration :  2   Loss :  0.102152333706
Iteration :  3   Loss :  0.100485554216
Iteration :  4   Loss :  0.0990406968059
Iteration :  5   Loss :  0.0977264564087
Iteration :  6   Loss :  0.0964985499395
Iteration :  7   Loss :  0.0953324829789
Iteration :  8   Loss :  0.0942134389493
Iteration :  9   Loss :  0.093131848234
Iteration :  10   Loss :  0.0920812098136
Iteration :  11   Loss :  0.0910569250506
Iteration :  12   Loss :  0.0900556307306
Iteration :  13   Loss :  0.0890747969464
Iteration :  14   Loss :  0.0881124739316
Iteration :  15   Loss :  0.0871671267997
Iteration :  16   Loss :  0.0862375243011
Iteration :  17   Loss :  0.0853226619375
Iteration :  18   Loss :  0.0844217075871
Iteration :  19   Loss :  0.0835339622661
Iteration :  20   Loss :  0.082658831304
Iteration :  21   Loss :  0.0817958028277
Iteration :  22   Loss :  0.0809444314702
Iteration :  23   Loss :  0.0801043258747
Iteration :  24   Loss :  0.0792751389962
Iteration :  25   Loss :  0.0784565604924
Iteration :  26   Loss :  0.077648310695
Iteration :  27   Loss :  0.0768501357867
Iteration :  28   Loss :  0.0760618039121
Iteration :  29   Loss :  0.0752831020131
Iteration :  30   Loss :  0.0745138332351
Iteration :  31   Loss :  0.0737538147847
Iteration :  32   Loss :  0.0730028761472
Iteration :  33   Loss :  0.0722608575928
Iteration :  34   Loss :  0.0715276089162
Iteration :  35   Loss :  0.0708029883652
Iteration :  36   Loss :  0.0700868617239
Iteration :  37   Loss :  0.0693791015227
Iteration :  38   Loss :  0.0686795863527
Iteration :  39   Loss :  0.0679882002671
Iteration :  40   Loss :  0.0673048322543
Iteration :  41   Loss :  0.0666293757715
Iteration :  42   Loss :  0.0659617283297
Iteration :  43   Loss :  0.0653017911216
Iteration :  44   Loss :  0.0646494686859
Iteration :  45   Loss :  0.064004668604
Iteration :  46   Loss :  0.0633673012239
Iteration :  47   Loss :  0.0627372794081
Iteration :  48   Loss :  0.0621145183028
Iteration :  49   Loss :  0.0614989351265
Iteration :  50   Loss :  0.060890448975
Iteration :  51   Loss :  0.0602889806431
Iteration :  52   Loss :  0.0596944524599
Iteration :  53   Loss :  0.0591067881381
Iteration :  54   Loss :  0.0585259126362
Iteration :  55   Loss :  0.057951752032
Iteration :  56   Loss :  0.0573842334086
Iteration :  57   Loss :  0.0568232847508
Iteration :  58   Loss :  0.0562688348526
Iteration :  59   Loss :  0.0557208132347
Iteration :  60   Loss :  0.055179150072
Iteration :  61   Loss :  0.0546437761311
Iteration :  62   Loss :  0.054114622717
Iteration :  63   Loss :  0.0535916216283
Iteration :  64   Loss :  0.0530747051216
Iteration :  65   Loss :  0.0525638058836
Iteration :  66   Loss :  0.0520588570115
Iteration :  67   Loss :  0.0515597920001
Iteration :  68   Loss :  0.0510665447359
Iteration :  69   Loss :  0.0505790494976
Iteration :  70   Loss :  0.0500972409621
Iteration :  71   Loss :  0.0496210542156
Iteration :  72   Loss :  0.0491504247689
Iteration :  73   Loss :  0.0486852885773
Iteration :  74   Loss :  0.0482255820626
Iteration :  75   Loss :  0.0477712421387
Iteration :  76   Loss :  0.0473222062379
Iteration :  77   Loss :  0.0468784123397
Iteration :  78   Loss :  0.0464397989994
Iteration :  79   Loss :  0.0460063053766
Iteration :  80   Loss :  0.0455778712639
Iteration :  81   Loss :  0.0451544371136
Iteration :  82   Loss :  0.0447359440636
Iteration :  83   Loss :  0.0443223339607
Iteration :  84   Loss :  0.0439135493816
Iteration :  85   Loss :  0.0435095336517
Iteration :  86   Loss :  0.0431102308595
Iteration :  87   Loss :  0.0427155858691
Iteration :  88   Loss :  0.0423255443284
Iteration :  89   Loss :  0.0419400526735
Iteration :  90   Loss :  0.0415590581303
Iteration :  91   Loss :  0.0411825087112
Iteration :  92   Loss :  0.040810353209
Iteration :  93   Loss :  0.0404425411863
Iteration :  94   Loss :  0.0400790229625
Iteration :  95   Loss :  0.039719749596
Iteration :  96   Loss :  0.0393646728646
Iteration :  97   Loss :  0.0390137452416
Iteration :  98   Loss :  0.0386669198704
Iteration :  99   Loss :  0.0383241505356
[-0.00059686  0.00036537 -0.0002274  ...,  0.00016953 -0.00010993
  0.00024425]
CROSS VALIDATION 9
Iteration :  0   Loss :  7.95139502861
Iteration :  1   Loss :  0.134567320311
Iteration :  2   Loss :  0.118830644697
Iteration :  3   Loss :  0.116938667813
Iteration :  4   Loss :  0.115300866661
Iteration :  5   Loss :  0.113805265955
Iteration :  6   Loss :  0.112401355269
Iteration :  7   Loss :  0.111062348527
Iteration :  8   Loss :  0.109772486346
Iteration :  9   Loss :  0.108521787661
Iteration :  10   Loss :  0.107303568823
Iteration :  11   Loss :  0.106113152649
Iteration :  12   Loss :  0.104947145951
Iteration :  13   Loss :  0.10380301135
Iteration :  14   Loss :  0.102678801419
Iteration :  15   Loss :  0.101572987144
Iteration :  16   Loss :  0.100484343563
Iteration :  17   Loss :  0.0994118713936
Iteration :  18   Loss :  0.0983547419961
Iteration :  19   Loss :  0.0973122579166
Iteration :  20   Loss :  0.0962838240562
Iteration :  21   Loss :  0.0952689262597
Iteration :  22   Loss :  0.0942671151753
Iteration :  23   Loss :  0.0932779939242
Iteration :  24   Loss :  0.0923012085663
Iteration :  25   Loss :  0.0913364406466
Iteration :  26   Loss :  0.0903834013079
Iteration :  27   Loss :  0.0894418265977
Iteration :  28   Loss :  0.0885114736943
Iteration :  29   Loss :  0.0875921178471
Iteration :  30   Loss :  0.0866835498764
Iteration :  31   Loss :  0.0857855741163
Iteration :  32   Loss :  0.0848980067087
Iteration :  33   Loss :  0.0840206741805
Iteration :  34   Loss :  0.0831534122476
Iteration :  35   Loss :  0.0822960648032
Iteration :  36   Loss :  0.0814484830571
Iteration :  37   Loss :  0.0806105247979
Iteration :  38   Loss :  0.0797820537565
Iteration :  39   Loss :  0.0789629390541
Iteration :  40   Loss :  0.0781530547196
Iteration :  41   Loss :  0.0773522792654
Iteration :  42   Loss :  0.0765604953126
Iteration :  43   Loss :  0.075777589257
Iteration :  44   Loss :  0.0750034509706
Iteration :  45   Loss :  0.0742379735331
Iteration :  46   Loss :  0.0734810529888
Iteration :  47   Loss :  0.0727325881267
Iteration :  48   Loss :  0.071992480279
Iteration :  49   Loss :  0.0712606331381
Iteration :  50   Loss :  0.0705369525881
Iteration :  51   Loss :  0.0698213465498
Iteration :  52   Loss :  0.0691137248394
Iteration :  53   Loss :  0.0684139990368
Iteration :  54   Loss :  0.0677220823659
Iteration :  55   Loss :  0.0670378895837
Iteration :  56   Loss :  0.0663613368789
Iteration :  57   Loss :  0.0656923417782
Iteration :  58   Loss :  0.065030823062
Iteration :  59   Loss :  0.0643767006859
Iteration :  60   Loss :  0.0637298957107
Iteration :  61   Loss :  0.0630903302381
Iteration :  62   Loss :  0.0624579273531
Iteration :  63   Loss :  0.0618326110728
Iteration :  64   Loss :  0.0612143062991
Iteration :  65   Loss :  0.0606029387783
Iteration :  66   Loss :  0.0599984350641
Iteration :  67   Loss :  0.0594007224855
Iteration :  68   Loss :  0.0588097291182
Iteration :  69   Loss :  0.0582253837591
Iteration :  70   Loss :  0.057647615905
Iteration :  71   Loss :  0.0570763557322
Iteration :  72   Loss :  0.0565115340796
Iteration :  73   Loss :  0.0559530824331
Iteration :  74   Loss :  0.0554009329111
Iteration :  75   Loss :  0.0548550182518
Iteration :  76   Loss :  0.0543152718011
Iteration :  77   Loss :  0.0537816275009
Iteration :  78   Loss :  0.053254019878
Iteration :  79   Loss :  0.052732384033
Iteration :  80   Loss :  0.05221665563
Iteration :  81   Loss :  0.0517067708855
Iteration :  82   Loss :  0.0512026665578
Iteration :  83   Loss :  0.0507042799371
Iteration :  84   Loss :  0.0502115488342
Iteration :  85   Loss :  0.0497244115715
Iteration :  86   Loss :  0.0492428069727
Iteration :  87   Loss :  0.0487666743541
Iteration :  88   Loss :  0.0482959535164
Iteration :  89   Loss :  0.0478305847374
Iteration :  90   Loss :  0.047370508766
Iteration :  91   Loss :  0.0469156668174
Iteration :  92   Loss :  0.0464660005701
Iteration :  93   Loss :  0.0460214521644
Iteration :  94   Loss :  0.0455819642027
Iteration :  95   Loss :  0.0451474797517
Iteration :  96   Loss :  0.0447179423467
Iteration :  97   Loss :  0.0442932959979
Iteration :  98   Loss :  0.0438734851984
Iteration :  99   Loss :  0.0434584549346
[ -5.44379060e-04   4.12382108e-04  -1.62528274e-04 ...,   3.84875780e-04
  -8.09979748e-05   2.44863486e-04]
CROSS VALIDATION 10
Iteration :  0   Loss :  7.32619818747
Iteration :  1   Loss :  0.130718896519
Iteration :  2   Loss :  0.128947672144
Iteration :  3   Loss :  0.127340813644
Iteration :  4   Loss :  0.125829532787
Iteration :  5   Loss :  0.124382283901
Iteration :  6   Loss :  0.122982175966
Iteration :  7   Loss :  0.121619198537
Iteration :  8   Loss :  0.120286969823
Iteration :  9   Loss :  0.118981187826
Iteration :  10   Loss :  0.117698820106
Iteration :  11   Loss :  0.116437648575
Iteration :  12   Loss :  0.115195998924
Iteration :  13   Loss :  0.113972572254
Iteration :  14   Loss :  0.112766336244
Iteration :  15   Loss :  0.111576452516
Iteration :  16   Loss :  0.110402226793
Iteration :  17   Loss :  0.109243073886
Iteration :  18   Loss :  0.108098492562
Iteration :  19   Loss :  0.10696804719
Iteration :  20   Loss :  0.105851354092
Iteration :  21   Loss :  0.104748071261
Iteration :  22   Loss :  0.1036578905
Iteration :  23   Loss :  0.102580531349
Iteration :  24   Loss :  0.101515736331
Iteration :  25   Loss :  0.100463267206
Iteration :  26   Loss :  0.0994229019818
Iteration :  27   Loss :  0.098394432511
Iteration :  28   Loss :  0.0973776625507
Iteration :  29   Loss :  0.0963724061751
Iteration :  30   Loss :  0.095378486474
Iteration :  31   Loss :  0.0943957344767
Iteration :  32   Loss :  0.0934239882577
Iteration :  33   Loss :  0.0924630921879
Iteration :  34   Loss :  0.0915128963052
Iteration :  35   Loss :  0.090573255782
Iteration :  36   Loss :  0.0896440304714
Iteration :  37   Loss :  0.0887250845191
Iteration :  38   Loss :  0.087816286029
Iteration :  39   Loss :  0.0869175067728
Iteration :  40   Loss :  0.0860286219369
Iteration :  41   Loss :  0.0851495098992
Iteration :  42   Loss :  0.0842800520323
Iteration :  43   Loss :  0.0834201325263
Iteration :  44   Loss :  0.0825696382304
Iteration :  45   Loss :  0.081728458509
Iteration :  46   Loss :  0.080896485109
Iteration :  47   Loss :  0.0800736120393
Iteration :  48   Loss :  0.079259735457
Iteration :  49   Loss :  0.0784547535625
Iteration :  50   Loss :  0.0776585665
Iteration :  51   Loss :  0.0768710762644
Iteration :  52   Loss :  0.0760921866117
Iteration :  53   Loss :  0.0753218029756
Iteration :  54   Loss :  0.0745598323859
Iteration :  55   Loss :  0.0738061833928
Iteration :  56   Loss :  0.0730607659926
Iteration :  57   Loss :  0.0723234915582
Iteration :  58   Loss :  0.0715942727721
Iteration :  59   Loss :  0.0708730235631
Iteration :  60   Loss :  0.0701596590454
Iteration :  61   Loss :  0.0694540954614
Iteration :  62   Loss :  0.0687562501276
Iteration :  63   Loss :  0.0680660413836
Iteration :  64   Loss :  0.0673833885439
Iteration :  65   Loss :  0.066708211853
Iteration :  66   Loss :  0.066040432444
Iteration :  67   Loss :  0.0653799722992
Iteration :  68   Loss :  0.064726754215
Iteration :  69   Loss :  0.0640807017684
Iteration :  70   Loss :  0.0634417392873
Iteration :  71   Loss :  0.0628097918235
Iteration :  72   Loss :  0.0621847851274
Iteration :  73   Loss :  0.0615666456268
Iteration :  74   Loss :  0.0609553004068
Iteration :  75   Loss :  0.0603506771929
Iteration :  76   Loss :  0.059752704336
Iteration :  77   Loss :  0.0591613107997
Iteration :  78   Loss :  0.0585764261495
Iteration :  79   Loss :  0.0579979805438
Iteration :  80   Loss :  0.0574259047272
Iteration :  81   Loss :  0.0568601300248
Iteration :  82   Loss :  0.0563005883384
Iteration :  83   Loss :  0.0557472121445
Iteration :  84   Loss :  0.055199934493
Iteration :  85   Loss :  0.0546586890072
Iteration :  86   Loss :  0.0541234098857
Iteration :  87   Loss :  0.053594031904
Iteration :  88   Loss :  0.0530704904182
Iteration :  89   Loss :  0.0525527213687
Iteration :  90   Loss :  0.0520406612844
Iteration :  91   Loss :  0.0515342472883
Iteration :  92   Loss :  0.051033417102
Iteration :  93   Loss :  0.0505381090521
Iteration :  94   Loss :  0.0500482620751
Iteration :  95   Loss :  0.0495638157232
Iteration :  96   Loss :  0.04908471017
Iteration :  97   Loss :  0.0486108862151
Iteration :  98   Loss :  0.0481422852891
Iteration :  99   Loss :  0.0476788494577
[ -5.61320168e-04   3.50289849e-04  -2.82268280e-04 ...,   3.62370741e-04
  -4.72706973e-05   2.59850289e-04]
CROSS VALIDATION 11
Iteration :  0   Loss :  7.9260147603
Iteration :  1   Loss :  0.115917868286
Iteration :  2   Loss :  0.102165971346
Iteration :  3   Loss :  0.100486928252
Iteration :  4   Loss :  0.0990363497508
Iteration :  5   Loss :  0.0977190754509
Iteration :  6   Loss :  0.0964894522865
Iteration :  7   Loss :  0.0953223935549
Iteration :  8   Loss :  0.0942027918339
Iteration :  9   Loss :  0.093120921135
Iteration :  10   Loss :  0.0920701903759
Iteration :  11   Loss :  0.0910459461032
Iteration :  12   Loss :  0.0900447901489
Iteration :  13   Loss :  0.0890641693731
Iteration :  14   Loss :  0.0881021179555
Iteration :  15   Loss :  0.0871570894856
Iteration :  16   Loss :  0.0862278441168
Iteration :  17   Loss :  0.0853133706802
Iteration :  18   Loss :  0.0844128316685
Iteration :  19   Loss :  0.0835255235776
Iteration :  20   Loss :  0.0826508478013
Iteration :  21   Loss :  0.0817882889262
Iteration :  22   Loss :  0.0809373983101
Iteration :  23   Loss :  0.0800977814979
Iteration :  24   Loss :  0.0792690884615
Iteration :  25   Loss :  0.0784510059499
Iteration :  26   Loss :  0.0776432514319
Iteration :  27   Loss :  0.0768455682569
Iteration :  28   Loss :  0.0760577217541
Iteration :  29   Loss :  0.0752794960634
Iteration :  30   Loss :  0.0745106915402
Iteration :  31   Loss :  0.073751122615
Iteration :  32   Loss :  0.0730006160144
Iteration :  33   Loss :  0.0722590092726
Iteration :  34   Loss :  0.0715261494779
Iteration :  35   Loss :  0.070801892208
Iteration :  36   Loss :  0.0700861006218
Iteration :  37   Loss :  0.0693786446778
Iteration :  38   Loss :  0.0686794004569
Iteration :  39   Loss :  0.0679882495733
Iteration :  40   Loss :  0.0673050786568
Iteration :  41   Loss :  0.0666297788959
Iteration :  42   Loss :  0.0659622456328
Iteration :  43   Loss :  0.0653023780003
Iteration :  44   Loss :  0.0646500785969
Iteration :  45   Loss :  0.064005253193
Iteration :  46   Loss :  0.0633678104652
Iteration :  47   Loss :  0.0627376617539
Iteration :  48   Loss :  0.0621147208426
Iteration :  49   Loss :  0.0614989037566
Iteration :  50   Loss :  0.0608901285773
Iteration :  51   Loss :  0.0602883152735
Iteration :  52   Loss :  0.0596933855452
Iteration :  53   Loss :  0.0591052626817
Iteration :  54   Loss :  0.0585238714306
Iteration :  55   Loss :  0.0579491378788
Iteration :  56   Loss :  0.0573809893434
Iteration :  57   Loss :  0.056819354273
Iteration :  58   Loss :  0.0562641621579
Iteration :  59   Loss :  0.0557153434494
Iteration :  60   Loss :  0.0551728294875
Iteration :  61   Loss :  0.0546365524361
Iteration :  62   Loss :  0.0541064452262
Iteration :  63   Loss :  0.0535824415058
Iteration :  64   Loss :  0.0530644755966
Iteration :  65   Loss :  0.0525524824567
Iteration :  66   Loss :  0.0520463976492
Iteration :  67   Loss :  0.0515461573168
Iteration :  68   Loss :  0.0510516981597
Iteration :  69   Loss :  0.0505629574203
Iteration :  70   Loss :  0.05007987287
Iteration :  71   Loss :  0.049602382801
Iteration :  72   Loss :  0.049130426021
Iteration :  73   Loss :  0.0486639418507
Iteration :  74   Loss :  0.0482028701238
Iteration :  75   Loss :  0.0477471511892
Iteration :  76   Loss :  0.0472967259153
Iteration :  77   Loss :  0.0468515356946
Iteration :  78   Loss :  0.0464115224502
Iteration :  79   Loss :  0.0459766286425
Iteration :  80   Loss :  0.0455467972763
Iteration :  81   Loss :  0.0451219719075
Iteration :  82   Loss :  0.0447020966497
Iteration :  83   Loss :  0.0442871161802
Iteration :  84   Loss :  0.0438769757447
Iteration :  85   Loss :  0.0434716211612
Iteration :  86   Loss :  0.0430709988219
Iteration :  87   Loss :  0.0426750556948
Iteration :  88   Loss :  0.0422837393218
Iteration :  89   Loss :  0.0418969978163
Iteration :  90   Loss :  0.0415147798586
Iteration :  91   Loss :  0.0411370346889
Iteration :  92   Loss :  0.0407637120986
Iteration :  93   Loss :  0.0403947624198
Iteration :  94   Loss :  0.0400301365129
Iteration :  95   Loss :  0.0396697857516
Iteration :  96   Loss :  0.0393136620077
Iteration :  97   Loss :  0.0389617176329
Iteration :  98   Loss :  0.0386139054404
Iteration :  99   Loss :  0.0382701786846
[-0.00059818  0.00036479 -0.0002311  ...,  0.00016839 -0.00010535
  0.00024438]
CROSS VALIDATION 12
Iteration :  0   Loss :  10.065049574
Iteration :  1   Loss :  0.0976893967731
Iteration :  2   Loss :  0.0916228480773
Iteration :  3   Loss :  0.0898329280758
Iteration :  4   Loss :  0.088445504203
Iteration :  5   Loss :  0.0872342926093
Iteration :  6   Loss :  0.0861232972927
Iteration :  7   Loss :  0.0850780073952
Iteration :  8   Loss :  0.0840799646581
Iteration :  9   Loss :  0.0831181986405
Iteration :  10   Loss :  0.082185684614
Iteration :  11   Loss :  0.0812776654173
Iteration :  12   Loss :  0.0803907747913
Iteration :  13   Loss :  0.0795225442123
Iteration :  14   Loss :  0.0786711088319
Iteration :  15   Loss :  0.0778350236896
Iteration :  16   Loss :  0.0770131442933
Iteration :  17   Loss :  0.0762045464448
Iteration :  18   Loss :  0.0754084708913
Iteration :  19   Loss :  0.0746242841827
Iteration :  20   Loss :  0.0738514503996
Iteration :  21   Loss :  0.0730895103488
Iteration :  22   Loss :  0.0723380659985
Iteration :  23   Loss :  0.0715967686567
Iteration :  24   Loss :  0.0708653098706
Iteration :  25   Loss :  0.0701434143329
Iteration :  26   Loss :  0.0694308342873
Iteration :  27   Loss :  0.0687273450708
Iteration :  28   Loss :  0.0680327415237
Iteration :  29   Loss :  0.0673468350719
Iteration :  30   Loss :  0.066669451334
Iteration :  31   Loss :  0.0660004281415
Iteration :  32   Loss :  0.0653396138875
Iteration :  33   Loss :  0.0646868661385
Iteration :  34   Loss :  0.0640420504585
Iteration :  35   Loss :  0.0634050394061
Iteration :  36   Loss :  0.0627757116737
Iteration :  37   Loss :  0.0621539513439
Iteration :  38   Loss :  0.0615396472432
Iteration :  39   Loss :  0.0609326923788
Iteration :  40   Loss :  0.0603329834447
Iteration :  41   Loss :  0.0597404203872
Iteration :  42   Loss :  0.0591549060219
Iteration :  43   Loss :  0.0585763456961
Iteration :  44   Loss :  0.0580046469898
Iteration :  45   Loss :  0.0574397194517
Iteration :  46   Loss :  0.0568814743668
Iteration :  47   Loss :  0.0563298245512
Iteration :  48   Loss :  0.0557846841726
Iteration :  49   Loss :  0.0552459685944
Iteration :  50   Loss :  0.0547135942399
Iteration :  51   Loss :  0.0541874784764
Iteration :  52   Loss :  0.0536675395176
Iteration :  53   Loss :  0.0531536963415
Iteration :  54   Loss :  0.0526458686242
Iteration :  55   Loss :  0.0521439766874
Iteration :  56   Loss :  0.0516479414588
Iteration :  57   Loss :  0.0511576844443
Iteration :  58   Loss :  0.0506731277107
Iteration :  59   Loss :  0.0501941938784
Iteration :  60   Loss :  0.0497208061229
Iteration :  61   Loss :  0.049252888184
Iteration :  62   Loss :  0.0487903643816
Iteration :  63   Loss :  0.0483331596388
Iteration :  64   Loss :  0.0478811995082
Iteration :  65   Loss :  0.0474344102046
Iteration :  66   Loss :  0.0469927186404
Iteration :  67   Loss :  0.0465560524636
Iteration :  68   Loss :  0.0461243400986
Iteration :  69   Loss :  0.0456975107886
Iteration :  70   Loss :  0.0452754946386
Iteration :  71   Loss :  0.0448582226584
Iteration :  72   Loss :  0.0444456268068
Iteration :  73   Loss :  0.0440376400334
Iteration :  74   Loss :  0.0436341963208
Iteration :  75   Loss :  0.0432352307241
Iteration :  76   Loss :  0.0428406794093
Iteration :  77   Loss :  0.0424504796893
Iteration :  78   Loss :  0.0420645700566
Iteration :  79   Loss :  0.0416828902146
Iteration :  80   Loss :  0.041305381104
Iteration :  81   Loss :  0.0409319849276
Iteration :  82   Loss :  0.0405626451703
Iteration :  83   Loss :  0.040197306616
Iteration :  84   Loss :  0.0398359153608
Iteration :  85   Loss :  0.0394784188215
Iteration :  86   Loss :  0.0391247657413
Iteration :  87   Loss :  0.0387749061902
Iteration :  88   Loss :  0.0384287915621
Iteration :  89   Loss :  0.0380863745678
Iteration :  90   Loss :  0.0377476092232
Iteration :  91   Loss :  0.0374124508347
Iteration :  92   Loss :  0.0370808559799
Iteration :  93   Loss :  0.0367527824851
Iteration :  94   Loss :  0.0364281893988
Iteration :  95   Loss :  0.0361070369625
Iteration :  96   Loss :  0.0357892865781
Iteration :  97   Loss :  0.0354749007724
Iteration :  98   Loss :  0.0351638431591
Iteration :  99   Loss :  0.034856078399
[ -1.13558532e-03   5.01870882e-06  -3.55412122e-04 ...,   6.26202521e-05
  -1.19746522e-04   1.39713389e-04]
CROSS VALIDATION 13
Iteration :  0   Loss :  8.02677875454
Iteration :  1   Loss :  0.129228198012
Iteration :  2   Loss :  0.124147812897
Iteration :  3   Loss :  0.121662616568
Iteration :  4   Loss :  0.119757475743
Iteration :  5   Loss :  0.118105696014
Iteration :  6   Loss :  0.116594904914
Iteration :  7   Loss :  0.115174425229
Iteration :  8   Loss :  0.113817580295
Iteration :  9   Loss :  0.112508809009
Iteration :  10   Loss :  0.111238335996
Iteration :  11   Loss :  0.109999663175
Iteration :  12   Loss :  0.108788272418
Iteration :  13   Loss :  0.107600904209
Iteration :  14   Loss :  0.106435132824
Iteration :  15   Loss :  0.105289104134
Iteration :  16   Loss :  0.104161367339
Iteration :  17   Loss :  0.103050763359
Iteration :  18   Loss :  0.101956348675
Iteration :  19   Loss :  0.100877342075
Iteration :  20   Loss :  0.0998130865949
Iteration :  21   Loss :  0.0987630218041
Iteration :  22   Loss :  0.0977266632747
Iteration :  23   Loss :  0.0967035871385
Iteration :  24   Loss :  0.0956934183122
Iteration :  25   Loss :  0.0946958214044
Iteration :  26   Loss :  0.0937104936153
Iteration :  27   Loss :  0.0927371591333
Iteration :  28   Loss :  0.0917755646708
Iteration :  29   Loss :  0.0908254758772
Iteration :  30   Loss :  0.0898866744316
Iteration :  31   Loss :  0.0889589556716
Iteration :  32   Loss :  0.0880421266435
Iteration :  33   Loss :  0.0871360044915
Iteration :  34   Loss :  0.0862404151178
Iteration :  35   Loss :  0.0853551920641
Iteration :  36   Loss :  0.0844801755716
Iteration :  37   Loss :  0.0836152117907
Iteration :  38   Loss :  0.0827601521118
Iteration :  39   Loss :  0.0819148525991
Iteration :  40   Loss :  0.0810791735094
Iteration :  41   Loss :  0.0802529788829
Iteration :  42   Loss :  0.079436136195
Iteration :  43   Loss :  0.0786285160593
Iteration :  44   Loss :  0.0778299919758
Iteration :  45   Loss :  0.0770404401157
Iteration :  46   Loss :  0.0762597391389
Iteration :  47   Loss :  0.0754877700397
Iteration :  48   Loss :  0.0747244160156
Iteration :  49   Loss :  0.0739695623571
Iteration :  50   Loss :  0.0732230963549
Iteration :  51   Loss :  0.0724849072218
Iteration :  52   Loss :  0.0717548860278
Iteration :  53   Loss :  0.0710329256459
Iteration :  54   Loss :  0.0703189207065
Iteration :  55   Loss :  0.0696127675602
Iteration :  56   Loss :  0.068914364246
Iteration :  57   Loss :  0.0682236104654
Iteration :  58   Loss :  0.0675404075598
Iteration :  59   Loss :  0.0668646584914
Iteration :  60   Loss :  0.0661962678267
Iteration :  61   Loss :  0.0655351417212
Iteration :  62   Loss :  0.0648811879057
Iteration :  63   Loss :  0.0642343156731
Iteration :  64   Loss :  0.0635944358659
Iteration :  65   Loss :  0.0629614608625
Iteration :  66   Loss :  0.0623353045651
Iteration :  67   Loss :  0.0617158823853
Iteration :  68   Loss :  0.0611031112306
Iteration :  69   Loss :  0.0604969094891
Iteration :  70   Loss :  0.0598971970144
Iteration :  71   Loss :  0.0593038951088
Iteration :  72   Loss :  0.0587169265069
Iteration :  73   Loss :  0.0581362153574
Iteration :  74   Loss :  0.0575616872048
Iteration :  75   Loss :  0.0569932689704
Iteration :  76   Loss :  0.0564308889326
Iteration :  77   Loss :  0.055874476707
Iteration :  78   Loss :  0.0553239632255
Iteration :  79   Loss :  0.0547792807156
Iteration :  80   Loss :  0.0542403626791
Iteration :  81   Loss :  0.0537071438704
Iteration :  82   Loss :  0.0531795602752
Iteration :  83   Loss :  0.0526575490881
Iteration :  84   Loss :  0.0521410486912
Iteration :  85   Loss :  0.0516299986315
Iteration :  86   Loss :  0.0511243395994
Iteration :  87   Loss :  0.0506240134064
Iteration :  88   Loss :  0.0501289629631
Iteration :  89   Loss :  0.0496391322574
Iteration :  90   Loss :  0.0491544663328
Iteration :  91   Loss :  0.048674911267
Iteration :  92   Loss :  0.04820041415
Iteration :  93   Loss :  0.0477309230634
Iteration :  94   Loss :  0.0472663870592
Iteration :  95   Loss :  0.0468067561396
Iteration :  96   Loss :  0.046351981236
Iteration :  97   Loss :  0.0459020141897
Iteration :  98   Loss :  0.0454568077319
Iteration :  99   Loss :  0.0450163154648
[ -8.81004846e-04  -1.26337240e-04  -4.27416689e-04 ...,   6.83709955e-05
   4.12804159e-05   2.23164371e-04]
CROSS VALIDATION 14
Iteration :  0   Loss :  7.9070370269
Iteration :  1   Loss :  0.114444088923
Iteration :  2   Loss :  0.10067068934
Iteration :  3   Loss :  0.0992571468025
Iteration :  4   Loss :  0.0979804855243
Iteration :  5   Loss :  0.096786442911
Iteration :  6   Loss :  0.095648848076
Iteration :  7   Loss :  0.0945531758114
Iteration :  8   Loss :  0.09349057252
Iteration :  9   Loss :  0.0924552632675
Iteration :  10   Loss :  0.0914432809478
Iteration :  11   Loss :  0.0904517854765
Iteration :  12   Loss :  0.089478673315
Iteration :  13   Loss :  0.0885223410572
Iteration :  14   Loss :  0.0875815358503
Iteration :  15   Loss :  0.0866552572301
Iteration :  16   Loss :  0.0857426906803
Iteration :  17   Loss :  0.0848431614599
Iteration :  18   Loss :  0.0839561017736
Iteration :  19   Loss :  0.0830810269594
Iteration :  20   Loss :  0.0822175179095
Iteration :  21   Loss :  0.0813652078873
Iteration :  22   Loss :  0.0805237725036
Iteration :  23   Loss :  0.0796929219952
Iteration :  24   Loss :  0.0788723952114
Iteration :  25   Loss :  0.0780619548783
Iteration :  26   Loss :  0.0772613838344
Iteration :  27   Loss :  0.0764704820102
Iteration :  28   Loss :  0.0756890639843
Iteration :  29   Loss :  0.0749169569878
Iteration :  30   Loss :  0.0741539992642
Iteration :  31   Loss :  0.0734000387079
Iteration :  32   Loss :  0.0726549317271
Iteration :  33   Loss :  0.0719185422851
Iteration :  34   Loss :  0.071190741086
Iteration :  35   Loss :  0.0704714048776
Iteration :  36   Loss :  0.0697604158488
Iteration :  37   Loss :  0.0690576611052
Iteration :  38   Loss :  0.0683630322078
Iteration :  39   Loss :  0.0676764247652
Iteration :  40   Loss :  0.0669977380688
Iteration :  41   Loss :  0.066326874765
Iteration :  42   Loss :  0.0656637405577
Iteration :  43   Loss :  0.0650082439374
Iteration :  44   Loss :  0.0643602959321
Iteration :  45   Loss :  0.0637198098784
Iteration :  46   Loss :  0.0630867012092
Iteration :  47   Loss :  0.0624608872569
Iteration :  48   Loss :  0.0618422870705
Iteration :  49   Loss :  0.0612308212457
Iteration :  50   Loss :  0.0606264117672
Iteration :  51   Loss :  0.0600289818615
Iteration :  52   Loss :  0.0594384558625
Iteration :  53   Loss :  0.0588547590858
Iteration :  54   Loss :  0.0582778177152
Iteration :  55   Loss :  0.0577075586987
Iteration :  56   Loss :  0.0571439096553
Iteration :  57   Loss :  0.0565867987915
Iteration :  58   Loss :  0.0560361548289
Iteration :  59   Loss :  0.0554919069408
Iteration :  60   Loss :  0.0549539846999
Iteration :  61   Loss :  0.0544223180348
Iteration :  62   Loss :  0.0538968371972
Iteration :  63   Loss :  0.0533774727377
Iteration :  64   Loss :  0.0528641554909
Iteration :  65   Loss :  0.0523568165696
Iteration :  66   Loss :  0.0518553873664
Iteration :  67   Loss :  0.051359799564
Iteration :  68   Loss :  0.0508699851516
Iteration :  69   Loss :  0.0503858764482
Iteration :  70   Loss :  0.0499074061321
Iteration :  71   Loss :  0.0494345072742
Iteration :  72   Loss :  0.0489671133765
Iteration :  73   Loss :  0.0485051584132
Iteration :  74   Loss :  0.0480485768743
Iteration :  75   Loss :  0.0475973038111
Iteration :  76   Loss :  0.0471512748819
Iteration :  77   Loss :  0.0467104263975
Iteration :  78   Loss :  0.0462746953662
Iteration :  79   Loss :  0.0458440195362
Iteration :  80   Loss :  0.045418337436
Iteration :  81   Loss :  0.044997588411
Iteration :  82   Loss :  0.0445817126574
Iteration :  83   Loss :  0.0441706512501
Iteration :  84   Loss :  0.0437643461672
Iteration :  85   Loss :  0.0433627403088
Iteration :  86   Loss :  0.04296577751
Iteration :  87   Loss :  0.0425734025496
Iteration :  88   Loss :  0.0421855611517
Iteration :  89   Loss :  0.0418021999828
Iteration :  90   Loss :  0.0414232666427
Iteration :  91   Loss :  0.0410487096508
Iteration :  92   Loss :  0.0406784784265
Iteration :  93   Loss :  0.0403125232658
Iteration :  94   Loss :  0.0399507953128
Iteration :  95   Loss :  0.0395932465275
Iteration :  96   Loss :  0.0392398296504
Iteration :  97   Loss :  0.038890498164
Iteration :  98   Loss :  0.0385452062516
Iteration :  99   Loss :  0.0382039087546
[-0.00057548  0.00037557 -0.00022409 ...,  0.0001524  -0.00012087
  0.00024138]
CROSS VALIDATION 15
Iteration :  0   Loss :  7.9555203543
Iteration :  1   Loss :  0.116311768609
Iteration :  2   Loss :  0.10216166835
Iteration :  3   Loss :  0.100501446277
Iteration :  4   Loss :  0.0990598942438
Iteration :  5   Loss :  0.0977474637215
Iteration :  6   Loss :  0.0965205709003
Iteration :  7   Loss :  0.0953550508868
Iteration :  8   Loss :  0.094236261456
Iteration :  9   Loss :  0.0931547333214
Iteration :  10   Loss :  0.0921040269291
Iteration :  11   Loss :  0.0910795831116
Iteration :  12   Loss :  0.0900780649004
Iteration :  13   Loss :  0.0890969602747
Iteration :  14   Loss :  0.0881343318428
Iteration :  15   Loss :  0.0871886533248
Iteration :  16   Loss :  0.0862586994209
Iteration :  17   Loss :  0.0853434696578
Iteration :  18   Loss :  0.0844421345126
Iteration :  19   Loss :  0.083553996524
Iteration :  20   Loss :  0.0826784617195
Iteration :  21   Loss :  0.0818150182857
Iteration :  22   Loss :  0.0809632204198
Iteration :  23   Loss :  0.0801226759441
Iteration :  24   Loss :  0.0792930366972
Iteration :  25   Loss :  0.0784739909979
Iteration :  26   Loss :  0.0776652576782
Iteration :  27   Loss :  0.0768665813141
Iteration :  28   Loss :  0.0760777283817
Iteration :  29   Loss :  0.0752984841346
Iteration :  30   Loss :  0.074528650047
Iteration :  31   Loss :  0.0737680417046
Iteration :  32   Loss :  0.0730164870529
Iteration :  33   Loss :  0.0722738249308
Iteration :  34   Loss :  0.0715399038352
Iteration :  35   Loss :  0.0708145808732
Iteration :  36   Loss :  0.0700977208648
Iteration :  37   Loss :  0.0693891955721
Iteration :  38   Loss :  0.0686888830294
Iteration :  39   Loss :  0.0679966669584
Iteration :  40   Loss :  0.0673124362529
Iteration :  41   Loss :  0.0666360845223
Iteration :  42   Loss :  0.065967509683
Iteration :  43   Loss :  0.0653066135914
Iteration :  44   Loss :  0.0646533017106
Iteration :  45   Loss :  0.0640074828075
Iteration :  46   Loss :  0.0633690686736
Iteration :  47   Loss :  0.0627379738693
Iteration :  48   Loss :  0.0621141154851
Iteration :  49   Loss :  0.0614974129213
Iteration :  50   Loss :  0.0608877876812
Iteration :  51   Loss :  0.0602851631784
Iteration :  52   Loss :  0.0596894645561
Iteration :  53   Loss :  0.0591006185182
Iteration :  54   Loss :  0.0585185531707
Iteration :  55   Loss :  0.0579431978731
Iteration :  56   Loss :  0.057374483101
Iteration :  57   Loss :  0.0568123403164
Iteration :  58   Loss :  0.0562567018494
Iteration :  59   Loss :  0.0557075007879
Iteration :  60   Loss :  0.0551646708776
Iteration :  61   Loss :  0.0546281464306
Iteration :  62   Loss :  0.0540978622443
Iteration :  63   Loss :  0.0535737535285
Iteration :  64   Loss :  0.0530557558431
Iteration :  65   Loss :  0.0525438050442
Iteration :  66   Loss :  0.0520378372395
Iteration :  67   Loss :  0.0515377887534
Iteration :  68   Loss :  0.0510435961002
Iteration :  69   Loss :  0.0505551959663
Iteration :  70   Loss :  0.0500725252007
Iteration :  71   Loss :  0.0495955208127
Iteration :  72   Loss :  0.049124119978
Iteration :  73   Loss :  0.0486582600504
Iteration :  74   Loss :  0.0481978785806
Iteration :  75   Loss :  0.0477429133395
Iteration :  76   Loss :  0.0472933023469
Iteration :  77   Loss :  0.0468489839042
Iteration :  78   Loss :  0.0464098966287
Iteration :  79   Loss :  0.0459759794925
Iteration :  80   Loss :  0.0455471718604
Iteration :  81   Loss :  0.0451234135297
Iteration :  82   Loss :  0.0447046447695
Iteration :  83   Loss :  0.0442908063589
Iteration :  84   Loss :  0.0438818396228
Iteration :  85   Loss :  0.0434776864664
Iteration :  86   Loss :  0.043078289406
Iteration :  87   Loss :  0.042683591596
Iteration :  88   Loss :  0.0422935368528
Iteration :  89   Loss :  0.0419080696736
Iteration :  90   Loss :  0.0415271352516
Iteration :  91   Loss :  0.0411506794854
Iteration :  92   Loss :  0.0407786489849
Iteration :  93   Loss :  0.0404109910718
Iteration :  94   Loss :  0.0400476537758
Iteration :  95   Loss :  0.0396885858266
Iteration :  96   Loss :  0.0393337366414
Iteration :  97   Loss :  0.0389830563093
Iteration :  98   Loss :  0.0386364955717
Iteration :  99   Loss :  0.0382940058002
[-0.00059923  0.00036757 -0.00023533 ...,  0.00016991 -0.0001065
  0.00024394]
CROSS VALIDATION 16
Iteration :  0   Loss :  7.93195491729
Iteration :  1   Loss :  0.101938788976
Iteration :  2   Loss :  0.100392539951
Iteration :  3   Loss :  0.0990420919583
Iteration :  4   Loss :  0.0977999505873
Iteration :  5   Loss :  0.0966276754683
Iteration :  6   Loss :  0.095505149721
Iteration :  7   Loss :  0.0944206155178
Iteration :  8   Loss :  0.0933666474967
Iteration :  9   Loss :  0.0923382765023
Iteration :  10   Loss :  0.0913320222661
Iteration :  11   Loss :  0.090345355301
Iteration :  12   Loss :  0.0893763792147
Iteration :  13   Loss :  0.088423633945
Iteration :  14   Loss :  0.0874859689784
Iteration :  15   Loss :  0.086562458904
Iteration :  16   Loss :  0.0856523455536
Iteration :  17   Loss :  0.0847549973788
Iteration :  18   Loss :  0.0838698803154
Iteration :  19   Loss :  0.0829965364929
Iteration :  20   Loss :  0.0821345684157
Iteration :  21   Loss :  0.0812836270329
Iteration :  22   Loss :  0.0804434026217
Iteration :  23   Loss :  0.0796136177331
Iteration :  24   Loss :  0.0787940216756
Iteration :  25   Loss :  0.0779843861553
Iteration :  26   Loss :  0.0771845017977
Iteration :  27   Loss :  0.0763941753493
Iteration :  28   Loss :  0.0756132274055
Iteration :  29   Loss :  0.0748414905524
Iteration :  30   Loss :  0.0740788078344
Iteration :  31   Loss :  0.0733250314812
Iteration :  32   Loss :  0.072580021842
Iteration :  33   Loss :  0.0718436464866
Iteration :  34   Loss :  0.0711157794418
Iteration :  35   Loss :  0.0703963005366
Iteration :  36   Loss :  0.069685094838
Iteration :  37   Loss :  0.0689820521595
Iteration :  38   Loss :  0.0682870666306
Iteration :  39   Loss :  0.067600036316
Iteration :  40   Loss :  0.0669208628772
Iteration :  41   Loss :  0.066249451268
Iteration :  42   Loss :  0.0655857094607
Iteration :  43   Loss :  0.0649295481963
Iteration :  44   Loss :  0.0642808807563
Iteration :  45   Loss :  0.0636396227535
Iteration :  46   Loss :  0.0630056919377
Iteration :  47   Loss :  0.062379008017
Iteration :  48   Loss :  0.0617594924911
Iteration :  49   Loss :  0.061147068496
Iteration :  50   Loss :  0.0605416606593
Iteration :  51   Loss :  0.059943194966
Iteration :  52   Loss :  0.0593515986322
Iteration :  53   Loss :  0.0587667999886
Iteration :  54   Loss :  0.0581887283716
Iteration :  55   Loss :  0.0576173140228
Iteration :  56   Loss :  0.0570524879961
Iteration :  57   Loss :  0.0564941820723
Iteration :  58   Loss :  0.0559423286808
Iteration :  59   Loss :  0.0553968608289
Iteration :  60   Loss :  0.0548577120379
Iteration :  61   Loss :  0.0543248162858
Iteration :  62   Loss :  0.053798107957
Iteration :  63   Loss :  0.0532775217986
Iteration :  64   Loss :  0.0527629928825
Iteration :  65   Loss :  0.0522544565739
Iteration :  66   Loss :  0.0517518485062
Iteration :  67   Loss :  0.051255104561
Iteration :  68   Loss :  0.0507641608539
Iteration :  69   Loss :  0.0502789537256
Iteration :  70   Loss :  0.0497994197376
Iteration :  71   Loss :  0.0493254956733
Iteration :  72   Loss :  0.0488571185429
Iteration :  73   Loss :  0.0483942255925
Iteration :  74   Loss :  0.0479367543173
Iteration :  75   Loss :  0.0474846424779
Iteration :  76   Loss :  0.0470378281192
Iteration :  77   Loss :  0.0465962495931
Iteration :  78   Loss :  0.0461598455819
Iteration :  79   Loss :  0.0457285551247
Iteration :  80   Loss :  0.0453023176445
Iteration :  81   Loss :  0.0448810729762
Iteration :  82   Loss :  0.0444647613958
Iteration :  83   Loss :  0.0440533236481
Iteration :  84   Loss :  0.0436467009759
Iteration :  85   Loss :  0.0432448351468
Iteration :  86   Loss :  0.0428476684796
Iteration :  87   Loss :  0.0424551438687
Iteration :  88   Loss :  0.0420672048069
Iteration :  89   Loss :  0.0416837954055
Iteration :  90   Loss :  0.0413048604123
Iteration :  91   Loss :  0.0409303452266
Iteration :  92   Loss :  0.0405601959109
Iteration :  93   Loss :  0.0401943592003
Iteration :  94   Loss :  0.0398327825079
Iteration :  95   Loss :  0.0394754139277
Iteration :  96   Loss :  0.0391222022335
Iteration :  97   Loss :  0.0387730968754
Iteration :  98   Loss :  0.0384280479729
Iteration :  99   Loss :  0.0380870063049
[-0.00058321  0.00036404 -0.00022936 ...,  0.00016678 -0.00010742
  0.00024874]
CROSS VALIDATION 17
Iteration :  0   Loss :  7.92328168937
Iteration :  1   Loss :  0.115870400767
Iteration :  2   Loss :  0.10213719266
Iteration :  3   Loss :  0.10045674965
Iteration :  4   Loss :  0.0990057259554
Iteration :  5   Loss :  0.0976884168974
Iteration :  6   Loss :  0.0964589664096
Iteration :  7   Loss :  0.0952921975197
Iteration :  8   Loss :  0.0941729577479
Iteration :  9   Loss :  0.0930914967283
Iteration :  10   Loss :  0.0920412094166
Iteration :  11   Loss :  0.0910174340485
Iteration :  12   Loss :  0.0900167673995
Iteration :  13   Loss :  0.089036653244
Iteration :  14   Loss :  0.0880751239209
Iteration :  15   Loss :  0.0871306319957
Iteration :  16   Loss :  0.0862019371533
Iteration :  17   Loss :  0.0852880281448
Iteration :  18   Loss :  0.0843880676643
Iteration :  19   Loss :  0.0835013526183
Iteration :  20   Loss :  0.0826272849708
Iteration :  21   Loss :  0.081765350003
Iteration :  22   Loss :  0.080915099868
Iteration :  23   Loss :  0.0800761409871
Iteration :  24   Loss :  0.0792481242763
Iteration :  25   Loss :  0.0784307374845
Iteration :  26   Loss :  0.077623699128
Iteration :  27   Loss :  0.0768267536426
Iteration :  28   Loss :  0.076039667476
Iteration :  29   Loss :  0.0752622259125
Iteration :  30   Loss :  0.0744942304711
Iteration :  31   Loss :  0.0737354967591
Iteration :  32   Loss :  0.0729858526866
Iteration :  33   Loss :  0.0722451369718
Iteration :  34   Loss :  0.0715131978803
Iteration :  35   Loss :  0.0707898921542
Iteration :  36   Loss :  0.0700750840954
Iteration :  37   Loss :  0.0693686447771
Iteration :  38   Loss :  0.0686704513584
Iteration :  39   Loss :  0.0679803864861
Iteration :  40   Loss :  0.067298337769
Iteration :  41   Loss :  0.0666241973115
Iteration :  42   Loss :  0.0659578612989
Iteration :  43   Loss :  0.0652992296254
Iteration :  44   Loss :  0.0646482055586
Iteration :  45   Loss :  0.0640046954365
Iteration :  46   Loss :  0.0633686083915
Iteration :  47   Loss :  0.0627398560986
Iteration :  48   Loss :  0.0621183525451
Iteration :  49   Loss :  0.06150401382
Iteration :  50   Loss :  0.0608967579201
Iteration :  51   Loss :  0.0602965045728
Iteration :  52   Loss :  0.0597031750734
Iteration :  53   Loss :  0.0591166921363
Iteration :  54   Loss :  0.0585369797593
Iteration :  55   Loss :  0.0579639631007
Iteration :  56   Loss :  0.0573975683677
Iteration :  57   Loss :  0.0568377227171
Iteration :  58   Loss :  0.0562843541659
Iteration :  59   Loss :  0.0557373915138
Iteration :  60   Loss :  0.0551967642749
Iteration :  61   Loss :  0.0546624026197
Iteration :  62   Loss :  0.054134237327
Iteration :  63   Loss :  0.0536121997436
Iteration :  64   Loss :  0.0530962217543
Iteration :  65   Loss :  0.0525862357585
Iteration :  66   Loss :  0.052082174655
Iteration :  67   Loss :  0.0515839718342
Iteration :  68   Loss :  0.0510915611759
Iteration :  69   Loss :  0.0506048770535
Iteration :  70   Loss :  0.0501238543431
Iteration :  71   Loss :  0.0496484284367
Iteration :  72   Loss :  0.0491785352598
Iteration :  73   Loss :  0.0487141112911
Iteration :  74   Loss :  0.048255093585
Iteration :  75   Loss :  0.047801419795
Iteration :  76   Loss :  0.0473530281983
Iteration :  77   Loss :  0.0469098577201
Iteration :  78   Loss :  0.0464718479572
Iteration :  79   Loss :  0.0460389392002
Iteration :  80   Loss :  0.0456110724544
Iteration :  81   Loss :  0.0451881894572
Iteration :  82   Loss :  0.0447702326933
Iteration :  83   Loss :  0.0443571454064
Iteration :  84   Loss :  0.0439488716075
Iteration :  85   Loss :  0.0435453560787
Iteration :  86   Loss :  0.0431465443736
Iteration :  87   Loss :  0.0427523828135
Iteration :  88   Loss :  0.0423628184788
Iteration :  89   Loss :  0.0419777991974
Iteration :  90   Loss :  0.0415972735284
Iteration :  91   Loss :  0.0412211907427
Iteration :  92   Loss :  0.0408495007993
Iteration :  93   Loss :  0.0404821543195
Iteration :  94   Loss :  0.0401191025581
Iteration :  95   Loss :  0.0397602973716
Iteration :  96   Loss :  0.0394056911853
Iteration :  97   Loss :  0.0390552369586
Iteration :  98   Loss :  0.0387088881495
Iteration :  99   Loss :  0.0383665986782
[-0.00059885  0.00036435 -0.00023313 ...,  0.0001688  -0.00010648
  0.0002444 ]
CROSS VALIDATION 18
Iteration :  0   Loss :  7.0408083108
Iteration :  1   Loss :  0.106399400744
Iteration :  2   Loss :  0.104311817091
Iteration :  3   Loss :  0.102596876251
Iteration :  4   Loss :  0.101087159187
Iteration :  5   Loss :  0.0997084534922
Iteration :  6   Loss :  0.098421172557
Iteration :  7   Loss :  0.0972017728801
Iteration :  8   Loss :  0.0960351857789
Iteration :  9   Loss :  0.094911233945
Iteration :  10   Loss :  0.0938227516074
Iteration :  11   Loss :  0.0927645215205
Iteration :  12   Loss :  0.0917326381
Iteration :  13   Loss :  0.0907241078685
Iteration :  14   Loss :  0.0897365891282
Iteration :  15   Loss :  0.0887682168882
Iteration :  16   Loss :  0.0878174819073
Iteration :  17   Loss :  0.0868831451547
Iteration :  18   Loss :  0.0859641760701
Iteration :  19   Loss :  0.0850597071936
Iteration :  20   Loss :  0.0841690002859
Iteration :  21   Loss :  0.0832914206618
Iteration :  22   Loss :  0.0824264174914
Iteration :  23   Loss :  0.081573508502
Iteration :  24   Loss :  0.0807322679687
Iteration :  25   Loss :  0.0799023171922
Iteration :  26   Loss :  0.0790833168804
Iteration :  27   Loss :  0.0782749610005
Iteration :  28   Loss :  0.0774769717792
Iteration :  29   Loss :  0.0766890956072
Iteration :  30   Loss :  0.0759110996605
Iteration :  31   Loss :  0.0751427690967
Iteration :  32   Loss :  0.0743839047138
Iteration :  33   Loss :  0.0736343209849
Iteration :  34   Loss :  0.0728938443995
Iteration :  35   Loss :  0.0721623120578
Iteration :  36   Loss :  0.0714395704734
Iteration :  37   Loss :  0.0707254745494
Iteration :  38   Loss :  0.0700198866999
Iteration :  39   Loss :  0.0693226760941
Iteration :  40   Loss :  0.068633718002
Iteration :  41   Loss :  0.0679528932285
Iteration :  42   Loss :  0.0672800876216
Iteration :  43   Loss :  0.0666151916431
Iteration :  44   Loss :  0.0659580999948
Iteration :  45   Loss :  0.0653087112891
Iteration :  46   Loss :  0.0646669277614
Iteration :  47   Loss :  0.0640326550149
Iteration :  48   Loss :  0.0634058017945
Iteration :  49   Loss :  0.0627862797861
Iteration :  50   Loss :  0.0621740034361
Iteration :  51   Loss :  0.0615688897882
Iteration :  52   Loss :  0.0609708583348
Iteration :  53   Loss :  0.0603798308807
Iteration :  54   Loss :  0.0597957314156
Iteration :  55   Loss :  0.0592184859945
Iteration :  56   Loss :  0.0586480226231
Iteration :  57   Loss :  0.0580842711484
Iteration :  58   Loss :  0.0575271631504
Iteration :  59   Loss :  0.056976631837
Iteration :  60   Loss :  0.0564326119379
Iteration :  61   Loss :  0.0558950395994
Iteration :  62   Loss :  0.0553638522784
Iteration :  63   Loss :  0.0548389886342
Iteration :  64   Loss :  0.0543203884206
Iteration :  65   Loss :  0.0538079923754
Iteration :  66   Loss :  0.0533017421091
Iteration :  67   Loss :  0.0528015799928
Iteration :  68   Loss :  0.0523074490452
Iteration :  69   Loss :  0.0518192928195
Iteration :  70   Loss :  0.0513370552914
Iteration :  71   Loss :  0.0508606807477
Iteration :  72   Loss :  0.0503901136776
Iteration :  73   Loss :  0.0499252986662
Iteration :  74   Loss :  0.0494661802923
Iteration :  75   Loss :  0.0490127030306
Iteration :  76   Loss :  0.0485648111597
Iteration :  77   Loss :  0.0481224486762
Iteration :  78   Loss :  0.0476855592163
Iteration :  79   Loss :  0.0472540859854
Iteration :  80   Loss :  0.0468279716967
Iteration :  81   Loss :  0.0464071585188
Iteration :  82   Loss :  0.0459915880337
Iteration :  83   Loss :  0.0455812012057
Iteration :  84   Loss :  0.0451759383602
Iteration :  85   Loss :  0.0447757391744
Iteration :  86   Loss :  0.044380542679
Iteration :  87   Loss :  0.0439902872713
Iteration :  88   Loss :  0.0436049107389
Iteration :  89   Loss :  0.0432243502946
Iteration :  90   Loss :  0.0428485426218
Iteration :  91   Loss :  0.0424774239291
Iteration :  92   Loss :  0.0421109300147
Iteration :  93   Loss :  0.0417489963384
Iteration :  94   Loss :  0.0413915581018
Iteration :  95   Loss :  0.0410385503342
Iteration :  96   Loss :  0.0406899079836
Iteration :  97   Loss :  0.0403455660125
Iteration :  98   Loss :  0.040005459496
Iteration :  99   Loss :  0.0396695237211
[ -5.34559986e-04  -1.52826403e-04  -9.47608311e-04 ...,   5.77791170e-04
  -8.73756210e-05   2.06220009e-04]
CROSS VALIDATION 19
Iteration :  0   Loss :  7.75881243812
Iteration :  1   Loss :  0.113778188237
Iteration :  2   Loss :  0.0991182387663
Iteration :  3   Loss :  0.0975174053308
Iteration :  4   Loss :  0.0961181119482
Iteration :  5   Loss :  0.0948416883664
Iteration :  6   Loss :  0.0936482686749
Iteration :  7   Loss :  0.0925151381165
Iteration :  8   Loss :  0.0914282608317
Iteration :  9   Loss :  0.0903784265193
Iteration :  10   Loss :  0.0893593022327
Iteration :  11   Loss :  0.0883663662894
Iteration :  12   Loss :  0.0873962877713
Iteration :  13   Loss :  0.0864465470103
Iteration :  14   Loss :  0.0855151937959
Iteration :  15   Loss :  0.0846006879586
Iteration :  16   Loss :  0.0837017911536
Iteration :  17   Loss :  0.0828174915262
Iteration :  18   Loss :  0.0819469501021
Iteration :  19   Loss :  0.0810894618923
Iteration :  20   Loss :  0.0802444271816
Iteration :  21   Loss :  0.0794113300068
Iteration :  22   Loss :  0.0785897217954
Iteration :  23   Loss :  0.0777792087705
Iteration :  24   Loss :  0.0769794421398
Iteration :  25   Loss :  0.076190110372
Iteration :  26   Loss :  0.0754109330544
Iteration :  27   Loss :  0.074641655963
Iteration :  28   Loss :  0.0738820470692
Iteration :  29   Loss :  0.07313189328
Iteration :  30   Loss :  0.0723909977531
Iteration :  31   Loss :  0.0716591776713
Iteration :  32   Loss :  0.0709362623814
Iteration :  33   Loss :  0.0702220918286
Iteration :  34   Loss :  0.0695165152307
Iteration :  35   Loss :  0.0688193899469
Iteration :  36   Loss :  0.0681305805079
Iteration :  37   Loss :  0.0674499577803
Iteration :  38   Loss :  0.0667773982412
Iteration :  39   Loss :  0.0661127833476
Iteration :  40   Loss :  0.065455998985
Iteration :  41   Loss :  0.0648069349847
Iteration :  42   Loss :  0.0641654846992
Iteration :  43   Loss :  0.0635315446293
Iteration :  44   Loss :  0.062905014096
Iteration :  45   Loss :  0.0622857949519
Iteration :  46   Loss :  0.0616737913281
Iteration :  47   Loss :  0.0610689094124
Iteration :  48   Loss :  0.0604710572567
Iteration :  49   Loss :  0.0598801446094
Iteration :  50   Loss :  0.0592960827717
Iteration :  51   Loss :  0.0587187844745
Iteration :  52   Loss :  0.0581481637753
Iteration :  53   Loss :  0.0575841359717
Iteration :  54   Loss :  0.0570266175314
Iteration :  55   Loss :  0.0564755260356
Iteration :  56   Loss :  0.0559307801353
Iteration :  57   Loss :  0.0553922995188
Iteration :  58   Loss :  0.0548600048889
Iteration :  59   Loss :  0.0543338179477
Iteration :  60   Loss :  0.05381366139
Iteration :  61   Loss :  0.0532994589012
Iteration :  62   Loss :  0.0527911351606
Iteration :  63   Loss :  0.0522886158483
Iteration :  64   Loss :  0.0517918276549
Iteration :  65   Loss :  0.0513006982929
Iteration :  66   Loss :  0.0508151565095
Iteration :  67   Loss :  0.0503351320992
Iteration :  68   Loss :  0.0498605559172
Iteration :  69   Loss :  0.0493913598907
Iteration :  70   Loss :  0.0489274770307
Iteration :  71   Loss :  0.0484688414408
Iteration :  72   Loss :  0.0480153883258
Iteration :  73   Loss :  0.0475670539974
Iteration :  74   Loss :  0.0471237758783
Iteration :  75   Loss :  0.0466854925043
Iteration :  76   Loss :  0.0462521435248
Iteration :  77   Loss :  0.0458236697003
Iteration :  78   Loss :  0.0454000128994
Iteration :  79   Loss :  0.0449811160924
Iteration :  80   Loss :  0.0445669233444
Iteration :  81   Loss :  0.044157379806
Iteration :  82   Loss :  0.0437524317028
Iteration :  83   Loss :  0.0433520263228
Iteration :  84   Loss :  0.0429561120029
Iteration :  85   Loss :  0.0425646381135
Iteration :  86   Loss :  0.0421775550422
Iteration :  87   Loss :  0.0417948141753
Iteration :  88   Loss :  0.0414163678786
Iteration :  89   Loss :  0.0410421694762
Iteration :  90   Loss :  0.0406721732286
Iteration :  91   Loss :  0.0403063343079
Iteration :  92   Loss :  0.0399446087731
Iteration :  93   Loss :  0.0395869535422
Iteration :  94   Loss :  0.0392333263639
Iteration :  95   Loss :  0.038883685787
Iteration :  96   Loss :  0.0385379911278
Iteration :  97   Loss :  0.0381962024367
Iteration :  98   Loss :  0.0378582804617
Iteration :  99   Loss :  0.037524186612
[-0.00070617  0.00036474 -0.00029426 ...,  0.00014902 -0.00011746
  0.0002583 ]
Accuracy (Logistic Loss):	0.95
Hinge Loss
CROSS VALIDATION 0
Iteration :  0   Loss :  24.8053666149
Iteration :  1   Loss :  0.143365085515
Iteration :  2   Loss :  0.141853366289
Iteration :  3   Loss :  0.14035758745
Iteration :  4   Loss :  0.138877580914
Iteration :  5   Loss :  0.137413180371
Iteration :  6   Loss :  0.13596422126
Iteration :  7   Loss :  0.134530540761
Iteration :  8   Loss :  0.133111977767
Iteration :  9   Loss :  0.13170837287
Iteration :  10   Loss :  0.130319568345
Iteration :  11   Loss :  0.128945408128
Iteration :  12   Loss :  0.127585737801
Iteration :  13   Loss :  0.126240404576
Iteration :  14   Loss :  0.124909257274
Iteration :  15   Loss :  0.123592146312
Iteration :  16   Loss :  0.122288923682
Iteration :  17   Loss :  0.120999442938
Iteration :  18   Loss :  0.119723559179
Iteration :  19   Loss :  0.11846112903
Iteration :  20   Loss :  0.117212010629
Iteration :  21   Loss :  0.11597606361
Iteration :  22   Loss :  0.114753149087
Iteration :  23   Loss :  0.113543129639
Iteration :  24   Loss :  0.112345869291
Iteration :  25   Loss :  0.111161233506
Iteration :  26   Loss :  0.109989089164
Iteration :  27   Loss :  0.108829304547
Iteration :  28   Loss :  0.107681749328
Iteration :  29   Loss :  0.106546294554
Iteration :  30   Loss :  0.105422812631
Iteration :  31   Loss :  0.10431117731
Iteration :  32   Loss :  0.103211263676
Iteration :  33   Loss :  0.102122948127
Iteration :  34   Loss :  0.101046108368
Iteration :  35   Loss :  0.099980623392
Iteration :  36   Loss :  0.098926373467
Iteration :  37   Loss :  0.0978832401251
Iteration :  38   Loss :  0.096851106147
Iteration :  39   Loss :  0.0958298555494
Iteration :  40   Loss :  0.094819373572
Iteration :  41   Loss :  0.0938195466648
Iteration :  42   Loss :  0.0928302624747
Iteration :  43   Loss :  0.0918514098338
Iteration :  44   Loss :  0.0908828787461
Iteration :  45   Loss :  0.0899245603755
Iteration :  46   Loss :  0.0889763470337
Iteration :  47   Loss :  0.0880381321676
Iteration :  48   Loss :  0.087109810348
Iteration :  49   Loss :  0.0861912772572
Iteration :  50   Loss :  0.0852824296776
Iteration :  51   Loss :  0.0843831654798
Iteration :  52   Loss :  0.0834933836115
Iteration :  53   Loss :  0.0826129840859
Iteration :  54   Loss :  0.0817418679704
Iteration :  55   Loss :  0.0808799373758
Iteration :  56   Loss :  0.0800270954448
Iteration :  57   Loss :  0.0791832463418
Iteration :  58   Loss :  0.0783482952415
Iteration :  59   Loss :  0.0775221483184
Iteration :  60   Loss :  0.0767047127367
Iteration :  61   Loss :  0.0758958966391
Iteration :  62   Loss :  0.0750956091371
Iteration :  63   Loss :  0.0743037603007
Iteration :  64   Loss :  0.073520261148
Iteration :  65   Loss :  0.0727450236352
Iteration :  66   Loss :  0.0719779606474
Iteration :  67   Loss :  0.0712189859877
Iteration :  68   Loss :  0.0704680143686
Iteration :  69   Loss :  0.0697249614015
Iteration :  70   Loss :  0.068989743588
Iteration :  71   Loss :  0.0682622783099
Iteration :  72   Loss :  0.0675424838202
Iteration :  73   Loss :  0.0668302792342
Iteration :  74   Loss :  0.0661255845196
Iteration :  75   Loss :  0.0654283204883
Iteration :  76   Loss :  0.0647384087871
Iteration :  77   Loss :  0.0640557718892
Iteration :  78   Loss :  0.063380333085
Iteration :  79   Loss :  0.0627120164739
Iteration :  80   Loss :  0.0620507469557
Iteration :  81   Loss :  0.061396450222
Iteration :  82   Loss :  0.0607490527479
Iteration :  83   Loss :  0.060108481784
Iteration :  84   Loss :  0.0594746653477
Iteration :  85   Loss :  0.0588475322158
Iteration :  86   Loss :  0.0582270119158
Iteration :  87   Loss :  0.0576130347184
Iteration :  88   Loss :  0.0570055316296
Iteration :  89   Loss :  0.056404434383
Iteration :  90   Loss :  0.0558096754317
Iteration :  91   Loss :  0.0552211879414
Iteration :  92   Loss :  0.0546389057825
Iteration :  93   Loss :  0.0540627635224
Iteration :  94   Loss :  0.0534926964189
Iteration :  95   Loss :  0.0529286404121
Iteration :  96   Loss :  0.0523705321178
Iteration :  97   Loss :  0.0518183088201
Iteration :  98   Loss :  0.0512719084644
Iteration :  99   Loss :  0.0507312696504
[ -1.21699449e-03   9.14019506e-05   1.36494486e-04 ...,   3.74954997e-04
   4.95785703e-04  -5.93497860e-05]
CROSS VALIDATION 1
Iteration :  0   Loss :  14.8012563555
Iteration :  1   Loss :  0.416466510017
Iteration :  2   Loss :  0.152726229387
Iteration :  3   Loss :  0.151115801182
Iteration :  4   Loss :  0.149522354205
Iteration :  5   Loss :  0.147945709398
Iteration :  6   Loss :  0.146385689589
Iteration :  7   Loss :  0.144842119474
Iteration :  8   Loss :  0.143314825601
Iteration :  9   Loss :  0.141803636342
Iteration :  10   Loss :  0.140308381883
Iteration :  11   Loss :  0.138828894198
Iteration :  12   Loss :  0.137365007033
Iteration :  13   Loss :  0.135916555889
Iteration :  14   Loss :  0.134483377999
Iteration :  15   Loss :  0.133065312314
Iteration :  16   Loss :  0.131662199484
Iteration :  17   Loss :  0.130273881835
Iteration :  18   Loss :  0.128900203362
Iteration :  19   Loss :  0.127541009699
Iteration :  20   Loss :  0.126196148111
Iteration :  21   Loss :  0.124865467474
Iteration :  22   Loss :  0.123548818255
Iteration :  23   Loss :  0.122246052499
Iteration :  24   Loss :  0.120957023812
Iteration :  25   Loss :  0.119681587343
Iteration :  26   Loss :  0.118419599768
Iteration :  27   Loss :  0.117170919274
Iteration :  28   Loss :  0.115935405545
Iteration :  29   Loss :  0.114712919743
Iteration :  30   Loss :  0.113503324494
Iteration :  31   Loss :  0.112306483874
Iteration :  32   Loss :  0.11112226339
Iteration :  33   Loss :  0.109950529969
Iteration :  34   Loss :  0.108791151942
Iteration :  35   Loss :  0.107643999025
Iteration :  36   Loss :  0.10650894231
Iteration :  37   Loss :  0.105385854249
Iteration :  38   Loss :  0.104274608638
Iteration :  39   Loss :  0.103175080604
Iteration :  40   Loss :  0.102087146589
Iteration :  41   Loss :  0.101010684341
Iteration :  42   Loss :  0.0999455728947
Iteration :  43   Loss :  0.0988916925612
Iteration :  44   Loss :  0.0978489249136
Iteration :  45   Loss :  0.0968171527737
Iteration :  46   Loss :  0.0957962601989
Iteration :  47   Loss :  0.0947861324691
Iteration :  48   Loss :  0.0937866560741
Iteration :  49   Loss :  0.0927977187003
Iteration :  50   Loss :  0.0918192092186
Iteration :  51   Loss :  0.0908510176716
Iteration :  52   Loss :  0.0898930352615
Iteration :  53   Loss :  0.0889451543375
Iteration :  54   Loss :  0.0880072683842
Iteration :  55   Loss :  0.087079272009
Iteration :  56   Loss :  0.0861610609311
Iteration :  57   Loss :  0.0852525319688
Iteration :  58   Loss :  0.0843535830287
Iteration :  59   Loss :  0.0834641130938
Iteration :  60   Loss :  0.0825840222123
Iteration :  61   Loss :  0.0817132114866
Iteration :  62   Loss :  0.0808515830614
Iteration :  63   Loss :  0.0799990401138
Iteration :  64   Loss :  0.0791554868414
Iteration :  65   Loss :  0.0783208284523
Iteration :  66   Loss :  0.077494971154
Iteration :  67   Loss :  0.076677822143
Iteration :  68   Loss :  0.0758692895944
Iteration :  69   Loss :  0.0750692826516
Iteration :  70   Loss :  0.0742777114159
Iteration :  71   Loss :  0.0734944869367
Iteration :  72   Loss :  0.0727195212012
Iteration :  73   Loss :  0.0719527271248
Iteration :  74   Loss :  0.0711940185411
Iteration :  75   Loss :  0.0704433101923
Iteration :  76   Loss :  0.0697005177195
Iteration :  77   Loss :  0.0689655576534
Iteration :  78   Loss :  0.0682383474049
Iteration :  79   Loss :  0.0675188052557
Iteration :  80   Loss :  0.0668068503493
Iteration :  81   Loss :  0.0661024026815
Iteration :  82   Loss :  0.0654053830921
Iteration :  83   Loss :  0.0647157132553
Iteration :  84   Loss :  0.0640333156714
Iteration :  85   Loss :  0.0633581136577
Iteration :  86   Loss :  0.0626900313403
Iteration :  87   Loss :  0.0620289936453
Iteration :  88   Loss :  0.0613749262903
Iteration :  89   Loss :  0.0607277557762
Iteration :  90   Loss :  0.0600874093791
Iteration :  91   Loss :  0.0594538151417
Iteration :  92   Loss :  0.0588269018657
Iteration :  93   Loss :  0.0582065991033
Iteration :  94   Loss :  0.0575928371497
Iteration :  95   Loss :  0.056985547035
Iteration :  96   Loss :  0.0563846605167
Iteration :  97   Loss :  0.0557901100718
Iteration :  98   Loss :  0.0552018288893
Iteration :  99   Loss :  0.0546197508627
[-0.00141059 -0.00019193 -0.00031947 ...,  0.00073923  0.00066932
  0.00017728]
CROSS VALIDATION 2
Iteration :  0   Loss :  10.5214596642
Iteration :  1   Loss :  2.64376011502
Iteration :  2   Loss :  0.153119039957
Iteration :  3   Loss :  0.151504469744
Iteration :  4   Loss :  0.149906924435
Iteration :  5   Loss :  0.148326224511
Iteration :  6   Loss :  0.146762192344
Iteration :  7   Loss :  0.14521465218
Iteration :  8   Loss :  0.14368343012
Iteration :  9   Loss :  0.142168354095
Iteration :  10   Loss :  0.140669253854
Iteration :  11   Loss :  0.139185960939
Iteration :  12   Loss :  0.137718308669
Iteration :  13   Loss :  0.136266132121
Iteration :  14   Loss :  0.13482926811
Iteration :  15   Loss :  0.133407555172
Iteration :  16   Loss :  0.132000833547
Iteration :  17   Loss :  0.130608945157
Iteration :  18   Loss :  0.129231733594
Iteration :  19   Loss :  0.127869044097
Iteration :  20   Loss :  0.126520723537
Iteration :  21   Loss :  0.1251866204
Iteration :  22   Loss :  0.12386658477
Iteration :  23   Loss :  0.122560468312
Iteration :  24   Loss :  0.121268124254
Iteration :  25   Loss :  0.119989407373
Iteration :  26   Loss :  0.118724173977
Iteration :  27   Loss :  0.117472281888
Iteration :  28   Loss :  0.116233590428
Iteration :  29   Loss :  0.115007960402
Iteration :  30   Loss :  0.113795254085
Iteration :  31   Loss :  0.1125953352
Iteration :  32   Loss :  0.111408068911
Iteration :  33   Loss :  0.110233321802
Iteration :  34   Loss :  0.109070961864
Iteration :  35   Loss :  0.107920858479
Iteration :  36   Loss :  0.106782882409
Iteration :  37   Loss :  0.105656905775
Iteration :  38   Loss :  0.10454280205
Iteration :  39   Loss :  0.103440446038
Iteration :  40   Loss :  0.102349713867
Iteration :  41   Loss :  0.101270482967
Iteration :  42   Loss :  0.100202632063
Iteration :  43   Loss :  0.0991460411579
Iteration :  44   Loss :  0.0981005915207
Iteration :  45   Loss :  0.0970661656715
Iteration :  46   Loss :  0.0960426473697
Iteration :  47   Loss :  0.0950299215999
Iteration :  48   Loss :  0.0940278745601
Iteration :  49   Loss :  0.0930363936477
Iteration :  50   Loss :  0.0920553674479
Iteration :  51   Loss :  0.0910846857204
Iteration :  52   Loss :  0.0901242393877
Iteration :  53   Loss :  0.089173920522
Iteration :  54   Loss :  0.0882336223339
Iteration :  55   Loss :  0.08730323916
Iteration :  56   Loss :  0.0863826664509
Iteration :  57   Loss :  0.0854718007598
Iteration :  58   Loss :  0.0845705397306
Iteration :  59   Loss :  0.0836787820865
Iteration :  60   Loss :  0.0827964276188
Iteration :  61   Loss :  0.0819233771752
Iteration :  62   Loss :  0.0810595326491
Iteration :  63   Loss :  0.0802047969681
Iteration :  64   Loss :  0.0793590740839
Iteration :  65   Loss :  0.0785222689605
Iteration :  66   Loss :  0.0776942875642
Iteration :  67   Loss :  0.0768750368528
Iteration :  68   Loss :  0.0760644247654
Iteration :  69   Loss :  0.0752623602114
Iteration :  70   Loss :  0.0744687530612
Iteration :  71   Loss :  0.0736835141353
Iteration :  72   Loss :  0.0729065551945
Iteration :  73   Loss :  0.0721377889303
Iteration :  74   Loss :  0.0713771289545
Iteration :  75   Loss :  0.0706244897901
Iteration :  76   Loss :  0.0698797868614
Iteration :  77   Loss :  0.0691429364842
Iteration :  78   Loss :  0.0684138558571
Iteration :  79   Loss :  0.0676924630516
Iteration :  80   Loss :  0.0669786770032
Iteration :  81   Loss :  0.066272417502
Iteration :  82   Loss :  0.0655736051841
Iteration :  83   Loss :  0.0648821615223
Iteration :  84   Loss :  0.0641980088176
Iteration :  85   Loss :  0.06352107019
Iteration :  86   Loss :  0.0628512695706
Iteration :  87   Loss :  0.0621885316922
Iteration :  88   Loss :  0.0615327820814
Iteration :  89   Loss :  0.0608839470503
Iteration :  90   Loss :  0.0602419536877
Iteration :  91   Loss :  0.0596067298513
Iteration :  92   Loss :  0.0589782041596
Iteration :  93   Loss :  0.0583563059837
Iteration :  94   Loss :  0.0577409654396
Iteration :  95   Loss :  0.0571321133799
Iteration :  96   Loss :  0.0565296813866
Iteration :  97   Loss :  0.055933601763
Iteration :  98   Loss :  0.0553438075262
Iteration :  99   Loss :  0.0547602323998
[ -1.18790789e-03  -4.98283604e-04  -5.14069487e-04 ...,   1.25731919e-03
   5.59408833e-04   5.91198486e-05]
CROSS VALIDATION 3
Iteration :  0   Loss :  20.9044707731
Iteration :  1   Loss :  1.46727120591
Iteration :  2   Loss :  0.856679375538
Iteration :  3   Loss :  0.159603157618
Iteration :  4   Loss :  0.157920215351
Iteration :  5   Loss :  0.156255018941
Iteration :  6   Loss :  0.154607381265
Iteration :  7   Loss :  0.152977117174
Iteration :  8   Loss :  0.151364043472
Iteration :  9   Loss :  0.149767978894
Iteration :  10   Loss :  0.148188744087
Iteration :  11   Loss :  0.146626161588
Iteration :  12   Loss :  0.145080055807
Iteration :  13   Loss :  0.143550253003
Iteration :  14   Loss :  0.14203658127
Iteration :  15   Loss :  0.140538870513
Iteration :  16   Loss :  0.139056952431
Iteration :  17   Loss :  0.137590660497
Iteration :  18   Loss :  0.13613982994
Iteration :  19   Loss :  0.134704297727
Iteration :  20   Loss :  0.133283902545
Iteration :  21   Loss :  0.131878484781
Iteration :  22   Loss :  0.130487886503
Iteration :  23   Loss :  0.129111951448
Iteration :  24   Loss :  0.127750524998
Iteration :  25   Loss :  0.126403454168
Iteration :  26   Loss :  0.125070587583
Iteration :  27   Loss :  0.123751775466
Iteration :  28   Loss :  0.122446869619
Iteration :  29   Loss :  0.121155723408
Iteration :  30   Loss :  0.119878191743
Iteration :  31   Loss :  0.118614131064
Iteration :  32   Loss :  0.117363399327
Iteration :  33   Loss :  0.116125855984
Iteration :  34   Loss :  0.114901361969
Iteration :  35   Loss :  0.113689779683
Iteration :  36   Loss :  0.112490972978
Iteration :  37   Loss :  0.111304807141
Iteration :  38   Loss :  0.11013114888
Iteration :  39   Loss :  0.108969866308
Iteration :  40   Loss :  0.10782082893
Iteration :  41   Loss :  0.106683907625
Iteration :  42   Loss :  0.105558974635
Iteration :  43   Loss :  0.104445903549
Iteration :  44   Loss :  0.103344569288
Iteration :  45   Loss :  0.102254848093
Iteration :  46   Loss :  0.101176617509
Iteration :  47   Loss :  0.100109756373
Iteration :  48   Loss :  0.0990541448001
Iteration :  49   Loss :  0.098009664168
Iteration :  50   Loss :  0.0969761971062
Iteration :  51   Loss :  0.0959536274817
Iteration :  52   Loss :  0.0949418403859
Iteration :  53   Loss :  0.0939407221221
Iteration :  54   Loss :  0.0929501601924
Iteration :  55   Loss :  0.0919700432849
Iteration :  56   Loss :  0.0910002612617
Iteration :  57   Loss :  0.0900407051462
Iteration :  58   Loss :  0.0890912671109
Iteration :  59   Loss :  0.0881518404652
Iteration :  60   Loss :  0.0872223196436
Iteration :  61   Loss :  0.0863026001938
Iteration :  62   Loss :  0.0853925787647
Iteration :  63   Loss :  0.0844921530952
Iteration :  64   Loss :  0.0836012220024
Iteration :  65   Loss :  0.0827196853703
Iteration :  66   Loss :  0.0818474441386
Iteration :  67   Loss :  0.0809844002915
Iteration :  68   Loss :  0.080130456847
Iteration :  69   Loss :  0.0792855178453
Iteration :  70   Loss :  0.0784494883387
Iteration :  71   Loss :  0.0776222743808
Iteration :  72   Loss :  0.0768037830155
Iteration :  73   Loss :  0.0759939222672
Iteration :  74   Loss :  0.0751926011298
Iteration :  75   Loss :  0.0743997295572
Iteration :  76   Loss :  0.0736152184526
Iteration :  77   Loss :  0.0728389796585
Iteration :  78   Loss :  0.0720709259474
Iteration :  79   Loss :  0.0713109710112
Iteration :  80   Loss :  0.070559029452
Iteration :  81   Loss :  0.0698150167725
Iteration :  82   Loss :  0.0690788493663
Iteration :  83   Loss :  0.0683504445086
Iteration :  84   Loss :  0.0676297203467
Iteration :  85   Loss :  0.0669165958914
Iteration :  86   Loss :  0.0662109910071
Iteration :  87   Loss :  0.0655128264035
Iteration :  88   Loss :  0.0648220236262
Iteration :  89   Loss :  0.064138505048
Iteration :  90   Loss :  0.0634621938605
Iteration :  91   Loss :  0.062793014065
Iteration :  92   Loss :  0.0621308904643
Iteration :  93   Loss :  0.0614757486539
Iteration :  94   Loss :  0.0608275150142
Iteration :  95   Loss :  0.0601861167015
Iteration :  96   Loss :  0.0595514816406
Iteration :  97   Loss :  0.058923538516
Iteration :  98   Loss :  0.0583022167644
Iteration :  99   Loss :  0.0576874465663
[-0.00174105 -0.00017526 -0.00031312 ...,  0.0013205   0.00079858
  0.00011892]
CROSS VALIDATION 4
Iteration :  0   Loss :  20.9044707731
Iteration :  1   Loss :  1.46727120591
Iteration :  2   Loss :  0.856679375538
Iteration :  3   Loss :  0.159603157618
Iteration :  4   Loss :  0.157920215351
Iteration :  5   Loss :  0.156255018941
Iteration :  6   Loss :  0.154607381265
Iteration :  7   Loss :  0.152977117174
Iteration :  8   Loss :  0.151364043472
Iteration :  9   Loss :  0.149767978894
Iteration :  10   Loss :  0.148188744087
Iteration :  11   Loss :  0.146626161588
Iteration :  12   Loss :  0.145080055807
Iteration :  13   Loss :  0.143550253003
Iteration :  14   Loss :  0.14203658127
Iteration :  15   Loss :  0.140538870513
Iteration :  16   Loss :  0.139056952431
Iteration :  17   Loss :  0.137590660497
Iteration :  18   Loss :  0.13613982994
Iteration :  19   Loss :  0.134704297727
Iteration :  20   Loss :  0.133283902545
Iteration :  21   Loss :  0.131878484781
Iteration :  22   Loss :  0.130487886503
Iteration :  23   Loss :  0.129111951448
Iteration :  24   Loss :  0.127750524998
Iteration :  25   Loss :  0.126403454168
Iteration :  26   Loss :  0.125070587583
Iteration :  27   Loss :  0.123751775466
Iteration :  28   Loss :  0.122446869619
Iteration :  29   Loss :  0.121155723408
Iteration :  30   Loss :  0.119878191743
Iteration :  31   Loss :  0.118614131064
Iteration :  32   Loss :  0.117363399327
Iteration :  33   Loss :  0.116125855984
Iteration :  34   Loss :  0.114901361969
Iteration :  35   Loss :  0.113689779683
Iteration :  36   Loss :  0.112490972978
Iteration :  37   Loss :  0.111304807141
Iteration :  38   Loss :  0.11013114888
Iteration :  39   Loss :  0.108969866308
Iteration :  40   Loss :  0.10782082893
Iteration :  41   Loss :  0.106683907625
Iteration :  42   Loss :  0.105558974635
Iteration :  43   Loss :  0.104445903549
Iteration :  44   Loss :  0.103344569288
Iteration :  45   Loss :  0.102254848093
Iteration :  46   Loss :  0.101176617509
Iteration :  47   Loss :  0.100109756373
Iteration :  48   Loss :  0.0990541448001
Iteration :  49   Loss :  0.098009664168
Iteration :  50   Loss :  0.0969761971062
Iteration :  51   Loss :  0.0959536274817
Iteration :  52   Loss :  0.0949418403859
Iteration :  53   Loss :  0.0939407221221
Iteration :  54   Loss :  0.0929501601924
Iteration :  55   Loss :  0.0919700432849
Iteration :  56   Loss :  0.0910002612617
Iteration :  57   Loss :  0.0900407051462
Iteration :  58   Loss :  0.0890912671109
Iteration :  59   Loss :  0.0881518404652
Iteration :  60   Loss :  0.0872223196436
Iteration :  61   Loss :  0.0863026001938
Iteration :  62   Loss :  0.0853925787647
Iteration :  63   Loss :  0.0844921530952
Iteration :  64   Loss :  0.0836012220024
Iteration :  65   Loss :  0.0827196853703
Iteration :  66   Loss :  0.0818474441386
Iteration :  67   Loss :  0.0809844002915
Iteration :  68   Loss :  0.080130456847
Iteration :  69   Loss :  0.0792855178453
Iteration :  70   Loss :  0.0784494883387
Iteration :  71   Loss :  0.0776222743808
Iteration :  72   Loss :  0.0768037830155
Iteration :  73   Loss :  0.0759939222672
Iteration :  74   Loss :  0.0751926011298
Iteration :  75   Loss :  0.0743997295572
Iteration :  76   Loss :  0.0736152184526
Iteration :  77   Loss :  0.0728389796585
Iteration :  78   Loss :  0.0720709259474
Iteration :  79   Loss :  0.0713109710112
Iteration :  80   Loss :  0.070559029452
Iteration :  81   Loss :  0.0698150167725
Iteration :  82   Loss :  0.0690788493663
Iteration :  83   Loss :  0.0683504445086
Iteration :  84   Loss :  0.0676297203467
Iteration :  85   Loss :  0.0669165958914
Iteration :  86   Loss :  0.0662109910071
Iteration :  87   Loss :  0.0655128264035
Iteration :  88   Loss :  0.0648220236262
Iteration :  89   Loss :  0.064138505048
Iteration :  90   Loss :  0.0634621938605
Iteration :  91   Loss :  0.062793014065
Iteration :  92   Loss :  0.0621308904643
Iteration :  93   Loss :  0.0614757486539
Iteration :  94   Loss :  0.0608275150142
Iteration :  95   Loss :  0.0601861167015
Iteration :  96   Loss :  0.0595514816406
Iteration :  97   Loss :  0.058923538516
Iteration :  98   Loss :  0.0583022167644
Iteration :  99   Loss :  0.0576874465663
[-0.00174105 -0.00017526 -0.00031312 ...,  0.0013205   0.00079858
  0.00011892]
CROSS VALIDATION 5
Iteration :  0   Loss :  9.75492111041
Iteration :  1   Loss :  0.148466075962
Iteration :  2   Loss :  0.146900569126
Iteration :  3   Loss :  0.145351569842
Iteration :  4   Loss :  0.143818904048
Iteration :  5   Loss :  0.142302399512
Iteration :  6   Loss :  0.140801885824
Iteration :  7   Loss :  0.139317194365
Iteration :  8   Loss :  0.137848158299
Iteration :  9   Loss :  0.136394612546
Iteration :  10   Loss :  0.134956393767
Iteration :  11   Loss :  0.133533340348
Iteration :  12   Loss :  0.132125292375
Iteration :  13   Loss :  0.130732091625
Iteration :  14   Loss :  0.129353581538
Iteration :  15   Loss :  0.12798960721
Iteration :  16   Loss :  0.126640015367
Iteration :  17   Loss :  0.125304654352
Iteration :  18   Loss :  0.123983374108
Iteration :  19   Loss :  0.12267602616
Iteration :  20   Loss :  0.121382463598
Iteration :  21   Loss :  0.120102541061
Iteration :  22   Loss :  0.118836114722
Iteration :  23   Loss :  0.117583042268
Iteration :  24   Loss :  0.116343182891
Iteration :  25   Loss :  0.115116397263
Iteration :  26   Loss :  0.113902547528
Iteration :  27   Loss :  0.112701497284
Iteration :  28   Loss :  0.111513111564
Iteration :  29   Loss :  0.110337256829
Iteration :  30   Loss :  0.109173800943
Iteration :  31   Loss :  0.108022613167
Iteration :  32   Loss :  0.10688356414
Iteration :  33   Loss :  0.105756525863
Iteration :  34   Loss :  0.10464137169
Iteration :  35   Loss :  0.103537976307
Iteration :  36   Loss :  0.102446215723
Iteration :  37   Loss :  0.101365967255
Iteration :  38   Loss :  0.100297109513
Iteration :  39   Loss :  0.0992395223867
Iteration :  40   Loss :  0.0981930870328
Iteration :  41   Loss :  0.0971576858608
Iteration :  42   Loss :  0.0961332025204
Iteration :  43   Loss :  0.095119521888
Iteration :  44   Loss :  0.0941165300541
Iteration :  45   Loss :  0.0931241143101
Iteration :  46   Loss :  0.0921421631361
Iteration :  47   Loss :  0.0911705661878
Iteration :  48   Loss :  0.0902092142848
Iteration :  49   Loss :  0.0892579993977
Iteration :  50   Loss :  0.0883168146364
Iteration :  51   Loss :  0.0873855542377
Iteration :  52   Loss :  0.0864641135537
Iteration :  53   Loss :  0.0855523890402
Iteration :  54   Loss :  0.0846502782445
Iteration :  55   Loss :  0.0837576797943
Iteration :  56   Loss :  0.0828744933863
Iteration :  57   Loss :  0.0820006197749
Iteration :  58   Loss :  0.0811359607609
Iteration :  59   Loss :  0.0802804191805
Iteration :  60   Loss :  0.0794338988947
Iteration :  61   Loss :  0.0785963047778
Iteration :  62   Loss :  0.0777675427077
Iteration :  63   Loss :  0.0769475195544
Iteration :  64   Loss :  0.07613614317
Iteration :  65   Loss :  0.0753333223784
Iteration :  66   Loss :  0.0745389669646
Iteration :  67   Loss :  0.0737529876652
Iteration :  68   Loss :  0.0729752961578
Iteration :  69   Loss :  0.0722058050515
Iteration :  70   Loss :  0.0714444278769
Iteration :  71   Loss :  0.0706910790762
Iteration :  72   Loss :  0.0699456739938
Iteration :  73   Loss :  0.069208128867
Iteration :  74   Loss :  0.0684783608161
Iteration :  75   Loss :  0.0677562878353
Iteration :  76   Loss :  0.0670418287837
Iteration :  77   Loss :  0.0663349033759
Iteration :  78   Loss :  0.0656354321731
Iteration :  79   Loss :  0.064943336574
Iteration :  80   Loss :  0.0642585388063
Iteration :  81   Loss :  0.0635809619177
Iteration :  82   Loss :  0.0629105297674
Iteration :  83   Loss :  0.0622471670174
Iteration :  84   Loss :  0.061590799124
Iteration :  85   Loss :  0.0609413523297
Iteration :  86   Loss :  0.0602987536546
Iteration :  87   Loss :  0.0596629308886
Iteration :  88   Loss :  0.0590338125827
Iteration :  89   Loss :  0.0584113280414
Iteration :  90   Loss :  0.0577954073148
Iteration :  91   Loss :  0.0571859811905
Iteration :  92   Loss :  0.0565829811858
Iteration :  93   Loss :  0.0559863395401
Iteration :  94   Loss :  0.0553959892077
Iteration :  95   Loss :  0.0548118638494
Iteration :  96   Loss :  0.0542338978256
Iteration :  97   Loss :  0.0536620261892
Iteration :  98   Loss :  0.0530961846775
Iteration :  99   Loss :  0.0525363097056
[-0.00059892  0.00048196 -0.00056809 ...,  0.00012075  0.00015931
  0.00035386]
CROSS VALIDATION 6
Iteration :  0   Loss :  20.9031987349
Iteration :  1   Loss :  1.46719058523
Iteration :  2   Loss :  0.151981928301
Iteration :  3   Loss :  0.15037934841
Iteration :  4   Loss :  0.148793666991
Iteration :  5   Loss :  0.147224705857
Iteration :  6   Loss :  0.1456722887
Iteration :  7   Loss :  0.144136241072
Iteration :  8   Loss :  0.142616390363
Iteration :  9   Loss :  0.141112565785
Iteration :  10   Loss :  0.139624598348
Iteration :  11   Loss :  0.138152320847
Iteration :  12   Loss :  0.136695567839
Iteration :  13   Loss :  0.135254175625
Iteration :  14   Loss :  0.133827982232
Iteration :  15   Loss :  0.132416827396
Iteration :  16   Loss :  0.131020552542
Iteration :  17   Loss :  0.129639000767
Iteration :  18   Loss :  0.128272016823
Iteration :  19   Loss :  0.1269194471
Iteration :  20   Loss :  0.125581139606
Iteration :  21   Loss :  0.124256943952
Iteration :  22   Loss :  0.122946711335
Iteration :  23   Loss :  0.121650294521
Iteration :  24   Loss :  0.120367547829
Iteration :  25   Loss :  0.119098327115
Iteration :  26   Loss :  0.117842489752
Iteration :  27   Loss :  0.116599894619
Iteration :  28   Loss :  0.115370402084
Iteration :  29   Loss :  0.114153873985
Iteration :  30   Loss :  0.112950173618
Iteration :  31   Loss :  0.11175916572
Iteration :  32   Loss :  0.110580716456
Iteration :  33   Loss :  0.1094146934
Iteration :  34   Loss :  0.108260965523
Iteration :  35   Loss :  0.10711940318
Iteration :  36   Loss :  0.105989878088
Iteration :  37   Loss :  0.104872263322
Iteration :  38   Loss :  0.103766433292
Iteration :  39   Loss :  0.102672263734
Iteration :  40   Loss :  0.101589631694
Iteration :  41   Loss :  0.100518415512
Iteration :  42   Loss :  0.0994584948156
Iteration :  43   Loss :  0.0984097504976
Iteration :  44   Loss :  0.0973720647085
Iteration :  45   Loss :  0.0963453208413
Iteration :  46   Loss :  0.0953294035183
Iteration :  47   Loss :  0.0943241985786
Iteration :  48   Loss :  0.0933295930651
Iteration :  49   Loss :  0.0923454752115
Iteration :  50   Loss :  0.0913717344305
Iteration :  51   Loss :  0.0904082613004
Iteration :  52   Loss :  0.0894549475536
Iteration :  53   Loss :  0.0885116860641
Iteration :  54   Loss :  0.0875783708353
Iteration :  55   Loss :  0.0866548969886
Iteration :  56   Loss :  0.0857411607511
Iteration :  57   Loss :  0.084837059444
Iteration :  58   Loss :  0.0839424914716
Iteration :  59   Loss :  0.083057356309
Iteration :  60   Loss :  0.0821815544918
Iteration :  61   Loss :  0.081314987604
Iteration :  62   Loss :  0.0804575582675
Iteration :  63   Loss :  0.0796091701309
Iteration :  64   Loss :  0.0787697278591
Iteration :  65   Loss :  0.077939137122
Iteration :  66   Loss :  0.0771173045841
Iteration :  67   Loss :  0.0763041378943
Iteration :  68   Loss :  0.0754995456751
Iteration :  69   Loss :  0.0747034375127
Iteration :  70   Loss :  0.0739157239466
Iteration :  71   Loss :  0.0731363164596
Iteration :  72   Loss :  0.072365127468
Iteration :  73   Loss :  0.0716020703114
Iteration :  74   Loss :  0.0708470592433
Iteration :  75   Loss :  0.0701000094215
Iteration :  76   Loss :  0.0693608368982
Iteration :  77   Loss :  0.0686294586109
Iteration :  78   Loss :  0.0679057923731
Iteration :  79   Loss :  0.0671897568645
Iteration :  80   Loss :  0.0664812716228
Iteration :  81   Loss :  0.0657802570338
Iteration :  82   Loss :  0.0650866343228
Iteration :  83   Loss :  0.0644003255459
Iteration :  84   Loss :  0.0637212535811
Iteration :  85   Loss :  0.0630493421194
Iteration :  86   Loss :  0.0623845156566
Iteration :  87   Loss :  0.0617266994846
Iteration :  88   Loss :  0.0610758196832
Iteration :  89   Loss :  0.0604318031114
Iteration :  90   Loss :  0.0597945773998
Iteration :  91   Loss :  0.0591640709416
Iteration :  92   Loss :  0.0585402128856
Iteration :  93   Loss :  0.0579229331273
Iteration :  94   Loss :  0.0573121623016
Iteration :  95   Loss :  0.0567078317748
Iteration :  96   Loss :  0.0561098736368
Iteration :  97   Loss :  0.0555182206939
Iteration :  98   Loss :  0.0549328064604
Iteration :  99   Loss :  0.0543535651522
[ -1.96457218e-03  -3.89910051e-04  -4.61131823e-04 ...,   1.23317056e-03
   7.19568574e-04   5.90730216e-05]
CROSS VALIDATION 7
Iteration :  0   Loss :  20.9031987349
Iteration :  1   Loss :  1.46719058523
Iteration :  2   Loss :  0.856431497464
Iteration :  3   Loss :  0.159606339622
Iteration :  4   Loss :  0.157923363803
Iteration :  5   Loss :  0.156258134193
Iteration :  6   Loss :  0.154610463668
Iteration :  7   Loss :  0.152980167075
Iteration :  8   Loss :  0.151367061213
Iteration :  9   Loss :  0.149770964814
Iteration :  10   Loss :  0.148191698522
Iteration :  11   Loss :  0.14662908487
Iteration :  12   Loss :  0.145082948264
Iteration :  13   Loss :  0.143553114961
Iteration :  14   Loss :  0.14203941305
Iteration :  15   Loss :  0.140541672433
Iteration :  16   Loss :  0.139059724806
Iteration :  17   Loss :  0.137593403638
Iteration :  18   Loss :  0.136142544156
Iteration :  19   Loss :  0.134706983324
Iteration :  20   Loss :  0.133286559823
Iteration :  21   Loss :  0.131881114038
Iteration :  22   Loss :  0.130490488037
Iteration :  23   Loss :  0.12911452555
Iteration :  24   Loss :  0.127753071957
Iteration :  25   Loss :  0.12640597427
Iteration :  26   Loss :  0.125073081112
Iteration :  27   Loss :  0.123754242702
Iteration :  28   Loss :  0.12244931084
Iteration :  29   Loss :  0.121158138887
Iteration :  30   Loss :  0.119880581751
Iteration :  31   Loss :  0.118616495871
Iteration :  32   Loss :  0.117365739198
Iteration :  33   Loss :  0.116128171182
Iteration :  34   Loss :  0.114903652754
Iteration :  35   Loss :  0.113692046313
Iteration :  36   Loss :  0.112493215707
Iteration :  37   Loss :  0.111307026222
Iteration :  38   Loss :  0.110133344562
Iteration :  39   Loss :  0.108972038838
Iteration :  40   Loss :  0.107822978551
Iteration :  41   Loss :  0.10668603458
Iteration :  42   Loss :  0.105561079162
Iteration :  43   Loss :  0.104447985884
Iteration :  44   Loss :  0.103346629666
Iteration :  45   Loss :  0.102256886745
Iteration :  46   Loss :  0.101178634665
Iteration :  47   Loss :  0.100111752259
Iteration :  48   Loss :  0.0990561196401
Iteration :  49   Loss :  0.0980116181842
Iteration :  50   Loss :  0.0969781305182
Iteration :  51   Loss :  0.0959555405067
Iteration :  52   Loss :  0.094943733239
Iteration :  53   Loss :  0.0939425950159
Iteration :  54   Loss :  0.0929520133373
Iteration :  55   Loss :  0.0919718768893
Iteration :  56   Loss :  0.0910020755316
Iteration :  57   Loss :  0.0900425002854
Iteration :  58   Loss :  0.0890930433212
Iteration :  59   Loss :  0.0881535979462
Iteration :  60   Loss :  0.0872240585928
Iteration :  61   Loss :  0.0863043208066
Iteration :  62   Loss :  0.0853942812344
Iteration :  63   Loss :  0.0844938376132
Iteration :  64   Loss :  0.0836028887579
Iteration :  65   Loss :  0.0827213345506
Iteration :  66   Loss :  0.081849075929
Iteration :  67   Loss :  0.0809860148755
Iteration :  68   Loss :  0.0801320544059
Iteration :  69   Loss :  0.0792870985586
Iteration :  70   Loss :  0.0784510523842
Iteration :  71   Loss :  0.0776238219341
Iteration :  72   Loss :  0.0768053142506
Iteration :  73   Loss :  0.0759954373561
Iteration :  74   Loss :  0.0751941002428
Iteration :  75   Loss :  0.0744012128627
Iteration :  76   Loss :  0.0736166861173
Iteration :  77   Loss :  0.0728404318474
Iteration :  78   Loss :  0.0720723628236
Iteration :  79   Loss :  0.0713123927362
Iteration :  80   Loss :  0.0705604361856
Iteration :  81   Loss :  0.0698164086728
Iteration :  82   Loss :  0.0690802265896
Iteration :  83   Loss :  0.0683518072097
Iteration :  84   Loss :  0.0676310686788
Iteration :  85   Loss :  0.0669179300059
Iteration :  86   Loss :  0.066212311054
Iteration :  87   Loss :  0.0655141325311
Iteration :  88   Loss :  0.0648233159812
Iteration :  89   Loss :  0.0641397837758
Iteration :  90   Loss :  0.0634634591047
Iteration :  91   Loss :  0.0627942659677
Iteration :  92   Loss :  0.0621321291662
Iteration :  93   Loss :  0.0614769742943
Iteration :  94   Loss :  0.0608287277308
Iteration :  95   Loss :  0.0601873166306
Iteration :  96   Loss :  0.059552668917
Iteration :  97   Loss :  0.0589247132731
Iteration :  98   Loss :  0.0583033791342
Iteration :  99   Loss :  0.0576885966794
[-0.00174105 -0.00017524 -0.00031311 ...,  0.00132053  0.00079862
  0.00011892]
CROSS VALIDATION 8
Iteration :  0   Loss :  23.7490674989
Iteration :  1   Loss :  0.889807645667
Iteration :  2   Loss :  0.246945740915
Iteration :  3   Loss :  0.159770714926
Iteration :  4   Loss :  0.158086005844
Iteration :  5   Loss :  0.156419061249
Iteration :  6   Loss :  0.154769693821
Iteration :  7   Loss :  0.153137718219
Iteration :  8   Loss :  0.151522951052
Iteration :  9   Loss :  0.149925210867
Iteration :  10   Loss :  0.14834431812
Iteration :  11   Loss :  0.146780095165
Iteration :  12   Loss :  0.145232366224
Iteration :  13   Loss :  0.143700957377
Iteration :  14   Loss :  0.142185696536
Iteration :  15   Loss :  0.140686413426
Iteration :  16   Loss :  0.139202939571
Iteration :  17   Loss :  0.13773510827
Iteration :  18   Loss :  0.136282754577
Iteration :  19   Loss :  0.13484571529
Iteration :  20   Loss :  0.133423828924
Iteration :  21   Loss :  0.1320169357
Iteration :  22   Loss :  0.13062487752
Iteration :  23   Loss :  0.129247497958
Iteration :  24   Loss :  0.127884642232
Iteration :  25   Loss :  0.126536157197
Iteration :  26   Loss :  0.125201891319
Iteration :  27   Loss :  0.123881694665
Iteration :  28   Loss :  0.12257541888
Iteration :  29   Loss :  0.121282917175
Iteration :  30   Loss :  0.12000404431
Iteration :  31   Loss :  0.118738656573
Iteration :  32   Loss :  0.117486611772
Iteration :  33   Loss :  0.116247769209
Iteration :  34   Loss :  0.115021989675
Iteration :  35   Loss :  0.113809135425
Iteration :  36   Loss :  0.112609070169
Iteration :  37   Loss :  0.111421659051
Iteration :  38   Loss :  0.11024676864
Iteration :  39   Loss :  0.109084266911
Iteration :  40   Loss :  0.107934023231
Iteration :  41   Loss :  0.106795908344
Iteration :  42   Loss :  0.105669794358
Iteration :  43   Loss :  0.104555554728
Iteration :  44   Loss :  0.103453064245
Iteration :  45   Loss :  0.102362199021
Iteration :  46   Loss :  0.10128283647
Iteration :  47   Loss :  0.100214855304
Iteration :  48   Loss :  0.0991581355111
Iteration :  49   Loss :  0.0981125583445
Iteration :  50   Loss :  0.0970780063107
Iteration :  51   Loss :  0.0960543631547
Iteration :  52   Loss :  0.0950415138474
Iteration :  53   Loss :  0.0940393445725
Iteration :  54   Loss :  0.0930477427141
Iteration :  55   Loss :  0.0920665968436
Iteration :  56   Loss :  0.0910957967073
Iteration :  57   Loss :  0.0901352332143
Iteration :  58   Loss :  0.0891847984237
Iteration :  59   Loss :  0.0882443855332
Iteration :  60   Loss :  0.0873138888662
Iteration :  61   Loss :  0.0863932038608
Iteration :  62   Loss :  0.0854822270576
Iteration :  63   Loss :  0.0845808560878
Iteration :  64   Loss :  0.0836889896625
Iteration :  65   Loss :  0.0828065275606
Iteration :  66   Loss :  0.0819333706177
Iteration :  67   Loss :  0.0810694207153
Iteration :  68   Loss :  0.0802145807693
Iteration :  69   Loss :  0.0793687547193
Iteration :  70   Loss :  0.078531847518
Iteration :  71   Loss :  0.0777037651202
Iteration :  72   Loss :  0.0768844144724
Iteration :  73   Loss :  0.0760737035022
Iteration :  74   Loss :  0.0752715411082
Iteration :  75   Loss :  0.0744778371497
Iteration :  76   Loss :  0.0736925024362
Iteration :  77   Loss :  0.0729154487179
Iteration :  78   Loss :  0.0721465886755
Iteration :  79   Loss :  0.0713858359105
Iteration :  80   Loss :  0.0706331049352
Iteration :  81   Loss :  0.0698883111637
Iteration :  82   Loss :  0.0691513709016
Iteration :  83   Loss :  0.0684222013375
Iteration :  84   Loss :  0.0677007205327
Iteration :  85   Loss :  0.0669868474129
Iteration :  86   Loss :  0.0662805017585
Iteration :  87   Loss :  0.0655816041958
Iteration :  88   Loss :  0.0648900761881
Iteration :  89   Loss :  0.0642058400268
Iteration :  90   Loss :  0.0635288188228
Iteration :  91   Loss :  0.0628589364975
Iteration :  92   Loss :  0.0621961177749
Iteration :  93   Loss :  0.0615402881724
Iteration :  94   Loss :  0.060891373993
Iteration :  95   Loss :  0.0602493023166
Iteration :  96   Loss :  0.0596140009923
Iteration :  97   Loss :  0.0589853986298
Iteration :  98   Loss :  0.0583634245915
Iteration :  99   Loss :  0.0577480089849
[ -1.33364724e-03   3.28009760e-04  -8.62335574e-04 ...,   1.06564589e-03
  -1.11058933e-04   5.74931010e-05]
CROSS VALIDATION 9
Iteration :  0   Loss :  11.1637142389
Iteration :  1   Loss :  0.5463922368
Iteration :  2   Loss :  0.158232001158
Iteration :  3   Loss :  0.156563517109
Iteration :  4   Loss :  0.154912626461
Iteration :  5   Loss :  0.153279143699
Iteration :  6   Loss :  0.151662885266
Iteration :  7   Loss :  0.150063669538
Iteration :  8   Loss :  0.148481316808
Iteration :  9   Loss :  0.146915649264
Iteration :  10   Loss :  0.145366490967
Iteration :  11   Loss :  0.143833667836
Iteration :  12   Loss :  0.142317007624
Iteration :  13   Loss :  0.140816339899
Iteration :  14   Loss :  0.139331496029
Iteration :  15   Loss :  0.137862309158
Iteration :  16   Loss :  0.13640861419
Iteration :  17   Loss :  0.134970247771
Iteration :  18   Loss :  0.133547048267
Iteration :  19   Loss :  0.132138855751
Iteration :  20   Loss :  0.130745511981
Iteration :  21   Loss :  0.129366860383
Iteration :  22   Loss :  0.128002746036
Iteration :  23   Loss :  0.12665301565
Iteration :  24   Loss :  0.125317517553
Iteration :  25   Loss :  0.123996101673
Iteration :  26   Loss :  0.122688619518
Iteration :  27   Loss :  0.121394924165
Iteration :  28   Loss :  0.120114870237
Iteration :  29   Loss :  0.118848313892
Iteration :  30   Loss :  0.117595112804
Iteration :  31   Loss :  0.116355126148
Iteration :  32   Loss :  0.115128214584
Iteration :  33   Loss :  0.113914240241
Iteration :  34   Loss :  0.112713066702
Iteration :  35   Loss :  0.111524558988
Iteration :  36   Loss :  0.110348583545
Iteration :  37   Loss :  0.109185008224
Iteration :  38   Loss :  0.108033702273
Iteration :  39   Loss :  0.106894536316
Iteration :  40   Loss :  0.105767382343
Iteration :  41   Loss :  0.104652113693
Iteration :  42   Loss :  0.10354860504
Iteration :  43   Loss :  0.102456732381
Iteration :  44   Loss :  0.10137637302
Iteration :  45   Loss :  0.100307405554
Iteration :  46   Loss :  0.0992497098606
Iteration :  47   Loss :  0.0982031670844
Iteration :  48   Loss :  0.0971676596229
Iteration :  49   Loss :  0.0961430711137
Iteration :  50   Loss :  0.0951292864216
Iteration :  51   Loss :  0.0941261916251
Iteration :  52   Loss :  0.0931336740043
Iteration :  53   Loss :  0.0921516220276
Iteration :  54   Loss :  0.0911799253397
Iteration :  55   Loss :  0.0902184747487
Iteration :  56   Loss :  0.0892671622143
Iteration :  57   Loss :  0.0883258808352
Iteration :  58   Loss :  0.0873945248376
Iteration :  59   Loss :  0.0864729895628
Iteration :  60   Loss :  0.0855611714558
Iteration :  61   Loss :  0.0846589680535
Iteration :  62   Loss :  0.0837662779733
Iteration :  63   Loss :  0.0828830009015
Iteration :  64   Loss :  0.0820090375822
Iteration :  65   Loss :  0.0811442898062
Iteration :  66   Loss :  0.0802886603999
Iteration :  67   Loss :  0.0794420532141
Iteration :  68   Loss :  0.0786043731137
Iteration :  69   Loss :  0.0777755259667
Iteration :  70   Loss :  0.0769554186336
Iteration :  71   Loss :  0.076143958957
Iteration :  72   Loss :  0.0753410557515
Iteration :  73   Loss :  0.0745466187928
Iteration :  74   Loss :  0.0737605588084
Iteration :  75   Loss :  0.0729827874668
Iteration :  76   Loss :  0.0722132173681
Iteration :  77   Loss :  0.071451762034
Iteration :  78   Loss :  0.0706983358979
Iteration :  79   Loss :  0.0699528542957
Iteration :  80   Loss :  0.0692152334559
Iteration :  81   Loss :  0.0684853904903
Iteration :  82   Loss :  0.0677632433848
Iteration :  83   Loss :  0.0670487109902
Iteration :  84   Loss :  0.0663417130126
Iteration :  85   Loss :  0.0656421700053
Iteration :  86   Loss :  0.0649500033588
Iteration :  87   Loss :  0.064265135293
Iteration :  88   Loss :  0.0635874888475
Iteration :  89   Loss :  0.0629169878737
Iteration :  90   Loss :  0.0622535570258
Iteration :  91   Loss :  0.0615971217527
Iteration :  92   Loss :  0.0609476082892
Iteration :  93   Loss :  0.0603049436479
Iteration :  94   Loss :  0.0596690556112
Iteration :  95   Loss :  0.0590398727229
Iteration :  96   Loss :  0.0584173242803
Iteration :  97   Loss :  0.0578013403261
Iteration :  98   Loss :  0.0571918516408
Iteration :  99   Loss :  0.0565887897349
[-0.00176885 -0.00015941 -0.00050168 ...,  0.00121157  0.00096236
  0.00011811]
CROSS VALIDATION 10
Iteration :  0   Loss :  13.4896867314
Iteration :  1   Loss :  11.7174513638
Iteration :  2   Loss :  0.161108805661
Iteration :  3   Loss :  0.159409987025
Iteration :  4   Loss :  0.157729081654
Iteration :  5   Loss :  0.156065900661
Iteration :  6   Loss :  0.15442025715
Iteration :  7   Loss :  0.152791966198
Iteration :  8   Loss :  0.151180844828
Iteration :  9   Loss :  0.149586711996
Iteration :  10   Loss :  0.148009388565
Iteration :  11   Loss :  0.146448697288
Iteration :  12   Loss :  0.144904462787
Iteration :  13   Loss :  0.143376511531
Iteration :  14   Loss :  0.141864671823
Iteration :  15   Loss :  0.140368773772
Iteration :  16   Loss :  0.138888649282
Iteration :  17   Loss :  0.137424132028
Iteration :  18   Loss :  0.135975057437
Iteration :  19   Loss :  0.134541262676
Iteration :  20   Loss :  0.133122586624
Iteration :  21   Loss :  0.131718869861
Iteration :  22   Loss :  0.13032995465
Iteration :  23   Loss :  0.128955684914
Iteration :  24   Loss :  0.127595906224
Iteration :  25   Loss :  0.126250465777
Iteration :  26   Loss :  0.124919212385
Iteration :  27   Loss :  0.12360199645
Iteration :  28   Loss :  0.122298669955
Iteration :  29   Loss :  0.121009086441
Iteration :  30   Loss :  0.119733100995
Iteration :  31   Loss :  0.118470570232
Iteration :  32   Loss :  0.117221352279
Iteration :  33   Loss :  0.115985306756
Iteration :  34   Loss :  0.114762294769
Iteration :  35   Loss :  0.113552178883
Iteration :  36   Loss :  0.112354823115
Iteration :  37   Loss :  0.111170092916
Iteration :  38   Loss :  0.109997855155
Iteration :  39   Loss :  0.108837978105
Iteration :  40   Loss :  0.107690331428
Iteration :  41   Loss :  0.106554786159
Iteration :  42   Loss :  0.105431214696
Iteration :  43   Loss :  0.10431949078
Iteration :  44   Loss :  0.103219489484
Iteration :  45   Loss :  0.102131087198
Iteration :  46   Loss :  0.101054161616
Iteration :  47   Loss :  0.0999885917216
Iteration :  48   Loss :  0.0989342577742
Iteration :  49   Loss :  0.0978910412959
Iteration :  50   Loss :  0.096858825058
Iteration :  51   Loss :  0.095837493068
Iteration :  52   Loss :  0.0948269305565
Iteration :  53   Loss :  0.0938270239643
Iteration :  54   Loss :  0.0928376609296
Iteration :  55   Loss :  0.0918587302754
Iteration :  56   Loss :  0.0908901219969
Iteration :  57   Loss :  0.0899317272496
Iteration :  58   Loss :  0.0889834383363
Iteration :  59   Loss :  0.0880451486957
Iteration :  60   Loss :  0.0871167528901
Iteration :  61   Loss :  0.0861981465933
Iteration :  62   Loss :  0.0852892265797
Iteration :  63   Loss :  0.0843898907117
Iteration :  64   Loss :  0.0835000379289
Iteration :  65   Loss :  0.0826195682365
Iteration :  66   Loss :  0.0817483826942
Iteration :  67   Loss :  0.0808863834048
Iteration :  68   Loss :  0.0800334735034
Iteration :  69   Loss :  0.0791895571467
Iteration :  70   Loss :  0.0783545395018
Iteration :  71   Loss :  0.0775283267359
Iteration :  72   Loss :  0.0767108260056
Iteration :  73   Loss :  0.0759019454463
Iteration :  74   Loss :  0.0751015941625
Iteration :  75   Loss :  0.0743096822167
Iteration :  76   Loss :  0.0735261206201
Iteration :  77   Loss :  0.0727508213219
Iteration :  78   Loss :  0.0719836972001
Iteration :  79   Loss :  0.0712246620511
Iteration :  80   Loss :  0.0704736305804
Iteration :  81   Loss :  0.069730518393
Iteration :  82   Loss :  0.0689952419835
Iteration :  83   Loss :  0.0682677187274
Iteration :  84   Loss :  0.067547866871
Iteration :  85   Loss :  0.0668356055231
Iteration :  86   Loss :  0.0661308546453
Iteration :  87   Loss :  0.0654335350429
Iteration :  88   Loss :  0.0647435683567
Iteration :  89   Loss :  0.0640608770534
Iteration :  90   Loss :  0.0633853844176
Iteration :  91   Loss :  0.0627170145425
Iteration :  92   Loss :  0.062055692322
Iteration :  93   Loss :  0.0614013434416
Iteration :  94   Loss :  0.0607538943708
Iteration :  95   Loss :  0.0601132723542
Iteration :  96   Loss :  0.0594794054036
Iteration :  97   Loss :  0.0588522222899
Iteration :  98   Loss :  0.0582316525352
Iteration :  99   Loss :  0.0576176264046
[ -1.17396654e-03  -7.56003514e-05  -8.37611053e-04 ...,   1.37255621e-03
   5.80550513e-04   2.35996081e-04]
CROSS VALIDATION 11
Iteration :  0   Loss :  20.9032219654
Iteration :  1   Loss :  1.46687840888
Iteration :  2   Loss :  0.855903857645
Iteration :  3   Loss :  0.159610122773
Iteration :  4   Loss :  0.157927107062
Iteration :  5   Loss :  0.156261837981
Iteration :  6   Loss :  0.154614128401
Iteration :  7   Loss :  0.152983793165
Iteration :  8   Loss :  0.151370649068
Iteration :  9   Loss :  0.149774514837
Iteration :  10   Loss :  0.148195211111
Iteration :  11   Loss :  0.146632560421
Iteration :  12   Loss :  0.145086387166
Iteration :  13   Loss :  0.143556517602
Iteration :  14   Loss :  0.142042779812
Iteration :  15   Loss :  0.140545003694
Iteration :  16   Loss :  0.13906302094
Iteration :  17   Loss :  0.137596665016
Iteration :  18   Loss :  0.136145771144
Iteration :  19   Loss :  0.134710176285
Iteration :  20   Loss :  0.133289719116
Iteration :  21   Loss :  0.131884240018
Iteration :  22   Loss :  0.130493581054
Iteration :  23   Loss :  0.129117585952
Iteration :  24   Loss :  0.12775610009
Iteration :  25   Loss :  0.126408970472
Iteration :  26   Loss :  0.12507604572
Iteration :  27   Loss :  0.12375717605
Iteration :  28   Loss :  0.122452213257
Iteration :  29   Loss :  0.121161010699
Iteration :  30   Loss :  0.119883423282
Iteration :  31   Loss :  0.118619307439
Iteration :  32   Loss :  0.117368521119
Iteration :  33   Loss :  0.116130923769
Iteration :  34   Loss :  0.114906376317
Iteration :  35   Loss :  0.113694741157
Iteration :  36   Loss :  0.112495882135
Iteration :  37   Loss :  0.111309664533
Iteration :  38   Loss :  0.110135955054
Iteration :  39   Loss :  0.108974621803
Iteration :  40   Loss :  0.10782553428
Iteration :  41   Loss :  0.10668856336
Iteration :  42   Loss :  0.105563581277
Iteration :  43   Loss :  0.104450461616
Iteration :  44   Loss :  0.103349079292
Iteration :  45   Loss :  0.102259310541
Iteration :  46   Loss :  0.101181032903
Iteration :  47   Loss :  0.100114125209
Iteration :  48   Loss :  0.0990584675682
Iteration :  49   Loss :  0.0980139413545
Iteration :  50   Loss :  0.0969804291917
Iteration :  51   Loss :  0.0959578149418
Iteration :  52   Loss :  0.0949459836912
Iteration :  53   Loss :  0.0939448217381
Iteration :  54   Loss :  0.0929542165798
Iteration :  55   Loss :  0.0919740568995
Iteration :  56   Loss :  0.0910042325546
Iteration :  57   Loss :  0.0900446345637
Iteration :  58   Loss :  0.0890951550945
Iteration :  59   Loss :  0.0881556874518
Iteration :  60   Loss :  0.0872261260655
Iteration :  61   Loss :  0.0863063664787
Iteration :  62   Loss :  0.0853963053359
Iteration :  63   Loss :  0.0844958403714
Iteration :  64   Loss :  0.0836048703979
Iteration :  65   Loss :  0.0827232952951
Iteration :  66   Loss :  0.0818510159984
Iteration :  67   Loss :  0.0809879344878
Iteration :  68   Loss :  0.0801339537767
Iteration :  69   Loss :  0.0792889779015
Iteration :  70   Loss :  0.0784529119102
Iteration :  71   Loss :  0.0776256618523
Iteration :  72   Loss :  0.0768071347677
Iteration :  73   Loss :  0.0759972386766
Iteration :  74   Loss :  0.0751958825693
Iteration :  75   Loss :  0.0744029763954
Iteration :  76   Loss :  0.0736184310543
Iteration :  77   Loss :  0.0728421583848
Iteration :  78   Loss :  0.0720740711555
Iteration :  79   Loss :  0.0713140830545
Iteration :  80   Loss :  0.0705621086803
Iteration :  81   Loss :  0.0698180635318
Iteration :  82   Loss :  0.0690818639989
Iteration :  83   Loss :  0.0683534273532
Iteration :  84   Loss :  0.0676326717386
Iteration :  85   Loss :  0.0669195161622
Iteration :  86   Loss :  0.066213880485
Iteration :  87   Loss :  0.0655156854132
Iteration :  88   Loss :  0.0648248524889
Iteration :  89   Loss :  0.0641413040817
Iteration :  90   Loss :  0.0634649633796
Iteration :  91   Loss :  0.0627957543808
Iteration :  92   Loss :  0.0621336018847
Iteration :  93   Loss :  0.0614784314836
Iteration :  94   Loss :  0.0608301695547
Iteration :  95   Loss :  0.0601887432512
Iteration :  96   Loss :  0.0595540804945
Iteration :  97   Loss :  0.0589261099662
Iteration :  98   Loss :  0.0583047610997
Iteration :  99   Loss :  0.0576899640728
[-0.00174109 -0.00017526 -0.00031315 ...,  0.00132053  0.00079865
  0.00011892]
CROSS VALIDATION 12
Iteration :  0   Loss :  19.3841718944
Iteration :  1   Loss :  4.61792316214
Iteration :  2   Loss :  0.173201894529
Iteration :  3   Loss :  0.171375559804
Iteration :  4   Loss :  0.169568482943
Iteration :  5   Loss :  0.167780460881
Iteration :  6   Loss :  0.166011292693
Iteration :  7   Loss :  0.164260779574
Iteration :  8   Loss :  0.162528724815
Iteration :  9   Loss :  0.160814933782
Iteration :  10   Loss :  0.15911921389
Iteration :  11   Loss :  0.157441374588
Iteration :  12   Loss :  0.155781227335
Iteration :  13   Loss :  0.154138585574
Iteration :  14   Loss :  0.152513264719
Iteration :  15   Loss :  0.150905082129
Iteration :  16   Loss :  0.149313857089
Iteration :  17   Loss :  0.147739410788
Iteration :  18   Loss :  0.146181566303
Iteration :  19   Loss :  0.144640148576
Iteration :  20   Loss :  0.143114984393
Iteration :  21   Loss :  0.141605902368
Iteration :  22   Loss :  0.140112732923
Iteration :  23   Loss :  0.138635308267
Iteration :  24   Loss :  0.137173462377
Iteration :  25   Loss :  0.135727030984
Iteration :  26   Loss :  0.134295851548
Iteration :  27   Loss :  0.132879763244
Iteration :  28   Loss :  0.131478606943
Iteration :  29   Loss :  0.130092225194
Iteration :  30   Loss :  0.128720462206
Iteration :  31   Loss :  0.127363163832
Iteration :  32   Loss :  0.126020177547
Iteration :  33   Loss :  0.124691352439
Iteration :  34   Loss :  0.123376539183
Iteration :  35   Loss :  0.122075590032
Iteration :  36   Loss :  0.120788358794
Iteration :  37   Loss :  0.11951470082
Iteration :  38   Loss :  0.118254472987
Iteration :  39   Loss :  0.11700753368
Iteration :  40   Loss :  0.115773742777
Iteration :  41   Loss :  0.114552961634
Iteration :  42   Loss :  0.113345053071
Iteration :  43   Loss :  0.11214988135
Iteration :  44   Loss :  0.110967312168
Iteration :  45   Loss :  0.109797212637
Iteration :  46   Loss :  0.108639451271
Iteration :  47   Loss :  0.107493897968
Iteration :  48   Loss :  0.106360424
Iteration :  49   Loss :  0.105238901997
Iteration :  50   Loss :  0.10412920593
Iteration :  51   Loss :  0.103031211101
Iteration :  52   Loss :  0.101944794124
Iteration :  53   Loss :  0.100869832918
Iteration :  54   Loss :  0.0998062066852
Iteration :  55   Loss :  0.0987537959046
Iteration :  56   Loss :  0.0977124823141
Iteration :  57   Loss :  0.096682148899
Iteration :  58   Loss :  0.0956626798782
Iteration :  59   Loss :  0.0946539606919
Iteration :  60   Loss :  0.0936558779877
Iteration :  61   Loss :  0.092668319609
Iteration :  62   Loss :  0.0916911745815
Iteration :  63   Loss :  0.0907243331012
Iteration :  64   Loss :  0.0897676865219
Iteration :  65   Loss :  0.088821127343
Iteration :  66   Loss :  0.0878845491976
Iteration :  67   Loss :  0.0869578468402
Iteration :  68   Loss :  0.086040916135
Iteration :  69   Loss :  0.0851336540446
Iteration :  70   Loss :  0.0842359586178
Iteration :  71   Loss :  0.0833477289785
Iteration :  72   Loss :  0.0824688653143
Iteration :  73   Loss :  0.0815992688654
Iteration :  74   Loss :  0.080738841913
Iteration :  75   Loss :  0.0798874877692
Iteration :  76   Loss :  0.0790451107652
Iteration :  77   Loss :  0.0782116162413
Iteration :  78   Loss :  0.0773869105358
Iteration :  79   Loss :  0.0765709009746
Iteration :  80   Loss :  0.0757634958608
Iteration :  81   Loss :  0.0749646044645
Iteration :  82   Loss :  0.0741741370124
Iteration :  83   Loss :  0.0733920046779
Iteration :  84   Loss :  0.072618119571
Iteration :  85   Loss :  0.0718523947284
Iteration :  86   Loss :  0.0710947441039
Iteration :  87   Loss :  0.0703450825585
Iteration :  88   Loss :  0.0696033258512
Iteration :  89   Loss :  0.0688693906289
Iteration :  90   Loss :  0.0681431944176
Iteration :  91   Loss :  0.0674246556132
Iteration :  92   Loss :  0.0667136934716
Iteration :  93   Loss :  0.0660102281005
Iteration :  94   Loss :  0.0653141804499
Iteration :  95   Loss :  0.0646254723032
Iteration :  96   Loss :  0.0639440262688
Iteration :  97   Loss :  0.0632697657711
Iteration :  98   Loss :  0.0626026150417
Iteration :  99   Loss :  0.0619424991116
[ -2.61881943e-03  -3.34410032e-04  -1.08376726e-03 ...,   1.23010352e-03
   7.48243280e-04   5.94389296e-05]
CROSS VALIDATION 13
Iteration :  0   Loss :  20.9058446834
Iteration :  1   Loss :  1.46624068706
Iteration :  2   Loss :  0.857006131776
Iteration :  3   Loss :  0.159611451173
Iteration :  4   Loss :  0.157928421455
Iteration :  5   Loss :  0.156263138515
Iteration :  6   Loss :  0.154615415221
Iteration :  7   Loss :  0.152985066416
Iteration :  8   Loss :  0.151371908893
Iteration :  9   Loss :  0.149775761378
Iteration :  10   Loss :  0.148196444508
Iteration :  11   Loss :  0.146633780812
Iteration :  12   Loss :  0.145087594689
Iteration :  13   Loss :  0.143557712392
Iteration :  14   Loss :  0.142043962003
Iteration :  15   Loss :  0.14054617342
Iteration :  16   Loss :  0.139064178331
Iteration :  17   Loss :  0.137597810203
Iteration :  18   Loss :  0.136146904256
Iteration :  19   Loss :  0.134711297448
Iteration :  20   Loss :  0.133290828457
Iteration :  21   Loss :  0.131885337662
Iteration :  22   Loss :  0.130494667124
Iteration :  23   Loss :  0.12911866057
Iteration :  24   Loss :  0.127757163376
Iteration :  25   Loss :  0.126410022547
Iteration :  26   Loss :  0.125077086701
Iteration :  27   Loss :  0.123758206054
Iteration :  28   Loss :  0.1224532324
Iteration :  29   Loss :  0.121162019096
Iteration :  30   Loss :  0.119884421045
Iteration :  31   Loss :  0.118620294682
Iteration :  32   Loss :  0.117369497952
Iteration :  33   Loss :  0.116131890302
Iteration :  34   Loss :  0.114907332658
Iteration :  35   Loss :  0.113695687413
Iteration :  36   Loss :  0.112496818414
Iteration :  37   Loss :  0.11131059094
Iteration :  38   Loss :  0.110136871691
Iteration :  39   Loss :  0.108975528775
Iteration :  40   Loss :  0.107826431689
Iteration :  41   Loss :  0.106689451306
Iteration :  42   Loss :  0.10556445986
Iteration :  43   Loss :  0.104451330935
Iteration :  44   Loss :  0.103349939444
Iteration :  45   Loss :  0.102260161623
Iteration :  46   Loss :  0.101181875011
Iteration :  47   Loss :  0.100114958437
Iteration :  48   Loss :  0.0990592920105
Iteration :  49   Loss :  0.0980147571034
Iteration :  50   Loss :  0.0969812363389
Iteration :  51   Loss :  0.095958613578
Iteration :  52   Loss :  0.0949467739061
Iteration :  53   Loss :  0.0939456036206
Iteration :  54   Loss :  0.0929549902177
Iteration :  55   Loss :  0.0919748223798
Iteration :  56   Loss :  0.0910049899632
Iteration :  57   Loss :  0.0900453839857
Iteration :  58   Loss :  0.0890958966142
Iteration :  59   Loss :  0.0881564211525
Iteration :  60   Loss :  0.0872268520297
Iteration :  61   Loss :  0.086307084788
Iteration :  62   Loss :  0.0853970160709
Iteration :  63   Loss :  0.084496543612
Iteration :  64   Loss :  0.0836055662232
Iteration :  65   Loss :  0.0827239837833
Iteration :  66   Loss :  0.0818516972268
Iteration :  67   Loss :  0.0809886085329
Iteration :  68   Loss :  0.0801346207143
Iteration :  69   Loss :  0.0792896378065
Iteration :  70   Loss :  0.0784535648569
Iteration :  71   Loss :  0.0776263079139
Iteration :  72   Loss :  0.0768077740169
Iteration :  73   Loss :  0.0759978711852
Iteration :  74   Loss :  0.0751965084084
Iteration :  75   Loss :  0.0744035956353
Iteration :  76   Loss :  0.0736190437646
Iteration :  77   Loss :  0.0728427646344
Iteration :  78   Loss :  0.0720746710124
Iteration :  79   Loss :  0.0713146765862
Iteration :  80   Loss :  0.0705626959535
Iteration :  81   Loss :  0.0698186446124
Iteration :  82   Loss :  0.0690824389523
Iteration :  83   Loss :  0.068353996244
Iteration :  84   Loss :  0.0676332346308
Iteration :  85   Loss :  0.0669200731189
Iteration :  86   Loss :  0.0662144315689
Iteration :  87   Loss :  0.0655162306861
Iteration :  88   Loss :  0.0648253920121
Iteration :  89   Loss :  0.0641418379159
Iteration :  90   Loss :  0.0634654915848
Iteration :  91   Loss :  0.0627962770163
Iteration :  92   Loss :  0.0621341190093
Iteration :  93   Loss :  0.0614789431554
Iteration :  94   Loss :  0.0608306758311
Iteration :  95   Loss :  0.0601892441891
Iteration :  96   Loss :  0.0595545761502
Iteration :  97   Loss :  0.0589266003955
Iteration :  98   Loss :  0.0583052463577
Iteration :  99   Loss :  0.0576904442139
[-0.00174106 -0.00017525 -0.00031315 ...,  0.00132053  0.00079865
  0.00011892]
CROSS VALIDATION 14
Iteration :  0   Loss :  20.9058446834
Iteration :  1   Loss :  0.145528033544
Iteration :  2   Loss :  0.14399350702
Iteration :  3   Loss :  0.142475161377
Iteration :  4   Loss :  0.140972825993
Iteration :  5   Loss :  0.139486332049
Iteration :  6   Loss :  0.138015512503
Iteration :  7   Loss :  0.136560202076
Iteration :  8   Loss :  0.135120237232
Iteration :  9   Loss :  0.133695456158
Iteration :  10   Loss :  0.132285698749
Iteration :  11   Loss :  0.130890806586
Iteration :  12   Loss :  0.129510622923
Iteration :  13   Loss :  0.128144992665
Iteration :  14   Loss :  0.126793762354
Iteration :  15   Loss :  0.125456780147
Iteration :  16   Loss :  0.124133895807
Iteration :  17   Loss :  0.122824960676
Iteration :  18   Loss :  0.121529827668
Iteration :  19   Loss :  0.120248351244
Iteration :  20   Loss :  0.118980387403
Iteration :  21   Loss :  0.11772579366
Iteration :  22   Loss :  0.116484429034
Iteration :  23   Loss :  0.115256154031
Iteration :  24   Loss :  0.114040830625
Iteration :  25   Loss :  0.112838322248
Iteration :  26   Loss :  0.111648493772
Iteration :  27   Loss :  0.110471211493
Iteration :  28   Loss :  0.109306343117
Iteration :  29   Loss :  0.108153757745
Iteration :  30   Loss :  0.107013325858
Iteration :  31   Loss :  0.105884919303
Iteration :  32   Loss :  0.10476841128
Iteration :  33   Loss :  0.103663676322
Iteration :  34   Loss :  0.102570590289
Iteration :  35   Loss :  0.101489030348
Iteration :  36   Loss :  0.100418874962
Iteration :  37   Loss :  0.0993600038751
Iteration :  38   Loss :  0.098312298099
Iteration :  39   Loss :  0.0972756399009
Iteration :  40   Loss :  0.0962499127891
Iteration :  41   Loss :  0.0952350015003
Iteration :  42   Loss :  0.0942307919867
Iteration :  43   Loss :  0.093237171403
Iteration :  44   Loss :  0.0922540280936
Iteration :  45   Loss :  0.0912812515806
Iteration :  46   Loss :  0.0903187325507
Iteration :  47   Loss :  0.0893663628436
Iteration :  48   Loss :  0.0884240354393
Iteration :  49   Loss :  0.0874916444463
Iteration :  50   Loss :  0.0865690850897
Iteration :  51   Loss :  0.0856562536993
Iteration :  52   Loss :  0.0847530476983
Iteration :  53   Loss :  0.0838593655913
Iteration :  54   Loss :  0.0829751069531
Iteration :  55   Loss :  0.0821001724177
Iteration :  56   Loss :  0.0812344636666
Iteration :  57   Loss :  0.0803778834182
Iteration :  58   Loss :  0.0795303354165
Iteration :  59   Loss :  0.0786917244207
Iteration :  60   Loss :  0.0778619561941
Iteration :  61   Loss :  0.0770409374938
Iteration :  62   Loss :  0.07622857606
Iteration :  63   Loss :  0.0754247806057
Iteration :  64   Loss :  0.0746294608068
Iteration :  65   Loss :  0.0738425272912
Iteration :  66   Loss :  0.0730638916295
Iteration :  67   Loss :  0.0722934663245
Iteration :  68   Loss :  0.0715311648019
Iteration :  69   Loss :  0.0707769014
Iteration :  70   Loss :  0.0700305913605
Iteration :  71   Loss :  0.0692921508189
Iteration :  72   Loss :  0.0685614967949
Iteration :  73   Loss :  0.0678385471833
Iteration :  74   Loss :  0.0671232207445
Iteration :  75   Loss :  0.0664154370957
Iteration :  76   Loss :  0.0657151167017
Iteration :  77   Loss :  0.0650221808657
Iteration :  78   Loss :  0.064336551721
Iteration :  79   Loss :  0.063658152222
Iteration :  80   Loss :  0.0629869061352
Iteration :  81   Loss :  0.0623227380313
Iteration :  82   Loss :  0.0616655732761
Iteration :  83   Loss :  0.0610153380226
Iteration :  84   Loss :  0.0603719592023
Iteration :  85   Loss :  0.0597353645173
Iteration :  86   Loss :  0.0591054824319
Iteration :  87   Loss :  0.0584822421649
Iteration :  88   Loss :  0.0578655736813
Iteration :  89   Loss :  0.0572554076847
Iteration :  90   Loss :  0.0566516756094
Iteration :  91   Loss :  0.0560543096125
Iteration :  92   Loss :  0.0554632425667
Iteration :  93   Loss :  0.0548784080525
Iteration :  94   Loss :  0.0542997403506
Iteration :  95   Loss :  0.0537271744349
Iteration :  96   Loss :  0.0531606459647
Iteration :  97   Loss :  0.052600091278
Iteration :  98   Loss :  0.052045447384
Iteration :  99   Loss :  0.0514966519561
[ -1.58638743e-03  -2.06751907e-04  -2.95179200e-04 ...,   9.77016005e-04
   4.63764254e-04  -4.90620906e-07]
CROSS VALIDATION 15
Iteration :  0   Loss :  20.9058446834
Iteration :  1   Loss :  1.46624068706
Iteration :  2   Loss :  0.857185509068
Iteration :  3   Loss :  0.159613018824
Iteration :  4   Loss :  0.157929972576
Iteration :  5   Loss :  0.15626467328
Iteration :  6   Loss :  0.154616933803
Iteration :  7   Loss :  0.152986568985
Iteration :  8   Loss :  0.151373395618
Iteration :  9   Loss :  0.149777232426
Iteration :  10   Loss :  0.148197900044
Iteration :  11   Loss :  0.146635221
Iteration :  12   Loss :  0.145089019691
Iteration :  13   Loss :  0.143559122368
Iteration :  14   Loss :  0.142045357112
Iteration :  15   Loss :  0.140547553818
Iteration :  16   Loss :  0.139065544174
Iteration :  17   Loss :  0.137599161643
Iteration :  18   Loss :  0.136148241446
Iteration :  19   Loss :  0.134712620538
Iteration :  20   Loss :  0.133292137596
Iteration :  21   Loss :  0.131886632996
Iteration :  22   Loss :  0.130495948799
Iteration :  23   Loss :  0.129119928731
Iteration :  24   Loss :  0.127758418165
Iteration :  25   Loss :  0.126411264104
Iteration :  26   Loss :  0.125078315167
Iteration :  27   Loss :  0.123759421566
Iteration :  28   Loss :  0.122454435095
Iteration :  29   Loss :  0.121163209109
Iteration :  30   Loss :  0.119885598511
Iteration :  31   Loss :  0.118621459731
Iteration :  32   Loss :  0.117370650716
Iteration :  33   Loss :  0.116133030911
Iteration :  34   Loss :  0.114908461239
Iteration :  35   Loss :  0.113696804095
Iteration :  36   Loss :  0.112497923321
Iteration :  37   Loss :  0.111311684195
Iteration :  38   Loss :  0.110137953419
Iteration :  39   Loss :  0.108976599097
Iteration :  40   Loss :  0.107827490724
Iteration :  41   Loss :  0.106690499174
Iteration :  42   Loss :  0.105565496679
Iteration :  43   Loss :  0.104452356821
Iteration :  44   Loss :  0.103350954513
Iteration :  45   Loss :  0.102261165989
Iteration :  46   Loss :  0.101182868786
Iteration :  47   Loss :  0.100115941733
Iteration :  48   Loss :  0.099060264938
Iteration :  49   Loss :  0.0980157197718
Iteration :  50   Loss :  0.0969821888565
Iteration :  51   Loss :  0.0959595560517
Iteration :  52   Loss :  0.0949477064419
Iteration :  53   Loss :  0.0939465263232
Iteration :  54   Loss :  0.0929559031908
Iteration :  55   Loss :  0.091975725726
Iteration :  56   Loss :  0.0910058837841
Iteration :  57   Loss :  0.0900462683817
Iteration :  58   Loss :  0.0890967716846
Iteration :  59   Loss :  0.0881572869957
Iteration :  60   Loss :  0.087227708743
Iteration :  61   Loss :  0.0863079324676
Iteration :  62   Loss :  0.0853978548121
Iteration :  63   Loss :  0.0844973735091
Iteration :  64   Loss :  0.0836063873694
Iteration :  65   Loss :  0.0827247962708
Iteration :  66   Loss :  0.081852501147
Iteration :  67   Loss :  0.0809894039761
Iteration :  68   Loss :  0.08013540777
Iteration :  69   Loss :  0.0792904165631
Iteration :  70   Loss :  0.0784543354018
Iteration :  71   Loss :  0.0776270703338
Iteration :  72   Loss :  0.0768085283974
Iteration :  73   Loss :  0.0759986176112
Iteration :  74   Loss :  0.0751972469636
Iteration :  75   Loss :  0.0744043264028
Iteration :  76   Loss :  0.0736197668265
Iteration :  77   Loss :  0.0728434800719
Iteration :  78   Loss :  0.072075378906
Iteration :  79   Loss :  0.0713153770154
Iteration :  80   Loss :  0.0705633889969
Iteration :  81   Loss :  0.069819330348
Iteration :  82   Loss :  0.0690831174571
Iteration :  83   Loss :  0.0683546675943
Iteration :  84   Loss :  0.067633898902
Iteration :  85   Loss :  0.0669207303857
Iteration :  86   Loss :  0.0662150819051
Iteration :  87   Loss :  0.0655168741648
Iteration :  88   Loss :  0.0648260287057
Iteration :  89   Loss :  0.0641424678958
Iteration :  90   Loss :  0.0634661149219
Iteration :  91   Loss :  0.0627968937806
Iteration :  92   Loss :  0.06213472927
Iteration :  93   Loss :  0.0614795469812
Iteration :  94   Loss :  0.0608312732898
Iteration :  95   Loss :  0.0601898353479
Iteration :  96   Loss :  0.0595551610756
Iteration :  97   Loss :  0.058927179153
Iteration :  98   Loss :  0.0583058190125
Iteration :  99   Loss :  0.0576910108303
[-0.00174109 -0.00017527 -0.00031316 ...,  0.00132056  0.00079867
  0.00011893]
CROSS VALIDATION 16
Iteration :  0   Loss :  20.9058446834
Iteration :  1   Loss :  0.138098003547
Iteration :  2   Loss :  0.13664182329
Iteration :  3   Loss :  0.135200997787
Iteration :  4   Loss :  0.13377536513
Iteration :  5   Loss :  0.132364765117
Iteration :  6   Loss :  0.130969039236
Iteration :  7   Loss :  0.129588030645
Iteration :  8   Loss :  0.128221584159
Iteration :  9   Loss :  0.126869546225
Iteration :  10   Loss :  0.125531764912
Iteration :  11   Loss :  0.124208089892
Iteration :  12   Loss :  0.122898372418
Iteration :  13   Loss :  0.121602465316
Iteration :  14   Loss :  0.120320222962
Iteration :  15   Loss :  0.119051501266
Iteration :  16   Loss :  0.117796157661
Iteration :  17   Loss :  0.116554051079
Iteration :  18   Loss :  0.115325041943
Iteration :  19   Loss :  0.114108992145
Iteration :  20   Loss :  0.112905765037
Iteration :  21   Loss :  0.111715225407
Iteration :  22   Loss :  0.110537239473
Iteration :  23   Loss :  0.109371674862
Iteration :  24   Loss :  0.108218400597
Iteration :  25   Loss :  0.107077287081
Iteration :  26   Loss :  0.105948206084
Iteration :  27   Loss :  0.10483103073
Iteration :  28   Loss :  0.10372563548
Iteration :  29   Loss :  0.102631896116
Iteration :  30   Loss :  0.101549689733
Iteration :  31   Loss :  0.100478894721
Iteration :  32   Loss :  0.0994193907533
Iteration :  33   Loss :  0.0983710587698
Iteration :  34   Loss :  0.0973337809672
Iteration :  35   Loss :  0.0963074407845
Iteration :  36   Loss :  0.0952919228894
Iteration :  37   Loss :  0.0942871131658
Iteration :  38   Loss :  0.093292898701
Iteration :  39   Loss :  0.0923091677727
Iteration :  40   Loss :  0.091335809837
Iteration :  41   Loss :  0.0903727155153
Iteration :  42   Loss :  0.0894197765826
Iteration :  43   Loss :  0.0884768859548
Iteration :  44   Loss :  0.0875439376773
Iteration :  45   Loss :  0.0866208269125
Iteration :  46   Loss :  0.0857074499283
Iteration :  47   Loss :  0.0848037040865
Iteration :  48   Loss :  0.083909487831
Iteration :  49   Loss :  0.0830247006768
Iteration :  50   Loss :  0.0821492431982
Iteration :  51   Loss :  0.0812830170182
Iteration :  52   Loss :  0.0804259247969
Iteration :  53   Loss :  0.0795778702208
Iteration :  54   Loss :  0.0787387579923
Iteration :  55   Loss :  0.0779084938182
Iteration :  56   Loss :  0.0770869843999
Iteration :  57   Loss :  0.0762741374225
Iteration :  58   Loss :  0.0754698615446
Iteration :  59   Loss :  0.0746740663878
Iteration :  60   Loss :  0.0738866625268
Iteration :  61   Loss :  0.0731075614792
Iteration :  62   Loss :  0.0723366756956
Iteration :  63   Loss :  0.0715739185499
Iteration :  64   Loss :  0.0708192043293
Iteration :  65   Loss :  0.0700724482248
Iteration :  66   Loss :  0.0693335663217
Iteration :  67   Loss :  0.0686024755902
Iteration :  68   Loss :  0.067879093876
Iteration :  69   Loss :  0.0671633398909
Iteration :  70   Loss :  0.066455133204
Iteration :  71   Loss :  0.0657543942327
Iteration :  72   Loss :  0.0650610442331
Iteration :  73   Loss :  0.064375005292
Iteration :  74   Loss :  0.0636962003177
Iteration :  75   Loss :  0.0630245530311
Iteration :  76   Loss :  0.0623599879579
Iteration :  77   Loss :  0.0617024304193
Iteration :  78   Loss :  0.0610518065241
Iteration :  79   Loss :  0.0604080431601
Iteration :  80   Loss :  0.0597710679863
Iteration :  81   Loss :  0.0591408094241
Iteration :  82   Loss :  0.0585171966502
Iteration :  83   Loss :  0.0579001595875
Iteration :  84   Loss :  0.0572896288984
Iteration :  85   Loss :  0.056685535976
Iteration :  86   Loss :  0.0560878129371
Iteration :  87   Loss :  0.0554963926141
Iteration :  88   Loss :  0.0549112085478
Iteration :  89   Loss :  0.0543321949797
Iteration :  90   Loss :  0.0537592868447
Iteration :  91   Loss :  0.0531924197639
Iteration :  92   Loss :  0.052631530037
Iteration :  93   Loss :  0.0520765546357
Iteration :  94   Loss :  0.051527431196
Iteration :  95   Loss :  0.0509840980118
Iteration :  96   Loss :  0.0504464940273
Iteration :  97   Loss :  0.0499145588309
Iteration :  98   Loss :  0.0493882326477
Iteration :  99   Loss :  0.0488674563333
[ -1.34042341e-03  -1.92406214e-04  -2.16929316e-04 ...,   9.68227032e-04
   4.66305487e-04   5.87758677e-05]
CROSS VALIDATION 17
Iteration :  0   Loss :  20.9058446834
Iteration :  1   Loss :  1.46655330317
Iteration :  2   Loss :  0.857666564269
Iteration :  3   Loss :  0.159614445696
Iteration :  4   Loss :  0.157931384402
Iteration :  5   Loss :  0.156266070219
Iteration :  6   Loss :  0.154618316012
Iteration :  7   Loss :  0.152987936619
Iteration :  8   Loss :  0.151374748831
Iteration :  9   Loss :  0.14977857137
Iteration :  10   Loss :  0.14819922487
Iteration :  11   Loss :  0.146636531856
Iteration :  12   Loss :  0.145090316725
Iteration :  13   Loss :  0.143560405725
Iteration :  14   Loss :  0.142046626937
Iteration :  15   Loss :  0.140548810253
Iteration :  16   Loss :  0.13906678736
Iteration :  17   Loss :  0.137600391721
Iteration :  18   Loss :  0.136149458553
Iteration :  19   Loss :  0.134713824811
Iteration :  20   Loss :  0.133293329171
Iteration :  21   Loss :  0.131887812006
Iteration :  22   Loss :  0.130497115377
Iteration :  23   Loss :  0.129121083008
Iteration :  24   Loss :  0.12775956027
Iteration :  25   Loss :  0.126412394167
Iteration :  26   Loss :  0.125079433314
Iteration :  27   Loss :  0.123760527923
Iteration :  28   Loss :  0.122455529786
Iteration :  29   Loss :  0.121164292257
Iteration :  30   Loss :  0.119886670237
Iteration :  31   Loss :  0.118622520156
Iteration :  32   Loss :  0.11737169996
Iteration :  33   Loss :  0.11613406909
Iteration :  34   Loss :  0.114909488472
Iteration :  35   Loss :  0.113697820496
Iteration :  36   Loss :  0.112498929004
Iteration :  37   Loss :  0.111312679274
Iteration :  38   Loss :  0.110138938005
Iteration :  39   Loss :  0.108977573301
Iteration :  40   Loss :  0.107828454656
Iteration :  41   Loss :  0.106691452942
Iteration :  42   Loss :  0.10556644039
Iteration :  43   Loss :  0.10445329058
Iteration :  44   Loss :  0.103351878427
Iteration :  45   Loss :  0.10226208016
Iteration :  46   Loss :  0.101183773317
Iteration :  47   Loss :  0.100116836727
Iteration :  48   Loss :  0.0990611504946
Iteration :  49   Loss :  0.0980165959906
Iteration :  50   Loss :  0.0969830558359
Iteration :  51   Loss :  0.0959604138892
Iteration :  52   Loss :  0.0949485552339
Iteration :  53   Loss :  0.0939473661651
Iteration :  54   Loss :  0.0929567341769
Iteration :  55   Loss :  0.0919765479498
Iteration :  56   Loss :  0.0910066973379
Iteration :  57   Loss :  0.0900470733569
Iteration :  58   Loss :  0.0890975681717
Iteration :  59   Loss :  0.0881580750842
Iteration :  60   Loss :  0.0872284885215
Iteration :  61   Loss :  0.0863087040237
Iteration :  62   Loss :  0.0853986182325
Iteration :  63   Loss :  0.0844981288796
Iteration :  64   Loss :  0.0836071347748
Iteration :  65   Loss :  0.0827255357952
Iteration :  66   Loss :  0.0818532328735
Iteration :  67   Loss :  0.0809901279869
Iteration :  68   Loss :  0.0801361241464
Iteration :  69   Loss :  0.0792911253856
Iteration :  70   Loss :  0.0784550367501
Iteration :  71   Loss :  0.0776277642867
Iteration :  72   Loss :  0.0768092150329
Iteration :  73   Loss :  0.0759992970064
Iteration :  74   Loss :  0.0751979191949
Iteration :  75   Loss :  0.0744049915457
Iteration :  76   Loss :  0.0736204249558
Iteration :  77   Loss :  0.0728441312616
Iteration :  78   Loss :  0.0720760232291
Iteration :  79   Loss :  0.0713160145444
Iteration :  80   Loss :  0.0705640198035
Iteration :  81   Loss :  0.0698199545031
Iteration :  82   Loss :  0.0690837350308
Iteration :  83   Loss :  0.0683552786559
Iteration :  84   Loss :  0.0676345035202
Iteration :  85   Loss :  0.0669213286285
Iteration :  86   Loss :  0.0662156738397
Iteration :  87   Loss :  0.0655174598577
Iteration :  88   Loss :  0.0648266082227
Iteration :  89   Loss :  0.0641430413021
Iteration :  90   Loss :  0.0634666822819
Iteration :  91   Loss :  0.062797455158
Iteration :  92   Loss :  0.062135284728
Iteration :  93   Loss :  0.0614800965821
Iteration :  94   Loss :  0.0608318170955
Iteration :  95   Loss :  0.0601903734193
Iteration :  96   Loss :  0.0595556934733
Iteration :  97   Loss :  0.0589277059369
Iteration :  98   Loss :  0.0583063402416
Iteration :  99   Loss :  0.0576915265634
[-0.00174112 -0.00017527 -0.00031317 ...,  0.00132056  0.00079867
  0.00011892]
CROSS VALIDATION 18
Iteration :  0   Loss :  24.3114147919
Iteration :  1   Loss :  15.2349883523
Iteration :  2   Loss :  0.168413111563
Iteration :  3   Loss :  0.166637272364
Iteration :  4   Loss :  0.164880158576
Iteration :  5   Loss :  0.163141572749
Iteration :  6   Loss :  0.161421319514
Iteration :  7   Loss :  0.159719205562
Iteration :  8   Loss :  0.158035039623
Iteration :  9   Loss :  0.156368632443
Iteration :  10   Loss :  0.154719796764
Iteration :  11   Loss :  0.153088347304
Iteration :  12   Loss :  0.151474100731
Iteration :  13   Loss :  0.149876875649
Iteration :  14   Loss :  0.148296492576
Iteration :  15   Loss :  0.146732773918
Iteration :  16   Loss :  0.145185543959
Iteration :  17   Loss :  0.143654628831
Iteration :  18   Loss :  0.142139856503
Iteration :  19   Loss :  0.140641056756
Iteration :  20   Loss :  0.139158061166
Iteration :  21   Loss :  0.137690703087
Iteration :  22   Loss :  0.136238817627
Iteration :  23   Loss :  0.134802241634
Iteration :  24   Loss :  0.133380813678
Iteration :  25   Loss :  0.13197437403
Iteration :  26   Loss :  0.130582764644
Iteration :  27   Loss :  0.129205829143
Iteration :  28   Loss :  0.127843412796
Iteration :  29   Loss :  0.126495362506
Iteration :  30   Loss :  0.125161526789
Iteration :  31   Loss :  0.12384175576
Iteration :  32   Loss :  0.122535901113
Iteration :  33   Loss :  0.121243816105
Iteration :  34   Loss :  0.119965355542
Iteration :  35   Loss :  0.118700375761
Iteration :  36   Loss :  0.117448734613
Iteration :  37   Loss :  0.116210291449
Iteration :  38   Loss :  0.1149849071
Iteration :  39   Loss :  0.113772443869
Iteration :  40   Loss :  0.112572765508
Iteration :  41   Loss :  0.111385737206
Iteration :  42   Loss :  0.110211225575
Iteration :  43   Loss :  0.109049098631
Iteration :  44   Loss :  0.107899225784
Iteration :  45   Loss :  0.106761477821
Iteration :  46   Loss :  0.105635726888
Iteration :  47   Loss :  0.104521846485
Iteration :  48   Loss :  0.10341971144
Iteration :  49   Loss :  0.102329197905
Iteration :  50   Loss :  0.101250183337
Iteration :  51   Loss :  0.100182546483
Iteration :  52   Loss :  0.0991261673717
Iteration :  53   Loss :  0.0980809272945
Iteration :  54   Loss :  0.0970467087956
Iteration :  55   Loss :  0.0960233956576
Iteration :  56   Loss :  0.0950108728884
Iteration :  57   Loss :  0.0940090267084
Iteration :  58   Loss :  0.0930177445381
Iteration :  59   Loss :  0.0920369149846
Iteration :  60   Loss :  0.0910664278299
Iteration :  61   Loss :  0.0901061740182
Iteration :  62   Loss :  0.0891560456436
Iteration :  63   Loss :  0.088215935938
Iteration :  64   Loss :  0.087285739259
Iteration :  65   Loss :  0.0863653510783
Iteration :  66   Loss :  0.0854546679699
Iteration :  67   Loss :  0.0845535875981
Iteration :  68   Loss :  0.0836620087066
Iteration :  69   Loss :  0.0827798311065
Iteration :  70   Loss :  0.0819069556655
Iteration :  71   Loss :  0.0810432842966
Iteration :  72   Loss :  0.0801887199472
Iteration :  73   Loss :  0.0793431665877
Iteration :  74   Loss :  0.0785065292016
Iteration :  75   Loss :  0.0776787137738
Iteration :  76   Loss :  0.076859627281
Iteration :  77   Loss :  0.0760491776804
Iteration :  78   Loss :  0.0752472739
Iteration :  79   Loss :  0.0744538258281
Iteration :  80   Loss :  0.073668744303
Iteration :  81   Loss :  0.0728919411034
Iteration :  82   Loss :  0.072123328938
Iteration :  83   Loss :  0.0713628214362
Iteration :  84   Loss :  0.0706103331381
Iteration :  85   Loss :  0.0698657794847
Iteration :  86   Loss :  0.069129076809
Iteration :  87   Loss :  0.0684001423258
Iteration :  88   Loss :  0.0676788941232
Iteration :  89   Loss :  0.0669652511529
Iteration :  90   Loss :  0.0662591332212
Iteration :  91   Loss :  0.0655604609799
Iteration :  92   Loss :  0.0648691559178
Iteration :  93   Loss :  0.0641851403512
Iteration :  94   Loss :  0.0635083374157
Iteration :  95   Loss :  0.0628386710576
Iteration :  96   Loss :  0.0621760660247
Iteration :  97   Loss :  0.0615204478587
Iteration :  98   Loss :  0.0608717428863
Iteration :  99   Loss :  0.060229878211
[-0.00232029 -0.00037449 -0.00136356 ...,  0.00160231  0.00077931
  0.0001179 ]
CROSS VALIDATION 19
Iteration :  0   Loss :  30.3424215194
Iteration :  1   Loss :  13.8506369342
Iteration :  2   Loss :  0.136970952803
Iteration :  3   Loss :  0.13552665678
Iteration :  4   Loss :  0.134097590198
Iteration :  5   Loss :  0.13268359247
Iteration :  6   Loss :  0.1312845047
Iteration :  7   Loss :  0.12990016967
Iteration :  8   Loss :  0.12853043182
Iteration :  9   Loss :  0.127175137229
Iteration :  10   Loss :  0.125834133599
Iteration :  11   Loss :  0.124507270239
Iteration :  12   Loss :  0.123194398046
Iteration :  13   Loss :  0.121895369489
Iteration :  14   Loss :  0.120610038594
Iteration :  15   Loss :  0.119338260926
Iteration :  16   Loss :  0.118079893571
Iteration :  17   Loss :  0.116834795123
Iteration :  18   Loss :  0.11560282567
Iteration :  19   Loss :  0.11438384677
Iteration :  20   Loss :  0.113177721445
Iteration :  21   Loss :  0.11198431416
Iteration :  22   Loss :  0.110803490808
Iteration :  23   Loss :  0.109635118699
Iteration :  24   Loss :  0.108479066538
Iteration :  25   Loss :  0.107335204419
Iteration :  26   Loss :  0.106203403802
Iteration :  27   Loss :  0.105083537505
Iteration :  28   Loss :  0.103975479686
Iteration :  29   Loss :  0.10287910583
Iteration :  30   Loss :  0.101794292734
Iteration :  31   Loss :  0.100720918495
Iteration :  32   Loss :  0.0996588624972
Iteration :  33   Loss :  0.0986080053936
Iteration :  34   Loss :  0.0975682290974
Iteration :  35   Loss :  0.0965394167664
Iteration :  36   Loss :  0.0955214527907
Iteration :  37   Loss :  0.0945142227793
Iteration :  38   Loss :  0.0935176135476
Iteration :  39   Loss :  0.0925315131041
Iteration :  40   Loss :  0.0915558106386
Iteration :  41   Loss :  0.0905903965091
Iteration :  42   Loss :  0.0896351622299
Iteration :  43   Loss :  0.0886900004591
Iteration :  44   Loss :  0.0877548049866
Iteration :  45   Loss :  0.0868294707225
Iteration :  46   Loss :  0.0859138936848
Iteration :  47   Loss :  0.085007970988
Iteration :  48   Loss :  0.0841116008315
Iteration :  49   Loss :  0.0832246824882
Iteration :  50   Loss :  0.0823471162929
Iteration :  51   Loss :  0.0814788036317
Iteration :  52   Loss :  0.0806196469301
Iteration :  53   Loss :  0.0797695496428
Iteration :  54   Loss :  0.0789284162425
Iteration :  55   Loss :  0.0780961522089
Iteration :  56   Loss :  0.0772726640188
Iteration :  57   Loss :  0.0764578591348
Iteration :  58   Loss :  0.0756516459955
Iteration :  59   Loss :  0.0748539340048
Iteration :  60   Loss :  0.0740646335221
Iteration :  61   Loss :  0.0732836558518
Iteration :  62   Loss :  0.0725109132336
Iteration :  63   Loss :  0.0717463188328
Iteration :  64   Loss :  0.0709897867301
Iteration :  65   Loss :  0.0702412319122
Iteration :  66   Loss :  0.0695005702624
Iteration :  67   Loss :  0.0687677185508
Iteration :  68   Loss :  0.0680425944251
Iteration :  69   Loss :  0.0673251164015
Iteration :  70   Loss :  0.0666152038554
Iteration :  71   Loss :  0.0659127770123
Iteration :  72   Loss :  0.0652177569388
Iteration :  73   Loss :  0.0645300655339
Iteration :  74   Loss :  0.0638496255202
Iteration :  75   Loss :  0.0631763604352
Iteration :  76   Loss :  0.0625101946223
Iteration :  77   Loss :  0.0618510532232
Iteration :  78   Loss :  0.0611988621685
Iteration :  79   Loss :  0.0605535481701
Iteration :  80   Loss :  0.0599150387124
Iteration :  81   Loss :  0.0592832620449
Iteration :  82   Loss :  0.0586581471732
Iteration :  83   Loss :  0.0580396238518
Iteration :  84   Loss :  0.0574276225758
Iteration :  85   Loss :  0.0568220745732
Iteration :  86   Loss :  0.0562229117973
Iteration :  87   Loss :  0.0556300669186
Iteration :  88   Loss :  0.0550434733179
Iteration :  89   Loss :  0.0544630650783
Iteration :  90   Loss :  0.0538887769781
Iteration :  91   Loss :  0.053320544483
Iteration :  92   Loss :  0.0527583037397
Iteration :  93   Loss :  0.0522019915676
Iteration :  94   Loss :  0.0516515454529
Iteration :  95   Loss :  0.0511069035406
Iteration :  96   Loss :  0.0505680046281
Iteration :  97   Loss :  0.050034788158
Iteration :  98   Loss :  0.0495071942115
Iteration :  99   Loss :  0.0489851635019
[-0.00125233 -0.00036496 -0.00094955 ...,  0.00064758  0.00022368
  0.00011775]
Accuracy (Hinge Loss):	0.7
